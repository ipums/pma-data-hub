[
  {
    "path": "posts/2021-12-15-cei-conflict/",
    "title": "How to integrate ACLED conflict data with PMA Client Exit Interview data",
    "description": "Conflict data provides important context for access to family planning services.",
    "author": [
      {
        "name": "Maya Luetke",
        "url": "https://www.linkedin.com/in/maya-luetke-12425829/"
      },
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [
      "Client Exit Interviews",
      "Armed Conflict",
      "ACLED",
      "Mapping",
      "Animation",
      "sf",
      "ggspatial",
      "gganimate"
    ],
    "contents": "\n\nContents\nSetup\nArmed conflict data\nShapefile\nFacility GPS coordinates\n\nMapping conflict in Uganda\nAnimation\n\nMerge with CEI data\nResults\n\nWhen we introduced the new PMA Client Exit Interview (CEI) surveys in our last post, we mentioned several variables that describe some of the barriers that women face when accessing care at a particular facility. Some of these deal with transportation, in particular:\nFACNEAREST indicates whether the woman visited the facility nearest to her home today.\nFor women who did not visit the nearest facility, NOTNEARESTWHY explains why.\nFACTRAVELHR and FACTRAVELMIN describe the total amount of time required to reach the facility.\nTRANSPORT describes the mode of transportation she used to get to the facility\nClient Exit Interview (CEI) surveys are a convenience sample of women receiving family planning services at a facility included in a contemporaneous Service Delivery Point (SDP) survey.\nTransportation challenges can make it difficult or impossible for women to access family planning services, and they can be rooted in multiple intersecting factors that may not be visible without additional context. In some places, for example, the presence of nearby or recent armed conflict can disrupt public transportation, cause destruction to transportation infrastructure (e.g., roads, bridges), make certain types of travel unsafe, or even discourage movement altogether. In this post, we will show you how to download, aggregate, and match externally sourced conflict data with a 2020 CEI survey from Uganda. We’ll see how the proximity and timing of armed conflict influences CEI transportation measures, and we’ll demonstrate how to map monthly conflict statistics with an animated plot built with the gganimate package.\nSetup\nLet’s begin by downloading a CEI data extract from the IPUMS PMA website. We’ll select the Uganda 2020 sample and the five variables listed above; a number of pre-selected variables will also be included automatically.\nWe’ve downloaded our data extract and saved it in the “data” subfolder of our working directory. We’ll load it here, along with a few key packages:\n\n\nlibrary(ipumsr)\nlibrary(tidyverse)\n\ncei <- read_ipums_micro(\n  ddi = \"data/pma_00058.xml\",\n  data = \"data/pma_00058.dat.gz\"\n) \n\n\n\nThe variables FACNEAREST, NOTNEARESTWHY, and TRANSPORT are labelled integers, but we’ll change them into factors in order to make our summary tables easier to read. In the process, we’ll also replace the top-code “NIU (not in universe)” with the label NA and drop any zero-frequency response options.\n\n\ncei <- cei %>% \n  mutate(across(\n    c(COUNTRY, FACNEAREST, TRANSPORT, NOTNEARESTWHY, FACILITYTYPEGEN), \n    ~.x %>% \n      lbl_na_if(~.lbl == \"NIU (not in universe)\") %>%\n      as_factor() %>% \n      fct_drop()\n  ))\n\n\n\nNext, we’ll want to combine FACTRAVELHR and FACTRAVELMIN together. These variables come from a combined question intended to solicit both hours and minutes:\n109. How much time did it take you to travel here today?\nEnter -88 for do not know in both, -99 for no response in both.\nMinutes ___\nPlease verify the time entered\n\nHours ___\nPlease verify the time entered\nNotably, however, some respondents appear to have provided the same time in both hours and minutes (e.g. 3 hours and 180 minutes). While you might decide to use these responses in your own analysis, we’ll exercise an abundance of caution here: we’ll drop any responses where FACTRAVELHR > 0, but the number of minutes in FACTRAVELMIN is longer than one hour. We’ll also drop cases where the number of hours in FACTRAVELHR exceeds one full day (24 hours). With the remaining cases, we’ll calculate FACTRAVEL as the sum of FACTRAVELMIN and 60 * FACTRAVELHR.\n\n\ncei <- cei %>% \n  filter(FACTRAVELHR < 24 & (FACTRAVELHR == 0 | FACTRAVELMIN < 60)) %>% \n  mutate(FACTRAVEL = 60*FACTRAVELHR + FACTRAVELMIN)\n\n\n\nNow that we’ve finished processing the CEI data extract, let’s take a look at a gtsummary table containing our key transportation variables.\n\n\nlibrary(gtsummary)\n\ncei %>% \n  select(FACNEAREST, NOTNEARESTWHY, TRANSPORT, FACTRAVEL) %>% \n  tbl_summary(\n    label = list(\n      FACNEAREST ~ \"Facility is nearest to residence\",\n      NOTNEARESTWHY ~ \"Reason for not visiting nearest facility\",\n      TRANSPORT ~ \"Mode of transportation taken to facility\",\n      FACTRAVEL ~ \"Total travel time to facility (minutes)\"),\n    statistic = list(\n      all_continuous() ~ \"{median} ({p25}, {p75})\", \n      all_categorical() ~ \"{n} ({p}%)\"\n    ),\n    digits = list(everything() ~ 0),       \n    missing = \"no\"\n  ) %>% \n  modify_spanning_header(\n    everything() ~ \"# Client Exit Interview Transportation Summary\"\n  ) %>% \n  modify_header(update = list(label ~ \" \")) %>% \n  bold_labels() %>%\n  italicize_labels() \n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#tvrrubopkq .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#tvrrubopkq .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#tvrrubopkq .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#tvrrubopkq .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#tvrrubopkq .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#tvrrubopkq .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#tvrrubopkq .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#tvrrubopkq .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#tvrrubopkq .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#tvrrubopkq .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#tvrrubopkq .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#tvrrubopkq .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#tvrrubopkq .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#tvrrubopkq .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#tvrrubopkq .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#tvrrubopkq .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#tvrrubopkq .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#tvrrubopkq .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#tvrrubopkq .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#tvrrubopkq .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#tvrrubopkq .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#tvrrubopkq .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#tvrrubopkq .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#tvrrubopkq .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#tvrrubopkq .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#tvrrubopkq .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#tvrrubopkq .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#tvrrubopkq .gt_left {\n  text-align: left;\n}\n\n#tvrrubopkq .gt_center {\n  text-align: center;\n}\n\n#tvrrubopkq .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#tvrrubopkq .gt_font_normal {\n  font-weight: normal;\n}\n\n#tvrrubopkq .gt_font_bold {\n  font-weight: bold;\n}\n\n#tvrrubopkq .gt_font_italic {\n  font-style: italic;\n}\n\n#tvrrubopkq .gt_super {\n  font-size: 65%;\n}\n\n#tvrrubopkq .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\n\n        Client Exit Interview Transportation Summary\n\n      \n    \n      N = 2,3491\n    Facility is nearest to residence\n1,878 (80%)Reason for not visiting nearest facility\nNo family planning services\n64 (14%)Inconvenient operating hours\n20 (4%)Bad reputation / Bad prior experience\n35 (7%)Do not like personnel\n17 (4%)No medicine\n35 (7%)Prefers to remain anonymous\n16 (3%)It is more expensive than other options\n129 (27%)Was referred\n23 (5%)Less convenient location\n13 (3%)Absence of provider\n20 (4%)Other\n99 (21%)Mode of transportation taken to facility\nMotor vehicle (car, motorcycle, bus)\n868 (37%)Bicycle / pedicab\n96 (4%)Walking\n1,382 (59%)Boat\n1 (0%)Other\n2 (0%)Total travel time to facility (minutes)\n30 (20, 60)\n        \n          1\n          \n           \n          n (%); Median (IQR)\n          \n      \n    \n\n\n\n\n\nWhile 80% of sampled women visited the facility nearest to their home, 20% went elsewhere. For the latter group, a plurality of women visited a different facility because of cost (27%), while the second most important reason is “other” (21%). A majority of women (59%) traveled to the facility on foot (“Walking”), and the median travel time for all women is half an hour (the IQR shows that 50% of women spent between 20 and 60 minutes traveling).\nBefore we dig into these variables a bit more, consider how each of these might be impacted by armed conflict. It may prevent women from visiting the nearest healthcare facility, particularly if there is localized violent conflict occurring between their home and the facility, and thus also increase travel times to facilities to avoid conflict-ridden areas. It may also increase wait times at the facilities and limit the stocks of available medicines, contraceptives, and other medical supplies at facilities if supply-chains are interrupted by violence. To test these effects, we’ll need to choose and download a dataset describing the location and timing of armed conflict events in Uganda.\nArmed conflict data\nThere are several sources of publicly available conflict data. Two notable and frequently used databases are The Armed Conflict Location & Event Data Project (ACLED) and Uppsala Conflict Data Program (UCDP). Some researchers also argue for the integration of such databases into a more comprehensive dataset. If you are interested in learning more about this technique, please refer to the R package, meltt. However, for the sake of this blog post, we will use the ACLED dataset.\nIn order to get access to the ACLED data and be able to download an extract of the data, you must first register for an account on the ACLED website.\n\n\n\nFigure 1: Image from https://acleddata.com/#/dashboard\n\n\n\nAfter verifying your email by clicking the link in the email sent to you from ACLED, you must agree to ACLED’s Terms of Use in the dashboard of your account. Next, you must request an “access key” (a string of upper and lower case letters and numbers) by clicking the “Add new key” button in the dashboard of your account.\n\n\n\nFigure 2: Image from https://developer.acleddata.com/dashboard/main/\n\n\n\nBe sure to copy and save this access key as you will need it later and will not be able to view it again (though you can revoke the key and request another one if you do misplace it).\nAfter these steps, you can use the access key to request data through the ACLED’s Data Export Tool. To request the dataset, enter the access key, dates of interest, and country or region of interest. We’ll examine one year (September 1, 2019 to August 31, 2020) of conflict data prior to the beginning of CEI data collection in Uganda in September 2020.\n\n\n\nFigure 3: Image from https://acleddata.com/data-export-tool/\n\n\n\nThe ACLED dataset you’ll receive is a simple CSV file:\n\n\nconflict <- read_csv(\"data/2019-09-01-2020-08-31-Uganda.csv\")\n\nconflict\n\n\n# A tibble: 515 × 31\n   data_id   iso event_id_cnty event_id_no_cnty event_date      year\n     <dbl> <dbl> <chr>                    <dbl> <chr>          <dbl>\n 1 7188645   800 UGA6189                   6189 31 August 2020  2020\n 2 7188646   800 UGA6188                   6188 30 August 2020  2020\n 3 7970125   800 UGA6187                   6187 30 August 2020  2020\n 4 7576369   800 UGA6186                   6186 29 August 2020  2020\n 5 7692471   800 UGA6185                   6185 29 August 2020  2020\n 6 7189967   800 UGA6183                   6183 27 August 2020  2020\n 7 7188639   800 UGA6181                   6181 26 August 2020  2020\n 8 7276451   800 UGA6182                   6182 26 August 2020  2020\n 9 7189965   800 UGA6180                   6180 25 August 2020  2020\n10 7262418   800 UGA6179                   6179 23 August 2020  2020\n# … with 505 more rows, and 25 more variables: time_precision <dbl>,\n#   event_type <chr>, sub_event_type <chr>, actor1 <chr>,\n#   assoc_actor_1 <chr>, inter1 <dbl>, actor2 <chr>,\n#   assoc_actor_2 <chr>, inter2 <dbl>, interaction <dbl>,\n#   region <chr>, country <chr>, admin1 <chr>, admin2 <chr>,\n#   admin3 <chr>, location <chr>, latitude <dbl>, longitude <dbl>,\n#   geo_precision <dbl>, source <chr>, source_scale <chr>, …\n\nYou’ll notice that there are 515 rows in our conflict dataset - each representing a specific event with a unique data_id number. The variables event_type and sub_event_type describe different types of recorded events. We’ll use the kableExtra package to create an easy-to-read table showing the frequency of each event type.\n\n\nlibrary(kableExtra)\n\nconflict %>% \n  count(event_type, sub_event_type) %>% \n  mutate(pct = 100*prop.table(n) %>% round(3)) %>% \n  kbl() \n\n\n\nevent_type\n\n\nsub_event_type\n\n\nn\n\n\npct\n\n\nBattles\n\n\nArmed clash\n\n\n67\n\n\n13.0\n\n\nExplosions/Remote violence\n\n\nRemote explosive/landmine/IED\n\n\n1\n\n\n0.2\n\n\nProtests\n\n\nExcessive force against protesters\n\n\n5\n\n\n1.0\n\n\nProtests\n\n\nPeaceful protest\n\n\n65\n\n\n12.6\n\n\nProtests\n\n\nProtest with intervention\n\n\n53\n\n\n10.3\n\n\nRiots\n\n\nMob violence\n\n\n112\n\n\n21.7\n\n\nRiots\n\n\nViolent demonstration\n\n\n57\n\n\n11.1\n\n\nStrategic developments\n\n\nArrests\n\n\n2\n\n\n0.4\n\n\nStrategic developments\n\n\nChange to group/activity\n\n\n1\n\n\n0.2\n\n\nStrategic developments\n\n\nDisrupted weapons use\n\n\n1\n\n\n0.2\n\n\nStrategic developments\n\n\nLooting/property destruction\n\n\n2\n\n\n0.4\n\n\nStrategic developments\n\n\nOther\n\n\n2\n\n\n0.4\n\n\nViolence against civilians\n\n\nAbduction/forced disappearance\n\n\n5\n\n\n1.0\n\n\nViolence against civilians\n\n\nAttack\n\n\n136\n\n\n26.4\n\n\nViolence against civilians\n\n\nSexual violence\n\n\n6\n\n\n1.2\n\n\n\n\n\n\nAs you can see from the table above, there are a number of different types and subtypes of conflict events. For projects with a specific interest in a type of conflict, users can filter these data to only include one or a couple of types of data (e.g., Battles, Violence against civilians, etc.). For this particular analysis, we are including all conflict events.\nEach event is associated with a particular event_date and a set of latitude and longitude coordinates. These are subject to different degrees of precision represented in time_precision and geo_precision, respectively, on a scale ranging from 1 (most precise) to 3 (least precise). The specific meaning for each value is described in the ACLED codebook, but we’ll summarize here.\nThe degrees of time_precision are:\nThe listed event_date matches the date of the event\nThe listed event_date is the middle date of an event that happened during a specified week or weekend\nThe listed event_date is the middle date of an event that happened during a specified month\n\n\nconflict %>% \n  count(time_precision) %>%\n  kbl()\n\n\n\ntime_precision\n\n\nn\n\n\n1\n\n\n428\n\n\n2\n\n\n79\n\n\n3\n\n\n8\n\n\nTo keep things simple, we’ll recode event_date to the least precise level in time_precision - the month and year in which an event happened. We’ll use the lubridate package to create a century month code (cmc) for each month, and we’ll then recode event_date as a string containing the month and year for each event.\n\n\nlibrary(lubridate)\n\nconflict <-conflict %>% \n  mutate(\n    event_date = event_date %>% as_date(format = '%d %B %Y'),\n    event_month = month(event_date),\n    event_year = year(event_date),\n    event_cmc = 12*(event_year - 1900) + event_month\n  ) %>% \n  arrange(event_cmc) %>% \n  mutate(\n    event_date = month(event_month, label = TRUE) %>% \n      paste(event_year) %>% \n      as_factor()\n  )\n\n\n\n\n\n\n\nThe degrees of geo_precision are:\nThe listed coordinates are the centroid of a town where the event happened\nThe listed coordinates are the centroid of a town in the district in which the event happened (the nearest town is used if possible)\nThe listed coordinates are represent a natural location (e.g. “border area”, “forest”, or “sea”) or a provincial capital located in a “larger region” in which the event happened\nMost locations are specifically geo-referenced within a particular town (geo_precision == 1), but we’ll need to decide how to handle those locations that are less precise:\n\n\nconflict %>% \n  count(geo_precision) %>%\n  kbl()\n\n\n\ngeo_precision\n\n\nn\n\n\n1\n\n\n303\n\n\n2\n\n\n199\n\n\n3\n\n\n13\n\n\nBecause all but 13 events are geo-referenced to towns within the same district, we’ll drop events where geo-precision == 3 and aggregate the others by districts represented in the variable admin2 (admin level 2).\n\n\nconflict <- conflict %>% filter(geo_precision < 3)\n\n\n\nDoing so will allow us to build a cloropleth map showing the number of conflicts in each district in each month. But first, we’ll need to locate an appropriate shapefile.\nShapefile\nIPUMS PMA offers shapefiles for each sampled country at the same administrative level shown in the variable SUBNATIONAL. For Uganda, the boundaries of 10 regions are included (admin level 1).\nAs we’ve seen, the ACLED conflict data are more precise: all of the events remaining in conflict are geo-referenced to the nearest town within the same district, which is recorded in the column admin2 (admin level 2).\n\n\nconflict %>% count(admin2)\n\n\n# A tibble: 94 × 2\n   admin2       n\n   <chr>    <int>\n 1 Abim         6\n 2 Adjumani     3\n 3 Agago       10\n 4 Amolatar     1\n 5 Amuru       14\n 6 Apac         4\n 7 Arua        14\n 8 Budaka       3\n 9 Bududa       1\n10 Bugiri       1\n# … with 84 more rows\n\nIn order to map these events, we’ll need to locate a shapefile from a different source that contains boundaries for all of the districts in Uganda. We’ll find the shapefile we need from The Humanitarian Data Exchange and save it in the “data” subfolder of our working directory. We’ll load it into R with the sf package and, for the sake of improved processing speed, we’ll apply a small amount of smoothing to the boundaries with st_simplify.\n\n\nlibrary(sf)\n\nshapefile <- st_read(\"data/uga_admbnda_ubos_20200824_shp\") %>% \n  rename(admin2 = ADM2_EN) %>% \n  count(admin2) %>% \n  select(-n) %>% \n  st_make_valid() %>% \n  st_simplify(dTolerance = 1000) \n\n\n\n\n\n\n\n\n\nshapefile\n\n\nSimple feature collection with 135 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 29.57268 ymin: -1.479916 xmax: 34.99451 ymax: 4.231367\nGeodetic CRS:  WGS 84\nFirst 10 features:\n     admin2                       geometry\n1      Abim MULTIPOLYGON (((33.59604 3....\n2  Adjumani MULTIPOLYGON (((32.05573 3....\n3     Agago MULTIPOLYGON (((33.4717 3.2...\n4  Alebtong MULTIPOLYGON (((33.03664 2....\n5  Amolatar MULTIPOLYGON (((32.95499 1....\n6    Amudat MULTIPOLYGON (((34.92851 2....\n7    Amuria MULTIPOLYGON (((33.47753 2....\n8     Amuru MULTIPOLYGON (((32.0598 3.5...\n9      Apac MULTIPOLYGON (((32.57423 2....\n10     Arua MULTIPOLYGON (((31.16639 3....\n\nFacility GPS coordinates\nThe final source of data we’ll use in this post are the displaced GPS coordinates for each of the enumeration areas used to select facilities for the CEI sample. IPUMS PMA does not disseminate these coordinates, so you’ll need to apply to download them directly from our partners at PMA. Once approved, you’ll receive a CSV file - we’ve saved a copy of this file into the “data” subfolder of our working directory.\n\n\ngps <- read_csv(\"data/PMA_UG_GPS_v1_19May2021.csv\") %>%\n  select(EAID = EA_ID, GPSLONG, GPSLAT, DATUM) %>%\n  st_as_sf(\n    coords = c(\"GPSLONG\", \"GPSLAT\"), \n    crs = 4326\n  )\n\ngps\n\n\nSimple feature collection with 122 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 29.65181 ymin: -1.311755 xmax: 34.94487 ymax: 3.450838\nGeodetic CRS:  WGS 84\n# A tibble: 122 × 3\n        EAID DATUM              geometry\n *     <dbl> <chr>           <POINT [°]>\n 1 800221003 WGS84  (32.33993 0.2208572)\n 2 800191002 WGS84   (29.9435 -1.157961)\n 3 800121008 WGS84 (30.19217 -0.5432807)\n 4 800151006 WGS84   (33.29807 0.688975)\n 5 800171005 WGS84  (32.62457 0.3097168)\n 6 800221009 WGS84  (32.54551 0.1381675)\n 7 800241002 WGS84   (30.66916 0.771588)\n 8 800141003 WGS84   (31.52249 1.627066)\n 9 800151008 WGS84  (33.45034 0.5236081)\n10 800241004 WGS84 (29.73013 0.03855568)\n# … with 112 more rows\n\nMapping conflict in Uganda\nNext, we’ll want to think about the best way to visualize monthly changes in the amount of conflict in each district. Previously, we’ve shown how to create a faceted plot, where you might arrange maps for each month in a grid. This works well in some cases, but here - where we’ll build one map for each of 12 months - this would easily overwhelm the available space on our page. Instead, we’ll create an animated image - specifically, a gif - that cycles through each month in sequence.\nTo get started, we’ll first need to count the monthly total number of events in each district. We’ll do this simply by counting the number of distinct data_id codes for each district in each month. We’ll call this summary table conflict_summary.\n\n\nconflict_summary <- conflict %>% \n  group_by(admin2, event_date) %>% \n  summarise(events = n_distinct(data_id), .groups = \"keep\") %>% \n  ungroup() \n\nconflict_summary\n\n\n# A tibble: 288 × 3\n   admin2   event_date events\n   <chr>    <fct>       <int>\n 1 Abim     Jan 2020        2\n 2 Abim     Feb 2020        1\n 3 Abim     Mar 2020        1\n 4 Abim     May 2020        1\n 5 Abim     Jun 2020        1\n 6 Adjumani Oct 2019        1\n 7 Adjumani Dec 2019        2\n 8 Agago    Apr 2020        3\n 9 Agago    May 2020        1\n10 Agago    Jun 2020        2\n# … with 278 more rows\n\nNotice the first district, Abim, has recorded events in only 5 of the 12 months in our timeline; in each of the remaining 7 months, there were no reported events in the database. We’ll want to fill these gaps with 0, and the easiest way to do this is to pivot_wider, placing each of the 12 months into a separate column. Any district with no recorded events in a particular month will show NA in that column.\n\n\n\n\n\n\nconflict_summary <- conflict_summary %>% \n  arrange(event_date) %>% \n  pivot_wider(\n    admin2,\n    names_from = event_date,\n    values_from = events\n  ) %>% \n  arrange(admin2)\n\nconflict_summary\n\n\n# A tibble: 94 × 13\n   admin2   `Sep 2019` `Oct 2019` `Nov 2019` `Dec 2019` `Jan 2020`\n   <chr>         <int>      <int>      <int>      <int>      <int>\n 1 Abim             NA         NA         NA         NA          2\n 2 Adjumani         NA          1         NA          2         NA\n 3 Agago            NA         NA         NA         NA         NA\n 4 Amolatar          1         NA         NA         NA         NA\n 5 Amuru            NA         NA         NA          1         NA\n 6 Apac             NA         NA         NA         NA         NA\n 7 Arua             NA         NA         NA         NA         NA\n 8 Budaka           NA          1         NA         NA         NA\n 9 Bududa           NA         NA         NA         NA          1\n10 Bugiri           NA         NA         NA         NA         NA\n# … with 84 more rows, and 7 more variables: Feb 2020 <int>,\n#   Mar 2020 <int>, Apr 2020 <int>, May 2020 <int>, Jun 2020 <int>,\n#   Jul 2020 <int>, Aug 2020 <int>\n\nNow, the complete monthly event totals for each district are stored in a single row. At this point, we’ll want to merge conflict_summary with our shapefile.\n\n\nconflict_summary <- shapefile %>% full_join(conflict_summary, by = \"admin2\")\n\n\n\nFinally, we’ll pivot_longer so that each row contains one month again, except that missing months will be represented with the value NA. We’ll replace these values with 0. (We’ll also need to transform event_date back into a factor with levels set in chronological order - these were removed when the dates were used as column names above).\n\n\nconflict_summary <- conflict_summary %>% \n  pivot_longer(\n    cols = contains(\" \"), \n    names_to = \"event_date\",\n    values_to = \"events\"\n  ) %>% \n  mutate(event_date = factor(\n    event_date, \n    levels = levels(conflict$event_date)\n  )) %>% \n  select(admin2, events, event_date) %>% \n  mutate(events = ifelse(is.na(events), 0, events))\n\nconflict_summary\n\n\nSimple feature collection with 1620 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 29.57268 ymin: -1.479916 xmax: 34.99451 ymax: 4.231367\nGeodetic CRS:  WGS 84\n# A tibble: 1,620 × 4\n   admin2 events event_date                                   geometry\n * <chr>   <dbl> <fct>                              <MULTIPOLYGON [°]>\n 1 Abim        0 Sep 2019   (((33.59604 3.140113, 33.56416 3.148248, …\n 2 Abim        0 Oct 2019   (((33.59604 3.140113, 33.56416 3.148248, …\n 3 Abim        0 Nov 2019   (((33.59604 3.140113, 33.56416 3.148248, …\n 4 Abim        0 Dec 2019   (((33.59604 3.140113, 33.56416 3.148248, …\n 5 Abim        2 Jan 2020   (((33.59604 3.140113, 33.56416 3.148248, …\n 6 Abim        1 Feb 2020   (((33.59604 3.140113, 33.56416 3.148248, …\n 7 Abim        1 Mar 2020   (((33.59604 3.140113, 33.56416 3.148248, …\n 8 Abim        0 Apr 2020   (((33.59604 3.140113, 33.56416 3.148248, …\n 9 Abim        1 May 2020   (((33.59604 3.140113, 33.56416 3.148248, …\n10 Abim        1 Jun 2020   (((33.59604 3.140113, 33.56416 3.148248, …\n# … with 1,610 more rows\n\nYou could now easily build a static map for any single month. We’ll build an example for August 2020 with our favorite ggplot-aligned package for spatial data, ggspatial.\n\n\nlibrary(ggspatial)\n\nggplot() + \n  layer_spatial(\n    conflict_summary %>% filter(event_date == \"Aug 2020\"), \n    aes(fill = events)\n  ) + \n  layer_spatial(gps, aes(shape = \"Enumeration Area Centroid\")) + \n  theme_minimal() + \n  theme(\n    text = element_text(family = \"cabrito\", size = 12, color = \"#00263A\"),\n    plot.title = element_text(size = 24),\n    legend.direction = \"horizontal\"\n  ) + \n  annotation_scale(aes(style = \"ticks\", location = \"br\")) +\n  scale_fill_gradient(low = \"#FFFFFF\", high = \"#7F0000\") + \n  guides(\n    shape = guide_legend(title = element_blank()),\n    fill = guide_colorbar(title = element_blank())\n  ) + \n  labs(\n    title = \"Number of armed conflict events: August 2020\", \n    caption = \"Data source: ACLED (https://acleddata.com/)\"\n  ) \n\n\n\n\nAnimation\nTo build an animated version of this map that shows the number of armed conflicts for each month in sequence, we’ll use the gganimate package. This code will look almost exactly like the code we used to create a static map for August 2020, except that we’ll use the function transition_states to cycle through each event_date, and animate to render the animation over a 24 second period (two seconds per month). Also notice the title, where we use the variable {closest_state} to import the correct event_date for each month.\n\n\n\n\n\n\nlibrary(gganimate) \n\ndynamic_map <- ggplot() + \n  layer_spatial(conflict_summary, aes(fill = events)) + \n  layer_spatial(gps, aes(shape = \"Enumeration Area Centroid\")) + \n  theme_minimal() + \n  theme(\n    text = element_text(size = 12, color = \"#00263A\"),\n    plot.title = element_text(size = 24),\n    legend.direction = \"horizontal\"\n  ) + \n  annotation_scale(aes(style = \"ticks\", location = \"br\")) +\n  scale_fill_gradient(low = \"#FFFFFF\", high = \"#7F0000\") + \n  guides(\n    shape = guide_legend(title = element_blank()),\n    fill = guide_colorbar(title = element_blank())\n  ) + \n  labs(\n    title = \"Number of armed conflict events: {closest_state}\", \n    caption = \"Data source: ACLED (https://acleddata.com/)\"\n  ) + \n  transition_states(event_date, transition_length = 0, state_length = 1)\n\nanimate(\n  dynamic_map, \n  width = 1200, \n  height = 800,\n  duration = 24 # 2 seconds per month (12 months)\n)\n\n\n\n\n\n\nMerge with CEI data\nAs a final processing step, we’ll now use conflict_summary to find the number of recent distict-level conflicts for each facility in the CEI survey. We’ll focus here on the 3 months prior to the first month of CEI data collection: June, July, and August 2020.\nFor comparison’s sake, let’s divide the 135 districts into two groups: “low” and “high” levels of recent conflict. We’ll define HIGH_CONFLICT districts as those with a three-month event total in the upper-most tertile relative to all district totals.\n\n\nconflict_summary <- conflict_summary %>% \n  group_by(admin2) %>% \n  slice(10:12) %>% # most recent 3 months\n  summarise(conflict_events_3mo = sum(events)) %>% \n  ungroup() %>% \n  transmute(HIGH_CONFLICT = ntile(conflict_events_3mo, 3) > 2) \n\nconflict_summary\n\n\nSimple feature collection with 135 features and 1 field\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 29.57268 ymin: -1.479916 xmax: 34.99451 ymax: 4.231367\nGeodetic CRS:  WGS 84\n# A tibble: 135 × 2\n   HIGH_CONFLICT                                              geometry\n * <lgl>                                                <GEOMETRY [°]>\n 1 FALSE         POLYGON ((33.59604 3.140113, 33.56416 3.148248, 33.5…\n 2 FALSE         POLYGON ((32.05573 3.58631, 31.96306 3.56222, 31.933…\n 3 TRUE          MULTIPOLYGON (((33.4717 3.297044, 33.40285 3.303162,…\n 4 FALSE         POLYGON ((33.03664 2.500747, 33.01045 2.496633, 33.0…\n 5 FALSE         POLYGON ((32.95499 1.807919, 32.86451 1.814944, 32.8…\n 6 FALSE         POLYGON ((34.92851 2.283174, 34.88737 2.269499, 34.8…\n 7 FALSE         POLYGON ((33.47753 2.23472, 33.44095 2.239425, 33.47…\n 8 TRUE          POLYGON ((32.0598 3.576683, 32.05748 3.413509, 32.01…\n 9 FALSE         POLYGON ((32.57423 2.218455, 32.53095 2.13281, 32.52…\n10 TRUE          POLYGON ((31.16639 3.006533, 31.18047 3.062847, 31.2…\n# … with 125 more rows\n\nNow, we’ll need to identify the correct district for every enumeration area. The GPS coordinates for each enumeration area are stored in gps, so we’ll use st_intersection to place them within the district boundaries shown in conflict_summary. In the rare event that the gps coordinates sit directly on a district boundary, we’ll label the enumeration area “high” conflict if either one of the districts is labelled “high”.\n\n\nconflict_summary <- gps %>% \n  st_intersection(conflict_summary) %>% \n  st_drop_geometry() %>% \n  group_by(EAID) %>% \n  summarise(HIGH_CONFLICT = any(HIGH_CONFLICT))\n\nconflict_summary\n\n\n# A tibble: 122 × 2\n        EAID HIGH_CONFLICT\n       <dbl> <lgl>        \n 1 800111001 TRUE         \n 2 800111002 TRUE         \n 3 800111003 TRUE         \n 4 800111004 TRUE         \n 5 800111005 TRUE         \n 6 800111006 TRUE         \n 7 800121001 TRUE         \n 8 800121002 FALSE        \n 9 800121003 FALSE        \n10 800121004 FALSE        \n# … with 112 more rows\n\nFinally, we’ll attach conflict_summary to the original cei data extract by EAID.\n\n\ncei <- cei %>% left_join(conflict_summary, by = \"EAID\")\n\n\n\nResults\nNow we will look at the variables in the CEI data that we think might be impacted by localized conflict. We mentioned these at the beginning of this post but to remind you, they are:\nFACNEAREST\nTRANSPORT\nFACTRAVELHR and FACTRAVELMIN\nNOTNEARESTWHY\nTo examine these variables and their relationship to conflict, we will again build a gtsummary table, but this time we will stratify this table by HIGH_CONFLICT.\n\n\ncei %>% \n  select(HIGH_CONFLICT, FACNEAREST, NOTNEARESTWHY, TRANSPORT, FACTRAVEL) %>%\n  tbl_summary(\n    by = HIGH_CONFLICT, \n    label = list(\n      FACNEAREST ~ \"Facility is nearest to residence\",\n      NOTNEARESTWHY ~ \"Reason for not visiting nearest facility\",\n      TRANSPORT ~ \"Mode of transportation taken to facility\",\n      FACTRAVEL ~ \"Total travel time to facility (minutes)\"\n    ),\n    statistic = list(\n      all_continuous() ~ \"{median} ({p25}, {p75})\", \n      all_categorical() ~ \"{n} ({p}%)\"\n    ),\n    digits = list(everything() ~ 0),       \n    missing = \"no\"\n  ) %>% \n  add_p(test = list(\n    all_continuous() ~ \"t.test\", \n    c(\"TRANSPORT\",\"NOTNEARESTWHY\") ~ \"fisher.test\",\n    c(\"FACNEAREST\") ~ \"chisq.test\"\n  )) %>% \n  bold_p() %>%\n  modify_spanning_header(update = list(\n   everything() ~ \"# **Client Exit Interview Transportation Summary**\"\n  )) %>%\n  modify_header(update = list(\n    label ~ \" \",\n    stat_1 ~ \"**Low/Moderate Conflict** <br> N = {n}\",\n    stat_2 ~ \"**High Conflict** <br> N = {n}\",\n    p.value ~ \"**p-value**\"\n  )) %>%\n  bold_labels() %>% \n  italicize_labels() \n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#njdvemfvzg .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#njdvemfvzg .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#njdvemfvzg .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#njdvemfvzg .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#njdvemfvzg .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#njdvemfvzg .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#njdvemfvzg .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#njdvemfvzg .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#njdvemfvzg .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#njdvemfvzg .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#njdvemfvzg .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#njdvemfvzg .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#njdvemfvzg .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#njdvemfvzg .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#njdvemfvzg .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#njdvemfvzg .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#njdvemfvzg .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#njdvemfvzg .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#njdvemfvzg .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#njdvemfvzg .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#njdvemfvzg .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#njdvemfvzg .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#njdvemfvzg .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#njdvemfvzg .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#njdvemfvzg .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#njdvemfvzg .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#njdvemfvzg .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#njdvemfvzg .gt_left {\n  text-align: left;\n}\n\n#njdvemfvzg .gt_center {\n  text-align: center;\n}\n\n#njdvemfvzg .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#njdvemfvzg .gt_font_normal {\n  font-weight: normal;\n}\n\n#njdvemfvzg .gt_font_bold {\n  font-weight: bold;\n}\n\n#njdvemfvzg .gt_font_italic {\n  font-style: italic;\n}\n\n#njdvemfvzg .gt_super {\n  font-size: 65%;\n}\n\n#njdvemfvzg .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\n\n        Client Exit Interview Transportation Summary\n\n      \n    \n      Low/Moderate Conflict  N = 9571\n      High Conflict  N = 13921\n      p-value2\n    Facility is nearest to residence\n788 (82%)\n1,090 (78%)\n0.019Reason for not visiting nearest facility\n\n\nNo family planning services\n28 (17%)\n36 (12%)\nInconvenient operating hours\n6 (4%)\n14 (5%)\nBad reputation / Bad prior experience\n9 (5%)\n26 (9%)\nDo not like personnel\n7 (4%)\n10 (3%)\nNo medicine\n15 (9%)\n20 (7%)\nPrefers to remain anonymous\n3 (2%)\n13 (4%)\nIt is more expensive than other options\n45 (27%)\n84 (28%)\nWas referred\n9 (5%)\n14 (5%)\nLess convenient location\n3 (2%)\n10 (3%)\nAbsence of provider\n8 (5%)\n12 (4%)\nOther\n36 (21%)\n63 (21%)\nMode of transportation taken to facility\n\n\n<0.001Motor vehicle (car, motorcycle, bus)\n307 (32%)\n561 (40%)\nBicycle / pedicab\n50 (5%)\n46 (3%)\nWalking\n600 (63%)\n782 (56%)\nBoat\n0 (0%)\n1 (0%)\nOther\n0 (0%)\n2 (0%)\nTotal travel time to facility (minutes)\n40 (20, 65)\n30 (15, 60)\n<0.001\n        \n          1\n          \n           \n          n (%); Median (IQR)\n          \n        \n          2\n          \n           \n          Pearson's Chi-squared test; Fisher's exact test; Welch Two Sample t-test\n          \n      \n    \n\nAs you can see in the table above, there are some significant differences between the respondents that have experienced High conflict in their area compared to those that experienced Low/Moderate conflict. More respondents in the high conflict areas did not visit the facility nearest to their residence compared to those in the low/moderate conflict areas. Overall, there were no significant differences in the reasons for not visiting the nearest facility. However, it may be notable that there are no possible responses that inquire about conflict as a possible reason and the closest one that might estimate this is less convenient location, which is demonstrably more prevalent among the High conflict group compared to the Low/Moderate group. We also see significant differences in the mode of transportation between the two groups, with less respondents in the High conflict area walking and biking to the clinic/facility and more taking a motor vehicle compared to respondents in the Low/Moderate area. Finally, the travel time to facility is also significantly different between respondents living in High conflict versus Low/Moderate conflict areas. The trend appears to be in an unexpected direction, with those in Low/Moderate conflict settings having greater travel times compared to those in High conflict settings. A possible explanation for this trend may be that women in the Low/Moderate conflict group are more likely to walk or bicycle to the facility and less likely to use motor vehicle transport compared to the women in the High conflict group.\nTaken together, the results of this analysis seem to indicate that conflict does indeed impact women’s ability to access family planning services. Importantly, it may be more expensive or even unaffordable for women to obtain a motor vehicle ride to a healthcare facility. Since the CEI data were collected from women at healthcare facilities, we do not know the magnitude of women who were unable to visit a healthcare facility due to the direct threat of localized violence or indirect cost of longer travel or the necessity of a motor vehicle in order to visit a healthcare facility and avoid local conflict.\n\n\n\n",
    "preview": "posts/2021-12-15-cei-conflict/cei-conflict_files/figure-html5/unnamed-chunk-27-1.png",
    "last_modified": "2021-12-15T16:58:57-06:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1536
  },
  {
    "path": "posts/2021-12-01-cei-discovery/",
    "title": "Introducing Client Exit Interviews",
    "description": "Women receiving family planning services assess their care in a new data series from PMA.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-12-01",
    "categories": [
      "Data Discovery",
      "New Data",
      "Client Exit Interviews",
      "Service Delivery Points"
    ],
    "contents": "\n\nContents\nSample Design\nMissing data\nSample weights\n\nTopics\nTechnical Variables\nInterview Location\nBackground\nServices & Information\nAssessment\n\nNext Steps\n\nThis fall, IPUMS PMA released baseline data from a new study examining women’s assessment of care received from providers of family planning services. Previously, we’ve seen how PMA service delivery point (SDP) surveys can be combined with surveys of households and women sampled within the same geographically defined enumeration areas. These new Client Exit Interview (CEI) surveys represent women who have received care from a specific provider included in a contemporaneous SDP sample.\nInterviews for samples included in this release were conducted between December 2019 and November 2020. Women aged 15-49 receiving services at a participating SDP were approached by interviewers and asked to reflect on their visit. As we’ll see, most questions were given only to women who received or discussed a family planning method with their provider. Questions address whether the woman received her preferred method, whether the advantages or disadvantages of certain methods were discussed, and how she felt about the overall experience.\n\nA convenience sample of women leaving a facility that participated in the SDP survey\nWomen who completed the baseline interview were invited to participate in a follow-up telephone interview six months afterward, but these follow-ups were canceled in most PMA countries because of disruptions caused by COVID-19. Coming this spring, IPUMS PMA plans to release data from a second round of baseline CEI interviews collected one year after the first round; from then onward, we expect to release six month follow-up interviews following each baseline survey.\nToday, we’re launching a new series devoted to showcasing the first round of CEI baseline surveys. Let’s see what’s included in the release!\nSample Design\nCEI surveys are currently available from six PMA countries (more countries will be included in the second round planned for release in 2022):\nBurkina Faso\nDemocratic Republic of Congo (DRC) - Kinshasa and Kongo Central only\nIndia - Rajasthan only\nKenya\nNigeria - Kano and Lagos only\nUganda\nAs we’ve discussed elsewhere, up to three public-sector and three private-sector facilities are sampled for each enumeration area in a given SDP survey. Sampled SDPs were eligible to host client exit interviews only if their daily client volume was three or more. (We’ll show how to link SDP and CEI data together in an upcoming post.)\nCEI interviewers were present at eligible SDPs for two business days during which family planning services were offered to clients. They invited all female clients to participate in the survey; participants are those who agreed and affirmed that their age was between 15 and 49. Interviewers then asked participants whether they received or discussed a family planning method with their provider; this information is reported in the variable FPINFOYN:\n\nFamily planning clients aged 15-49\n100. Did you receive any family planning information or a method during your \nvisit today?\n\nIf no, thank her for her time and end the interview.\n\n[] Yes\n[] No\n[] No response\nMissing data\nWomen who indicated that they did not receive or discuss a method in FPINFOYN were asked no further questions, but you’ll find that these cases are included in all CEI data extracts. For all other variables derived from the CEI questionnaire (except for those that describe the facility and the woman’s age), these cases will be labelled NIU (not in universe).\n\nPMA allows respondents to skip questions if they are deemed irrelevant based on answers to a previous question. Those cases will be marked NIU (not in universe) as well.\nA small number of women declined consent to be interviewed. These cases are labelled Not interviewed (female questionnaire), and they may be automatically excluded from your data extract by selecting “Female Respondents” rather than “All cases (Respondents and Non-respondents)” at checkout.\nIf a woman declined to answer a particular question - but otherwise consented to the interview - her response will be labelled No response or missing.\nIn many cases, you’ll want to consolidate all types of “missing data” by marking them NA with help from the ipumsr package. For example, let’s say you’ve downloaded the following data extract and placed it in the data folder of your working directory.\n\n\nlibrary(ipumsr)\nlibrary(tidyverse)\n\ndat <- read_ipums_micro(\n  ddi = \"data/pma_00054.xml\",\n  data = \"data/pma_00054.dat.gz\"\n)\n\n\n\nVariables that you’d expect to be binary - with response options for “yes” or “no” - will have additional top codes for the three types of missing responses we’ve discussed. For example DISCFPTODAY indicates whether the woman discussed family planning with her provider today:\n\n\ndat %>% count(DISCFPTODAY)\n\n\n# A tibble: 5 × 2\n                                  DISCFPTODAY     n\n                                    <int+lbl> <int>\n1  0 [No]                                       735\n2  1 [Yes]                                     5428\n3 95 [Not interviewed (female questionnaire)]     5\n4 98 [No response or missing]                     2\n5 99 [NIU (not in universe)]                   2838\n\nNotice that the top codes for missing data are numeric: 95, 98, and 99. These codes will will be the same for all binary variables, but not necessarily for continuous variables or variables with many response categories. For example, look at FPMETHGIVEN, which describes the method or prescription given to the woman by her provider during the visit:\n\n\ndat %>% count(FPMETHGIVEN)\n\n\n# A tibble: 22 × 2\n                                    FPMETHGIVEN     n\n                                      <int+lbl> <int>\n 1 101 [Female Sterilization]                     111\n 2 102 [Male Sterilization]                         1\n 3 111 [Implants]                                2229\n 4 112 [IUD]                                      347\n 5 113 [Post-partum IUD]                           89\n 6 114 [Post-abortion IUD]                          1\n 7 120 [Injectables]                             3704\n 8 123 [Injectables (Depo Provera)]               268\n 9 124 [Injectables (Sayana Press)]               163\n10 131 [Pill]                                     946\n11 132 [Emergency Contraception]                   47\n12 141 [Male condom]                              117\n13 142 [Female condom]                              6\n14 151 [Diaphragm]                                  2\n15 160 [Standard Days/Cycle Beads Method]           8\n16 170 [Lactational amenorrhea method (LAM)]        2\n17 210 [Rhythm]                                     1\n18 220 [Withdrawal]                                 2\n19 900 [None of the above]                          4\n20 995 [Not interviewed (female questionnaire)]     5\n21 998 [No response or missing]                     2\n22 999 [NIU (not in universe)]                    953\n\nHere, the top codes are left-padded to match the number of digits required for all of the response categories. They are now 995, 998, and 999.\nYou’ll find the value NA in some variables, but only if the associated question was excluded from one or more of the samples in your data extract. For instance, GEOCD indicates whether respondents to the DRC sample were interviewed in Kinshasa or Konga Central; it’s not included in the samples from other countries:\n\n\ndat %>% count(COUNTRY, GEOCD)\n\n\n# A tibble: 7 × 3\n                          COUNTRY              GEOCD     n\n                        <int+lbl>          <int+lbl> <int>\n1  1 [Burkina Faso]               NA                   946\n2  2 [Congo, Democratic Republic]  1 [Kinshasa]         96\n3  2 [Congo, Democratic Republic]  2 [Kongo Central]    76\n4  6 [India]                      NA                   532\n5  7 [Kenya]                      NA                  3935\n6  9 [Nigeria]                    NA                  1016\n7 10 [Uganda]                     NA                  2407\n\nIf you want to mark all top codes as NA, the easiest way to do so is to reference the labels rather than the numeric codes. We’ll provide them to the ipumsr function lbl_na_if applied to all variables in our extract:\n\n\ndat <- dat %>% \n  mutate(across(everything(), ~lbl_na_if(.x,\n    ~.lbl %in% c(\n        \"Not interviewed (female questionnaire)\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n  )))\n\n\n\n\nThe value NA may be useful if, for example, you don’t want to create dummy variables for missing values in an analytic model. R will usually drop NA cases by default.\nNow, missing responses for DISCFPTODAY and FPMETHGIVEN are simply NA.\n\n\ndat %>% count(DISCFPTODAY)\n\n\n# A tibble: 3 × 2\n  DISCFPTODAY     n\n    <int+lbl> <int>\n1     0 [No]    735\n2     1 [Yes]  5428\n3    NA        2845\n\ndat %>% count(FPMETHGIVEN)\n\n\n# A tibble: 20 × 2\n                                 FPMETHGIVEN     n\n                                   <int+lbl> <int>\n 1 101 [Female Sterilization]                  111\n 2 102 [Male Sterilization]                      1\n 3 111 [Implants]                             2229\n 4 112 [IUD]                                   347\n 5 113 [Post-partum IUD]                        89\n 6 114 [Post-abortion IUD]                       1\n 7 120 [Injectables]                          3704\n 8 123 [Injectables (Depo Provera)]            268\n 9 124 [Injectables (Sayana Press)]            163\n10 131 [Pill]                                  946\n11 132 [Emergency Contraception]                47\n12 141 [Male condom]                           117\n13 142 [Female condom]                           6\n14 151 [Diaphragm]                               2\n15 160 [Standard Days/Cycle Beads Method]        8\n16 170 [Lactational amenorrhea method (LAM)]     2\n17 210 [Rhythm]                                  1\n18 220 [Withdrawal]                              2\n19 900 [None of the above]                       4\n20  NA                                         960\n\nWe encourage you to use caution when labelling values NA. You may find it useful to mark other values NA (e.g. “None of the above” or “Don’t know”), but doing so may have unintended consequences for your analysis!\nSample weights\nNotably, no sample weights are available for CEI surveys: as we’ve discussed, PMA constructed a convenience sample of women visiting eligible facilities over a two day period. Those facilities are, themselves, selected to reflect the service environment experienced by women in the household and female sample - neither CEI nor SDP samples are designed to be nationally or subnationally representative.\nTopics\nYou can browse an English version of the common CEI questionnaire here, or you can find country-specific variations in both English and French (where appropriate) along with PMA dataset notes on this page.\nIPUMS PMA harmonizes variables from all PMA samples to ensure that the same variable names, labels, and response options are used wherever possible. In this case, all six of the CEI samples are highly comparable, except that some variables are not available for all samples.\nYou’ll find CEI data on the IPUMS PMA website under the new “Client Exit Interview” unit of analysis.\n\n\n\nVariables are arranged by topic, but you may also browse them alphabetically or search for a particular variable by name, label, value label, or description. We’ll give a broad overview here.\nTechnical Variables\nAs with all IPUMS PMA data, you’ll find several preselected variables included automatically with your extract. The include “technical variables” reporting the SAMPLE, COUNTRY, YEAR, and ROUND (all samples included in this release are “round 1”; data from “round 2” will be available next spring).\nAdditionally, you’ll find several unique identification numbers:\nEAID identifies each enumeration area\nFACILITYID identifies each facility\nENUMID identifies each enumerator (interviewer)\nINTFQMON identifies the month of the interview\nINTFQYEAR identifies the year of the interview\nThe variable CONSENTFQ indicates whether the woman provided informed consent to participate in the interview, and RESULTCQ provides details about whether the interview was completed.\nInterview Location\nAs we’ve mentioned, CEI baseline interviews took place at facilities sampled in a contemporaneous SDP survey. In an upcoming post, we’ll show how to use FACILITYID to obtain detailed information about each facility and attach it to a CEI data extract. However, CEI extracts do contain some basic information about the facility:\nFACILITYTYPE and FACILITYTYPEGEN describe the facility type (hospital, health clinic, pharmacy, etc.)\nFACILITYADV indicates whether the service delivery point is an “advanced facility.” Generally speaking, advanced facilities include hospitals, health centers, and clinics, whereas pharmacies and drug shops are not considered advanced facilities. This definition varies across countries.\nAUTHORITY indicates whether the managing authority for the facility is government, private, an NGO, a faith-based organization, or “other”.\nURBAN indicates whether the facility is located in an urban enumeration area (not available for DRC, Nigeria, or Kenya).\nSUBNATIONAL identifies the subnational region in which the facility is located - variables starting with the prefix GEO provide the same information, but are country-specific.\nBackground\nAll women who received family planning information or a family planning method during their visit (see FPINFOYN) are asked to provide their AGE, but women whose age is younger than 15 or older the 49 are excluded from the data file. All remaining women are asked to provide their:\neducation level EDUCATT and EDUCATTGEN\nmarital status MARSTAT\nnumber of times given birth BIRTHEVENT\nOther important background information concerns each woman’s experience using family planning before the visit. PREVMETH indicates whether the woman ever used any family planning method before; if so, PREVMETHDUR and PREVMETHDURVAL indicate how long she’s been using her most recent method. For any woman who received a method today, FPUSEPREV indicates whether she had previously used it; if so, PREVMETHYR indicates whether she’d used it in the previous 12 months.\nServices & Information\nVISITREASON gives the main reason for the woman’s visit, and VISITFPTODAY indicates whether the main reason included family planning. The availability of most other variables depends on whether the woman received a method or prescription FPVISITGIVEN, or whether she discussed family planning during the visit DISCFPTODAY .\nFor women who received a method or prescription, FPMETHGIVEN describes the method that was given (if the method was injectable, FPINJTYPETODAY gives the injectable type). You’ll also find variables explaining the woman’s expectations prior to the visit, and whether the method she was given met those expectations. These describe:\nwhich method she initially wanted FP1STMETHWANT\nwhether she obtained the method she initially wanted FPGETDESIREDTODAY\nthe reason why she didn’t receive the method she initially wanted FPGETWHYTODAY\nwho made the final decision about the method she ultimately received FPDECIDEMETHOD\nSeveral variables describe what the provider told her about the method she was given. Specifically, did the provider:\nexplain how to use the method TELLMETHUSE\nexplain possible side effects TELLSIDEEFF\ntell her what to do if she experienced problems TELLSEPROB\ntell her when to return for follow-up TELLWHENRETURN\ntell her about methods other than the one she was given DISCOTHFP\ntalk about methods that protect against HIV/AIDs and STIs DISCSTIPROTECT\nask about her preferred method ASKFPPREF\ntell her she could switch methods in the future TELLSWITCH\nWhether a woman received a method or only discussed family planning, the variable DISCMETHPROCON indicates whether the advantages and disadvantages of a particular method were discussed. If so, the woman was asked to report whether the discussion covered any of the following advantages:\ndiscrete METHPRODISCRETE\neasy to use METHPROEASY\neffective METHPROEFFECT\nreturn to fertility METHPROFERT\nfew side effects METHPROFEWSE\nless bleeding METHPROLESSBLD\nmore regular bleeding METHPROMOREBLD\nno hormones METHPRONOHORMONE\nlong term protection METHPROLONG\nother advantages METHPROOTH\nLikewise, she was asked to report whether the discussion covered any of the following disadvantages:\ndifficult to use METHCONDIFF\nineffective METHCONNOTEFF\nmore bleeding METHCONMOREBLD\nirregular bleeding METHCONIRRBLD\ncramping METHCONCRAMP\nheadache METHCONHEADACHE\nfew / no periods METHCONLESSPRD\nnausea METHCONNAUS\nweight gain METHCONWTGAIN\nother METHCONOTH\nFinally, women in the Kenya sample were also asked whether this discussion included any of the following methods, in particular:\nstandard days / cycle beads METHCOUNBEADS\ndiaphragm METHCOUNDIA\nemergency contraception METHCOUNEC\nfemale condoms METHCOUNFC\nfoam / jelly METHCOUNFOAM\nfemale sterilization METHCOUNFSTER\nimplants METHCOUNIMP\ninjectables METHCOUNINJ\nIUD METHCOUNIUD\nLAM METHCOUNLAM\nmale condom METHCOUNMC\nmale sterilization METHCOUNMSTER\npill METHCOUNPILL\nrhythm method METHCOUNRHY\nwithdrawal METHCOUNWD\nnone of the above METHCOUNNONE\nAssessment\nWomen give an overall rating (5 point scale) of their satisfaction with services received during their visit SERVSATIS, the overall clarity of the information they received FPINFOCLARITY, and the politeness of staff at the facility POLITESTAFF. They’re also asked to report whether they:\nwould return to the facility RETURNFAC\nwould refer the facility to a friend or relative REFERFAC\nwere given a change to ask questions PROVLETQUESTION\nfelt the provider answers all questions clearly CLEARANSWERS\nOther variables concern strains on the woman’s time or financial resources. FPPAYTODAY indicates whether she paid for family planning services received during the visit (some samples include FPINSTODAY indicating whether those services were covered by insurance). FACTRAVELHR and FACTRAVELMIN describe the total travel time needed to reach the facility, while TRANSPORT gives the mode of transportation; the related variable FACNEAREST indicates whether this facility was the option nearest to the woman’s home, and - if not - NOTNEARESTWHY explains why she did not visit the nearest facility. Finally, TODAYWAITHR together with TODAYWAITMIN gives the total wait time the woman experienced before seeing her provider during the visit.\nNext Steps\nOver the coming weeks, we’ll be exploring some of the exciting research questions you might explore with CEI surveys (particularly when combined with SDP surveys and external spatial data). In the meantime, reach out to us on Twitter and let us know what excites you most about this incredible new data series from PMA.\n\n\n\n",
    "preview": "posts/2021-12-01-cei-discovery/../../images/new_data.png",
    "last_modified": "2021-12-15T16:59:28-06:00",
    "input_file": {},
    "preview_width": 2555,
    "preview_height": 1437
  },
  {
    "path": "posts/2021-11-15-find-an-example/",
    "title": "2021 Blog Round-Up",
    "description": "Where to find example code for all of the key concepts we've covered so far.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-11-15",
    "categories": [
      "R tips",
      "R packages"
    ],
    "contents": "\n\nContents\nIntroduction Course\nData Discovery\nData Manipulation\nIPUMS labelled data\nIteration with columns\nIteration with rows\nJoining and Reshaping\nVisualization\nSpatial data\n\nData Analysis\nPlans for 2022\n\nWe launched the IPUMS PMA Data Analysis Hub one year ago this week, and we’ve covered a lot since then! As we’re gearing up for new series exploring Client Exit Interviews and longitudinal analysis with Round 2 Panel Data in the coming months, we thought it might be a nice time to pause and take inventory of the important concepts we’ve covered in our first year.\nIntroduction Course\nLooking for a place to start learning about IPUMS PMA data with coding examples in your preferred programming language? Check out our new online introduction course! This course is free and open to all registered IPUMS PMA data users. You’ll learn all about PMA surveys, the IPUMS data extract system, and basic analysis tools for R or Stata.\nData Discovery\nOur Data Discovery posts are where you’ll find announcements about new data releases and deep-dives into available data for a featured topic. So far, we’ve explored:\nService Delivery Point (SDP) surveys\nCOVID-19 surveys\nNutrition surveys for women and young children\nMigration history data\nExternal sources for climate, population density, and infrastructure data that complement PMA surveys\n\n\n\nData Discovery posts help you navigate the IPUMS PMA website to find the most important variables related to a particular topic.\nData Manipulation\nA Data Manipulation post typically showcases tips for recoding, reshaping, describing, or visualizing variables included in an IPUMS PMA data extract. Often, we’ll feature tools from one or two of our favorite R packages; while we always recommend visiting package documentation websites, our goal here is to show why these tools are important specifically for working with data from IPUMS PMA.\nIPUMS labelled data\n\nAlmost every post uses the ipumsr package to import an IPUMS PMA data extract into R. Once loaded, you’ll find that many IPUMS PMA variables are labelled; the ipumsr package contains functions for exploring and manipulating those labels. We’ve shown how to:\nload an IPUMS PMA data extract into R\nrecode or label NA values for a particular variable\nconvert labelled variables into factors for tables and figures\n\n\n\n\n\n\n\n\n© IPUMS (MPL 2.0)\n\nThe ipumsr package provides tools to access and manipulate labels.\nIteration with columns\nWhen you want to apply changes to several IPUMS PMA variables, it’s usually most efficient to leverage the across function from dplyr. We use across all the time, but some of our best examples show how to:\nuse tidy selection to identify variables by location or a naming pattern\nrecode or label NA values for several variables with tools from ipumsr\ncheck conditions for multiple variables with if_any and if_all\nIteration with rows\n\nMany functions in R are vectorized: when you apply them to a column in your data frame, you typically get one result for each row.\nNon-vectorized functions instead return one result summarizing the entire column. For instance, the function mean gives you one mean value derived from all rows in a given column. Tools from dplyr like rowwise and group_by allow us to apply non-vectorized functions to individual rows or groups of rows. We’ve used these functions to:\nsummarize variables within groups\nsummarize variables within the same row with c_across\niterate through groups with help from cur_group\n\n\n\n\n\n© RStudio (CC0 1.0)\nJoining and Reshaping\n\nAll of the IPUMS PMA data extracts we’ve examined so far are rectangular in shape: each row represents one person or facility. Sometimes, it’s necessary to change this structure: we might want to spread multiple observations of the same individual into separate rows, or we might want to leverage hierarchical data to situate a person within some larger context (like a household, a region, or a country). With help from data-structuring packages like tidyr, we’ve shown how to:\nmerge summary SDP data to records for individual women\nmerge baseline and COVID-19 follow up surveys for women in a panel study\npivot event-history data from a wide to long format and back again\nuse nested data structures for individuals grouped by household\n\n\n\n\n\n© RStudio (CC0 1.0)\nVisualization\n\nWhenever we summarize variables or model output on this blog, we like to make figures with ggplot2 and tables with gtsummary. For maps, we’ve focused on ggspatial - an extension of ggplot2 that supports raster data from both the raster package and the terra package. Topics include:\ndesigning a theme for ggplot2 (custom fonts, colors, and layout)\nbasic bar charts\ngrouped bar charts\ndivergent stacked bar charts\nfaceted bar charts\nlollipop charts\nradar charts\nalluvial charts\ndot and whisker charts\nerror bars and text annotation\nmaps with ggspatial\ndescriptive tables and model summary tables with gtsummary\n\n\n\n\n\n© RStudio (CC0 1.0)\n\n\n\nData Manipulation posts include tips for data wrangling, exploration, and presentation.\nSpatial data\n\nLastly, we’ve devoted several Data Manipulation posts to exploring external sources for spatial data. If you’re new to spatial data - or if you’re an ArcGIS user interested in learning more about R - we recommend checking out the free, open-source text Geocomputation with R. You’ll be introduced to both of the major packages we use on this blog: sf for vector data and terra for raster data. We’re planning to cover much more spatial content in the coming months, but so far we’ve explained how to:\nfind and use PMA GPS coordinates\nfind and use IPUMS PMA shapefiles\ncreate enumeration area buffer zones\nunderstand and manipulate coordinate reference systems\nmerge external vector data with the sf package\nmerge external raster data with both the raster package and the newer terra package\n\n\n\n\n\n© Edzer Pebesma (GPLv2)\n\n\n\n\n© Robert J. Hijmans et al. (GPLv3)\nData Analysis\nWe like to end every series with a Data Analysis post. Here, we use what we’ve learned in earlier posts to build a model and discuss important findings. Often, we’ll use this as an opportunity to showcase new and exciting published research from authors working with PMA data. These posts discuss:\n\ndesign elements for the household and female surveys like sample weights and information about cluster sampling\ndesign elements for COVID-19 follow up surveys\nordinary least squares regression\nbinary logistic regression\nmultinomial models\nmulti-level mixed effect models\nsurvival analysis\ncluster robust standard error estimation\nbootstrapped standard error estimation\n\n\n\n\n\n© Greg Freedman Ellis et al. (GPL 2 | GPL 3)\n\n\n\n\nWe walk through source code from recent publications with PMA data in a Data Analysis post.\nPlans for 2022\nWe’ve been trilled to connect with so many of you - more than 2,000 readers in over 100 countries - in our first year! Coming in the months ahead, we’ll continue providing bi-weekly blog posts covering the latest IPUMS PMA data releases as they become available. We’ve mentioned upcoming series planned for Client Exit Interviews and Round 2 Panel Data, and we’ll also return to the relationship between COVID-19 and family planning in new samples. We plan to continue emphasizing spatial analysis made possible with external data sources, and we’d love to expand our coverage of PMA publications in collaboration with researchers using IPUMS PMA data in their own work. We’re also planning a new course on longitudinal analysis using the first two rounds of PMA panel data; look for new announcements on this in the early spring. And, as always, we’ll continue exploring free and powerful tools for working with survey data in R that we hope will make IPUMS PMA accessible to more researchers than ever before.\nSpecial thanks to all of our outstanding partners at the Bill & Melinda Gates Foundation, Johns Hopkins University, and here at IPUMS for tremendous support as we launched this new project in 2021!\nThis blog would not be possible without brilliant insight and ongoing encouragement from the whole team at IPUMS PMA: professors Kathryn Grace, Elizabeth Heger Boyle, and Nina Brooks; project director Devon Kristiansen; post-doctoral associates Maya Luetke and Jiao Yu; and graduate research assistants Shelby Rutzick, Saeun Park, and Tayler Nelson who contributed posts this year - thanks to each of you!\n\n\n\n",
    "preview": "posts/2021-11-15-find-an-example/../../images/logo-navy.png",
    "last_modified": "2021-12-01T12:18:12-06:00",
    "input_file": {},
    "preview_width": 1113,
    "preview_height": 312
  },
  {
    "path": "posts/2021-11-01-nutrition-analysis/",
    "title": "Measuring agricultural conditions and their relationship with infant nutrition in Burkina Faso",
    "description": "We've learned how to build key indicators with spatially referenced nutrition data from PMA. Now let's see how researchers have used them.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-11-05",
    "categories": [
      "Nutrition",
      "Climate",
      "Data Analysis",
      "Mixed Effects",
      "PMA Publications",
      "terra",
      "sf",
      "lme4",
      "gtsummary"
    ],
    "contents": "\n\nContents\nPMA Nutrition Data\nLivelihood Zones\nWRSI\nMulti-level Mixed Effects Models\nModeling MUAC\nModeling MDD and MMF\n\nWrap-up\n\nWe’re wrapping up our series on PMA nutrition surveys this week with an exciting new paper published in the journal Environmental Research Communications. Authors Jessie Pinchoff, William Turner, and Kathryn Grace look at the relationship between agricultural conditions and infant malnutrition in Burkina Faso, so their work mobilizes many of the tools we’ve demonstrated in this series so far. If you’ve been reading along, you’ll already know how to:\nfind harmonized nutrition data for infants,\nfind and summarize data collected from mothers of sampled infants,\nbuild key nutrition indicators, and\nexplore climate data together with PMA nutrition surveys\nIn this post, we’ll see how Pinchoff et al. put these pieces together to build a multi-level mixed effects logistic regression model for each of three nutrition outcomes for infants aged 6-23 months. Their results suggest that - in Burkina Faso, where many households rely on rainfed, subsistence agriculture - the quality of a particular growing season has a statistically significant impact on infant nutrition and growth.\nAs always in a PMA Publications post, we’ll share R code and discuss decisions about measurement and analysis as we review the author’s approach. Our hope is that you’ll find something useful for your own project linking climate and nutrition data! For background information on the prevalence of rainfed food systems and infant malnutrition in Burkina Faso - or for a discussion of the implications of these findings for policy and further research - we encourage you to check out the published paper.\n\nBig thanks to Jessie Pinchoff for sharing Stata source code for this paper! We’ve adapted it for R and used IPUMS PMA variables where available.\nPMA Nutrition Data\nThe authors derive three dependent variables from the 2017 Burkina Faso nutrition sample. We’ve already demonstrated how to make two of these - Minimum Dietary Diversity MDD and Minimum Meal Frequency MMF - in an earlier post. Both MDD and MMF are defined by World Health Organization (WHO) recommendations for Infant and Young Child Feeding (IYCF) practices necessary for healthy growth and development in children during the first two years of life.\n\nBe sure to check out our post discussing WHO guidelines for MDD and MMF in much greater detail.\nThe third dependent variable is an anthropometric measurement of each infant’s physical development. IPUMS PMA nutrition surveys include INFARMCIRCVAL, a measure of the infant’s mid-upper arm circumference (MUAC) taken by the interviewer on the day of the interview. According to WHO guidelines, MUAC measurements below 11.5 centimeters indicate acute malnutrition in children aged 6-23 months, and measurements between 11.5 and 12.5 centimeters indicate risk of malnutrition. Pinchoff et al. point to a range of studies suggesting that these thresholds may be too low to properly identify malnourishment, and they note that PMA surveys were conducted largely in June or July - early months of the Burkina Faso “hunger season” before food shortages were likely to manifest in anthropometric measures. To address these issues, the authors build a binary threshold for MUAC measures below 13.5 centimeters that we’ll call MUAC_LOW.\nWe’ll construct one model for each of these three outcomes with a range of independent variables recoded from IPUMS PMA variables and two external data sources (more on these in a moment). We’ll be using harmonized IPUMS PMA data where possible, but we’ll mention here that the authors used source data downloaded directly from PMA before harmonized data from IPUMS PMA became available; these differences will not change our main findings, but they did impact certain design choices. For example, the authors were not able to control for the SEX of each infant, as this information was collected on the household screening questionnaire but not on the child nutrition questionnaire; IPUMS PMA has matched records from the two questionnaires together, so you might choose to include this information in your own work.\nNotably absent from IPUMS PMA data are 1) the infant’s precise age in months, and 2) several variables related to perceived food insecurity in the infant’s household. The authors obtained this information directly from PMA source data (although it may be available from IPUMS PMA soon). We’ve previously shown how to use currently available IPUMS PMA data to accurately estimate each infant’s age within one month, but we’ll use the precise age of the infant on the day of the interview here. We’ve also pre-constructed a measure of household food insecurity HHFOODINSEC from source data: food insecurity is ranked “Severe” if 6 or more of the following statements were true, “Moderate” if 4-5 were true, and “Low/None” otherwise.\n\n\n\nWe’ve also recoded some existing IPUMS PMA variables beforehand, and we’ve located information about each child’s mother (including her own MUAC measurements) using a procedure described here. We removed all cases from the file except for infants aged 6-23 months without missing responses, and then saved this “cleaned” dataset as an R data file. We’ll use read_rds to load it into R now:\n\n\nlibrary(tidyverse)\n\ndat <- read_rds(\"data/dat_clean.rds\")\n\n\n\nWe’ll use our favorite table-making package, gtsummary, to preview all of the PMA nutrition variables we’ve made.\n\n\nlibrary(gtsummary)\n\ndat %>% \n  select(\n    INFAGE_3, INFDIAR, MOMAGE_4, MOMEDUC, MOMMUAC_3,\n    HHKIDS_4, HHFOODINSEC, MAD, MDD, MMF, MUAC_LOW\n  ) %>% \n  tbl_summary() %>% \n  modify_spanning_header(everything() ~ \"# PMA Nutrition Variables\") %>% \n  modify_header(update = list(label ~ \" \")) %>% \n  italicize_labels() %>% \n  as_gt() %>% \n  gt::tab_options(table.width = gt::pct(75))\n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#njwpbjfspx .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: 75%;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#njwpbjfspx .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#njwpbjfspx .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#njwpbjfspx .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#njwpbjfspx .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#njwpbjfspx .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#njwpbjfspx .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#njwpbjfspx .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#njwpbjfspx .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#njwpbjfspx .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#njwpbjfspx .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#njwpbjfspx .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#njwpbjfspx .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#njwpbjfspx .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#njwpbjfspx .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#njwpbjfspx .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#njwpbjfspx .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#njwpbjfspx .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#njwpbjfspx .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#njwpbjfspx .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#njwpbjfspx .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#njwpbjfspx .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#njwpbjfspx .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#njwpbjfspx .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#njwpbjfspx .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#njwpbjfspx .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#njwpbjfspx .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#njwpbjfspx .gt_left {\n  text-align: left;\n}\n\n#njwpbjfspx .gt_center {\n  text-align: center;\n}\n\n#njwpbjfspx .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#njwpbjfspx .gt_font_normal {\n  font-weight: normal;\n}\n\n#njwpbjfspx .gt_font_bold {\n  font-weight: bold;\n}\n\n#njwpbjfspx .gt_font_italic {\n  font-style: italic;\n}\n\n#njwpbjfspx .gt_super {\n  font-size: 65%;\n}\n\n#njwpbjfspx .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\n\n        PMA Nutrition Variables\n\n      \n    \n      N = 1,7221\n    Infant's age\n6-11\n638 (37%)12-17\n586 (34%)18-23\n498 (29%)Infant diarrhea last 2 wks\n580 (34%)Mother's age\n35+\n336 (20%)25-34\n809 (47%)20-24\n431 (25%)<20\n146 (8.5%)Mother ever attended school\n632 (37%)Mother's MUAC (cm)\nNormal (>25 cm)\n1,270 (74%)Risk (22.1-25 cm)\n412 (24%)Acute (22 cm or less)\n40 (2.3%)Household total kids under 5\n1\n646 (38%)2\n457 (27%)3\n291 (17%)4+\n328 (19%)Household food insecurity\nLow/None\n1,012 (59%)Moderate\n314 (18%)Severe\n396 (23%)Minimum Acceptable Diet (MAD)\n178 (10%)Minimum Dietary Diversity (MDD)\n265 (15%)Minimum Meal Frequency (MMF)\n910 (53%)MUAC 13.5 cm or less\n568 (33%)\n        \n          1\n          \n           \n          n (%)\n          \n      \n    \n\nThe authors derive two additional independent variables - including their key predictor of interest - from external datasets distributed by the Famine Early Warning Systems Network (FEWSNET).\nLivelihood Zones\nThe first external data source concerns livelihood zones for the enumeration areas used to identify households in the PMA nutrition sample. Livelihood zones describe the main ways that workers in a particular area earn income and procure food, and they are used widely in climate-health research. You can download a shapefile from FEWSNET describing nine distinct livelihood zones for Burkina Faso here.\nWe’ve downloaded the shapefile BF_LHZ_2014 and saved it in the “data” folder of our working directory. Let’s load it into R with the sf package and call the resulting object lhz.\n\n\nlibrary(sf)\n\nlhz <- st_read(\"data/BF_LHZ_2014\") %>% \n  st_transform(crs = 4326) %>%                 # Set to WGS 84\n  mutate(label = paste(LZCODE, LZNAMEEN))      # Label for our map below\n\n\n\nWe’ll want to find a livelihood zone classification for each PMA enumeration area, so we’ll also need to obtain their displaced GPS coordinates from PMA. In our last post, we described steps for creating buffer zones containing the actual centroid location for each enumeration area, subject to certain rules:\nurban centroids are displaced up to 2 kilometers in any direction\n99% of rural centroids are displaced up to 5 kilometers in any direction\n1% of rural centroids are displaced up to 10 kilometers in any direction\ndisplacement across administrative boundaries is not permitted\nWe’ve downloaded 1) a CSV file containing the displaced GPS coordinates, and 2) a shapefile containing administrative boundaries for Burkina Faso. Both are saved in the “data” folder of our working directory. We’ll load both into R and create gps_buf to represent the buffer zones for each enumeration area.\n\nMake sure to review our last post, where we describe these steps in detail.\n\n\n# Displaced gps coordinates \ngps <- read_csv(\"data/gps_bf.csv\") %>% \n  select(EAID = EA_ID, GPSLONG, GPSLAT) %>% \n  st_as_sf(coords = c(\"GPSLONG\", \"GPSLAT\"), crs = 4326) %>% \n  left_join(\n    by = \"EAID\",\n    dat %>% select(EAID, URBAN) %>% distinct()\n  )\n\n# Shapefile \nshape <- st_read(\"data/shape_bf\") %>% select(ADMIN_NAME)\n\n# Project `gps` and `shape` to meters \ngps <- gps %>% st_transform(crs = 32630)\nshape <- shape %>% st_transform(crs = 32630)\n\n# Create buffers\ngps_buf <- gps %>% \n  st_buffer(if_else(.$URBAN, 2000, 5000)) %>% \n  st_intersection(shape) %>% \n  st_filter(gps)\n\n# Revert to WGS84\ngps <- gps %>% st_transform(crs = 4326)\nshape <- shape %>% st_transform(crs = 4326)\ngps_buf <- gps_buf %>% st_transform(crs = 4326)\n\n\n\nNow, let’s plot the nine FEWNET livelihood zones in lhz with ggspatial.\n\n\nlibrary(ggspatial)\nsource(\"make_theme.R\")\n\nggplot() + \n  layer_spatial(lhz, aes(fill = label)) + \n  theme_pma(\n    title = \"FEWSNET Livelihood Zones for Burkina Faso\",\n    caption = paste0(\n      \"https://fews.net/west-africa/burkina-faso/\",\n      \"livelihood-zone-map/november-2009\"\n    )\n  ) + \n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#7B4D30\", \"#0E440F\", \"#29682C\", \"#7E9C32\", \"#587211\",\n      \"#D20501\", \"#B5D55A\", \"#4FA252\", \"#5C2D0F\")\n  ) \n\n\n\n\nWe’ve defined theme_pma in a source script called make_theme.R. If you’re curious, you can find it on our GitHub page here.\n\n\n\nTo simplify things a bit, the authors collapse these nine zones into three major categories: urban (Ouaga), agricultural, and pastoral (or agropastoral). We’ll do the same here, and then map the result.\n\n\nlhz <- lhz %>% \n  mutate(LIVZ_3 = case_when(\n    LZCODE == \"BF06\" ~ \"Ouaga/Urban\",\n    LZCODE %in% c(\"BF07\", \"BF08\", \"BF09\") ~ \"Pastoral\",\n    T ~ \"Agricultural\"\n  ))\n\nggplot() + \n  layer_spatial(\n    lhz %>% count(LIVZ_3) %>% st_make_valid() , \n    aes(fill = LIVZ_3)\n  ) + \n  layer_spatial(gps_buf, color = \"#00263A\", alpha = 0) + \n  theme_pma(\n    title = \"Distribution of PMA Enumeration Areas within Major Livelihood Zones\",\n    show_eas = TRUE\n  ) + \n  scale_fill_manual(\n    name = NULL,\n    values = c( \"#899DA4\", \"#C93311\", \"#DC863B\")\n  ) \n\n\n\n\nWe’ll now assign each enumeration area one of those three categories in dat. In the rare case where a buffer overlaps with a boundary between livelihood zones, we’ll use whichever livelihood zone contains the centroid. As we’ll see, a majority of the sampled infants reside in “agricultural” areas (54%) or “pastoral” areas (30%). Only a small minority reside in the Ouaga “urban” area (16%).\n\n\ndat <- lhz %>%\n  mutate(LIVZ_3 = case_when(\n    LZCODE == \"BF06\" ~ \"Ouaga/Urban\",\n    LZCODE %in% c(\"BF07\", \"BF08\", \"BF09\") ~ \"Pastoral\",\n    T ~ \"Agricultural\"\n  )) %>%\n  st_intersection(gps_buf) %>%      # find the intersection with EA buffers\n  st_filter(gps) %>%                # keep only if it contains the centroid\n  tibble() %>%\n  select(EAID, LIVZ_3) %>%\n  right_join(dat, by = \"EAID\")\n\n\n\n\n\ndat %>% \n  select(LIVZ_3) %>% \n  tbl_summary() %>% \n  modify_spanning_header(\n    everything() ~ \"# Recoded FEWSNET Livelihood Zones\"\n  ) %>% \n  modify_header(update = list(label ~ \" \")) %>% \n  italicize_labels() %>% \n  as_gt() %>% \n  gt::tab_options(table.width = gt::pct(75))\n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#ivmxgspknk .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: 75%;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#ivmxgspknk .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#ivmxgspknk .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#ivmxgspknk .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#ivmxgspknk .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#ivmxgspknk .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#ivmxgspknk .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#ivmxgspknk .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#ivmxgspknk .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#ivmxgspknk .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#ivmxgspknk .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#ivmxgspknk .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#ivmxgspknk .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#ivmxgspknk .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#ivmxgspknk .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#ivmxgspknk .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#ivmxgspknk .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#ivmxgspknk .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ivmxgspknk .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#ivmxgspknk .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ivmxgspknk .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#ivmxgspknk .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#ivmxgspknk .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#ivmxgspknk .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#ivmxgspknk .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#ivmxgspknk .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#ivmxgspknk .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#ivmxgspknk .gt_left {\n  text-align: left;\n}\n\n#ivmxgspknk .gt_center {\n  text-align: center;\n}\n\n#ivmxgspknk .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#ivmxgspknk .gt_font_normal {\n  font-weight: normal;\n}\n\n#ivmxgspknk .gt_font_bold {\n  font-weight: bold;\n}\n\n#ivmxgspknk .gt_font_italic {\n  font-style: italic;\n}\n\n#ivmxgspknk .gt_super {\n  font-size: 65%;\n}\n\n#ivmxgspknk .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\n\n        Recoded FEWSNET Livelihood Zones\n\n      \n    \n      N = 1,7221\n    Livelihood zone\nAgricultural\n924 (54%)Pastoral\n517 (30%)Ouaga/Urban\n281 (16%)\n        \n          1\n          \n           \n          n (%)\n          \n      \n    \n\nWRSI\nThe final data source concerns the authors’ main predictor of interest: agricultural conditions in the infant’s enumeration area that may influence the abundance and variety of available food, or the livelihoods of families involved with agricultural work.\nHere, the authors use a crop-specific Water Requirement Satisfaction Index (WRSI) - also developed by FEWSNET - to model precipitation relative to other meteorological factors that drive evaporation and transpiration (e.g. solar radiation, air temperature, humidity, and wind). Rainfall, alone, can sometimes serve as a good proxy for agricultural conditions, but not if the water evaporates before it becomes available for plants. WRSI also models the growing season for a selected reference crop - in this case, millet - to test whether rainfall occurs during the most crucial stages of plant development.\nTo see why all of this matters, let’s take a look at a simple rainfall total for the 2017 growing season. Following the steps outlined in our last post, we’ve download a dataset from the Climate Hazards center InfraRed Precipitation with Station (CHIRPS) series spanning the years 1981 (the first year of data collection) through 2017. We calculated a simple terra::sum for each year’s growing season (approximated as June 1 to Oct 1) and saved the result in a raster with one layer for each year’s total seasonal accumulation (in millimeters).\n\n\nlibrary(terra)\n\nyr_sums <- rast(\"data/chirps_sums.tif\")\n\nyr_sums\n\n\nclass       : SpatRaster \ndimensions  : 115, 159, 37  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -5.549997, 2.400003, 9.349999, 15.1  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : chirps_sums.tif \nnames       :      1981,      1982,      1983,      1984,      1985,      1986, ... \nmin values  : 156.65630, 170.00345, 139.44662,  63.97786, 121.26834, 171.29820, ... \nmax values  :  985.3304,  834.6878,  862.3036, 1024.9135, 1053.9604, 1017.6069, ... \n\nSuppose we simply wanted to gauge whether 2017 was especially wetter or drier than average in a particular place. We could calculate a simple z-score and map the result:\n\n\nz_scores <- (yr_sums$`2017` - mean(yr_sums)) / stdev(yr_sums)\n\nggplot() + \n  layer_spatial(mask(z_scores, vect(shape), touches = FALSE)) +\n  layer_spatial(lhz %>% count(LIVZ_3) %>% st_make_valid(), alpha = 0) +\n  layer_spatial(gps_buf, color = \"#00263A\", alpha = 0) +\n  theme_pma(\n    title = \"2017 Rainfall Accumulation: June 1 - Oct 1\",\n    subtitle = \"Z-scores relative to mean seasonal accumuation 1981-2017\",\n    caption = \"https://www.chc.ucsb.edu/data/chirps\",\n    show_eas = TRUE\n  ) + \n  scale_fill_gradient2(\n    name = \"Z-score\",\n    limits = c(-2.3, 2.3),\n    low = \"#FAEFD1\", \n    high = \"#00263A\", \n    na.value = \"transparent\"\n  ) \n\n\n\n\nAs you can see, there’s a good deal of spatial variation in the Burkina Faso rainfall totals for 2017. In our map, the z-score “0” indicates a perfectly average year, whereas z-scores “1” and “-1” indicate rainfall totals exactly 1 standard deviation above or below average, respectively. Enumeration areas located in the northern and eastern regions experienced more rainfall than normal in 2017, while those in the south and west experienced less.\nWe’ll assign one z-score to each enumeration area by taking a weighted average of the pixels that overlap with each buffer: the extract function will produce a “weight” equal to the proportion of each pixel included within the boundaries of a given buffer. Finally we’ll match these z-scores to each of the infants in dat.\n\nCheck out this post explaining why we take a weighted mean of the pixels in each buffer.\n\n\ndat <- z_scores %>%\n  extract(gps_buf %>% vect(), weights = TRUE) %>%\n  tibble() %>%\n  right_join(gps_buf %>% rowid_to_column(\"ID\"), by = \"ID\") %>%\n  group_by(EAID) %>%\n  summarise(CHIRPS_Z = weighted.mean(`2017`, weight)) %>%\n  full_join(dat, by = \"EAID\")\n\n\n\nNow, let’s compare these relative rainfall totals to WRSI scores obtained for each enumeration area. FEWSNET provides free software called GeoWRSI that you can use to specify a reference crop, source data, and other parameters used to calculate WRSI. The authors used CHIRPS data combined with NOAA ESRL PSD Global Reference Evapotranspiration for the FEWS NET Science Community (RefET) to calculate WRSI scores for millet. They’ve shared the program output with us as WRSI.dta, so we’ll simply attach the WRSI score for each EA to dat.\n\n\ndat <- haven::read_dta(\"data/WRSI.dta\") %>%\n  select(EAID = EA_ID, WRSI_17 = eos_wrsi_anomaly_2017) %>%\n  right_join(dat, by = \"EAID\")\n\n\n\nBecause CHIRPS precipitation totals were used to derive WRSI, you might expect to see a strong correlation between the two measures. We should see this if we create a scatterplot for each of the enumeration areas in dat:\n\n\ndat %>% \n  ggplot(aes(x = CHIRPS_Z, y = WRSI_17)) + \n  geom_point() + \n  geom_abline() + \n  theme_minimal() + \n  xlim(-2, 2) + \n  ylim(-20, 20)\n\n\n\n\nIn fact, the correlation between rainfall and WRSI is positive, but somewhat weak. Here, a negative value for WRSI indicates poor growing conditions for millet. There are a handful of enumeration areas where conditions were wetter than normal (CHIRPS_Z > 0), but overall conditions for growing millet were poor (WRSI < 0). This can happen if, for example, if conditions in 2017 were also hotter than normal (driving evaporation), or if rainfall occurred at the wrong stage of plant development. Overall, most enumeration areas were located in areas that experienced less rainfall than normal (CHIRPS_Z < 0), but only a few of these experienced exceptionally poor growing conditions for millet.\nFor these reasons, the authors use WRSI rather than the simpler CHIRPS z-scores we’ve derived. As we’ll see, they find that - controlling for all of the other independent variables we’ve discussed so far - there is a statistically significant relationship between “good agricultural conditions” and positive nutritional outcomes for infants.\n\n\ndat %>% \n  select(WRSI_17, CHIRPS_Z) %>% \n  tbl_summary(label = list(\n    WRSI_17 = \"WRSI Anomaly\",\n    CHIRPS_Z = \"CHIRPS Z-score\"\n  )) %>% \n  modify_spanning_header(\n    everything() ~ \"# 2017 Agricultural Conditions\"\n  ) %>% \n  modify_header(update = list(label ~ \" \")) %>% \n  italicize_labels() %>% \n  as_gt() %>% \n  gt::tab_options(table.width = gt::pct(75))\n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#uvuzgzmzok .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: 75%;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#uvuzgzmzok .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#uvuzgzmzok .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#uvuzgzmzok .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#uvuzgzmzok .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#uvuzgzmzok .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#uvuzgzmzok .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#uvuzgzmzok .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#uvuzgzmzok .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#uvuzgzmzok .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#uvuzgzmzok .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#uvuzgzmzok .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#uvuzgzmzok .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#uvuzgzmzok .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#uvuzgzmzok .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#uvuzgzmzok .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#uvuzgzmzok .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#uvuzgzmzok .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#uvuzgzmzok .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#uvuzgzmzok .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#uvuzgzmzok .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#uvuzgzmzok .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#uvuzgzmzok .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#uvuzgzmzok .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#uvuzgzmzok .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#uvuzgzmzok .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#uvuzgzmzok .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#uvuzgzmzok .gt_left {\n  text-align: left;\n}\n\n#uvuzgzmzok .gt_center {\n  text-align: center;\n}\n\n#uvuzgzmzok .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#uvuzgzmzok .gt_font_normal {\n  font-weight: normal;\n}\n\n#uvuzgzmzok .gt_font_bold {\n  font-weight: bold;\n}\n\n#uvuzgzmzok .gt_font_italic {\n  font-style: italic;\n}\n\n#uvuzgzmzok .gt_super {\n  font-size: 65%;\n}\n\n#uvuzgzmzok .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\n\n        2017 Agricultural Conditions\n\n      \n    \n      N = 1,7221\n    WRSI Anomaly\n0.0 (-4.0, 1.0)CHIRPS Z-score\n-0.54 (-0.92, -0.18)\n        \n          1\n          \n           \n          Median (IQR)\n          \n      \n    \n\nMulti-level Mixed Effects Models\nIf you’re a regular reader of this blog, you might remember that we’ve previously shown how to model binary outcomes like unmet need for family planning and current contraceptive use with cluster-robust standard error estimation via the survey package. When we do so, we address the likelihood that neighboring households - those in the same enumeration area - probably share many common features. While the point-estimate for each predictor is identical to the estimate you’d obtain from logistic regression, the estimated standard error for each predictor is typically larger. Practically speaking, we make it harder for each predictor to pass the same test for statistical significance (e.g. “p < .05”) because we suspect that the clustered sample design produces artificial agreement between respondents.\nPinchoff et al. use a different approach to solve the same problem, except that their multi-level mixed effects model incorporates variation within and between enumeration areas into the estimate associated with each predictor. As a result, both the point-estimate and the standard error associated with each is different compared to what you’d obtain from logistic regression.\nYou might imagine this approach in contrast to a naive fixed effects model where we simply plugged in EAID as an independent variable. Because EAID is categorical, we’d effectively create one “dummy variable” for every enumeration area (categorical variables like LIVZ_3 or MOMEDUC are handled this way). The estimates for all of our other independent variables would be “controlled for” variation between enumeration areas.\nWhen we treat enumeration areas as random effects, we explicitly incorporate this sort of variation into our model. Compared with the fixed effects approach - where each “dummy variable” is defined by infants from the same enumeration area - the random effects approach combines what we know about one enumeration area with information about how it compares with the others. The result is an improved estimation for every “dummy variable,” because each incorporates information from the full sample. More importantly, this generally reduces the standard error for each predictor. Let’s see how the authors implemented this approach to model MUAC_LOW, MDD, and MFF.\nModeling MUAC\nMost of the functions we use for multi-level modeling come from the lme4 package. We’ll need an analogue to the glm function that allows us to specify a “binomial” link function and a formula that specifies EAID as a random effect.\n\n\nlibrary(lme4)\n\n\n\nHere, we use glmer to build three models: one that controls for MDD, one that controls for MMF, and one that controls for neither. The notation (1|EAID) signals that we’ll be fitting 1 intercept for each enumeration area (note, however, that these intercepts are not reported in the model output; instead, we’ll see a standard deviation summarizing the dispersion between these intercepts).\n\n\nMUAC1 <- glmer(\n  data = dat,\n  family = \"binomial\",\n  formula = MUAC_LOW ~ WRSI_17 + LIVZ_3 + MOMAGE_4 + MOMMUAC_3 + \n    INFDIAR + MOMEDUC + HHKIDS_4 + INFAGE_3 + (1|EAID)\n)\n\nMUAC2 <- glmer(\n  data = dat,\n  family = \"binomial\",\n  formula = MUAC_LOW ~ WRSI_17 + MDD + LIVZ_3 + MOMAGE_4 + MOMMUAC_3 + \n    INFDIAR + MOMEDUC + HHKIDS_4 + INFAGE_3 + (1|EAID)\n)\n\nMUAC3 <- glmer(\n  data = dat,\n  family = \"binomial\",\n  formula = MUAC_LOW ~ WRSI_17 + MMF + LIVZ_3 + MOMAGE_4 + MOMMUAC_3 + \n    INFDIAR + MOMEDUC + HHKIDS_4 + INFAGE_3 + (1|EAID)\n)\n\n\n\nThe next step looks messy, but it’s mostly just formatting the output to fit in a gtsummary table. Basically, we’ll use purrr::map to create one table for each model, and then store them in a list. Then we use gtsummary::tbl_merge to merge them together. The rest is just creating labels, moving rows, and setting the overall look of the table on this page:\n\n\nlist(MUAC1, MUAC2, MUAC3) %>% \n  map(\n    ~.x %>%   \n      tbl_regression(\n        exp = TRUE, \n        conf.int = TRUE, \n        show_single_row = where(is.logical),\n        tidy_fun = broom.mixed::tidy, \n        label = list(\n          `EAID.sd__(Intercept)` = \"Random Effects: EAID (standard deviation)\"\n        )\n      ) %>% \n      add_significance_stars() \n  ) %>% \n  tbl_merge() %>%  \n  italicize_labels() %>% \n  modify_header(update = list(label ~ \" \")) %>% \n  modify_spanning_header(update = list(\n    estimate_1 ~ \"**Model 1**\",\n    std.error_1 ~ \"**Model 1**\",\n    estimate_2 ~ \"**Model 2**\",\n    std.error_2 ~ \"**Model 2**\",\n    estimate_3 ~ \"**Model 3**\",\n    std.error_3 ~ \"**Model 3**\"\n  )) %>% \n  modify_table_body(\n    ~.x %>% \n      mutate(variable = variable %>% fct_relevel(c(\n        \"WRSI_17\", \"MDD\", \"MMF\", \"LIVZ_3\", \"MOMAGE_4\", \"MOMMUAC_3\",\n        \"INFDIAR\", \"MOMEDUC\", \"HHKIDS_4\", \"INFAGE_3\", \"EAID.sd__(Intercept)\"\n      ))) %>% \n      arrange(variable)\n  ) %>% \n  as_gt() %>% \n  gt::tab_options(table.width = gt::pct(100)) %>% \n  gt::tab_header(\n    gt::md(\"# Factors Associated with <br> MUAC <= 13.5 cm\")\n  )\n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#rtsmtbhsta .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: 100%;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#rtsmtbhsta .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#rtsmtbhsta .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#rtsmtbhsta .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#rtsmtbhsta .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#rtsmtbhsta .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#rtsmtbhsta .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#rtsmtbhsta .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#rtsmtbhsta .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#rtsmtbhsta .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#rtsmtbhsta .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#rtsmtbhsta .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#rtsmtbhsta .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#rtsmtbhsta .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#rtsmtbhsta .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#rtsmtbhsta .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#rtsmtbhsta .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#rtsmtbhsta .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#rtsmtbhsta .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#rtsmtbhsta .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#rtsmtbhsta .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#rtsmtbhsta .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#rtsmtbhsta .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#rtsmtbhsta .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#rtsmtbhsta .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#rtsmtbhsta .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#rtsmtbhsta .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#rtsmtbhsta .gt_left {\n  text-align: left;\n}\n\n#rtsmtbhsta .gt_center {\n  text-align: center;\n}\n\n#rtsmtbhsta .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#rtsmtbhsta .gt_font_normal {\n  font-weight: normal;\n}\n\n#rtsmtbhsta .gt_font_bold {\n  font-weight: bold;\n}\n\n#rtsmtbhsta .gt_font_italic {\n  font-style: italic;\n}\n\n#rtsmtbhsta .gt_super {\n  font-size: 65%;\n}\n\n#rtsmtbhsta .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\nFactors Associated with  MUAC <= 13.5 cm\n\n    \n      \n        Model 1\n      \n      \n        Model 2\n      \n      \n        Model 3\n      \n    OR1,2\n      SE2\n      OR1,2\n      SE2\n      OR1,2\n      SE2\n    WRSI anomaly 2017\n0.97*\n0.016\n0.97*\n0.016\n0.97*\n0.016Minimum Dietary Diversity (MDD)\n\n\n0.67*\n0.115\n\nMinimum Meal Frequency (MMF)\n\n\n\n\n1.03\n0.118Livelihood zone\n\n\n\n\n\nAgricultural\n—\n—\n—\n—\n—\n—Pastoral\n1.20\n0.220\n1.19\n0.214\n1.20\n0.220Ouaga/Urban\n1.09\n0.254\n1.08\n0.249\n1.09\n0.254Mother's age\n\n\n\n\n\n35+\n—\n—\n—\n—\n—\n—25-34\n0.96\n0.141\n0.97\n0.142\n0.96\n0.14120-24\n0.93\n0.156\n0.92\n0.155\n0.93\n0.156<20\n1.17\n0.260\n1.16\n0.260\n1.17\n0.261Mother's MUAC (cm)\n\n\n\n\n\nNormal (>25 cm)\n—\n—\n—\n—\n—\n—Risk (22.1-25 cm)\n1.64***\n0.208\n1.65***\n0.210\n1.64***\n0.209Acute (22 cm or less)\n2.71**\n0.937\n2.71**\n0.935\n2.71**\n0.937Infant diarrhea last 2 wks\n1.22\n0.143\n1.20\n0.141\n1.22\n0.144Mother ever attended school\n0.71**\n0.092\n0.73*\n0.094\n0.71**\n0.092Household total kids under 5\n\n\n\n\n\n1\n—\n—\n—\n—\n—\n—2\n1.17\n0.164\n1.17\n0.164\n1.17\n0.1643\n1.13\n0.186\n1.13\n0.186\n1.13\n0.1874+\n1.21\n0.196\n1.20\n0.194\n1.21\n0.196Infant's age\n\n\n\n\n\n6-11\n—\n—\n—\n—\n—\n—12-17\n0.80\n0.104\n0.84\n0.109\n0.80\n0.10418-23\n0.81\n0.110\n0.83\n0.113\n0.81\n0.110Random Effects: EAID (standard deviation)\n0.48\n\n0.46\n\n0.48\n\n        \n          1\n          \n           \n          *p<0.05; **p<0.01; ***p<0.001\n          \n        \n          2\n          \n           \n          OR = Odds Ratio, SE = Standard Error\n          \n      \n    \n\nGenerally speaking, all three of these models suggest that improved agricultural conditions in the enumeration area (measured by WRSI) are associated with lower odds of infant malnourishment (measured by a MUAC 13.5 cm or lower). This effect is roughly the same whether we control for IYCF practices (MDD and MMF) or not. The authors note that “for some children who are near the margin or threshold for intervention, seasonal variation in agricultural yield could therefore play an important role in shifting them from (or into) different risk categories.”\nControlling for WRSI, infants who consume a diverse diet (measured by MDD) appear significantly less likely to have MUAC measurements below 13.5 cm. However, meal frequency (measured by MMF) had no statistically significant effect. Other significant factors are related to the mother’s nutritional status and her education level.\nModeling MDD and MMF\nWe’ll use the same appraoch to recreate the authors models for MDD and MMF:\n\n\nMDD <- glmer(\n  data = dat,\n  family = \"binomial\",\n  formula = MDD ~ WRSI_17 + LIVZ_3 + MOMAGE_4 + HHFOODINSEC + MOMEDUC + \n    HHKIDS_4 + INFAGE_3 + (1|EAID)\n) \n\nMMF <- glmer(\n  data = dat,\n  family = \"binomial\",\n  formula = MMF ~ WRSI_17 + LIVZ_3 + MOMAGE_4 + HHFOODINSEC + MOMEDUC + \n    HHKIDS_4 + INFAGE_3 + (1|EAID)\n) \n\nlist(MMF, MDD) %>% \n  map(\n    ~.x %>%   \n      tbl_regression(\n        exp = TRUE, \n        conf.int = TRUE, \n        show_single_row = where(is.logical),\n        tidy_fun = broom.mixed::tidy, \n        label = list(\n          `EAID.sd__(Intercept)` = \"Random Effects: EAID (standard deviation)\"\n        )\n      ) %>% \n      add_significance_stars()\n  ) %>% \n  tbl_merge() %>%  \n  italicize_labels() %>% \n  modify_header(update = list(label ~ \" \")) %>% \n  modify_spanning_header(update = list(\n    estimate_1 ~ \"**Minimum Meal Frequency (MMF)**\",\n    std.error_1 ~ \"**Minimum Meal Frequency (MMF)**\",\n    estimate_2 ~ \"**Minimum Dietary Diversity (MDD)**\",\n    std.error_2 ~ \"**Minimum Dietary Diversity (MDD)**\"\n  )) %>% \n  modify_table_body(\n    ~.x %>% \n      mutate(variable = variable %>% fct_relevel(c(\n        \"WRSI_17\", \"LIVZ_3\", \"HHFOODINSEC\", \"MOMAGE_4\", \"MOMEDUC\",\n        \"HHKIDS_4\", \"INFAGE_3\", \"EAID.sd__(Intercept)\"\n      ))) %>% \n      arrange(variable)\n  ) %>% \n  as_gt() %>% \n  gt::tab_options(table.width = gt::pct(100)) %>% \n  gt::tab_header(\n    gt::md(\"# Factors Associated with <br> Recommended IYCF Practices\") \n  )\n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#ouatgsjukp .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: 100%;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#ouatgsjukp .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#ouatgsjukp .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#ouatgsjukp .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#ouatgsjukp .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#ouatgsjukp .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#ouatgsjukp .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#ouatgsjukp .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#ouatgsjukp .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#ouatgsjukp .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#ouatgsjukp .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#ouatgsjukp .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#ouatgsjukp .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#ouatgsjukp .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#ouatgsjukp .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#ouatgsjukp .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#ouatgsjukp .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#ouatgsjukp .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ouatgsjukp .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#ouatgsjukp .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ouatgsjukp .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#ouatgsjukp .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#ouatgsjukp .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#ouatgsjukp .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#ouatgsjukp .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#ouatgsjukp .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#ouatgsjukp .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#ouatgsjukp .gt_left {\n  text-align: left;\n}\n\n#ouatgsjukp .gt_center {\n  text-align: center;\n}\n\n#ouatgsjukp .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#ouatgsjukp .gt_font_normal {\n  font-weight: normal;\n}\n\n#ouatgsjukp .gt_font_bold {\n  font-weight: bold;\n}\n\n#ouatgsjukp .gt_font_italic {\n  font-style: italic;\n}\n\n#ouatgsjukp .gt_super {\n  font-size: 65%;\n}\n\n#ouatgsjukp .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\nFactors Associated with  Recommended IYCF Practices\n\n    \n      \n        Minimum Meal Frequency (MMF)\n      \n      \n        Minimum Dietary Diversity (MDD)\n      \n    OR1,2\n      SE2\n      OR1,2\n      SE2\n    WRSI anomaly 2017\n1.02\n0.025\n1.07*\n0.034Livelihood zone\n\n\n\nAgricultural\n—\n—\n—\n—Pastoral\n1.48\n0.390\n0.83\n0.276Ouaga/Urban\n1.46\n0.450\n0.87\n0.331Household food insecurity\n\n\n\nLow/None\n—\n—\n—\n—Moderate\n0.86\n0.129\n0.77\n0.166Severe\n0.91\n0.139\n0.58*\n0.128Mother's age\n\n\n\n35+\n—\n—\n—\n—25-34\n1.04\n0.151\n1.13\n0.22720-24\n0.85\n0.141\n0.81\n0.190<20\n0.61*\n0.138\n0.90\n0.289Mother ever attended school\n1.10\n0.143\n1.75***\n0.297Household total kids under 5\n\n\n\n1\n—\n—\n—\n—2\n1.09\n0.150\n0.93\n0.1723\n0.81\n0.133\n0.89\n0.2034+\n1.07\n0.174\n0.76\n0.184Infant's age\n\n\n\n6-11\n—\n—\n—\n—12-17\n1.38*\n0.176\n2.78***\n0.52018-23\n1.96***\n0.263\n2.18***\n0.422Random Effects: EAID (standard deviation)\n0.83\n\n0.99\n\n        \n          1\n          \n           \n          *p<0.05; **p<0.01; ***p<0.001\n          \n        \n          2\n          \n           \n          OR = Odds Ratio, SE = Standard Error\n          \n      \n    \n\nHere, improved agricultural conditions in the enumeration area are again broadly associated with improved nutritional outcomes, but only MDD is shown to be significantly improved by higher WRSI scores. Infants whose mother perceived “severe” food insecurity in their household (answering 6 or more questions positively) were significantly less likely to achieve MDD, but the effect on infant MMF was ambiguous.\nWhat do we make of the difference between dietary diversity and meal frequency? The authors suggest that it might be easier for respondents to recall types of foods compared to the frequency of food consumption if, for example, mothers were not present for every meal (or if small meals are not recalled, if meal times are inconsistent, etc). Similarly, they may be more sensitive to changes in the composition of their family’s diet, whereas meal timing / occurrence may be less connected with an individual’s perception of food security.\nWrap-up\nThe kinds of dietary recall questions you’ll find in PMA nutrition surveys are common, but they’re rarely accompanied by the kind of spatial data Pinchoff et al. bring to this analysis. If you’re working on a research project that uses spatially referenced nutrition data - or if you’re using remotely sensed climate data together with other global health surveys - we’d love to hear from you. As always, you can reach us in the comments below or on Twitter.\nComing up later this month: we’ll be diving into the brand new SDP client exit interview data available from IPUMS PMA. As we’ll see, there are plenty of exciting ways to explore spatial dynamics of service delivery, as well.\nSpecial thanks to Greg Husak at the the Climate Hazards Center, and Jiao Yu at the University of Minnesota for excellent help with this post!\n\n\n\n“Indicators for Assessing Infant and Young Child Feeding Practices: Definitions and Measurement Methods.” 2021. World Health Organization; the United Nations Children’s Fund (UNICEF). https://apps.who.int/iris/bitstream/handle/10665/340706/9789240018389-eng.pdf?sequence=1.\n\n\nPinchoff, Jessie, William Turner, and Kathryn Grace. 2021. “The Association Between Agricultural Conditions and Multiple Dimensions of Undernutrition in Children 6-23 Months of Age in Burkina Faso.” Environmental Research Communications 3 (6): 065004. https://iopscience.iop.org/article/10.1088/2515-7620/ac07f5/meta.\n\n\n\n\n",
    "preview": "posts/2021-11-01-nutrition-analysis/nutrition-analysis_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2021-11-05T15:15:39-05:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1536
  },
  {
    "path": "posts/2021-10-15-nutrition-climate/",
    "title": "How to use CHIRPS Climate Data with PMA Nutrition Surveys",
    "description": "Remotely sensed daily precipitation data is an incredible resource for understanding rainfed food systems.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-10-15",
    "categories": [
      "Nutrition",
      "CHIRPS",
      "Precipitation",
      "Climate",
      "Agriculture",
      "terra",
      "sf",
      "ggspatial"
    ],
    "contents": "\n\nContents\nSetup\nSimple features objects\nPlot theme\n\nEnumeration Area Buffers\nCHIRPS data\nRaster data with terra\nRaster layers\n\nRainfall within Enumeration Areas\n\nIf you’ve been following along with this series, you already know that PMA nutrition surveys offer researchers a unique opportunity to connect nutrition data for women and young children with data about their local health and nutrition services environment. PMA promotes this type of contextual research by cluster sampling both households and service delivery points within the same geographically-defined enumeration areas.\nIn this post, we’ll take a look at another approach to studying contextual factors that impact nutrition. Specifically, we’ll be laying the groundwork for our next post, where we’ll dive into a recent paper published in Environmental Research Communications that explores the impact of climate change on rainfed agriculture in Burkina Faso. As we’ll see, the authors use PMA nutrition data together with local precipitation measures from the Climate Hazards center InfraRed Precipitation with Station dataset (CHIRPS) to show how spatial patterns of growing season quality impact the quality and variety of foods available for women and young children.\nResearch like this is made possible when we know the approximate centroid location for enumeration areas used in PMA nutrition surveys. PMA offers displaced GPS coordinates for all of its nutrition surveys here. In preparation for our next post, we’ll show how to combine these GPS coordinates with an IPUMS PMA data extract and precipitation raster data downloaded from CHIRPS.\nSetup\nTo get started, we’ll download a 2017 Burkina Faso nutrition data extract from IPUMS PMA and load it into R. We’ve selected responses only for “Females and Children with Nutrition Information” (all other household members have been omitted). As usual, we’ll save the “dat” file together with the “xml” codebook in our working directory, and we’ll then load both into R with the tidyverse and ipumsr packages.\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\n\ndat <- read_ipums_micro(\n  ddi = \"data/pma_00002.xml\",\n  data = \"data/pma_00002.dat.gz\"\n)\n\n\n\nNotably, IPUMS PMA does not disseminate the GPS coordinates we’ll be using in this post. Instead, you’ll need to apply to download them directly from our partners at PMA. Once approved, you’ll receive a CSV file containing the displaced centroid for every enumeration area represented in our data extract dat. You’ll also receive complete documentation detailing the procedure PMA used to displace each centroid from its original location.\nIf you’re new to spatial analysis in R, you might expect to begin by loading the CSV into R the usual way:\n\n\ngps <- read_csv(\"data/gps_bf.csv\") %>% \n  select(EA_ID, GPSLONG, GPSLAT, DATUM)\ngps \n\n\n# A tibble: 83 × 4\n   EA_ID GPSLONG GPSLAT DATUM\n   <dbl>   <dbl>  <dbl> <chr>\n 1  7610  -1.07    12.9 WGS84\n 2  7820  -4.07    12.7 WGS84\n 3  7271  -1.58    12.4 WGS84\n 4  7799  -1.54    12.4 WGS84\n 5  7243   0.380   12.1 WGS84\n 6  7026  -2.37    12.3 WGS84\n 7  7859  -1.57    12.3 WGS84\n 8  7725  -1.55    12.3 WGS84\n 9  7390  -2.21    12.1 WGS84\n10  7104  -1.91    12.8 WGS84\n# … with 73 more rows\n\nHere, we see one row for each of the 83 enumeration areas included in the 2017 Burkina Faso sample. The column EA_ID corresponds to the variable EAID, and the displaced latitude and longitude points are displayed in GPSLAT and GPSLONG, respectively. The column DATUM shows the coordinate reference system for those points: “WGS84” for the World Geodetic System 1984.\nIn order to perform geometrical operations with these gps coordinates, we’ll need to load the sf package designed for manipulating simple features used by geographic information systems. We’ve covered the basics of this package in previous posts here and here, so we’ll skip over some of the introductory details this time; however, it’s crucial to know that sf requires three operating system dependencies:\nGEOS for geometrical operations on projected coordinates\nPRØJ for coordinate reference system conversion and transformation\nGDAL for driver options\nMake sure to follow these instructions for installing GEOS, PRØJ, and GDAL on your operating system. You may also need to update R, and then run install.packages(\"sf\").\nSimple features objects\nOnce you’ve installed sf, load it into R and use st_as_sf to coerce gps into the “simple features” object class. We’ll tell sf that the coordinates data are stored in GPSLONG and GPSLAT, and that the points are modeled with the coordinate reference system (crs) 4326 (this is the EPSG code corresponding to the World Geodetic System 1984).\n\n\n\n\n© (GPLv2)\n\n\nlibrary(sf)\ngps <- gps %>% \n  st_as_sf(\n    coords = c(\"GPSLONG\", \"GPSLAT\"), \n    crs = 4326\n  )\ngps \n\n\nSimple feature collection with 83 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -5.18865 ymin: 9.883331 xmax: 1.634087 ymax: 14.39679\nGeodetic CRS:  WGS 84\n# A tibble: 83 × 3\n   EA_ID DATUM             geometry\n * <dbl> <chr>          <POINT [°]>\n 1  7610 WGS84 (-1.065238 12.93839)\n 2  7820 WGS84 (-4.065366 12.72985)\n 3  7271 WGS84 (-1.583361 12.37884)\n 4  7799 WGS84 (-1.540024 12.40452)\n 5  7243 WGS84 (0.3797186 12.07097)\n 6  7026 WGS84 (-2.366313 12.26087)\n 7  7859 WGS84 (-1.570808 12.33761)\n 8  7725 WGS84 (-1.552744 12.33411)\n 9  7390 WGS84 (-2.211783 12.09784)\n10  7104 WGS84 (-1.909244 12.76427)\n# … with 73 more rows\n\nThe result of this transformation looks something like a tibble, except that it contains a header describing a simple feature collection with 83 “features” (one per enumeration area) and 2 “fields” (EA_ID and DATUM). The new column geometry replaces GPSLONG and GPSLAT, and it contains the latitude / longitude for each displaced centroid.\nThere are several graphics packages available for mapping simple feature collections, but we’ll focus here on ggspatial - an extension of the ggplot2 package we’ve introduced elsewhere on this blog. For example, we can now easily lay out the displaced centroid for each EAID as a point on a grid like so:\n\nIn this post, we’ll be plotting SpatRaster objects from the terra package. Support for terra objects is provided in ggspatial version 1.1.5.9000 (currently available on GitHub).\n\n\nlibrary(ggspatial)\nggplot() + layer_spatial(gps)\n\n\n\n\nThe ggspatial package comes with several base-map options, accessible via annotation_map_tile. However, we’ll use a shapefile we’ve downloaded from IPUMS PMA and saved in the “data” folder of our working directory. We’ll use sf::st_read to load it as another simple feature collection (we’ll also drop some columns that we won’t be using in this post).\n\n\nshape <- st_read(\"data/shape_bf\") %>% select(ADMIN_NAME)\n\n\nReading layer `geobf' from data source \n  `/Users/Matt/R/pma-data-hub/_posts/2021-10-15-nutrition-climate/data/shape_bf' \n  using driver `ESRI Shapefile'\nSimple feature collection with 13 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -5.521112 ymin: 9.393889 xmax: 2.404293 ymax: 15.08511\nGeodetic CRS:  WGS 84\n\nshape\n\n\nSimple feature collection with 13 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -5.521112 ymin: 9.393889 xmax: 2.404293 ymax: 15.08511\nGeodetic CRS:  WGS 84\nFirst 10 features:\n          ADMIN_NAME                       geometry\n1  Boucle du Mouhoun MULTIPOLYGON (((-3.206306 1...\n2           Cascades MULTIPOLYGON (((-5.388849 1...\n3             Centre MULTIPOLYGON (((-1.565052 1...\n4         Centre-Est MULTIPOLYGON (((-0.2517975 ...\n5        Centre-Nord MULTIPOLYGON (((-0.6722373 ...\n6       Centre-Ouest MULTIPOLYGON (((-2.547486 1...\n7         Centre-Sud MULTIPOLYGON (((-1.470801 1...\n8                Est MULTIPOLYGON (((0.06834642 ...\n9      Hauts-Bassins MULTIPOLYGON (((-4.483203 1...\n10              Nord MULTIPOLYGON (((-2.952616 1...\n\nWith ggspatial, you can layer different simple feature collections together, much like you would layer multiple geometries on a bar chart or any other figure in ggplot2. We’ll build a transparent layer for shape (by setting alpha = 0), and then we’ll build a layer for gps.\n\n\nggplot() + \n  layer_spatial(shape, alpha = 0) + # `alpha = 0` makes the shapes transparent\n  layer_spatial(gps) \n\n\n\n\nPlot theme\nIn previous posts, we’ve shown how to make a custom theme for plots made with ggplot2. Because ggspatial is a spatial extension of ggplot2, we’ll do so again here to create theme_pma_rainfall. This theme builds on theme_minimal by specifying a font, several label and color options, a few custom mapping options:\ntitle gives us a quick way to provide a title for our map\nsubtitle provides an optional subtitle\nshow_legend allows us to show or hide a pre-designed legend describing the shapes we’ll use to show urban and rural enumeration areas\nmanual_grid describes the coordinates of one particular enumeration area we’ll be focusing on in an example below\n\n\nsysfonts::font_add(\n  family = \"cabrito\", \n  regular = \"../../fonts/cabritosansnormregular-webfont.ttf\"\n)\nshowtext::showtext_auto()\n\ntheme_pma_rainfall <- function(\n  title, \n  subtitle = NULL, \n  show_legend = TRUE,\n  manual_grid = FALSE\n){\n  components <- list(\n    theme_minimal() %+replace% \n      theme(\n        text = element_text(family = \"cabrito\", size = 10), \n        plot.title = element_text(\n          hjust = 0,\n          size = 18, \n          color = \"#00263A\", # IPUMS navy\n          margin = margin(b = 5)\n        ), \n        plot.subtitle = element_text(\n          size = 12, \n          hjust = 0,\n          margin = margin(b = 10)\n        )\n      ),\n    labs(\n      title = title, \n      subtitle = subtitle,\n      fill = \"Rainfall total (mm)\",\n      size = \"EA displacement buffer\",\n      x = NULL,\n      y = NULL\n    ),\n    guides(size = guide_legend(override.aes = list(alpha = 1))),\n    annotation_scale(aes(style = \"ticks\", location = \"br\")),\n    if(show_legend){c(\n      geom_point(\n        mapping = aes(size = URBAN + 1, x = 0, y = 14),\n        data = gps,\n        alpha = 0, \n        shape = 21, \n        fill = \"white\"\n      ),\n      scale_size(\n        breaks = c(1, 2), \n        range = c(.75, 3), \n        labels = c(\"Urban (2 km)\", \"Rural (5 km)\")\n      )\n    )},\n    if(manual_grid){c(\n      scale_x_continuous(breaks = seq(from = -0.4, to = -0.2, by = 0.025)),\n      scale_y_continuous(breaks = seq(from = 13.4, to = 13.6, by = 0.025)),\n      scale_fill_gradient2(high = \"#BEC4CB\")\n    )}\n  )\n}\n\n\n\nWe’ll be layering theme_pma_rainfall onto our ggspatial maps like so:\n\n\nggplot() + \n  layer_spatial(shape, alpha = 0) + \n  layer_spatial(gps) + \n  theme_pma_rainfall(\n    title = \"2017 Burkina Faso Nutrition Survey Enumeration Areas\",\n    subtitle = \"Displaced centroid location for household sample clusters\",\n    show_legend = FALSE\n  )\n\n\n\n\nEnumeration Area Buffers\nWe mentioned above that the gps coordinates downloaded from PMA are not the actual centroid locations for each EA. These coordinates are randomly displaced from the actual centroid, subject to certain rules:\nurban EAs were displaced up to 2 kilometers in any direction\n99% of rural EAs were displaced up to 5 kilometers in any direction\n1% of rural EAs were discplaced up to 10 kilometers in any direction\ndisplaced coordinates could not cross one of the administrative boundaries shown in our shape file\n\n\n\nFigure 1: Image courtesy https://pmadata.org\n\n\n\nIn order to better represent the actual centroid location for each EA, we’ll use sf::st_buffer to create an appropriately sized buffer zone around the displaced gps coordinates for urban and rural areas. First, we’ll need to project both gps and shape with a coordinate reference system that uses meters rather than degrees of latitude / longitude. We’ll use EPSG code 32630 to select an appropriate projection for Burkina Faso. Notice that the geometry column for each now describes a point in meters:\n\n\n# Project `gps` to meters \ngps <- gps %>% st_transform(crs = 32630)\ngps\n\n\nSimple feature collection with 83 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 261303.6 ymin: 1092514 xmax: 1005762 ymax: 1593223\nProjected CRS: WGS 84 / UTM zone 30N\n# A tibble: 83 × 3\n   EA_ID DATUM           geometry\n * <dbl> <chr>        <POINT [m]>\n 1  7610 WGS84 (709895.9 1431117)\n 2  7820 WGS84 (384340.5 1407498)\n 3  7271 WGS84 (654009.7 1368854)\n 4  7799 WGS84 (658706.4 1371720)\n 5  7243 WGS84 (868012.7 1336673)\n 6  7026 WGS84 (568916.8 1355481)\n 7  7859 WGS84   (655399 1364301)\n 8  7725 WGS84 (657365.6 1363924)\n 9  7390 WGS84   (585776 1337496)\n10  7104 WGS84 (618400.2 1411317)\n# … with 73 more rows\n\n# Project `shape` to meters \nshape <- shape %>% st_transform(crs = 32630)\nshape\n\n\nSimple feature collection with 13 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 223985.4 ymin: 1038409 xmax: 1089380 ymax: 1669244\nProjected CRS: WGS 84 / UTM zone 30N\nFirst 10 features:\n          ADMIN_NAME                       geometry\n1  Boucle du Mouhoun MULTIPOLYGON (((477692.2 15...\n2           Cascades MULTIPOLYGON (((238964.1 12...\n3             Centre MULTIPOLYGON (((655859.9 13...\n4         Centre-Est MULTIPOLYGON (((798642.2 13...\n5        Centre-Nord MULTIPOLYGON (((751439.8 15...\n6       Centre-Ouest MULTIPOLYGON (((549102.6 14...\n7         Centre-Sud MULTIPOLYGON (((666365.7 13...\n8                Est MULTIPOLYGON (((832139.5 14...\n9      Hauts-Bassins MULTIPOLYGON (((338580.2 13...\n10              Nord MULTIPOLYGON (((505125.2 15...\n\nNext, we’ll need to identify which of the EAs in gps are located in urban areas. To do so, we’ll use the URBAN indicator for each EAID in our data extract dat.\n\n\ngps <- dat %>% \n  count(EAID, URBAN) %>% \n  select(EAID, URBAN) %>% \n  mutate(URBAN = if_else(URBAN == 1, TRUE, FALSE)) %>% \n  full_join(gps %>% rename(EAID = EA_ID), ., by = \"EAID\")\ngps\n\n\nSimple feature collection with 83 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 261303.6 ymin: 1092514 xmax: 1005762 ymax: 1593223\nProjected CRS: WGS 84 / UTM zone 30N\n# A tibble: 83 × 4\n    EAID DATUM           geometry URBAN\n   <dbl> <chr>        <POINT [m]> <lgl>\n 1  7610 WGS84 (709895.9 1431117) FALSE\n 2  7820 WGS84 (384340.5 1407498) FALSE\n 3  7271 WGS84 (654009.7 1368854) TRUE \n 4  7799 WGS84 (658706.4 1371720) TRUE \n 5  7243 WGS84 (868012.7 1336673) TRUE \n 6  7026 WGS84 (568916.8 1355481) TRUE \n 7  7859 WGS84   (655399 1364301) TRUE \n 8  7725 WGS84 (657365.6 1363924) TRUE \n 9  7390 WGS84   (585776 1337496) FALSE\n10  7104 WGS84 (618400.2 1411317) FALSE\n# … with 73 more rows\n\nThe function st_buffer will draw a circle around each centroid. We’ll specify that the radius of our circle should be 2000 meters if a particular EA is URBAN, and 5000 meters otherwise. We’ll then bisect each circle with st_intersection if it crosses an administrative boundary in shape, and we’ll use st_filter to discard any resulting section that does not contain one of the original centroids in gps.\n\nWe use a 5 km buffer for all rural EAs in our maps, but remember that an unknown 1% of rural EAs were actually displaced 10 km.\n\n\ngps <- gps %>% \n  st_buffer(if_else(.$URBAN, 2000, 5000)) %>% \n  st_intersection(shape) %>% \n  st_filter(gps) \ngps \n\n\nSimple feature collection with 83 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 257997.5 ymin: 1090514 xmax: 1010762 ymax: 1598223\nProjected CRS: WGS 84 / UTM zone 30N\n# A tibble: 83 × 5\n    EAID DATUM URBAN ADMIN_NAME                               geometry\n * <dbl> <chr> <lgl> <chr>                               <POLYGON [m]>\n 1  7820 WGS84 FALSE Boucle du Mouhoun ((389340.5 1407498, 389333.6 1…\n 2  7139 WGS84 TRUE  Boucle du Mouhoun ((452887.2 1376644, 452884.4 1…\n 3  7869 WGS84 FALSE Boucle du Mouhoun ((524026.7 1425830, 524019.9 1…\n 4  7602 WGS84 FALSE Boucle du Mouhoun ((473865.1 1373194, 473858.3 1…\n 5  7185 WGS84 TRUE  Boucle du Mouhoun ((383533.7 1346964, 383531 134…\n 6  7016 WGS84 FALSE Boucle du Mouhoun ((359037.3 1313005, 359030.4 1…\n 7  7212 WGS84 FALSE Boucle du Mouhoun ((531483.7 1275510, 531476.8 1…\n 8  7813 WGS84 FALSE Cascades          ((412468.5 1144108, 412461.6 1…\n 9  7521 WGS84 FALSE Cascades          ((289412.7 1156524, 289405.9 1…\n10  7335 WGS84 TRUE  Cascades          ((307168.9 1176806, 307166.1 1…\n# … with 73 more rows\n\nNotice that the geometry column now describes a “polygon” rather than a “point”. This polygon is defined by a series of latitude / longitude pairs that form the circumference of a buffer zone.\nWe’re finished measuring distance in meters, so we’ll revert back to degrees of latitude / longitude and plot the result. (This time, we’ll use the argument show_legend = TRUE to adopt the custom legend we designed for theme_pma_rainfall).\n\n\ngps <- gps %>% st_transform(crs = 4326)\nshape <- shape %>% st_transform(crs = 4326)\n\nggplot() + \n  layer_spatial(shape, alpha = 0) + \n  layer_spatial(gps, alpha = 0) + \n  theme_pma_rainfall(\n    title = \"2017 Burkina Faso Nutrition Survey Enumeration Areas\",\n    subtitle = \"Displaced centroid location for household sample clusters\",\n    show_legend = TRUE\n  )\n\n\n\n\nNow that we’ve identified the buffer zone for each cluster of sampled households, we’re ready to calculate a measure of local precipitation for each household with data from CHIRPS.\nCHIRPS data\nThe complete CHIRPS precipitation data series can be downloaded directly from the UCSB Climate Hazards Center, but we imagine most users will want to select an area of interest through the CHIRPS API provider, ClimateServ. If you’re familiar with tools for downloading and saving raster files, you can submit a request to ClimateServ directly from R via the chirps R package. Or, if you’re more comfortable using a graphic user interface, you can simply navigate to the ClimateServ homepage in your browser:\n\n\n\nThe ClimateServ website allows us to simply select the country boundaries for Burkina Faso. We’ll select “download raw data” to download daily rainfall totals for a roughly 30 year period from June 1, 2017 (nutrition interviews were conducted between June and August).\n\n\n\nOnce your request has been processed, you’ll receive a compressed folder containing one “tif” image for each day in the 30 year timespan: that’s 10,959 files containing comprising potentially several gigabytes of raster data. We’ve previously shown how to use the raster package to handle this type of data, but we’ll now introduce the terra package as a newer, faster alternative.\nRaster data with terra\nIf you’ve installed all of the dependencies needed for sf above, you’ll be able to install terra with install.packages(\"terra\") and then load it into R.\n\n\nlibrary(terra)\n\n\n\nThe magic behind terra is that it avoids reading every image into R at once. Instead, it reads metadata about each image - information about its spatial extent, coordinate reference system, and pixel count. Let’s take a look at the metadata for the image associated with a particularly rainy day in 1997:\n\n\nday1 <- rast(\"data/chirps_bf/19970301.tif\")\nday1\n\n\nclass       : SpatRaster \ndimensions  : 115, 159, 1  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -5.549997, 2.400003, 9.349999, 15.1  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : 19970301.tif \nname        : 19970301 \nmin value   :        0 \nmax value   : 8.319133 \n\n\n\n\n\n© (GPLv3)\nAs you can see, this image contains 115 rows and 159 columns of pixels. The value in each pixel represents the rainfall - in millimeters - for an area 0.05 degrees longitude by 0.05 degrees latitutde (shown in the “resolution” field). If you just want to preview the values associated with each pixel, we recommend coercing the default matrix as a tibble:\n\n\nday1_vals <- values(day1, dataframe = TRUE) %>% tibble()\nday1_vals\n\n\n# A tibble: 18,285 × 1\n   X19970301\n       <dbl>\n 1         0\n 2         0\n 3         0\n 4         0\n 5         0\n 6         0\n 7         0\n 8         0\n 9         0\n10         0\n# … with 18,275 more rows\n\nThe result is a single column with precipitation totals for 18,285 pixels. This is the CHIRPS data we’ve obtained for a single day in our 30-year timespan. Alone, it’s not a lot of data:\n\n\nobject.size(day1_vals) %>% format(\"Mb\")\n\n\n[1] \"0.1 Mb\"\n\nIf you’d like, you can plot the rainfall pixels from March 1, 1997 with the same ggspatial tools shown above. We’ll also ensure that all pixels representing “0” rainfall are transparent (revealing the underlying coordinate grid).\n\n\nggplot() +\n  layer_spatial(\n    day1,\n    alpha = if_else(values(day1) == 0, 0, 1) # makes 0 values transparent\n  ) +\n  layer_spatial(shape, alpha = 0) +\n  layer_spatial(gps,  alpha = 0) +\n  theme_pma_rainfall(\n    title = \"Burkina Faso Rainfall Totals: March 1, 1997\",\n    subtitle = paste(\n      \"National precipitation with displaced centroid locations for 2017 PMA\",\n      \"Nutrition Survey enumeration areas\"\n    )\n  ) +    \n  scale_fill_gradient2(\n    low = \"#000000\",   # white\n    high = \"#00263A\"  # IPUMS navy\n  )\n\n\n\n\nRaster layers\nNow, imagine working with data for each of the 10,959 days in our data series. Whereas the total size of the dataset from a single day was small - only about 0.1 megabytes - the amount of data required to represent 18,285 pixels from daily collection over a 30-year period could easily overwhelm the amount of memory available to R. Instead, we’ll only read metadata for each image into a large list with purrr::map:\n\n\nchirps <- list.files(\"data/chirps\", full.names = TRUE) %>%\n  map(~rast(.x))\n\n\n\nWe can now “stack” all of the list-items in chirps as “layers” in a single metadata object:\n\n\nchirps <- rast(chirps)\nchirps\n\n\nclass       : SpatRaster \ndimensions  : 115, 159, 14031  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -5.549997, 2.400003, 9.349999, 15.1  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsources     : 19810101.tif  \n              19810102.tif  \n              19810103.tif  \n              ... and 14028 more source(s)\nnames       : 19810101, 19810102, 19810103, 19810104, 19810105, 19810106, ... \n\nNotice that the amount of data actually loaded into R is only about one kilobyte:\n\n\nobject.size(chirps) %>% format(\"Kb\")\n\n\n[1] \"1.3 Kb\"\n\nThe terra package contains several summary methods you can use to quickly explore summary statistics over all of the layers included in our 30-year timespan. For example, terra::mean computes the mean value for each pixel across every layer of chirps. This can be particularly helpful if you want to create a map showing the 30-year average rainfall for selected enumeration areas relative to neighboring areas:\n\n\nbf_means <- terra::mean(chirps) \n\n\n\nWhen we build a map for bf_means we’ll use mask simply to hide any pixels outside the boundaries of our shape file:\n\n\nggplot() +\n  layer_spatial(mask(bf_means, vect(shape), touches = FALSE)) + \n  layer_spatial(shape, alpha = 0) +\n  layer_spatial(gps,  alpha = 0) +\n  theme_pma_rainfall(\n    title = \"Burkina Faso 30-year Average Daily Rainfall\",\n    subtitle = paste(\n      \"National precipitation with displaced centroid locations for 2017 PMA\",\n      \"Nutrition Survey enumeration areas\"\n    )\n  ) +\n  scale_fill_continuous(\n    low = \"#FAEFD1BB\", high = \"#00263ABB\", na.value = \"transparent\"\n  ) + \n  labs(caption = paste(\n    sep = \"\\n\",\n    \"Climate Hazards Center InfraRed Precipitation with Station data (CHIRPS)\",\n    \"06-01-1987 to 06-01-2017\"\n  ))\n\n\n\n\nFor most analytic purposes, you’ll want to do more than simply layer summary data beneath a map of the PMA enumeration areas. Instead, we’ll want to extract the pixels associated with the area covered by the buffer zones we created above.\nRainfall within Enumeration Areas\nLet’s start by zooming-in one of the enumeration areas listed in our gps dataset. This rural EA is located near the eastern border in the Boucle du Mouhoun region, so you’ll notice that its 5 kilometer buffer zone is cropped by an administrative boundary.\n\n\nggplot() + \n  layer_spatial(gps %>% filter(EAID == 7003), alpha = 0) +\n  theme_pma_rainfall(\n    \"Enumeration Area 7003\", \n    \"A rural sample cluster in the Boucle du Mouhoun region\",\n    show_legend = FALSE,\n    manual_grid = TRUE\n  )   \n\n\n\n\nIn our single-day map above, we plotted national rainfall totals for March 1, 1997. We’ll plot rainfall from this date again, but this time we’ll use the crop function to focus only on pixels in the immediate vicinity of EAID == 7003.\n\n\nday1_7003 <- crop(\n  day1, \n  gps %>% filter(EAID == 7003), \n  snap = \"out\"\n)\n\nggplot() + \n  layer_spatial(day1_7003) + \n  layer_spatial(gps %>% filter(EAID == 7003), alpha = 0) + \n  theme_pma_rainfall(\n    title = \"Rural EA Rainfall Total: March 1, 1997\", \n    subtitle = \"One daily total is reported for every 0.05 arc-degrees\",\n    show_legend = FALSE, \n    manual_grid = TRUE\n  ) \n\n\n\n\nInterestingly, we see a range of rainfall totals across the 9 pixels in this area: trace amounts of rain were detected in the 6 western-most pixels, but not in the 3 pixels on the eastern side. How should we summarise the rainfall experienced by a household in this enumeration area, given that its centroid might be located any one of several different pixels?\nYou might consider taking the mean daily total for all 9 pixels, but 2 pixels (top-right and bottom-right) are not included in the buffer zone at all. Most of the remaining pixels only partially overlap with the buffer zone - you might reasonably conclude the centroid is more likely to fall within the bottom 2 rows of pixels than in the top row.\nFortunately, the terra function extract gives both the rainfall total and the proportion of each pixel overlapping with buffer zone if we specify weights = TRUE. We’ll need to use vect to coerce the gps object into the SpatVector class used by most terra functions.\n\n\nday1_7003 <- extract(\n  day1_7003, \n  gps %>% filter(EAID == 7003) %>% vect(), \n  weights = TRUE\n)\n\nday1_7003 \n\n\n  ID  19970301 weight\n1  1 1.2160333   0.01\n2  1 0.8472163   0.13\n3  1 1.6922826   0.49\n4  1 1.4724811   1.00\n5  1 0.0000000   0.22\n6  1 1.3903536   0.26\n7  1 1.3996017   0.64\n\nThe result is a data.frame containing one row for each pixel that overlaps with our buffer (the top-right and bottom-right pixels are omitted). The column 19970301 gives the rainfall for each pixel, and the column weight shows the proportion of each pixel that falls within the buffer. We can think of these weight values as probabilities representing the likelihood that each pixel contains the real centroid location for EA == 7003. Let’s calculate a weighted.mean using the probabilities represented by weight:\n\n\nday1_7003 %>% \n  summarise(\n    EAID = 7003,\n    wtd_mean = weighted.mean(`19970301`, weight)\n  )\n\n\n  EAID wtd_mean\n1 7003 1.338631\n\nThis represents the approximate single-day rainfall total for March 1, 1997 spatially averaged for all pixels overlapping with the buffer for EA == 7003. In order to produce the 30-year average daily rainfall for this buffer, we’ll simply repeat the same process for each day between June 1, 1987 and June 1, 2017 (the entire stack represented by chirps). We’ll arrange the results in a tibble, and we’ll replace the default ID column with the unique identifier for each pixel shown above.\n\n\nchirps30_7003 <- extract(\n  chirps,\n  gps %>% filter(EAID == 7003) %>% vect(),\n  weights = TRUE\n)\n\nchirps30_7003 <- chirps30_7003 %>%\n  tibble() %>%\n  dplyr::select(-ID) %>%\n  rowid_to_column(\"pixel\") %>%\n  relocate(weight, .after = pixel)\n\nchirps30_7003\n\n\n\n\n# A tibble: 7 × 10,961\n  pixel weight `19870601` `19870602` `19870603` `19870604` `19870605`\n  <int>  <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>\n1     1   0.01       8.81          0       2.94       5.87          0\n2     2   0.13       8.43          0       2.81       5.62          0\n3     3   0.49       9.46          0       3.15       6.31          0\n4     4   1.00       9.10          0       3.03       6.07          0\n5     5   0.22       9.23          0       3.08       6.16          0\n6     6   0.26       9.99          0       3.33       6.66          0\n7     7   0.64       9.45          0       3.15       6.30          0\n# … with 10,954 more variables: 19870606 <dbl>, 19870607 <dbl>,\n#   19870608 <dbl>, 19870609 <dbl>, 19870610 <dbl>, 19870611 <dbl>,\n#   19870612 <dbl>, 19870613 <dbl>, 19870614 <dbl>, 19870615 <dbl>,\n#   19870616 <dbl>, 19870617 <dbl>, 19870618 <dbl>, 19870619 <dbl>,\n#   19870620 <dbl>, 19870621 <dbl>, 19870622 <dbl>, 19870623 <dbl>,\n#   19870624 <dbl>, 19870625 <dbl>, 19870626 <dbl>, 19870627 <dbl>,\n#   19870628 <dbl>, 19870629 <dbl>, 19870630 <dbl>, 19870701 <dbl>, …\n\nThe rainfall total for each day is stored in one of 10,959 columns to the right of pixel and weight. We’ll use pivot_longer to rearrange these data in rows, and then we’ll calculate a simple weighted.mean to obtain the 30-year average daily rainfall.\n\n\nchirps30_7003 <- chirps30_7003 %>% pivot_longer(-c(pixel, weight)) \nchirps30_7003   \n\n\n# A tibble: 76,713 × 4\n   pixel weight name     value\n   <int>  <dbl> <chr>    <dbl>\n 1     1   0.01 19870601  8.81\n 2     1   0.01 19870602  0   \n 3     1   0.01 19870603  2.94\n 4     1   0.01 19870604  5.87\n 5     1   0.01 19870605  0   \n 6     1   0.01 19870606 30.6 \n 7     1   0.01 19870607  3.39\n 8     1   0.01 19870608  0   \n 9     1   0.01 19870609  0   \n10     1   0.01 19870610  0   \n# … with 76,703 more rows\n\nchirps30_7003 %>% \n  summarise(\n    EAID = 7003,\n    MEAN_20YR = weighted.mean(value, weight)\n  )\n\n\n# A tibble: 1 × 2\n   EAID MEAN_20YR\n  <dbl>     <dbl>\n1  7003      1.50\n\nNow that we’ve developed a strategy for summarising all of the rainfall pixels for a single enumeration area, we’ll see that it’s easy to generalize our approach to all of the enumeration areas associated with the 2017 Burkina Faso sample. Although we’ll save a great deal of time by focusing only on the CHIRPS pixels that fall within each of our 83 buffer zones, the extract procedure below will still require a few minutes of patience:\n\n\nchirps30_all <- chirps %>%\n  extract(gps %>% vect(), weights = TRUE) %>%\n  as.data.frame() %>%\n  tibble()\nchirps30_all\n\n\n\n\n# A tibble: 416 × 10,961\n      ID `19870601` `19870602` `19870603` `19870604` `19870605`\n   <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>\n 1     1          0       5.91       5.91       11.8       5.91\n 2     1          0       5.76       5.76       11.5       5.76\n 3     1          0       5.73       5.73       11.5       5.73\n 4     1          0       6.56       6.56       13.1       0   \n 5     1          0       6.14       6.14       12.3       6.14\n 6     1          0       6.15       6.15       12.3       6.15\n 7     1          0       6.80       0          13.6       0   \n 8     1          0       6.83       0          13.7       0   \n 9     2          0       5.95       0          17.9       5.95\n10     2          0       6.07       0          18.2       6.07\n# … with 406 more rows, and 10,955 more variables: 19870606 <dbl>,\n#   19870607 <dbl>, 19870608 <dbl>, 19870609 <dbl>, 19870610 <dbl>,\n#   19870611 <dbl>, 19870612 <dbl>, 19870613 <dbl>, 19870614 <dbl>,\n#   19870615 <dbl>, 19870616 <dbl>, 19870617 <dbl>, 19870618 <dbl>,\n#   19870619 <dbl>, 19870620 <dbl>, 19870621 <dbl>, 19870622 <dbl>,\n#   19870623 <dbl>, 19870624 <dbl>, 19870625 <dbl>, 19870626 <dbl>,\n#   19870627 <dbl>, 19870628 <dbl>, 19870629 <dbl>, 19870630 <dbl>, …\n\nThe only difference here is that ID represents an index number for each of the 83 enumeration areas, and each enumeration area may include anywhere between 1 and 9 pixels. We haven’t bothered assigning numbers to each pixel, but we’ll need to find the correct EAID for each ID.\n\n\nchirps30_all <- gps %>% \n  rowid_to_column(\"ID\") %>% \n  tibble() %>% \n  dplyr::select(ID, EAID) %>% \n  full_join(chirps30_all, by = \"ID\")\nchirps30_all\n\n\n# A tibble: 416 × 10,962\n      ID  EAID `19870601` `19870602` `19870603` `19870604` `19870605`\n   <dbl> <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>\n 1     1  7820          0       5.91       5.91       11.8       5.91\n 2     1  7820          0       5.76       5.76       11.5       5.76\n 3     1  7820          0       5.73       5.73       11.5       5.73\n 4     1  7820          0       6.56       6.56       13.1       0   \n 5     1  7820          0       6.14       6.14       12.3       6.14\n 6     1  7820          0       6.15       6.15       12.3       6.15\n 7     1  7820          0       6.80       0          13.6       0   \n 8     1  7820          0       6.83       0          13.7       0   \n 9     2  7139          0       5.95       0          17.9       5.95\n10     2  7139          0       6.07       0          18.2       6.07\n# … with 406 more rows, and 10,955 more variables: 19870606 <dbl>,\n#   19870607 <dbl>, 19870608 <dbl>, 19870609 <dbl>, 19870610 <dbl>,\n#   19870611 <dbl>, 19870612 <dbl>, 19870613 <dbl>, 19870614 <dbl>,\n#   19870615 <dbl>, 19870616 <dbl>, 19870617 <dbl>, 19870618 <dbl>,\n#   19870619 <dbl>, 19870620 <dbl>, 19870621 <dbl>, 19870622 <dbl>,\n#   19870623 <dbl>, 19870624 <dbl>, 19870625 <dbl>, 19870626 <dbl>,\n#   19870627 <dbl>, 19870628 <dbl>, 19870629 <dbl>, 19870630 <dbl>, …\n\nFinally, we’ll pivot_longer and calculate a separate weighted.mean for each EAID. The result can be merged directly to gps if we’d like to create a map:\n\n\ngps <- chirps30_all %>% \n  pivot_longer(-c(ID, EAID, weight)) %>% \n  group_by(EAID) %>% \n  summarise(CHIRPS_30 = weighted.mean(value, weight)) %>% \n  full_join(gps, by = \"EAID\")\n\nggplot() + \n  layer_spatial(shape, alpha = 0) + \n  layer_spatial(gps, aes(fill = CHIRPS_30, stroke = 0)) + \n  theme_pma_rainfall(\n    title = \"Burkina Faso 30-year Average Daily Rainfall\", \n    subtitle = \"PMA 2017 Nutrition Survey enumeration area centroid locations\"\n  ) + \n  scale_fill_steps(low = \"#FAEFD1\", high = \"#00263A\") +\n  labs(caption = paste(sep = \"\\n\",\n    \"Climate Hazards Center InfraRed Precipitation with Station data (CHIRPS)\",\n    \"06-01-1987 to 06-01-2017\"\n  ))\n\n\n\n\nWe can also attach these local rainfall totals to the records for individual women and children in the 2017 Burkina Faso nutrition dataset dat. At this point, the simple features collection class is no longer necessary; we can simply join CHIRPS_30 directly to dat.\n\n\ngps %>% \n  select(EAID, CHIRPS_30) %>% \n  full_join(dat, by = \"EAID\") \n\n\n# A tibble: 8,305 × 978\n    EAID CHIRPS_30         SAMPLE COUNTRY  YEAR ROUND HHID   PERSONID \n   <dbl>     <dbl>      <int+lbl> <int+l> <int> <dbl> <chr>  <chr>    \n 1  7003      1.50 85406 [Burkin… 1 [Bur…  2017     1 07003… 07003000…\n 2  7003      1.50 85406 [Burkin… 1 [Bur…  2017     1 07003… 07003000…\n 3  7003      1.50 85406 [Burkin… 1 [Bur…  2017     1 07003… 07003000…\n 4  7003      1.50 85406 [Burkin… 1 [Bur…  2017     1 07003… 07003000…\n 5  7003      1.50 85406 [Burkin… 1 [Bur…  2017     1 07003… 07003000…\n 6  7003      1.50 85406 [Burkin… 1 [Bur…  2017     1 07003… 07003000…\n 7  7003      1.50 85406 [Burkin… 1 [Bur…  2017     1 07003… 07003000…\n 8  7003      1.50 85406 [Burkin… 1 [Bur…  2017     1 07003… 07003000…\n 9  7003      1.50 85406 [Burkin… 1 [Bur…  2017     1 07003… 07003000…\n10  7003      1.50 85406 [Burkin… 1 [Bur…  2017     1 07003… 07003000…\n# … with 8,295 more rows, and 970 more variables:\n#   RESPONDENT <int+lbl>, LINENO <int>, ENUMID <dbl+lbl>,\n#   CONSENTHQ <int+lbl>, AVAILABLEHQ <int+lbl>, VISITNUMHQ <int+lbl>,\n#   PREVINTERVIEWHQ <int+lbl>, RESULTHQ <int+lbl>,\n#   ELIGIBLEHH <int+lbl>, RESULTFCQ <int+lbl>, HHIDORIG <chr>,\n#   ELIGTYPE <int+lbl>, ELIGIBLEFC <int+lbl>, ELIGIBLESEL <int+lbl>,\n#   ELIGIBLEKID <int+lbl>, ELIGIBLEHHKID <int+lbl>, …\n\nIn our next post, we’ll learn much more about the relationship between CHIRPS precipitation data and nutritional outcomes for the women and young children in this survey. Stay tuned!\n\n\n\n",
    "preview": "posts/2021-10-15-nutrition-climate/nutrition-climate_files/figure-html5/unnamed-chunk-32-1.png",
    "last_modified": "2021-11-05T15:16:07-05:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 1536
  },
  {
    "path": "posts/2021-10-01-nutrition-indicators/",
    "title": "Infant and Young Child Feeding (IYCF) Indicators for PMA Nutrition Data",
    "description": "Use WHO guidelines to calculate Minimum Dietary Diversity (MDD), Minimum Meal Frequency (MMF), and Minimum Acceptable Diet (MAD).",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-10-01",
    "categories": [
      "Nutrition",
      "IYCF",
      "Indicators",
      "Data Manipulation"
    ],
    "contents": "\n\nContents\nSetup\nMinimum Age with Century Month Codes\nMDD: Minimum Dietary Diversity\nMMF: Minimum Meal Frequency\nMAD: Minimum Acceptable Diet\nNext Steps\n\nGlobal indicators for Infant and Young Child Feeding (IYCF) practices have been used in large-scale nutrition surveys like the Demographic and Health Surveys (DHS) and Multiple Indicator Cluster Surveys (MICS) since 2008. These indicators provide benchmark dietary standards necessary for healthy growth and development in children during the first two years of life.\nIn 2017 and 2018 - the same years that PMA nutrition surveys were fielded in Burkina Faso and Kenya - the World Health Organization (WHO), UNICEF, and other international aid organizations convened an inter-agency review of IYCF guidelines. Their findings included recommendations for revised measures outlined here.\nIn this post, we’ll highlight three of the most commonly used IYCF indicators for children age 6-23 months, and we’ll show to calculate them with dietary recall data from the 2017 PMA nutrition survey from Burkina Faso. We’ll use the revised WHO definitions for:\nMDD: Minimum Dietary Diversity\nMMF: Minimum Meal Frequency\nMAD: Minimum Acceptable Diet\nSetup\nWhile you’ll be able to use the code in this post for any of the four PMA nutrition samples, we’ll focus on a data extract containing only the Burkina Faso 2017 sample. We’ve selected cases marked “Females and Children with Nutrition Information,” dropping all other household members from the file.\nWe’ll load the data extract into R with the packages ipumsr and tidyverse.\n\n\nlibrary(ipumsr)\nlibrary(tidyverse)\n\ndat <- read_ipums_micro(\n  ddi = \"data/pma_00002.xml\",\n  data = \"data/pma_00002.dat.gz\"\n)\n\n\n\nFor all graphs in this post, we’ll use a custom ggplot2 theme we’ll call theme_pma. Check out this post for more information about how to develop and use a graphics theme like this one:\n\n\nsysfonts::font_add(\n  family = \"cabrito\", \n  regular = \"../../fonts/cabritosansnormregular-webfont.ttf\"\n)\nshowtext::showtext_auto()\n\ntheme_pma <- function(title, subtitle){\n  components <- list(\n    theme_minimal() %+replace% \n      theme(\n        text = element_text(family = \"cabrito\", size = 10), \n        plot.title = element_text(\n          size = 18, color = \"#00263A\", margin = margin(b = 10)\n        ), \n        plot.subtitle = element_text(size = 12, margin = margin(b = 10)),\n        legend.title = element_blank(),        \n        panel.grid.minor = element_blank(),     \n        axis.ticks = element_blank(),          \n        legend.position = \"bottom\",\n        panel.spacing = unit(4, \"lines\"), \n        strip.text = element_blank()\n      ),\n    scale_fill_manual(\n      values = alpha(\n        alpha = .85,\n        colour = c(\n          \"#98579B\",   # PMA Pink\n          \"#00263A\",  # IPUMS Navy\n          \"#81A88D\"   # Green\n        )\n      )\n    ),\n    labs(\n      title = toupper(title),\n      subtitle = subtitle,\n      x = NULL,\n      y = NULL,\n      fill = NULL\n    ),\n    ylim(0, 100)\n  )\n}\n\n\n\nMinimum Age with Century Month Codes\nThe IYCF indicators we’ll calculate below only apply to infants who were between 6 and 23 months of age on the day of the nutrition interview. We know the birth month KIDBIRTHMO and year KIDBIRTHYR for each sampled child, but the interview month INTFCQMON and year INTFCQYEAR are only reported on the record for the woman who responded on their behalf (usually the child’s mother). In order to calculate each child’s age, we’ll need to use the procedure outlined in our last post to identify the mother for each child living in a household where nutrition surveys were administered in more than one month.\nTo make this calculation easier, we’ll combine each month and year into a single century month code (CMC). Dates reported as a CMC reflect the number of months since January 1900:\n\\[CMC = 12*(Year – 1900) + Month\\]\nYou’ll find CMC dates in many demographic surveys because they make it easy to measure the minimum passage of time between two events (particularly when the exact day is unknown). In our case, we’ll subtract the CMC code for each child’s birthdate KIDBIRTHCMC from the CMC code for the nutrition interview INTFCQCMC; we’ll then subtract one additional month to account for births that happened nearer to the end of the birth month than the date of the interview was to the end of the interview month.\nFor example, imagine a child born on January 1, 2017 and whose nutrition interview happened on February 1, 2017. The CMC code for their birthdate would be calculated as:\n\\[\\begin{eqnarray} \nKIDBIRTHCMC &=& 12*(2017 – 1900) + 1 \\\\ \n&=& 1405\n\\end{eqnarray}\\]\nAnd the CMC code for their interview date would be:\n\\[\\begin{eqnarray} \nINTFCQCMC &=& 12*(2017 – 1900) + 2 \\\\\n&=& 1406\n\\end{eqnarray}\\]\nSuppose we calculate each child’s age in months KIDAGEMO by subtracting KIDBIRTHCMC from INTFCQCMC:\n\\[\\begin{eqnarray} \nKIDAGEMO &=& INTFCQCMC - KIDBIRTHCMC \\\\\n&=& 1406 - 1405 \\\\ \n&=& 1\n\\end{eqnarray}\\]\nThis particular child aged one complete month between January 1 and February 1, so our calculation for KIDAGEMO works out. Unfortunately, this creates incorrect calculations for all of the children born after January 1, since they won’t have aged one complete month by February 1. In practice, we don’t know the precise day of either event; this is why we subtract an additional month in our calculation for KIDAGEMO, which reports each child’s minimum age in months.\n\\[\\begin{eqnarray} \nKIDAGEMO &=& INTFCQCMC - KIDBIRTHCMC - 1\\\\\n&=& 1406 - 1405 -1 \\\\ \n&=& 0\n\\end{eqnarray}\\]\nNote that all of the date variables contain special top-codes designating missing values; we’ll use dplyr::case_when to calculate CMC codes only for non-missing dates (all other cases will be marked NA automatically).\n\n\ndat <- dat %>% \n  mutate(\n    KIDBIRTHCMC = case_when(\n      KIDBIRTHMO <= 12 & KIDBIRTHYR <= 2017 ~ \n        12*(KIDBIRTHYR - 1900) + KIDBIRTHMO\n    ),\n    INTFCQCMC = case_when(\n      INTFCQMON <= 12 & INTFCQYEAR <= 2017 ~ \n        12*(INTFCQYEAR - 1900) + INTFCQMON\n    )\n  )\n\n\n\nAs we mentioned, the interview date is only located on the record for the woman in each household who provided nutrition information for herself and on behalf of each sampled child. As a result, INTFCQCMC is currently NA for each child.\n\n\ndat %>% count(ELIGTYPE, INTFCQCMC)\n\n\n# A tibble: 7 × 3\n                             ELIGTYPE INTFCQCMC     n\n                            <int+lbl>     <dbl> <int>\n1 11 [Infant under age 2 (INF)]              NA  2436\n2 12 [Youngest aged 2-5 (YK)]                NA   343\n3 13 [Older aged 2-5 (OTK)]                  NA  1032\n4 20 [Selected women aged 10-49 (WN)]      1410  3008\n5 20 [Selected women aged 10-49 (WN)]      1411  1231\n6 20 [Selected women aged 10-49 (WN)]      1412   253\n7 20 [Selected women aged 10-49 (WN)]      1413     2\n\nBefore we can calculate the age of each child KIDAGEMO, we need to match each child to the correct woman who provided responses on their behalf (usually their mother). Following the steps outlined in our last post, we’ll group the data by household HHID and then perform a rowwise search for each person in the sample. If that person is a woman aged 10-49 (ELIGTYPE == 20), we’ll leave INTFCQCMC as-is; otherwise, we’ll use these criteria to determine INTFCQCMC:\nIf all nutrition questionnaires administered in the household happened in the same month (or if only one nutrition questionnaire was administered), report the only INTFCQCMC available.\nIf women in the household received the nutrition questionnaire in different months, locate the woman in the household whose AGE and education level EDUCATT match the available information about each child’s mother in AGEMOM and EDUCATTMOM. Report the value in INTFCQCMC for the woman matched as the child’s mother.\nIf multiple women in the child’s household share the same AGE and education level EDUCATT, return NA. In this case, the interview date for the child cannot be determined.\n\n\ndat <- dat %>%  \n  select(\n    HHID, \n    PERSONID,\n    ELIGTYPE, \n    AGE, \n    AGEMOM,\n    EDUCATT, \n    EDUCATTMOM, \n    INTFCQCMC\n  ) %>% \n  group_by(HHID) %>% \n  mutate(HH_DATA = list(cur_data())) %>%\n  ungroup()  %>% \n  rowwise() %>% \n  mutate(INTFCQCMC = ifelse(ELIGTYPE == 20, INTFCQCMC, {\n    unique_dates <- HH_DATA %>% \n      filter(!is.na(INTFCQCMC)) %>% \n      distinct(INTFCQCMC) %>% \n      pull(INTFCQCMC)\n    ifelse(length(unique_dates) == 1, unique_dates, {\n      agemom <- AGEMOM\n      educattmom <- EDUCATTMOM\n      moms <- HH_DATA %>%\n        filter(AGE == agemom, EDUCATT == educattmom) %>%\n        pull(INTFCQCMC)\n      ifelse(length(moms) == 1, moms, NA)\n    })\n  })) %>% \n  ungroup() %>% \n  select(PERSONID, INTFCQCMC) %>% \n  right_join(dat %>% select(!INTFCQCMC), by = \"PERSONID\")\n\n\n\nLet’s now check to see whether an interview date INTFCQCMC was determined for every child:\n\n\ndat %>% count(ELIGTYPE, INTFCQCMC)\n\n\n# A tibble: 14 × 3\n                              ELIGTYPE INTFCQCMC     n\n                             <int+lbl>     <dbl> <int>\n 1 11 [Infant under age 2 (INF)]            1410  1663\n 2 11 [Infant under age 2 (INF)]            1411   648\n 3 11 [Infant under age 2 (INF)]            1412   124\n 4 11 [Infant under age 2 (INF)]            1413     1\n 5 12 [Youngest aged 2-5 (YK)]              1410   236\n 6 12 [Youngest aged 2-5 (YK)]              1411    99\n 7 12 [Youngest aged 2-5 (YK)]              1412     8\n 8 13 [Older aged 2-5 (OTK)]                1410   746\n 9 13 [Older aged 2-5 (OTK)]                1411   231\n10 13 [Older aged 2-5 (OTK)]                1412    55\n11 20 [Selected women aged 10-49 (WN)]      1410  3008\n12 20 [Selected women aged 10-49 (WN)]      1411  1231\n13 20 [Selected women aged 10-49 (WN)]      1412   253\n14 20 [Selected women aged 10-49 (WN)]      1413     2\n\nSuccess! Now, we can calculate the minimum age of every sampled child as KIDAGEMO.\n\n\ndat <- dat %>% mutate(KIDAGEMO = INTFCQCMC - KIDBIRTHCMC - 1)\n\n\n\nFor the remainder of the post, we’ll focus on children who were at least 6 months of age defined by KIDAGEMO. Unfortunately, because precise ages are suppressed in the public IPUMS PMA dataset, we’ve rounded down and likely excluded some children who are, in reality, exactly 6 months of age. On the other hand, we’ll keep all children where INFCHECK indicates that the respondent confirmed that the child was under 24 months of age (regardless of their recorded birthdate).\n\nThe official PMA summary report, which uses precise ages, shows a sample of 1732 children aged 6-23 months. Our approximation yields a smaller sample with 1706 children aged 6-23 months, a difference of 26 children.\n\n\ndat <- dat %>% filter(KIDAGEMO >= 6 & INFCHECK == 1)\n\n\n\nFor convenience, we’ll also create a factor KIDAGEMO_3 that aggregates children into three groups:\n\n\ndat <- dat %>% \n  mutate(KIDAGEMO_3 = case_when(\n    KIDAGEMO < 12 ~ \"6-11 months\",\n    KIDAGEMO < 18 ~ \"12-17 months\",\n    KIDAGEMO >= 18 ~ \"18-23 months\"\n  ) %>% \n    factor(levels = c(\"6-11 months\", \"12-17 months\", \"18-23 months\"))\n  ) \n\n\n\n\n\n\nMDD: Minimum Dietary Diversity\nWHO guidelines define Minimum Dietary Diversity (MDD) for children aged 6-23 months as consumption of food or beverages from at least 5 out of 8 defined food groups during the previous day. The PMA nutrition questionnaire for infants includes the following recall questions designed to meet the criteria for calculating MDD.\nAre you currently breastfeeding ${under_2yr_child_name}?\n\n[] Yes\n[] No\n\n\nNow I will ask you about liquids that ${under_2yr_child_name} had \nyesterday during the day or at night. I am interested in whether your child had \nthe item I mention even if it was combined with other foods.\n\nProceed to the next screen when ready.\n\nDid ${under_2yr_child_name} eat or drink:\n\n[] Plain water\n[] Juice or juice base\n[] Soup\n[] Milk such as canned milk, powder or fresh animal milk\n[] Primary milk / infant formula or marketed infant formula (Breastmilk \n   substitute: NAN, Nativa, Guigoz, etc.)\n[] Sweet drinks (sodas, zom-koom, bissap, ginger juice)\n[] Other liquids\n[] Yogurt\n[] A commercially prepared baby formula, such as Cerelac, vitacasui, vitaline\n[] Porridge\n\n\nNow I would like to ask you about foods that ${under_2yr_child_name} had \nyesterday during the day or at night. I am interested in whether your child had \nthe item I mention even if it was combined with other foods.\n\nProceed to the next screen when ready.\n\nDid ${under_2yr_child_name} eat or drink:\n\n[] Any fortified food like Cerelac?\n[] Maize, rice, wheat, thick porridge, sorghum, bread, or other foods made from \n   grains?\n[] Pumpkin, carrots, squash or yellow sweet potatoes that are yellow or orange \n   inside\n[] Irish potatoes, yams, cassava, white sweet potatoes, or any other foods made \n   from roots?\n[] Sukumu wiki or any dark green, leafy vegetables?\n[] Ripe mangoes, pawpaw and fruits that are orange or yellow inside?\n[] Any other fruits or vegetables?\n[] Liver, kidney, heart or other organ meats?\n[] Any meat, such as beef, pork, lamb, goat, chicken, duck?\n[] Eggs?\n[] Fresh or dried fish or shellfish?\n[] Any foods made from beans, peas, lentils, or nuts?\n[] Cheese or other food made from milk?\n[] Sugary foods, jiggery (sukari nguru), mandaazi, donuts, cake, sweet biscuits \n   or candies?\n[] Savory snacks like fried chips, crisps, samosas, or other fried foods?\n[] Any other solid, semi-solid or soft food?\nIPUMS PMA provides harmonized indicators for each of the response options shown. We’ll group the relevant options into each of the 8 food groups defined in WHO guidelines for MDD.\nBreast Milk\nINFBFNOW\n\nGrains, Roots, or Tubers\nINFYESTFORM\nINFYESTPORR\nINFYESTGRAIN\nINFYESTWHTVEG\nINFYESTFORT\n\nLegumes or Nuts\nINFYESTBEAN\n\nDairy\nINFYESTMILK\nINFYESTYOG\nINFYESTFORMP\nINFYESTDAIRY\n\nFlesh Foods\nINFYESTFISH\nINFYESTMEAT\nINFYESTORG\n\nEggs\nINFYESTEGG\n\nVitamin A-rich Fruits and Vegetables\nINFYESTYLWVEG\nINFYESTGRNVEG\nINFYESTYLWFRT\n\nOther Fruits and Vegetables\nINFYESTOTHFRTVEG\n\nFor each variable, the response code 1 indicates that the infant consumed the food yesterday (in INFBFNOW, it indicates that the child is currently being breastfed, so we assume that they were breastfed yesterday); we’ll label these responses TRUE. Because we removed all non-infant cases from our extract, we can safely collapse all of the remaining responses together as FALSE (the only remaining cases that are “NIU (not in universe)” are infants in INFBFNOW who were never breastfed).\nWe’ll create one “group” variable with the prefix GRP_ for each food group. In cases where more than one harmonized variable is used, we’ll use dplyr::if_any to indicate whether any of the constituent foods was eaten yesterday.\n\n\ndat <- dat %>% \n  mutate(\n    GRP_BF = INFBFNOW == 1,\n    GRP_NUT = INFYESTBEAN == 1,\n    GRP_EGG = INFYESTEGG == 1,\n    GRP_OTH = INFYESTOTHFRTVEG == 1,\n    GRP_GRAIN = if_any(\n      c(INFYESTFORM, INFYESTPORR, INFYESTGRAIN, INFYESTWHTVEG, INFYESTFORT),\n      ~.x == 1\n    ),\n    GRP_DAIRY = if_any(\n      c(INFYESTMILK, INFYESTYOG, INFYESTFORMP, INFYESTDAIRY),\n      ~.x == 1\n    ),\n    GRP_FLESH = if_any(\n      c(INFYESTFISH, INFYESTMEAT, INFYESTORG),\n      ~.x == 1\n    ),\n    GRP_VITA = if_any(\n      c(INFYESTYLWVEG, INFYESTGRNVEG, INFYESTYLWFRT),\n      ~.x == 1\n    )\n  ) \n\n\n\nLet’s preview the proportion of infants in each age range from KIDAGEMO_3 who consumed foods from each of the 8 food groups yesterday. First, we’ll make a summary table with dplyr::summarise showing the proportion of children in each age range that consumed foods from each group:\n\n\ngrp_summary <- dat %>% \n  group_by(KIDAGEMO_3) %>% \n  summarise(across(starts_with(\"GRP\"), ~100*mean(.x))) \n\ngrp_summary\n\n\n# A tibble: 3 × 9\n  KIDAGEMO_3   GRP_BF GRP_NUT GRP_EGG GRP_OTH GRP_GRAIN GRP_DAIRY\n  <fct>         <dbl>   <dbl>   <dbl>   <dbl>     <dbl>     <dbl>\n1 6-11 months    94.6    6.95    4.90    5.69      83.6      20.2\n2 12-17 months   95.5    8.80   11.3     8.47      90.5      26.2\n3 18-23 months   70.1   14.0    11.0    11.3       94.7      24.6\n# … with 2 more variables: GRP_FLESH <dbl>, GRP_VITA <dbl>\n\nWe’ll use tidyr::pivot_longer to create a long version of the table containing labels for each food group written exactly as we want them to appear on a ggplot.\n\n\ngrp_summary <- grp_summary %>% \n  pivot_longer(\n    !KIDAGEMO_3,\n    names_pattern = \"GRP_(.*)\",\n    names_to = c(\"group\")\n  ) %>% \n  mutate(\n    group = group %>% \n      as_factor() %>% \n      fct_relevel(\"GRAIN\", \"NUT\", \"FLESH\", \"EGG\", \"VITA\", \"OTH\", \"DAIRY\") %>% \n      fct_recode(\n        `Grains, roots, \\n tubers` = \"GRAIN\",\n        `Legumes and nuts` = \"NUT\",\n        `Flesh foods \\n (meat, fish, poultry)` = \"FLESH\",\n        `Eggs` = \"EGG\",\n        `Vitamin A-rich \\n fruits / veg` = \"VITA\",\n        `Other \\n fruits / veg` = \"OTH\",\n        `Dairy` = \"DAIRY\",\n        `Breastmilk` = \"BF\"\n      )\n  )\n\ngrp_summary\n\n\n# A tibble: 24 × 3\n   KIDAGEMO_3   group                                  value\n   <fct>        <fct>                                  <dbl>\n 1 6-11 months  \"Breastmilk\"                           94.6 \n 2 6-11 months  \"Legumes and nuts\"                      6.95\n 3 6-11 months  \"Eggs\"                                  4.90\n 4 6-11 months  \"Other \\n fruits / veg\"                 5.69\n 5 6-11 months  \"Grains, roots, \\n tubers\"             83.6 \n 6 6-11 months  \"Dairy\"                                20.2 \n 7 6-11 months  \"Flesh foods \\n (meat, fish, poultry)\" 25.6 \n 8 6-11 months  \"Vitamin A-rich \\n fruits / veg\"       49.1 \n 9 12-17 months \"Breastmilk\"                           95.5 \n10 12-17 months \"Legumes and nuts\"                      8.80\n# … with 14 more rows\n\nNow, we’ll create a grouped bar chart using the custom theme_pma created above.\n\n\ngrp_summary %>% \n  ggplot(aes(fill = KIDAGEMO_3, x = group, y = value)) + \n  geom_bar(width = .6, position = position_dodge(0.6), stat = \"identity\") + \n  theme_pma(\n    title = \"Infant consumption of MDD food groups\",\n    subtitle = \"Consumption of foods from 5 of 8 groups is required for MDD\"\n  )\n\n\n\n\nFinally, we’ll calculate MDD. Because MDD combines indicators from multiple columns, we’ll use dplyr::c_across. If the total sum of values in all columns with the prefix GRP is at least 5, the child will meet the WHO criteria for “Minimum Dietary Diversity.”\n\n\ndat <- dat %>% \n  rowwise() %>% \n  mutate(MDD = sum(c_across(starts_with(\"GRP\"))) >= 5) %>% \n  ungroup() \n\n\n\nWe’ll add MDD as a facet to the plot we made above.\n\n\ndat %>% \n  group_by(KIDAGEMO_3) %>% \n  summarise(group = \"MDD\", value = 100*mean(MDD), aside = TRUE) %>% \n  bind_rows(grp_summary %>% mutate(aside = FALSE)) %>% \n  ggplot(aes(fill = KIDAGEMO_3, x = group, y = value)) + \n  geom_bar(width = 0.8, position = position_dodge(0.8), stat = \"identity\") + \n  facet_grid(\n    cols = vars(aside), \n    scales = \"free_x\", \n    space = \"free_x\",\n  ) + \n  theme_pma(\n    title = \"Minimum Dietary Diversity (MDD) in Infants 6-23 Months\",\n    subtitle = \"Consumption of foods from 5 of 8 groups is required for MDD\"\n  ) \n\n\n\n\nMMF: Minimum Meal Frequency\nIn order to achieve Minimum Meal Frequency (MMF), WHO guidelines recommend that an infant should receive solid, semi-solid, or soft foods multiple times during the previous day. The precise quantity of meals depends on the child’s age and whether they are breastfed:\nBreastfed infants aged 6-8 months should receive two feedings of solid, semi-solid or soft foods\nBreastfed infants aged 9-23 months should receive three feedings of of solid, semi-solid or soft foods\nNon-breastfed infants aged 6-23 months should receive four feedings of solid, semi-solid or soft foods or milk feeds, provided that at least one of the feedings includes solid, semi-solid or soft foods\nHere, a “feeding” includes both meals and snacks, and “milk feeds” include “any formula (e.g. infant formula, follow-on formula, “toddler milk”) or any animal milk other than human breast milk, (e.g. cow milk, goat milk, evaporated milk or reconstituted powdered milk) as well as semi-solid and fluid/drinkable yogurt and other fluid/drinkable fermented products made with animal milk\".1\nFollowing this definition, we’ll first calculate the combined total number of MILKFEEDS from INFMILKNUM, INFYOGNUM, and INFFORMNUM; then, we’ll combine MILKFEEDS with INFFOODNUM appropriately for each child to create MMF.\nFor each NUM variable, we should be careful to note the reason why cases may appear “NIU (not in universe)”: these are children whose mother answered in a previous question that they did not consume the food yesterday. We’ll re-code these NIU cases as 0, rather than treat them as “missing.” On the other hand, we’ll assign the value NA to any child whose mother “did not know” whether they received the food yesterday.\n\n\ndat <- dat %>% \n  mutate(\n    across(\n      c(INFMILKNUM, INFYOGNUM, INFFORMNUM, INFFOODNUM), \n      ~case_when(.x < 90 ~ as.integer(.x))\n    ),\n    INFMILKNUM = ifelse(INFYESTMILK == 0, 0, INFMILKNUM),\n    INFYOGNUM = ifelse(INFYESTYOG == 0, 0, INFYOGNUM),\n    INFFORMNUM = ifelse(INFYESTFORMP == 0, 0, INFFORMNUM), \n    INFFOODNUM = ifelse(INFFOODYEST == 0, 0, INFFOODNUM),\n  )  \n\n\n\nNow, we’ll calculate MILKFEEDS as a sum of the three constituent NUM variables for dairy products (unless all are NA, in which case we’ll return NA).\n\n\ndat <- dat %>% \n  rowwise() %>% \n  mutate(MILKFEEDS = case_when(\n    !if_all(c(INFMILKNUM, INFYOGNUM, INFFORMNUM), is.na) ~ \n      sum(c_across(c(INFMILKNUM, INFYOGNUM, INFFORMNUM)), na.rm = TRUE)\n  )) %>% \n  ungroup()\n\n\n\nFinally, we’ll use case_when to apply separate calculations of MMF depending on whether the child is under 9 months, and whether they are breastfeeding.\n\n\ndat <- dat %>% \n  mutate(MMF = case_when(\n    KIDAGEMO < 9 & GRP_BF ~ INFFOODNUM >= 2,\n    KIDAGEMO >= 9 & GRP_BF ~ INFFOODNUM >= 3,\n    !GRP_BF ~ INFFOODNUM + MILKFEEDS >= 4 & INFFOODNUM >= 1\n  ))\n\n\n\nLet’s compare the percentage of children who achieved MMF to the percentage who achieved MDD as shown above.\n\n\ndat %>% \n  group_by(KIDAGEMO_3) %>%\n  summarise(MDD = 100*mean(MDD), MMF = 100*mean(MMF, na.rm = T)) %>% \n  pivot_longer(!KIDAGEMO_3) %>% \n  mutate(name = factor(name, levels = c(\"MDD\", \"MMF\"))) %>% \n  ggplot(aes(fill = KIDAGEMO_3, x = name, y = value)) + \n  geom_bar(width = 0.6, position = position_dodge(0.6), stat = \"identity\") + \n  geom_text(\n    aes(label = round(value, 1)), \n    position = position_dodge(0.6),\n    vjust = -0.5\n  ) +\n  theme_pma(\n    title = \"Key IYCF Indicators for Infants Age 6-23 Months\",\n    subtitle = paste(\n      \"Minimum Dietary Diversity (MDD)\",\n      \"and Minimum Meal Frequency (MMF)\"\n    )\n  ) \n\n\n\n\nMAD: Minimum Acceptable Diet\nLastly, the WHO definition for Minimum Acceptable Diet (MAD) combines the two measures we’ve calculated above. MAD uses separate crieteria for breastfed and non-breastfed children:\nBreastfed children should achieve both MDD and MMF - as appropriately defined for their age - during the previous day\nNon-breastfed children should achieve both MDD and MMF - as appropriately defined for their age - during the previous day and also receive at least two milk feeds.\nWe’ve already defined all of the intermediate variables needed to calculate MAD, so we’ll simply combine them as appropriate for breastfed and non-breastfed children with one final case_when function:\n\n\ndat <- dat %>% \n  mutate(MAD = case_when(\n    GRP_BF ~ MMF & MDD,\n    !GRP_BF ~ MMF & MDD & MILKFEEDS >= 2\n  )) \n\n\n\nTo wrap up, we’ll add MAD to the chart we made comparing MDD and MMF.\n\n\ndat %>% \n  group_by(KIDAGEMO_3) %>%\n  summarise(\n    MDD = 100*mean(MDD), \n    MMF = 100*mean(MMF, na.rm = T),\n    MAD = 100*mean(MAD, na.rm = T)\n  ) %>% \n  pivot_longer(!KIDAGEMO_3) %>% \n  mutate(name = factor(name, levels = c(\"MDD\", \"MMF\", \"MAD\"))) %>% \n  ggplot(aes(fill = KIDAGEMO_3, x = name, y = value)) + \n  geom_bar(width = 0.6, position = position_dodge(0.6), stat = \"identity\") + \n  geom_text(\n    aes(label = round(value, 1)), \n    position = position_dodge(0.6),\n    vjust = -0.5\n  ) +\n  theme_pma(\n    title = \"IYCF Indicators for Infants Age 6-23 Months\",\n    subtitle = paste(\n      \"Minimum Dietary Diversity (MDD),\",\n      \"Minimum Meal Frequency (MMF), \\n\",\n      \"and Minimum Acceptable Diet (MAD)\"\n    )\n  ) \n\n\n\n\nNext Steps\nIn the coming weeks, we’ll show how researchers have used the IYCF indicators we’ve created to learn more about environmental and household factors that contribute to child malnutrition. We’ll also dig into supplementary data sources that connect food security outcomes with local agricultural conditions. In the meantime, make sure to connect with us on Twitter and let us know how you’re using PMA Nutrition data.\n\n\n\n“Indicators for Assessing Infant and Young Child Feeding Practices: Definitions and Measurement Methods.” 2021. World Health Organization; the United Nations Children’s Fund (UNICEF). https://apps.who.int/iris/bitstream/handle/10665/340706/9789240018389-eng.pdf?sequence=1.\n\n\nPinchoff, Jessie, William Turner, and Kathryn Grace. 2021. “The Association Between Agricultural Conditions and Multiple Dimensions of Undernutrition in Children 6-23 Months of Age in Burkina Faso.” Environmental Research Communications 3 (6): 065004. https://iopscience.iop.org/article/10.1088/2515-7620/ac07f5/meta.\n\n\n“Pma2020 Nutrition Survey - Burkina Faso: Key Results | September 2017.” 2017. Performance Monitoring for Action. https://www.pmadata.org/sites/default/files/data_product_results/PMA2020-Burkina-R1-Nutrition-Brief-EN.pdf.\n\n\nsee page 10, “Indicators for Assessing Infant and Young Child Feeding Practices: Definitions and Measurement Methods” (2021)↩︎\n",
    "preview": "posts/2021-10-01-nutrition-indicators/nutrition-indicators_files/figure-html5/unnamed-chunk-22-1.png",
    "last_modified": "2021-10-18T16:11:16-05:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1152
  },
  {
    "path": "posts/2021-09-15-nutrition-linking/",
    "title": "Update: Matching Records in 2018 Nutrition Surveys",
    "description": "It's now much easier to perform row-wise operations with pre-grouped data!",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-09-15",
    "categories": [
      "Nutrition",
      "Data Manipulation",
      "rowwise",
      "group_by",
      "cur_data",
      "pmap"
    ],
    "contents": "\n\nContents\nSetup\nMatching with multiple varialbes\nIdentifying the “current group”\nRow-wise search within groups\nExpanded search\nNew research questions\n\nWhen IPUMS PMA first released harmonized 2018 nutrition surveys from households in Burkina Faso and Kenya, we published a user note describing how to match children and mothers in the same household with pmap - a function designed to iterate over several variables simultaneously.\nOur challenge was this: IPUMS PMA nutrition data are actually a harmonized version of three separate surveys. When you download and open a nutrition data extract, any given row may contain variables associated with:\na household screening questionnaire,\na nutrition questionnaire for all children under age 5, and\na nutrition questionnaire for selected women aged 10-49\nAs we mentioned in our last post, all children under age 5 living in one of the screened households were sampled in the subsequent child nutrition survey. However, women aged 10-49 were only selected for the female nutrition survey from a random sub-sample of screened households (45% of sampled households in Burkina Faso, 25% in Kenya).\nWe imagine that many users will want to link children to mothers where possible, particularly because the female nutrition survey includes questions about antenatal care and nutrtion support the mother received during her most recent pregnancy. Our user note offered code you could use to, at minimum, link sampled mothers to their youngest sampled child.\nIn this post, we’d like to offer an updated version of this code that uses the rowwise function released with dplyr 1.0.0 last year. Both approaches work great! But, we believe that the rowwise approach is a quite bit easier to read and understand.\nSetup\nTo keep things simple, we’ll use a data extract from Burkina Faso 2018 that contains only household members who participated in either the child nutrition survey or the female nutrition survey (you can request a file with all household members if you select “All Cases” at checkout). We’ll use the tidyverse and ipumsr packages to load our data into R.\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\n\ndat <- read_ipums_micro(\n  ddi = \"data/pma_00002.xml\",\n  data = \"data/pma_00002.dat.gz\"\n)\n\n\n\n\n\n\n\n© 2017 (MPL 2.0)\nFor the purposes of this post, we’ll be working with only a small subset of variables (feel free to include any number of variables when you create your own extract). Also: in order to make our coding examples a bit more readable, we’ll take an extra step of reassigning shorter identification numbers to each household and person - this is not recommended practice in general and we do so only to reduce the number of characters in HHID (20) and PERSONID (22) to 4.\n\n\ndat <- dat %>% \n  select(\n    HHID,\n    ELIGTYPE,\n    AGEHQ,\n    RELATEKID,\n    KIDBIRTHYR,\n    KIDBIRTHMO,\n    LASTBIRTHYR,\n    LASTBIRTHMO,\n    KIDARMCIRCVAL,\n    RPANCPREGMO\n  ) %>% \n  arrange(HHID, ELIGTYPE) %>% \n  rowid_to_column(var = \"PERSONID\") %>% \n  group_by(HHID) %>% \n  mutate(HHID = cur_group_id()) %>% \n  ungroup()\n\n\n\n\n\n\n\n© 2018 RStudio (CC0 1.0)\nMatching with multiple varialbes\nFor any given household in our dataset, there should always be at least one child under age 5 who participated in the child nutrition survey. We also expect roughly 45% of households should also include at least one woman aged 10-49 who particpated in the female nutrition survey.\nA typical household might look something like this:\n\n\n\nHHID\n\n\nPERSONID\n\n\nAGEHQ\n\n\nELIGTYPE\n\n\nKIDBIRTHMO\n\n\nKIDBIRTHYR\n\n\nLASTBIRTHMO\n\n\nLASTBIRTHYR\n\n\n1598\n\n\n3521\n\n\n0\n\n\n11 - Infant under age 2 (INF)\n\n\nDecember\n\n\n2017\n\n\nNA\n\n\nNA\n\n\n1598\n\n\n3522\n\n\n0\n\n\n11 - Infant under age 2 (INF)\n\n\nJuly\n\n\n2017\n\n\nNA\n\n\nNA\n\n\n1598\n\n\n3523\n\n\n0\n\n\n11 - Infant under age 2 (INF)\n\n\nApril\n\n\n2018\n\n\nNA\n\n\nNA\n\n\n1598\n\n\n3524\n\n\n4\n\n\n14 - Aged 2-5 (K)\n\n\nJuly\n\n\n2013\n\n\nNA\n\n\nNA\n\n\n1598\n\n\n3525\n\n\n20\n\n\n20 - Selected women aged 10-49 (WN)\n\n\nNA\n\n\nNA\n\n\nApril\n\n\n2018\n\n\n1598\n\n\n3526\n\n\n25\n\n\n20 - Selected women aged 10-49 (WN)\n\n\nNA\n\n\nNA\n\n\nDecember\n\n\n2017\n\n\n1598\n\n\n3527\n\n\n45\n\n\n20 - Selected women aged 10-49 (WN)\n\n\nNA\n\n\nNA\n\n\nJuly\n\n\n2017\n\n\n\nAll members of the household share the same household ID in HHID. Each person has a unique PERSONID, while AGEHQ gives their age (when the household questionnaire was completed) and ELIGTYPE describes their eligibility for sub-modules on either the child or female nutrition questionnaire (both the numeric code and label are shown).\nIn this household, it’s easy to spot the mother for each infant: each infant’s birth month KIDBIRTHMO and year KIDBIRTHYR match the month and year of one woman’s most recent delivery shown in LASTBIRTHMO and LASTBIRTHYR. Unfortunately, it’s not possible to match the 4 year-old child to a mother (women were only asked about the date of their most recent birth).\nOnce the matches are identified by both month and year, we could attach any number of variables from the mother’s record onto the record for the child. For demonstration purposes, we’ll just attach her PERSONID as a new variable called MOMID. If the child’s mother cannot be determined - or if a particular row contains data from a mother, rather than a child - we’ll assign the value NA to MOMID.\nIdentifying the “current group”\nImagine dividing our search in two basic steps: first, we’ll want to group our data into households. Then, we’ll iterate through person in the dataset: if that person is a child, we’ll search the members of their group for a woman with a matching birth month and year.\nLet’s focus on the first step for a moment. A simple way to minimize the number of records involved with the search for each child’s mother is to group_by HHID, and then create a unique subset of the larger dataset for each child. After we’ve grouped the data, we’ll use cur_data to return the current data for each group. For convenience, you might save this miniature dataset as a column in dat - we’ll call ours HH_DATA:\n\n\ndat <- dat %>% \n  group_by(HHID) %>% \n  mutate(HH_DATA = list(cur_data())) %>% \n  ungroup()\n\n\n\n\nFor grouped data, cur_data() returns a dataset with one row for each member of the group.\nIf you now look at dat, you’ll see we’ve stored one table in HH_DATA for each person. If you opened one of those tables, you’d see:\none row for each person living in the individual’s household\nall columns from dat except for the grouping column HHID (10 in total)\n\n\ndat %>% select(PERSONID, HHID, HH_DATA)\n\n\n# A tibble: 5,267 × 3\n   PERSONID  HHID HH_DATA          \n      <int> <int> <list>           \n 1        1     1 <tibble [3 × 10]>\n 2        2     1 <tibble [3 × 10]>\n 3        3     1 <tibble [3 × 10]>\n 4        4     2 <tibble [2 × 10]>\n 5        5     2 <tibble [2 × 10]>\n 6        6     3 <tibble [2 × 10]>\n 7        7     3 <tibble [2 × 10]>\n 8        8     4 <tibble [1 × 10]>\n 9        9     5 <tibble [5 × 10]>\n10       10     5 <tibble [5 × 10]>\n# … with 5,257 more rows\n\nLet’s pull HH_DATA for the first four individuals in dat (essentially printing the contents of the list):\n\n\ndat %>% \n  slice(1:4) %>% \n  pull(HH_DATA)\n\n\n[[1]]\n# A tibble: 3 × 10\n  PERSONID  ELIGTYPE AGEHQ RELATEKID KIDBIRTHYR KIDBIRTHMO LASTBIRTHYR\n     <int> <int+lbl> <int> <int+lbl>      <dbl>  <int+lbl>   <int+lbl>\n1        1 14 [Aged…     4 1 [Mothe…       2014  6 [June]  9999 [NIU …\n2        2 14 [Aged…     2 1 [Mothe…       2016  3 [March] 9999 [NIU …\n3        3 14 [Aged…     2 2 [Fathe…       2016  5 [May]   9999 [NIU …\n# … with 3 more variables: LASTBIRTHMO <int+lbl>,\n#   KIDARMCIRCVAL <dbl+lbl>, RPANCPREGMO <int+lbl>\n\n[[2]]\n# A tibble: 3 × 10\n  PERSONID  ELIGTYPE AGEHQ RELATEKID KIDBIRTHYR KIDBIRTHMO LASTBIRTHYR\n     <int> <int+lbl> <int> <int+lbl>      <dbl>  <int+lbl>   <int+lbl>\n1        1 14 [Aged…     4 1 [Mothe…       2014  6 [June]  9999 [NIU …\n2        2 14 [Aged…     2 1 [Mothe…       2016  3 [March] 9999 [NIU …\n3        3 14 [Aged…     2 2 [Fathe…       2016  5 [May]   9999 [NIU …\n# … with 3 more variables: LASTBIRTHMO <int+lbl>,\n#   KIDARMCIRCVAL <dbl+lbl>, RPANCPREGMO <int+lbl>\n\n[[3]]\n# A tibble: 3 × 10\n  PERSONID  ELIGTYPE AGEHQ RELATEKID KIDBIRTHYR KIDBIRTHMO LASTBIRTHYR\n     <int> <int+lbl> <int> <int+lbl>      <dbl>  <int+lbl>   <int+lbl>\n1        1 14 [Aged…     4 1 [Mothe…       2014  6 [June]  9999 [NIU …\n2        2 14 [Aged…     2 1 [Mothe…       2016  3 [March] 9999 [NIU …\n3        3 14 [Aged…     2 2 [Fathe…       2016  5 [May]   9999 [NIU …\n# … with 3 more variables: LASTBIRTHMO <int+lbl>,\n#   KIDARMCIRCVAL <dbl+lbl>, RPANCPREGMO <int+lbl>\n\n[[4]]\n# A tibble: 2 × 10\n  PERSONID  ELIGTYPE AGEHQ RELATEKID KIDBIRTHYR KIDBIRTHMO LASTBIRTHYR\n     <int> <int+lbl> <int> <int+lbl>      <dbl>  <int+lbl>   <int+lbl>\n1        4 14 [Aged…     3  1 [Moth…       2014  9 [Septe… 9999 [NIU …\n2        5 20 [Sele…    25 99 [NIU …       9999 99 [NIU (… 2014       \n# … with 3 more variables: LASTBIRTHMO <int+lbl>,\n#   KIDARMCIRCVAL <dbl+lbl>, RPANCPREGMO <int+lbl>\n\nThe first three individuals are members of the same household, so they appear in eachother’s HH_DATA. The fourth is a member of a second household, and so on.\nFor our purposes, it’s perfectly fine to leave the household datasets in a list. Once we define a search function to find each child’s mother, we’ll simply apply it to each row of HH_DATA. For example, suppose we just wanted to find the age of the oldest person in each household. We would use rowwise to iterate over each row in dat, looking for the maximum value of AGEHQ in the HH_DATA table stored in each row.\n\n\ndat %>% \n  rowwise() %>% \n  mutate(HH_AGE_OLDEST = max(HH_DATA$AGEHQ)) %>% \n  select(PERSONID, HHID, AGEHQ, HH_AGE_OLDEST)\n\n\n# A tibble: 5,267 × 4\n# Rowwise: \n   PERSONID  HHID     AGEHQ HH_AGE_OLDEST\n      <int> <int> <int+lbl>     <int+lbl>\n 1        1     1         4             4\n 2        2     1         2             4\n 3        3     1         2             4\n 4        4     2         3            25\n 5        5     2        25            25\n 6        6     3         1             4\n 7        7     3         4             4\n 8        8     4         2             2\n 9        9     5         4            27\n10       10     5         2            27\n# … with 5,257 more rows\n\nThe main difference between max and the function we’ll use for our search is that max returns the same value for every person in the household; strictly speaking, it’s not necessary to do a row-wise search through pre-grouped household data in this case. Our problem is more complex: we want to return a specific value for each person via rowwise, but we also want to restrict each person’s search to members of a pre-defined group.\nRow-wise search within groups\nNow that we’ve isolated data for each person’s household in HH_DATA, we need to write a custom function that will match birth months and years, and then return a value for MOMID only if exactly one match is found.\nTo save time, we’ll only perform our search if ELIGTYPE shows that the current row contains data for a sampled child. In other words, the value for ELIGTYPE should be less than 20.\n\n\ndat %>% count(ELIGTYPE)\n\n\n# A tibble: 3 × 2\n                             ELIGTYPE     n\n                            <int+lbl> <int>\n1 11 [Infant under age 2 (INF)]        1194\n2 14 [Aged 2-5 (K)]                    1662\n3 20 [Selected women aged 10-49 (WN)]  2411\n\nThe case_when function makes this job simple: we’ll explicitly define a function for the “case when” ELIGTYPE < 20, but we’ll not specify what to in the “case when” ELIGTYPE == 20. A nice feature of case_when is that it automatically returns the value NA for any cases that are not handled explicitly:\n\n\ndat %>% \n  count(ELIGTYPE) %>% \n  mutate(EXAMPLE = case_when(ELIGTYPE < 20 ~ TRUE))\n\n\n# A tibble: 3 × 3\n                             ELIGTYPE     n EXAMPLE\n                            <int+lbl> <int> <lgl>  \n1 11 [Infant under age 2 (INF)]        1194 TRUE   \n2 14 [Aged 2-5 (K)]                    1662 TRUE   \n3 20 [Selected women aged 10-49 (WN)]  2411 NA     \n\nAs we’ve seen, one way to identify the likely mother for each child is to match the mother’s LASTBIRTHMO and LASTBIRTHYR to the child’s KIDBIRTHMO and KIDBIRTHYR. If there is only one match in the household, we’ll return the mother’s PERSONID from HH_DATA as a new column MOMID. On the other hand, if multiple women could be the mother or if no match could be found, we’ll return the value NA.\n\n\ndat <- dat %>% \n  rowwise() %>%\n  mutate(MOMID = case_when(ELIGTYPE < 20 ~ {\n    # re-name these in order to avoid confusion with the columns in HH_DATA:\n    kid_month <- KIDBIRTHMO\n    kid_year <- KIDBIRTHYR\n    \n    # pull the PERSONID for any match\n    moms <- HH_DATA %>% \n      filter(LASTBIRTHMO == kid_month, LASTBIRTHYR == kid_year) %>% \n      pull(PERSONID)\n    \n    # if exactly one match was found, return that PERSONID\n    # otherwise, return NA\n    ifelse(length(moms) == 1, moms, NA) \n  })) %>% \n  ungroup()\n\n\n\nLet’s return to our example household:\n\n\n\nHHID\n\n\nPERSONID\n\n\nMOMID\n\n\nAGEHQ\n\n\nELIGTYPE\n\n\nKIDBIRTHMO\n\n\nKIDBIRTHYR\n\n\nLASTBIRTHMO\n\n\nLASTBIRTHYR\n\n\n1598\n\n\n3521\n\n\n3526\n\n\n0\n\n\n11 - Infant under age 2 (INF)\n\n\nDecember\n\n\n2017\n\n\nNA\n\n\nNA\n\n\n1598\n\n\n3522\n\n\n3527\n\n\n0\n\n\n11 - Infant under age 2 (INF)\n\n\nJuly\n\n\n2017\n\n\nNA\n\n\nNA\n\n\n1598\n\n\n3523\n\n\n3525\n\n\n0\n\n\n11 - Infant under age 2 (INF)\n\n\nApril\n\n\n2018\n\n\nNA\n\n\nNA\n\n\n1598\n\n\n3524\n\n\nNA\n\n\n4\n\n\n14 - Aged 2-5 (K)\n\n\nJuly\n\n\n2013\n\n\nNA\n\n\nNA\n\n\n1598\n\n\n3525\n\n\nNA\n\n\n20\n\n\n20 - Selected women aged 10-49 (WN)\n\n\nNA\n\n\nNA\n\n\nApril\n\n\n2018\n\n\n1598\n\n\n3526\n\n\nNA\n\n\n25\n\n\n20 - Selected women aged 10-49 (WN)\n\n\nNA\n\n\nNA\n\n\nDecember\n\n\n2017\n\n\n1598\n\n\n3527\n\n\nNA\n\n\n45\n\n\n20 - Selected women aged 10-49 (WN)\n\n\nNA\n\n\nNA\n\n\nJuly\n\n\n2017\n\n\n\nSuccess! MOMID shows the correct PERSONID for the mother of each infant, and it contains expected NA values as discussed above.\nBut how many children were matched to a mother, overall? Remember that a slight majority of sampled children (approximately 55%) live in households where no women were sampled for the female nutrition survey. We’ll flag the rest with an indicator WOMEN_IN_HH, then test whether each child was LINKED with a mother:\n\n\ndat %>%\n  group_by(HHID) %>% \n  mutate(WOMEN_IN_HH = any(ELIGTYPE == 20)) %>% \n  filter(ELIGTYPE < 20) %>% \n  ungroup() %>% \n  count(WOMEN_IN_HH, LINKED = !is.na(MOMID)) %>% \n  mutate(pct = 100*prop.table(n))\n\n\n# A tibble: 3 × 4\n  WOMEN_IN_HH LINKED     n   pct\n  <lgl>       <lgl>  <int> <dbl>\n1 FALSE       FALSE   1579  55.3\n2 TRUE        FALSE    481  16.8\n3 TRUE        TRUE     796  27.9\n\nSo far, we’ve managed to locate the mother for only about 28% of the children in our sample. Around 17% of the remaining children do live with a sampled woman, but could not be linked with a mother by matched birthdates.\nLet’s see if we can improve on these results.\nExpanded search\nIn our first search, our use of LASTBIRTHMO and LASTBIRTHYR ensured that only mother’s most recent child could be linked to her record. It’s possible to expand these criteria in certain circumstances using RELATEKID, which describes the relationship between each child and the person who provided responses to the interviewer on their behalf.\n\n\ndat %>% count(RELATEKID)\n\n\n# A tibble: 7 × 2\n                   RELATEKID     n\n                   <int+lbl> <int>\n1  1 [Mother]                 2646\n2  2 [Father]                   57\n3  3 [Grandmother]              94\n4  4 [Grandfather]               7\n5  5 [Sister]                    4\n6 94 [Other]                    48\n7 99 [NIU (not in universe)]  2411\n\nWhen RELATEKID == 1, this respondent is the child’s mother. So, suppose we have a household like this one:\n\n\n\nHHID\n\n\nPERSONID\n\n\nMOMID\n\n\nAGEHQ\n\n\nELIGTYPE\n\n\nRELATEKID\n\n\nKIDBIRTHMO\n\n\nKIDBIRTHYR\n\n\nLASTBIRTHMO\n\n\nLASTBIRTHYR\n\n\n61\n\n\n147\n\n\n149\n\n\n0\n\n\n11 - Infant under age 2 (INF)\n\n\nMother\n\n\nMay\n\n\n2018\n\n\nNA\n\n\nNA\n\n\n61\n\n\n148\n\n\nNA\n\n\n1\n\n\n11 - Infant under age 2 (INF)\n\n\nMother\n\n\nDecember\n\n\n2016\n\n\nNA\n\n\nNA\n\n\n61\n\n\n149\n\n\nNA\n\n\n19\n\n\n20 - Selected women aged 10-49 (WN)\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nMay\n\n\n2018\n\n\n\nWe’ve already matched the youngest child in this household with our first search, but the older child remains unlinked. Fortunately, RELATEKID tells us that the older child’s mother also lives in the household, since she provided responses on their behalf.\nBecause there is only one reproductive age woman living in this particular household, it’s reasonable to assume that the same woman is the mother for both children.\nLet’s implement these criteria in a new search, creating MOMID2.\n\n\ndat <- dat %>% \n  rowwise() %>%\n  mutate(MOMID2 = case_when(ELIGTYPE < 20 ~ {\n    # This is the same as above: if we find a match, we'll call it `mom`\n    kid_month <- KIDBIRTHMO\n    kid_year <- KIDBIRTHYR\n    moms <- HH_DATA %>%\n      filter(LASTBIRTHMO == kid_month, LASTBIRTHYR == kid_year) %>%\n      pull(PERSONID)\n    mom <- ifelse(length(moms) == 1, moms, NA)\n    \n    # Additionally, if `mom` is NA, we'll look at `RELATEKID`\n    # If the respondent was the child's mother, and only one woman in the \n    # household has ever given birth, we'll define that woman as `mom`\n    if(is.na(mom)){\n      moms <- HH_DATA %>% \n        filter(LASTBIRTHYR < 9000) %>% \n        pull(PERSONID) \n      mom <- ifelse(length(moms) == 1 & RELATEKID == 1, moms, NA)\n    }\n    mom\n  })) %>% \n  ungroup() \n\n\n\nNow, both of the children in our example household will show the same MOMID2:\n\n\n\nHHID\n\n\nPERSONID\n\n\nMOMID\n\n\nMOMID2\n\n\nAGEHQ\n\n\nELIGTYPE\n\n\nRELATEKID\n\n\nKIDBIRTHMO\n\n\nKIDBIRTHYR\n\n\nLASTBIRTHMO\n\n\nLASTBIRTHYR\n\n\n61\n\n\n147\n\n\n149\n\n\n149\n\n\n0\n\n\n11 - Infant under age 2 (INF)\n\n\nMother\n\n\nMay\n\n\n2018\n\n\nNA\n\n\nNA\n\n\n61\n\n\n148\n\n\nNA\n\n\n149\n\n\n1\n\n\n11 - Infant under age 2 (INF)\n\n\nMother\n\n\nDecember\n\n\n2016\n\n\nNA\n\n\nNA\n\n\n61\n\n\n149\n\n\nNA\n\n\nNA\n\n\n19\n\n\n20 - Selected women aged 10-49 (WN)\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nMay\n\n\n2018\n\n\n\nHow much does MOMID2 improve on the results from MOMID?\n\n\ndat %>%\n  group_by(HHID) %>% \n  mutate(WOMEN_IN_HH = any(ELIGTYPE == 20)) %>% \n  filter(ELIGTYPE < 20) %>% \n  ungroup() %>% \n  count(WOMEN_IN_HH, LINKED1 = !is.na(MOMID), LINKED2 = !is.na(MOMID2)) %>% \n  mutate(pct = 100*prop.table(n))\n\n\n# A tibble: 4 × 5\n  WOMEN_IN_HH LINKED1 LINKED2     n   pct\n  <lgl>       <lgl>   <lgl>   <int> <dbl>\n1 FALSE       FALSE   FALSE    1579 55.3 \n2 TRUE        FALSE   FALSE     198  6.93\n3 TRUE        FALSE   TRUE      283  9.91\n4 TRUE        TRUE    TRUE      796 27.9 \n\nOur second search found mothers for an additional 283 children, or about 10% of the overall child nutrition sample. Setting aside the 55% of child who could not possibly be linked because of sample design restrictions, we were unable to find a mother in only 7% of cases (many of these may not live with their mother, or else we have insufficient information to identify a match).\nNew research questions\nNow that we’ve linked as many children as possible to their mother’s data, we’ll be able to explore how certain antenatal interventions might impact child growth and nutrition. Consider our first example household again:\n\n\n\nPERSONID\n\n\nMOMID\n\n\nAGEHQ\n\n\nKIDARMCIRCVAL\n\n\nRPANCPREGMO\n\n\n3521\n\n\n3526\n\n\n0\n\n\n13.3\n\n\nNA\n\n\n3522\n\n\n3527\n\n\n0\n\n\n13.6\n\n\nNA\n\n\n3523\n\n\n3525\n\n\n0\n\n\n10.9\n\n\nNA\n\n\n3524\n\n\nNA\n\n\n4\n\n\n14.1\n\n\nNA\n\n\n3525\n\n\nNA\n\n\n20\n\n\nNA\n\n\n7\n\n\n3526\n\n\nNA\n\n\n25\n\n\nNA\n\n\n3\n\n\n3527\n\n\nNA\n\n\n45\n\n\nNA\n\n\n3\n\n\n\nWe’ve added two new variables here:\nKIDARMCIRCVAL shows the Mid-Upper Arm Circumference (MUAC) for each child in centimeters. For children older than 6 months, a MUAC smaller than 11.5 centimeters is commonly used to screen for acute malnutrition.\nRPANCPREGMO reports the first month of the mother’s most recent pregnancy when she first received antenatal care.\nNow that we’ve established a link between the infants and mothers, it’s possible to investigate how infants are impacted by, for example, the timing or quality of antenatal care provided to their mother during pregnancy. In this household, the KIDARMCIRCVAL shown for the child in row 3 suggests that they may be at risk for acute malnutrition. We also observe in RPANCPREGMO that their mother received no antenatal care until the third trimester of her pregnancy (month 7). If we took the additional step of attaching the value in RPANCPREGMO to the appropriate child’s record - for example, in a variable we might call MOM_ANCPREGMO - we could then build a model examining this relationship in the larger dataset.\nIn our next post, we’ll continue our series on PMA nutrition surveys as we consider more ways to measure nutritional outcomes in children. In particular, we’ll be looking at common assessment tools for recommended infant and young child feeding practices (IYCFP) and how to create them from dietary intake data in the child nutrition survey.\n\n\n\n",
    "preview": "posts/2021-09-15-nutrition-linking/images/logos.png",
    "last_modified": "2021-10-19T09:48:16-05:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 540
  },
  {
    "path": "posts/2021-09-01-nutrition-discovery/",
    "title": "Introducing PMA Nutrition Data",
    "description": "Nutrition surveys are available for women, young children, and health service providers in their area.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-09-01",
    "categories": [
      "Nutrition",
      "Data Discovery",
      "Service Delivery Points",
      "Infant and Young Child Feeding"
    ],
    "contents": "\n\nContents\nData Access\nHousehold Nutrition Surveys\n2017 Sample Design\n2018 Sample Design\n\nSDP Nutrition Surveys\nNext steps\n\n\n\n\nToday, we’re excited to launch a new series exploring nutrition surveys from Performance Monitoring for Action! Much like the family planning surveys we’ve discussed so far, PMA nutrition surveys feature contemporaneous samples of both households and service delivery points (SDPs). Several public use data series cover maternal and young child nutrition, but the inclusion of service delivery point data in PMA surveys gives researchers a unique opportunity to see how local service provision impacts the nutritional health of women and their households.\nPMA household nutrition surveys focus on women aged 10-49 and children up to age 5 living within geographically defined sample clusters - or enumeration areas - in Burkina Faso and Kenya. Two rounds of nationally representative, cross-sectional samples were collected from each country: one in 2017, and another in 2018.\nNutrition surveys for SDPs are designed to reflect the health service environment experienced by women and children in a corresponding household nutrition sample (SDP surveys are not nationally representative). PMA worked with officials in Burkina Faso and Kenya to obtain a census of public-sector facilities, and it sampled all public-sector facilities providing services to an enumeration area represented in the household nutrition sample. Additionally, PMA sampled up to three private-sector facilities that were located within the boundaries of any such enumeration area.\nIPUMS offers a harmonized version of all PMA nutrition surveys at no cost to registered users. At the household level, we’ve merged separate questionnaires administered to household heads, women age 10-49, and caregivers for children under age 5 in a process described here. We’ve also created integrated variable names, response codes, and documentation highlighting comparability issues between samples.\nHarmonized data from IPUMS PMA are designed to help you find and compare data from multiple PMA nutrition samples as quickly and easily as possible. In this post, we’ll guide you through some of the topics and comparability issues you’re likely to encounter. Then, in the coming weeks, we’ll take a deeper dive into analytic topics like:\nconstructing assessment indicators for infant and young child feeding (IYCF) practices\nlinking antenatal / postnatal interventions with child nutrition outcomes\ncontextualizing household nutrition with data from SDP surveys and external sources (e.g. livelihood zones and climate data)\nrecent publications featuring PMA nutrition data, and opportunities for additional analysis\nData Access\nYou’ll find nutrition data on the IPUMS PMA website by clicking on the “CHANGE” link in the box where the current unit of analysis is shown:\n\n\n\n\nAnyone can browse IPUMS PMA documentation, but you’ll need to register for a free account before you can download data.\nAs with family planning surveys, you’ll find separate units of analysis for nutrition surveys collected from households and SDPs. If you intend to use both, you will need to download two extracts: one for SDPs and one for households.\n\n\n\nThroughout this series, we’ll use tools from ipumsr and the tidyverse to work with IPUMS PMA nutrition data in R. Once you’ve downloaded extracts from one or both units of analysis, you can load them into R as follows (changing the file paths as needed):\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\n\n# we'll call data from the SDP unit of analysis `sdp`\nsdp <- read_ipums_micro(\n  ddi = \"data/pma_00001.xml\",\n  data = \"data/pma_00001.dat.gz\"\n)\n\n# we'll call data from the household unit of analysis `hh`\nhh <- read_ipums_micro(\n  ddi = \"data/pma_00002.xml\",\n  data = \"data/pma_00002.dat.gz\"\n)\n\n\n\nHousehold Nutrition Surveys\nThere are nearly 1,000 variables available from the four household nutrition samples, so navigation can seem overwhelming at first. Fortunately, a large proportion of these come from multiple response questions repeated separately for women and each sampled child.\nHarmonized variable names repeat the same stem for variables that come from the same original question, and they have a specific prefix indicating whether the question was administered to a woman or a particular child. If the question was multiple response, a unique suffix was given to the binary indicator derived from each response.\nFor example, a common question for both women and children deals with the type of foods eaten yesterday. Here is the text of the question administered to women in 2017 (both countries):\n501. Now I'd like to ask you about foods and drinks that you ate or drank\nyesterday during the day or night, whether you ate it at home or anywhere else.\n\nI am interested in whether you had the food items I will mention even if they\nwere combined with other foods. For example, if you had a soup made with\ncarrots, potatoes and meat, you should reply \"yes\" for each of these\ningredients when I read you the list. However, if you consumed only the broth\nof a soup, but not the meat or vegetable, do not say \"yes\" for the meat or\nvegetable.\n\nAs I ask you about foods and drinks, please think of foods and drinks you had\nas snacks or small meals as well as during any main meals. Please also remember\nfoods you may have eaten while preparing meals or preparing food for others.\nPlease do not include any food used in a small amount for seasoning or\ncondiments (like chilies, spices, herbs or fish powder). I will ask you about\nthose foods separately.\n\nCannot select 'no response' with other options.\n\n  [] Any foods made from grains, like maize, rice, wheat, porridge, sorghum, \n     bread\n  [] Any vegetables or roots that are orange or yellow inside like pumpkin,\n     carrots, squash or yellow sweet potatoes\n  [] Any white roots and tubers or plantains like Irish potatoes, yams, \n     cassava, white sweet potatoes\n  [] Any dark green, leafy vegetables like sukumu wiki\n  [] Any fruits that are dark yellow or orange inside like ripe mangoes, pawpaw\n  [] Any other fruits\n  [] Any other vegetables\n  [] Any meat made from animal organs like liver, kidney, heart\n  [] Any other meat, such as beef, pork, lamb, goat, chicken, duck, or dik dik\n  [] Eggs\n  [] Fresh or dried fish or shellfish\n  [] Any foods made from beans, peas, lentils\n  [] Any nuts and seeds like groundnut or groundnut paste\n  [] Any milk or milk products like cheese or mala\n  [] Any savory and fried snacks like fried chips, crisps, samosas, or other\n     fried foods\n  [] Sugary foods, jiggery (sukari nguru), mandaazi, donuts, cake, sweet\n     biscuits or candies\n  [] Any sugar-sweetened beverages like sweet fruit drinks, fizzy drinks, \n     sweet tea\n  [] Any condiments and seasonings used in small amounts for flavor, like\n     spices, herbs, fish powder, tomato paste\n  [] Other beverages and foods like unsweetened tea or coffee, clear broth, \n     alcohol\n  [] None of the above\n  [] No response\n\nThe format of questions included in each PMA household nutrition survey closely resembles nutrition questions found in other prominent global health surveys, including those you’ll find at IPUMS DHS.\nEach one of these response options is represented as a binary indicator (with top-codes for different types of non-response). For variables where this question was administered to women, each indicator received the same prefix and stem:\nWN (women) + YEST (yesterday)\nEach response is represented by the addition of a suffix like:\nWNYESTGRAIN - Woman consumed yesterday: grains\nWNYESTMEAT - Woman consumed yesterday: meats\nWNYESTFRIED - Woman consumed yesterday: savory and fried snacks\nThis question was also administered to caregivers responding on behalf of sampled children. The prefix for children can take one of several values associated with changes in sample design made between 2017 and 2018:\nINF - a child under 24 months\nYK - the caregiver’s youngest child aged 24-60 months (2017 samples only)\nOK - the caregiver’s other children aged 24-60 months (2017 samples only)\nK - any child aged 24-60 months (2018 samples only)\nKID - any child aged 0-60 months\n\nSee ELIGTYPE for more information on the use of each prefix.\nThis results in several variables for differently sampled children, for example:\nINFYESTFRIED - Infant under 2 consumed yesterday: savory/fried snacks (all samples)\nYKYESTFRIED - Youngest child aged 2-5 consumed yesterday: savory/fried snacks (2017 samples only)\nOTKYESTFRIED - Older child aged 2-5 consumed yesterday: savory/fried snacks (2017 samples only)\nKIDYESTFRIED - Child under age 5 consumed yesterday: savory/fried snacks (2018 samples only)\nBefore we continue, let’s dig a bit deeper into the changes made to sample design between the 2017 and 2018 surveys.\n2017 Sample Design\nLike all cross-sectional PMA surveys, the 2017 household nutrition survey features a multi-stage stratified cluster sample design. Officials in each country drew a sample of enumeration areas from urban/rural strata (83 enumeration areas for Burkina Faso, 151 for Kenya). Within each enumeration area, a set number of households were randomly selected for screening (89 households per enumeration area for Burkina Faso, 140 for Kenya). In preliminary screening, interviewers determined whether each household included both\n\nFull 2017 dataset notes for Burkina Faso and Kenya\nat least one woman aged 10-49 (see ELIGIBLEHHFEM), and\nat least one child under age 2 (see ELIGIBLEHHKID).\nHouseholds meeting both criteria (see ELIGIBLEHH) were given additional questions related to household food security, wealth / assets, access to water, sanitation, and so forth.\nFor example, consider the following household. In preliminary screening, the RESPONDENT provided a roster of all persons living in the household, including each person’s AGEHQ and SEX. Because the household contains both one woman aged 10-49 (row 1) and one child under age 2 (row 2), the RESPONDENT was asked to provide additional information about the household, including information used to determine WEALTHT, WATERDRINKMAIN, and more.\n\n\n\nRESPONDENT\n\n\nAGEHQ\n\n\nSEX\n\n\nWEALTHT\n\n\nWATERDRINKMAIN\n\n\nYes\n\n\n25\n\n\nFemale\n\n\nLowest tertile\n\n\nProtected spring\n\n\nNo\n\n\n0\n\n\nFemale\n\n\nLowest tertile\n\n\nProtected spring\n\n\nNo\n\n\n3\n\n\nMale\n\n\nLowest tertile\n\n\nProtected spring\n\n\nNo\n\n\n5\n\n\nMale\n\n\nLowest tertile\n\n\nProtected spring\n\n\nNo\n\n\n29\n\n\nMale\n\n\nLowest tertile\n\n\nProtected spring\n\n\n\nThe nutrition questionnaire was administered in a second stage, and only in households meeting both of the above criteria. This questionnaire was given separately to each woman age 10-49 in qualifying households:\nEach woman answered questions about her own nutritional health (prefix WN), and specific questions about nutrition during her current pregnancy (prefix CP) or recent pregnancy (prefix RP) if applicable;\nEach woman then answered questions about the youngest child in her care. Women with children under 24 months received certain questions related to infant nutrition (prefix INF). We found that, if a woman was the primary caregiver for more than one child under 24 months of age (e.g. twins), only infant feeding practices for one child were reported;\nIf the youngest child under age 5 in the woman’s care was 24 months or older, women received certain questions related to young child nutrition (prefix YK). Note: these women live with a child under 2 (see sample design), but are not the primary caretaker for that child;\nAll other children under age 5 in the woman’s care are represented by a subset of repeated questions related to young child nutrition (prefix OTK).\n\nThere are additional restrictions on children represented by YK and OTK variables: they must also be biological offspring of the responding woman. Children represented by INF variables need only be the youngest child in her care.\nEach woman aged 10-49 living in the household would have received the nutrition questionnaire; in our example household, there is only one such woman. She answered nutrition-related questions for herself, and on behalf of each of her children under age 5. You’ll see the stem for each qualifying woman and child in the variable ELIGTYPE; this will help you find the relevant nutrition variables for each person.\n\n\n\nAGEHQ\n\n\nSEX\n\n\nELIGTYPE\n\n\nWNYESTFRIED\n\n\nINFYESTFRIED\n\n\nYKYESTFRIED\n\n\nOTKYESTFRIED\n\n\nKIDYESTFRIED\n\n\n25\n\n\nFemale\n\n\nSelected women aged 10-49 (WN)\n\n\nNo\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNA\n\n\n0\n\n\nFemale\n\n\nInfant under age 2 (INF)\n\n\nNIU (not in universe)\n\n\nNo\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNA\n\n\n3\n\n\nMale\n\n\nOlder aged 2-5 (OTK)\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNo\n\n\nNA\n\n\n5\n\n\nMale\n\n\nMember of household included in nutrition sample\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNA\n\n\n29\n\n\nMale\n\n\nMember of household included in nutrition sample\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNA\n\n\n\nA few things to note here:\nELIGTYPE - Explains eligibility for nutrition questions associated with each prefix. Members of this household who were not eligible for the nutrition questionnaire are shown in rows 4 and 5 (you can exclude these cases from your extract by selecting only “Females and Children with Nutrition Information”).\nWNYESTFRIED - The 25 year old woman ate no fried food yesterday. All other household members are ineligible for this question.\nINFYESTFRIED - The newborn child (age 0) ate no friend food yesterday. All other household members are ineligible for this question.\nYKYESTFRIED - The woman’s youngest child is already represented by INFYESTFRID, so no child is eligible for questions with the stem YK. All household members are ineligible for this question\nOTKYESTFRIED - The woman has one other child under age 5. This child (age 3) ate no fried food yesterday. The final child (age 5) is too old the qualify for the nutrition questionnaire; he and all other household members are ineligible for this question.\nKIDYESTFRIED - This variable is not available for 2017 samples, so it appears as the value NA.\n2018 Sample Design\nThe 2018 household nutrition survey also features a multi-stage stratified cluster sample design. Officials in each country again drew a sample of enumeration areas from urban/rural strata (83 enumeration areas for Burkina Faso, 151 for Kenya). A set number of households were randomly selected for screening within each enumeration area (43 households per enumeration area for Burkina Faso, 56 for Kenya).\n\nFull 2018 dataset notes for Burkina Faso and Kenya\nIn contrast with the 2017 design, all screened households received questions about household food security, wealth / assets, access to water, sanitation, and so forth. Related variables are available even if no members of the household were eligible for the nutrition questionnaire (see ELIGIBLEHH).\nThe nutrition questionnaire was again administered in a second stage. This time, all children under age 5 living in screened households were selected (see ELIGIBLEKID). Because the 2017 sampling criteria were changed, 2018 samples include children under 5 in households where no women were selected. For example:\n\n\n\nAGEHQ\n\n\nSEX\n\n\nELIGTYPE\n\n\nWNYESTFRIED\n\n\nINFYESTFRIED\n\n\nYKYESTFRIED\n\n\nOTKYESTFRIED\n\n\nKIDYESTFRIED\n\n\n4\n\n\nMale\n\n\nAged 2-5 (K)\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNA\n\n\nNA\n\n\nNo\n\n\n6\n\n\nFemale\n\n\nMember of household included in nutrition sample\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNA\n\n\nNA\n\n\nNIU (not in universe)\n\n\n22\n\n\nFemale\n\n\nMember of household included in nutrition sample\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNA\n\n\nNA\n\n\nNIU (not in universe)\n\n\n39\n\n\nFemale\n\n\nMember of household included in nutrition sample\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNA\n\n\nNA\n\n\nNIU (not in universe)\n\n\n\nIn this household, only one child (age 4) was represented in the nutrition sample. The variables YKYESTFRIED and OTKYESTFRIED are only available for 2017 samples, so nutrition data for the 4 year old is recorded in KIDYESTFRIED (there is no child under 2 in the household, so no household member is eligible for INFYESTFRIED).\nNotice that there are, in fact, two women in the household aged 10-49, but they were not included in the nutrition sample. Women were selected for the 2018 nutrition survey through a separate sampling process. A given proportion of households was randomly selected to screen for eligible women (45% of sampled households in Burkina Faso, 25% in Kenya); individuals living in selected households are indicated in FQSELECTED. Any woman aged 10-49 (see ELIGIBLEFEM) residing in one of these randomly selected households would have been given questions about her own nutritional health (prefix WN), and specific questions about nutrition during her current pregnancy (prefix CP) or recent pregnancy (prefix RP) if applicable.\nIn short: it is possible - but not necessary - for sampled women and children to reside in the same household. Moreover, the respondent answering on behalf of a child may be their mother, but other caretakers could respond if the mother was not available (see RELATEKID). In an upcoming post in this series, we’ll explain how to link maternal and child records together (where possible), opening the possibility for analyses that track antenatal and postnatal interventions to child nutrition outcomes.\nSDP Nutrition Surveys\nUnlike the household nutrition surveys, you’ll find no differences in sample design between the 2017 and 2018 SDP nutrition surveys. In both rounds, PMA selected SDPs with the same set of enumeration areas used to select households (83 enumeration areas for Burkina Faso, 151 for Kenya). As mentioned above, SDP samples include:\nall public-sector facilities whose service catchment area includes a selected enumeration area\nup to three private-sector facilities located within the boundaries of a selected enumeration area\nBy design, SDP samples represent the service environment experienced by women and children in a corresponding household sample (SDP samples are not nationally representative). We anticipate that most users will want to aggregate data collected from several facilities serving the same enumeration area.\nFor example, let’s take a look at a few of the facilities surveyed in Kenya 2017:\n\n\n\nFACILITYID\n\n\nFACILITYTYPEGEN\n\n\nAUTHORITY\n\n\nEAID\n\n\nEASERVED1\n\n\nEASERVED2\n\n\nEASERVED3\n\n\nNUTFAC\n\n\n4047\n\n\nHospital\n\n\nGovernment\n\n\n4486\n\n\n4092\n\n\n4819\n\n\n4036\n\n\nYes\n\n\n4080\n\n\nHospital\n\n\nGovernment\n\n\n4036\n\n\n4092\n\n\n4036\n\n\nNIU (not in universe)\n\n\nYes\n\n\n4149\n\n\nPharmacy/chemist/drug shop\n\n\nPrivate\n\n\n4092\n\n\n4092\n\n\nNIU (not in universe)\n\n\nNIU (not in universe)\n\n\nNo\n\n\n\nAll three of these facilities provide services to the enumeration area 4092, but only one - the private-sector pharmacy - is actually located there (see EAID). The two public-sector hospitals serve multiple enumeration areas shown in variables sharing the stem EASERVED (some public-sector facilities serve up to 18 different enumeration areas listed in variables EASERVED1 to EASERVED18).\nPrivate facilities will only list one EASERVED - it will always match the facility’s EAID.\nPublic facilities may serve multiple EASERVED, usually including their own EAID.\nThe last variable NUTFAC reports whether each SDP is a “nutritional facility”, defined as a facility that “supervises, supports, or supplies community nutrition services provided either by paid staff or community health volunteers (CHVs)”. A simple way to integrate SDP data with household data from the same country-year might be to count the number of “nutritional facilities” serving each enumeration area. We recommend using pivot_longer() to create one row for each EASERVED in every facility’s catchment list:\n\n\nsdp <- sdp %>% \n  pivot_longer(\n    starts_with(\"EASERVED\"),\n    values_to = \"EASERVED\",\n    names_to = NULL\n  )\n\n\n\n\nYou’ll find “nutritional facility” defined on the NUTFAC description tab.\n\n\n\nFACILITYID\n\n\nFACILITYTYPEGEN\n\n\nAUTHORITY\n\n\nEAID\n\n\nNUTFAC\n\n\nEASERVED\n\n\n4047\n\n\nHospital\n\n\nGovernment\n\n\n4486\n\n\nYes\n\n\n4092\n\n\n4047\n\n\nHospital\n\n\nGovernment\n\n\n4486\n\n\nYes\n\n\n4819\n\n\n4047\n\n\nHospital\n\n\nGovernment\n\n\n4486\n\n\nYes\n\n\n4036\n\n\n4080\n\n\nHospital\n\n\nGovernment\n\n\n4036\n\n\nYes\n\n\n4092\n\n\n4080\n\n\nHospital\n\n\nGovernment\n\n\n4036\n\n\nYes\n\n\n4036\n\n\n4080\n\n\nHospital\n\n\nGovernment\n\n\n4036\n\n\nYes\n\n\nNIU (not in universe)\n\n\n4149\n\n\nPharmacy/chemist/drug shop\n\n\nPrivate\n\n\n4092\n\n\nNo\n\n\n4092\n\n\n4149\n\n\nPharmacy/chemist/drug shop\n\n\nPrivate\n\n\n4092\n\n\nNo\n\n\nNIU (not in universe)\n\n\n4149\n\n\nPharmacy/chemist/drug shop\n\n\nPrivate\n\n\n4092\n\n\nNo\n\n\nNIU (not in universe)\n\n\n\nEach of the three facilities in our example serves up to three enumeration areas shown in EASERVED; if a facility serves fewer than three, the remaining values are NIU (not in universe). We’re now able to count the number of “nutritional facilities” serving each enumeration area, and we can easily attach those counts to a household sample from the same year. Let’s join a count of the “nutritional facilities” N_NUTFAC with a count of sampled household members N_PERSONID living in each service area represented by our three example facilities.\n\n\nmerged <- sdp %>% \n  filter(EASERVED != \"NIU (not in universe)\") %>% \n  group_by(EASERVED) %>% \n  summarise(N_NUTFAC = sum(NUTFAC == \"Yes\")) %>%\n  left_join(hh %>% rename(EASERVED = EAID), by = \"EASERVED\") %>% \n  group_by(EASERVED, N_NUTFAC) %>% \n  summarise(\n    .groups = \"keep\",\n    N_PERSONID = n_distinct(PERSONID)\n  )\n\n\n\n\n\n\nEASERVED\n\n\nN_NUTFAC\n\n\nN_PERSONID\n\n\n4036\n\n\n2\n\n\n565\n\n\n4092\n\n\n2\n\n\n641\n\n\n4819\n\n\n1\n\n\n668\n\n\n\nCounting the number of “nutritional facilities” is only the beginning - you’ll find variables dealing with a range of service measures, like:\navailability of specific nutrition services\nmalnutrition monitoring and supplementary food assistance\nmaternal and pediatric care\nstaffing and community health workers\nmedicines\nfees\nNext steps\nIn two weeks, we’ll continue our discussion of PMA nutrition data as we dig into some of the common ways that researchers assess infant and young child feeding (IYCF) practices. Specifically, we’ll show how to construct indicators for Minimum Dietary Diversity (MDD) and Minimum Meal Frequency (MMF) used by the World Health Organization (WHO).\nIn the meantime, we hope you’ll check out PMA nutrition data on the IPUMS PMA website and begin exploring variables to use in your own analysis projects. As always, leave a comment below or reach out to us on Twitter with your questions.\n\n\n\n",
    "preview": "posts/2021-09-01-nutrition-discovery/images/unitofanalysis.png",
    "last_modified": "2021-09-17T14:26:15-05:00",
    "input_file": {},
    "preview_width": 2062,
    "preview_height": 1654
  },
  {
    "path": "posts/2021-08-15-covid-analysis/",
    "title": "Multinomial Regression with Complex Survey Design",
    "description": "The PMA COVID-19 survey is part of a broader panel study. We discuss how to merge it with the baseline survey, and how to specify sample weights and cluster information with the new svyVGAM package.",
    "author": [
      {
        "name": "Devon Kristiansen",
        "url": "https://www.linkedin.com/in/devonkristiansen"
      },
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-08-15",
    "categories": [
      "COVID-19",
      "Data Analysis",
      "svyVGAM",
      "Multinomial Logit"
    ],
    "contents": "\n\nContents\nSetup\nSampling Weights\nChange in purchasing power\nMultinomial modeling\nWrap-up\n\nThroughout our discussion of the PMA COVID-19 survey this summer, we’ve mentioned that PMA COVID-19 data were collected in telephone interviews with women following a baseline survey as part of an ongoing panel study focused on family planning and reproductive health.\nWhile the scope of the COVID-19 survey is narrower than the baseline survey, it does contain several repeated questions designed to help researchers evaluate change over the first few months of the pandemic. For example, both surveys contain questions about women’s financial independence and the dynamics of household decision-making. In the baseline survey, you’ll find a variable called HHDECDAILY representing a question that asks:\nThe baseline survey was administered in-person between November 2019 and February 2020.\nThe COVID-19 follow up administered by telephone between May and August 2020.\nWho usually makes decisions about making household purchases for daily \nneeds: you, your husband/partner, you and your husband/partner jointly, \nor someone else?\n\n[] Respondent\n[] Husband/partner\n[] Respondent and husband/partner\n[] Someone else\n[] No response\nIn the covid survey, a nearly identical question asks women to reflect on changes that happened since COVID-19 restrictions began. Responses are recorded in CVBUYDECIDER:\nSince the Coronavirus (COVID-19) restrictions began, who usually makes\ndecisions about making household purchases for daily needs: you, your\nhusband/partner, you and your husband/partner jointly, or someone else?\n\n[] Respondent\n[] Husband/partner\n[] Respondent and husband/partner\n[] Someone else\n[] No response\nOne way you might model change over time with these variables is to look at women’s net change in purchasing power with a derived factor that takes the value:\n“More” if a woman has gained autonomy,\n“Less” if she has lost autonomy,\n“Same” if she experienced no change, or\nNA if either HHDECDAILY or CVBUYDECIDER is not available\nIn this post, we’ll demonstrate how to create and model this type of variable by merging the baseline and COVID-19 surveys together. We’ve modeled continuous and binary variables in previous posts, but we’ll need a different approach for a dependent variable with three outcomes: in this case, we’ll fit a multinomial logit model for the odds that a woman will experience the “Less” or “More” autonomy relative to maintaining the “Same” level of autonomy. This task is complicated by two things:\n\nIf you’d like to explore an ordered logit model for these outcomes, you can use the same tools we highlight in this post. Check out this guide for testing the “proportional odds assumption” underlying ordered logit models.\nOnly around half of the women who participated in the in-person baseline survey were reached by telephone for the COVID-19 follow up. We’ll show how changes in the sample design introduce bias to the merged data, and we’ll explain how to mitigate that bias with a sampling weight called CVQWEIGHT.\nPMA screens households for reproductive age women within geographically defined sampling clusters represented by the variable EAID. Our multinomial model should include cluster-robust standard error estimates - in other words, we want to account for correlation between neighboring women who live in the same cluster.\nUntil recently, popular R packages for multinomial regression had no built-in way to handle elements of complex survey design. Fortunately, the new svyVGAM package offers a simple interface built on the very same survey package we’ve featured throughout this blog. We’ll demonstrate fitting a multinomial model with CVQWEIGHT and EAID here, but this package also provides a general interface to any generalized linear / additive model supported by the underlying VGAM package.\nSetup\nWe’ve mentioned in previous posts that you’ll find harmonized PMA COVID-19 data if you navigate to the COVID-19 unit of analysis on the IPUMS PMA website. In this post, we’ll use an extract containing all four of the available COVID-19 samples (Female Respondents only):\nBurkina Faso 2020\nDemocratic Republic of Congo (DRC) 2020 - Kinshasa\nKenya 2020\nNigeria 2020 - Lagos and Kano\nData from the baseline surveys are located under the “Family Planning - Person” unit of analysis. Because data extracts can only contain samples with the same unit of analysis, you’ll need to create and download two separate extracts. Once you’ve downloaded an extract containing COVID-19 data, switch units of analysis by clicking the “CHANGE” link in the “Currently Browsing” window at the top of your screen:\n\n\nknitr::include_graphics(\"images/change.png\")\n\n\n\n\nThen, select the “Family Planning - Person” unit of analysis in this menu:\n\n\nknitr::include_graphics(\"images/bchoose.png\")\n\n\n\n\nYou’ll need to create an extract that contains these samples (again, Female Respondents only):\nBurkina Faso 2020\nDemocratic Republic of Congo (DRC) 2019a - Kinshasa\nKenya\nNigeria 2019a - Kano\nNigeria 2019b - Lagos\nIn the example workflow shown below, we’ve downloaded both extracts and saved them together in the “data” folder of our R working directory. We’ve also installed and loaded the following packages:\n\n\nknitr::opts_chunk$set(echo = TRUE)\noptions(tibble.print_min = 30)\n\n\n\n\n\nlibrary(tidyverse)   # 1.3.1\nlibrary(broom)       # 0.7.6\nlibrary(ipumsr)      # 0.4.5\nlibrary(srvyr)       # 1.0.1\nlibrary(svyVGAM)     # 1.0\nlibrary(kableExtra)  # 1.3.4\nlibrary(ggalluvial)  # 0.12.3\nlibrary(dotwhisker)  # 0.6.0\n\n\n\nNow, we’ll load both of our data extracts into R. We’ll create two separate dataframes named covid and baseline (we’ll also edit the variable COUNTRY in each extract for additional clarity):\n\n\ncovid <- read_ipums_micro(\n  ddi = \"data/pma_00032.xml\",\n  data_file = \"data/pma_00032.dat.gz\"\n) %>% \n  mutate(COUNTRY = as_factor(COUNTRY %>% lbl_relabel(\n    lbl(2, \"DRC (Kinshasa)\") ~ .val == 2, \n    lbl(9, \"Nigeria (Lagos/Kano)\") ~ .val == 9\n  )))  \n\nbaseline <- read_ipums_micro(\n  ddi = \"data/pma_00039.xml\",\n  data_file = \"data/pma_00039.dat.gz\"\n) %>% \n  mutate(COUNTRY = as_factor(COUNTRY %>% lbl_relabel(\n    lbl(2, \"DRC (Kinshasa)\") ~ .val == 2, \n    lbl(9, \"Nigeria (Lagos/Kano)\") ~ .val == 9\n  )))  \n\n\n\nOne last thing here: we’ll be making several graphs to help illustrate our workflow in this post. We’ll create a custom theme theme_pma() for those graphs so that the fonts, colors, and other design elements match the overall look of this blog.\n\n\nlibrary(showtext)\nfont_add(\n  family = \"cabrito\", \n  regular = \"../../fonts/cabritosansnormregular-webfont.ttf\"\n)\nshowtext_auto()\n\ntheme_pma <- function(title, subtitle = NULL, x = NULL, legend.position){\n  components <- list(\n    theme_minimal() %+replace% \n      theme(\n        text = element_text(family = \"cabrito\", size = 14), \n        plot.title = element_text(\n          size = 18, color = \"#00263A\", margin = margin(b = 10)\n        ), \n        plot.subtitle = element_text(\n          size = 16, color = \"#00263A\", margin = margin(b = 5)\n        ),\n        legend.title = element_blank(),        \n        legend.position = legend.position,\n        strip.text.y = element_text(size = 12, angle = 0),\n        panel.spacing = unit(2, \"lines\")\n      ),\n    scale_fill_manual(\n      values = alpha(\n        colour =  c(\n          \"#00263A\",   # IPUMS Navy\n          \"#7A99AC\",   # IPUMS Blue Grey\n          \"#BDD2DE\",   # IPUMS Medium Grey\n          \"#98579B\",   # PMA Pink\n          \"#CCBA72\",   # Tan\n          \"#81A88D\"    # Green\n        )\n      ),\n      na.value = \"#F0E6F0\"\n    ),\n    scale_color_manual(\n      values = alpha(\n        alpha = .85,   # .85 creates bars that are 15% transparent\n        colour = c(\n          \"#98579B\",   # PMA Pink\n          \"#00263A\"    # IPUMS Navy\n        )\n      )\n    ),\n    labs(\n      title = toupper(title), \n      subtitle = subtitle, \n      x = x, \n      y = NULL, \n      fill = NULL\n    ),\n    guides(fill = guide_legend(reverse = TRUE, byrow = TRUE))\n  )\n}\n\n\n\n\nWe discuss custom themes in detail here\nSampling Weights\nRemember that interviews for the baseline survey were administered in-person, but the covid interviews were administered via telephone. Suppose we ignored the sampling weight CVQWEIGHT completely: how might the requirement of access to a telephone bias the combined sample of women who participated in both surveys?\nLet’s take a look at a few of the demographic variables that appear in both baseline and covid. We’ll use imap_dfr() build a data frame by iterating over both of the data extracts, and we’ll use summarise() to create the following summary statistics:\nRESPONDENTS - the total number of women in the sample\nMEAN AGE - the mean age of women in the sample\nPCT URBAN - the percentage of sampled women living in “urban” areas\nPCT NO SCHOOL - the percentage of sampled women who “never attended” school\nPCT MARRIED - the percentage of sampled women who are currently married\n\n\nsamples_tbl <- list(baseline, covid) %>% \n  imap_dfr(\n    ~.x %>% \n      group_by(COUNTRY) %>%\n      summarise(\n        SAMPLE = if_else(.y == 1, \"baseline\", \"covid\"),\n        RESPONDENTS = n(),\n        `MEAN AGE` = mean(AGE),\n        `PCT URBAN` = 100 * mean(URBAN), \n        `PCT NO SCHOOL` = 100 * mean(EDUCATTGEN <= 2), \n        `PCT MARRIED` = 100 * mean(MARSTAT == 21) \n      ) \n  ) %>% \n  arrange(COUNTRY)\n\n\n\n\nThe variable URBAN is not available for the DRC sample, which represents the Kinshasa region. We’ll assume that all women in Kinshasa live in an “urban” area in our analysis below.\n\n\nsamples_tbl %>% kbl(digits = 1) \n\n\n\nCOUNTRY\n\n\nSAMPLE\n\n\nRESPONDENTS\n\n\nMEAN AGE\n\n\nPCT URBAN\n\n\nPCT NO SCHOOL\n\n\nPCT MARRIED\n\n\nBurkina Faso\n\n\nbaseline\n\n\n6765\n\n\n28.5\n\n\n59.9\n\n\n64.1\n\n\n59.6\n\n\nBurkina Faso\n\n\ncovid\n\n\n3528\n\n\n29.7\n\n\n74.7\n\n\n54.3\n\n\n62.0\n\n\nDRC (Kinshasa)\n\n\nbaseline\n\n\n2634\n\n\n28.3\n\n\nNA\n\n\n8.0\n\n\n26.6\n\n\nDRC (Kinshasa)\n\n\ncovid\n\n\n1324\n\n\n29.5\n\n\nNA\n\n\n3.3\n\n\n34.0\n\n\nKenya\n\n\nbaseline\n\n\n9549\n\n\n28.8\n\n\n35.3\n\n\n49.5\n\n\n53.5\n\n\nKenya\n\n\ncovid\n\n\n5986\n\n\n31.1\n\n\n38.2\n\n\n45.4\n\n\n62.1\n\n\nNigeria (Lagos/Kano)\n\n\nbaseline\n\n\n2627\n\n\n29.5\n\n\n76.4\n\n\n32.3\n\n\n62.0\n\n\nNigeria (Lagos/Kano)\n\n\ncovid\n\n\n1346\n\n\n31.4\n\n\n87.8\n\n\n16.3\n\n\n63.5\n\n\nComparing the two samples for each country, we see that the covid follow up represents a sub-sample of the baseline participants that is generally a bit older (adjusting for the passage of time), more urban, more educated, and are more likely to be currently married. If we built a multinomial model without sample weights, our predictions would reflect these biases as well.\nFortunately, the variable CVQWEIGHT corrects the baseline sample weight FQWEIGHT for predicted telephone access and other sources of loss to follow up. If we use the correct weights for each sample in our imap_dfr() function above, we should produce similar estimates for each sample’s target population:\n\nRead more about the construction of CVQWEIGHT here.\n\n\npopulations_tbl <- list(baseline, covid) %>% \n  imap_dfr(~{\n    if(\"FQWEIGHT\" %in% names(.x)){\n      wt <- \"FQWEIGHT\"\n    } else {\n      wt <- \"CVQWEIGHT\"\n    }\n    .x %>% \n      as_survey_design(weights = !!wt, ids = EAID) %>% \n      group_by(COUNTRY) %>% \n      summarise(\n        SAMPLE = if_else(.y == 1, \"baseline\", \"covid\"),\n        `MEAN AGE` = survey_mean(AGE, vartype = NULL),\n        `PCT URBAN` = 100 * survey_mean(URBAN, vartype = NULL), \n        `PCT NO SCHOOL` = 100 * survey_mean(EDUCATTGEN == 1, vartype = NULL), \n        `PCT MARRIED` = 100 * survey_mean(MARSTAT == 21, vartype = NULL)   \n      ) \n  }) %>% \n  arrange(COUNTRY)\n\n\n\nTip: If you would like to produce confidence intervals for each of these population-level estimates, use vartype == \"ci\". Note that EAID has no impact on the point estimates we’ve shown.\n\n\npopulations_tbl %>% kbl(digits = 1) \n\n\n\nCOUNTRY\n\n\nSAMPLE\n\n\nMEAN AGE\n\n\nPCT URBAN\n\n\nPCT NO SCHOOL\n\n\nPCT MARRIED\n\n\nBurkina Faso\n\n\nbaseline\n\n\n28.9\n\n\n22.8\n\n\n59.9\n\n\n68.6\n\n\nBurkina Faso\n\n\ncovid\n\n\n27.9\n\n\n21.4\n\n\n55.1\n\n\n68.9\n\n\nDRC (Kinshasa)\n\n\nbaseline\n\n\n28.3\n\n\nNA\n\n\n0.4\n\n\n27.0\n\n\nDRC (Kinshasa)\n\n\ncovid\n\n\n28.9\n\n\nNA\n\n\n0.4\n\n\n31.4\n\n\nKenya\n\n\nbaseline\n\n\n28.8\n\n\n30.2\n\n\n4.5\n\n\n53.3\n\n\nKenya\n\n\ncovid\n\n\n28.9\n\n\n27.5\n\n\n1.9\n\n\n54.3\n\n\nNigeria (Lagos/Kano)\n\n\nbaseline\n\n\n29.7\n\n\n72.1\n\n\n22.6\n\n\n64.1\n\n\nNigeria (Lagos/Kano)\n\n\ncovid\n\n\n30.4\n\n\n81.9\n\n\n9.8\n\n\n63.3\n\n\nHere, we’ve used as_survey_design() to specify the correct weight wt in each iteration of imap_dfr(). Then, we used survey_mean() to estimate the mean / proportion for each sample’s target population. Although you might observe some differences between the point estimates derived from baseline and covid, the systematic biases we identified in the samples are no longer present. This will be easier to see in a faceted lollipop plot where we chart both summary tables side-by-side.\n\n\npopulations_tbl %>%  \n  mutate(TBL = \"weighted\") %>% \n  rbind(\n    samples_tbl %>% select(-RESPONDENTS) %>% mutate(TBL = \"unweighted\")\n  ) %>% \n  pivot_longer(-c(COUNTRY, SAMPLE, TBL)) %>% \n  ggplot(aes(x = value, y = TBL, color = SAMPLE)) + \n  facet_grid(\n    rows = vars(COUNTRY), \n    cols = vars(name),\n    scale = \"free_x\",\n    as.table = TRUE\n  ) +\n  geom_point(size = 3) + \n  geom_line(aes(group = TBL), alpha = .5, color = \"#000000\") + \n  theme_pma(\"Sample design: weighted vs unweighted\", legend.position = \"top\")\n\n\n\n\nIn nearly every case, the gap between baseline and covid is narrower for the weighted population estimates than it is for the unweighted sample statistics.\nFor example, consider the summary statistic PCT URBAN. 59.9% of the women in the Burkina Faso baseline sample reside in urban areas, compared to 74.7% of women in the covid sample. That’s a 14.8% difference! With weights applied, we use baseline to estimate that only 22.8% of women in Burkina Faso actually live in urban areas; compare this to the 21.4% estimate produced from covid. The difference between the two population estimates is much smaller - just 1.4%.\nChange in purchasing power\nWe’ll now focus our attention only on the respondents who participated in both baseline and covid as we explore changes that occurred during the intervening months. We’ll create a merged dataset that contains one row per person and drops anyone who didn’t participate in the covid survey. We should also preserve duplicate variables - or variables that appear in both extracts; we’ll need to change their names so that it’s easy to determine which version came from which survey.\nEither left_join() or right_join() from the dplyr package can be used here; the only difference is that left_join() discards rows from the data provided to “y”, while right_join() discards rows from the data provided to “x” (we could put baseline in either position). We’ll use left_join().\nEach participant in baseline will have a unique identification code in FQINSTID that appears again in covid. So, we’ll tell left_join() to merge rows connected by the same FQINSTID with by = \"FQINSTID\"\nFinally, we’ll use the suffix argument to handle variables that appear in both extracts. For example, AGE from covid will be renamed AGE_CV, while AGE from baseline will be renamed AGE_BASE.\n\n\nmerged <- left_join(\n  x = covid,\n  y = baseline,\n  by = \"FQINSTID\",\n  suffix = c(\"_CV\", \"_BASE\")\n) \n\n\n\nThe variables we’ll use to construct our model’s dependent variable - HHDECDAILY and CVBUYDECIDER - will now appear together in the same row for each woman who participated in both surveys. For both variables, the universe is women who were currently married or living with male partner. Before we get started, let’s take a quick look at an alluvial plot showing the general shifts from one variable to the other:\n\n\nmerged %>% \n  select(FQINSTID, HHDECDAILY, CVBUYDECIDER) %>% \n  pivot_longer(!FQINSTID) %>% \n  mutate(\n    name = if_else(name == \"HHDECDAILY\", \"Before COVID-19\", \"Summer 2020\") %>% \n      as_factor() %>% \n      fct_rev(),\n    value = value %>% \n      as_factor() %>% \n      fct_recode(\n        \"Resdponent\" = \"1\", \"Husband/partner\" = \"2\", \"Joint\" = \"3\",\n        \"Someone else\" = \"4\", \"No response\" = \"98\", \"Not partnered\" =  \"99\"\n      ) %>% \n      fct_rev()\n  ) %>% \n  ggplot(aes(\n    x = name,\n    stratum = value,\n    fill = value,\n    alluvium = FQINSTID\n  )) + \n  geom_stratum(alpha = 0.7) + \n  geom_flow() +\n  coord_flip() + \n  theme_pma(\n    title = \"Who usually makes decisions about \\n household purchases for daily needs?\",\n    subtitle = \"Female respondents to the PMA baseline and COVID-19 surveys\",\n    legend.position = \"bottom\"\n  ) \n\n\n\n\nHere, we see that a majority of women who independently made purchasing decisions at baseline began either making decisions jointly or deferred to their husband/partner or someone else after COVID-19 restrictions began. At the same time, a sizable portion of the women who made decisions jointly at baseline no longer reported any role in the covid survey. We’ll say that all of these women have “Less” purchasing power when we create our dependent variable.\nSome women experienced change in the opposite direction: these include women who gained some authority after having none at baseline, and also women who previously made decisions jointly, but now bear bear full responsibility. We’ll say that these women have “More” purchasing power.\nFinally, all other women will be said to have the “Same” level of purchasing power unless either variable contains “No response or missing” or “NIU (not in universe)”. In the latter case, we’ll use the value NA. This means we will be analyzing a sample of women who were married or living with a male partner both before and during COVID-19 restrictions.\n\n\nmerged <- merged %>% \n   mutate(PURCHASE_PWR = factor(\n     case_when(\n       HHDECDAILY == 1 & CVBUYDECIDER %in% 2:4 ~ \"Less\",\n       HHDECDAILY == 3 & CVBUYDECIDER %in% c(2, 4) ~  \"Less\",\n       HHDECDAILY == 2 & CVBUYDECIDER %in% c(1, 3) ~ \"More\",\n       HHDECDAILY == 3 & CVBUYDECIDER == 1 ~ \"More\",\n       HHDECDAILY == 4 & CVBUYDECIDER %in% c(1, 3) ~ \"More\",\n       HHDECDAILY < 90 & CVBUYDECIDER < 90 ~ \"Same\"\n     ),\n     levels = c( \"Less\", \"Same\", \"More\")\n   ))\n\n\n\nMultinomial modeling\nLet’s now see what our derived variable PURCHASE_PWR can tell us about each sample’s target population. As shown above, we’ll again specify the sampling weight CVQWEIGHT and the cluster IDs - this time using the COVID-19 version in EAID_CV - as survey design information; we’ll then pass this information to a survey_mean() summary function to produce point-estimates and a cluster-robust 95% confidence interval for each level of PURCHASE_POWER. We’ll plot these estimates in a grouped bar chart, omitting the percentage of women who are not married / cohabitating with a partner.\n\nKeep in mind: only the Burkina Faso and Kenya samples are nationally representative. The DRC and Nigeria samples represent sub-national regions.\n\n\nmerged %>% \n  as_survey_design(\n    weights = CVQWEIGHT,\n    ids = EAID_CV\n  ) %>% \n  group_by(COUNTRY_BASE, PURCHASE_PWR) %>% \n  summarise(est = 100 * survey_mean(vartype = \"ci\")) %>% \n  filter(!is.na(PURCHASE_PWR)) %>% \n  ggplot(aes(x = COUNTRY_BASE, y = est, fill = PURCHASE_PWR)) + \n  geom_bar(width = .6, position = position_dodge(0.6), stat = \"identity\") +\n  geom_errorbar(\n    aes(ymin = est_low, ymax = est_upp),\n    width = 0.2,\n    position = position_dodge(0.6),\n    alpha = 0.5\n  ) + \n  theme_pma(\n    title = \"Net change in purchasing power for daily needs\",\n    subtitle = \"Estimated percentages for populations of women age 15-49\",\n    legend.position = \"bottom\"\n  ) + \n  coord_flip()\n\n\n\n\nThe percentage of women in each population who gained “More” purchasing power after COVID-19 restrictions began is significantly lower than the percentage of women who maintained the “Same” or experienced “Less” purchasing power (we know this because the 95% confidence intervals do not overlap). The comparison between “Same” and “Less” is not so clear: while the point estimates suggest that there may be fewer women with “Less” purchasing power, the difference is not statistically significant (except in Kenya, where the two confidence intervals do not overlap).\nTo demonstrate a multinomial model for these outcomes, we’ll select a set of variables that describe demographic features and economic circumstances for each woman. As a reminder: for variables that appeared in both baseline and covid, we’ll need to specify which to include in our model by appending the suffix _BASE or _CV - for the most part, we’ll use the baseline version. We’ll include these covariates (modifying / recoding as needed):\nAGE_BASE - age at baseline\nURBAN_BASE - lived in an urban area at baseline: TRUE or FALSE\nWEALTHT - wealth score tertile: Highest, Middle, Lowest (reference group)\nRELIGION - religion of the household head recoded as Muslim, Christian, Other (reference group)\nPARTNERECON indicates whether married / partnered women said on the covid survey that there were “currently economically reliant” on their husband / partner for basic needs: TRUE or FALSE\nWORKWK indicates whether women indicated on the baseline survey that she had done any work outside the home in the past week: TRUE or FALSE\n\n\nmerged <- merged %>% \n  mutate(\n    RELIGION = factor(\n      case_when(\n        RELIGION == 100 ~ \"Muslim\",\n        RELIGION %in% 200:300 ~ \"Christian\",\n        RELIGION < 990 ~ \"Other\"\n      ),\n      levels = c(\"Other\", \"Muslim\", \"Christian\")\n    ),\n    WEALTHT = as_factor(WEALTHT),\n    URBAN_BASE = case_when(\n      COUNTRY_BASE == \"DRC (Kinshasa)\" ~ TRUE,\n      URBAN_BASE == 1 ~ TRUE,\n      URBAN_BASE == 0 ~ FALSE\n    ),\n    across(c(PARTNERECON, WORKWK), ~case_when(\n      .x == 1 ~ TRUE,\n      .x == 0 ~ FALSE\n    ))\n  ) \n\n\n\nThe svyVGAM package can interpret the very same call to as_survey_design() shown above. We’ll use the generic modeling function svy_vglm() to build our model; it looks similar to the svyglm function we’ve used to build linear and binary logit models elsewhere, except that the argument family takes a function describing the type of model we want to fit. Use multinomial(refLevel = \"Same\") to specify a multinomial model with “Same” as a reference group:\n\nSee package documentation for other supported modeling functions\n\n\npma_model <- merged %>%  \n  as_survey_design(\n    weights = CVQWEIGHT,\n    id = EAID_CV\n  ) %>% \n  svy_vglm(\n    formula = PURCHASE_PWR ~ \n      AGE_BASE +\n      RELIGION +\n      WEALTHT +\n      URBAN_BASE +\n      PARTNERECON + \n      WORKWK + \n      COUNTRY_BASE,\n    design = ., \n    family = multinomial(refLevel = \"Same\")\n  ) \n\n\n\nOne drawback here is that the output of a model created by svyVGAM (or by the VGAM package) cannot be handled by broom::tidy(), which we normally use to clean and standardize model output. Fortunately, broom contains a non-exported function that’s capable of handling something similar: the model output created by svyglm() from the sibling survey package! You’ll find it in the broom namespace if you use ::: like so:\n\n\nbroom:::tidy.svyglm\n\n\nfunction (x, conf.int = FALSE, conf.level = 0.95, exponentiate = FALSE, \n    ...) \n{\n    ret <- as_tibble(summary(x)$coefficients, rownames = \"term\")\n    colnames(ret) <- c(\"term\", \"estimate\", \"std.error\", \"statistic\", \n        \"p.value\")\n    coefs <- tibble::enframe(stats::coef(x), name = \"term\", value = \"estimate\")\n    ret <- left_join(coefs, ret, by = c(\"term\", \"estimate\"))\n    if (conf.int) {\n        ci <- broom_confint_terms(x, level = conf.level, ...)\n        ret <- dplyr::left_join(ret, ci, by = \"term\")\n    }\n    if (exponentiate) {\n        ret <- exponentiate(ret)\n    }\n    ret\n}\n<bytecode: 0x7feeb0bb15f8>\n<environment: namespace:broom>\n\nWe’ll modify this function a bit, creating our own tidy method tidy.svyVGAM() (you may need to make modifications for general use):\n\n\ntidy.svyVGAM <- function(\n  x, \n  conf.int = FALSE, \n  conf.level = 0.95,\n  exponentiate = FALSE, \n  ...\n){\n  # Replace `summary(x)$coefficients` with `summary(x)$coeftable`\n  ret <- as_tibble(summary(x)$coeftable, rownames = \"term\")\n  \n  # All of this stays the same:\n  colnames(ret) <- c(\"term\", \"estimate\", \"std.error\", \"statistic\", \"p.value\")\n  coefs <- tibble::enframe(stats::coef(x), name = \"term\", value = \"estimate\")\n  ret <- left_join(coefs, ret, by = c(\"term\", \"estimate\"))\n  if (conf.int){\n    ci <- broom:::broom_confint_terms(x, level = conf.level, ...)\n    ret <- dplyr::left_join(ret, ci, by = \"term\")\n  }\n  if (exponentiate){ret <- broom:::exponentiate(ret)}\n  \n  # This part only works for the multinomial case, and only if your covariates\n  # have no \":\" in their names - NOT FOR GENERAL USE\n  ret %>% \n    separate(term, into = c(\"term\", \"y.level\"), sep = \":\") %>% \n    arrange(y.level) %>% \n    relocate(y.level, .before = term)\n}\n\n\n\nNow, let’s look at our model results. We’ll exponentiate our coefficients, add significance “stars”, and label each y.level manually to make this table easier to read:\n\n\ntidy_pma_model <- tidy.svyVGAM(pma_model, exponentiate = TRUE, conf.int = TRUE)\n\n\n\n\n\ntidy_pma_model %>% \n  select(-y.level) %>% \n  rename(PURCHASE_PWR = term) %>% \n  mutate(sig = case_when(\n    p.value < 0.001 ~ \"\\\\*\\\\*\\\\*\",\n    p.value < 0.01 ~ \"\\\\*\\\\*\",\n    p.value < 0.05 ~ \"\\\\*\",\n    T ~ \"\"\n  )) %>% \n  kbl(digits = 3) %>% \n  pack_rows(\"Less\", 1, 12) %>% \n  pack_rows(\"More\", 13, 24)\n\n\n\nPURCHASE_PWR\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\nconf.low\n\n\nconf.high\n\n\nsig\n\n\nLess\n\n\n(Intercept)\n\n\n1.102\n\n\n0.302\n\n\n0.322\n\n\n0.747\n\n\n0.609\n\n\n1.994\n\n\n\n\nAGE_BASE\n\n\n1.001\n\n\n0.005\n\n\n0.100\n\n\n0.920\n\n\n0.991\n\n\n1.011\n\n\n\n\nRELIGIONMuslim\n\n\n0.602\n\n\n0.212\n\n\n-2.398\n\n\n0.016\n\n\n0.398\n\n\n0.912\n\n\n*\n\n\nRELIGIONChristian\n\n\n0.823\n\n\n0.149\n\n\n-1.312\n\n\n0.189\n\n\n0.615\n\n\n1.101\n\n\n\n\nWEALTHTMiddle tertile\n\n\n0.875\n\n\n0.107\n\n\n-1.244\n\n\n0.214\n\n\n0.709\n\n\n1.080\n\n\n\n\nWEALTHTHighest tertile\n\n\n0.939\n\n\n0.121\n\n\n-0.523\n\n\n0.601\n\n\n0.741\n\n\n1.189\n\n\n\n\nURBAN_BASETRUE\n\n\n1.176\n\n\n0.116\n\n\n1.398\n\n\n0.162\n\n\n0.937\n\n\n1.476\n\n\n\n\nPARTNERECONTRUE\n\n\n1.465\n\n\n0.094\n\n\n4.055\n\n\n0.000\n\n\n1.218\n\n\n1.762\n\n\n***\n\n\nWORKWKTRUE\n\n\n1.158\n\n\n0.084\n\n\n1.756\n\n\n0.079\n\n\n0.983\n\n\n1.365\n\n\n\n\nCOUNTRY_BASEDRC (Kinshasa)\n\n\n0.585\n\n\n0.221\n\n\n-2.429\n\n\n0.015\n\n\n0.380\n\n\n0.902\n\n\n*\n\n\nCOUNTRY_BASEKenya\n\n\n0.676\n\n\n0.156\n\n\n-2.503\n\n\n0.012\n\n\n0.497\n\n\n0.919\n\n\n*\n\n\nCOUNTRY_BASENigeria (Lagos/Kano)\n\n\n0.763\n\n\n0.201\n\n\n-1.348\n\n\n0.178\n\n\n0.514\n\n\n1.131\n\n\n\n\nMore\n\n\n(Intercept)\n\n\n0.677\n\n\n0.306\n\n\n-1.277\n\n\n0.201\n\n\n0.372\n\n\n1.232\n\n\n\n\nAGE_BASE\n\n\n0.991\n\n\n0.006\n\n\n-1.451\n\n\n0.147\n\n\n0.980\n\n\n1.003\n\n\n\n\nRELIGIONMuslim\n\n\n1.525\n\n\n0.240\n\n\n1.760\n\n\n0.078\n\n\n0.953\n\n\n2.439\n\n\n\n\nRELIGIONChristian\n\n\n1.069\n\n\n0.207\n\n\n0.321\n\n\n0.749\n\n\n0.712\n\n\n1.603\n\n\n\n\nWEALTHTMiddle tertile\n\n\n0.941\n\n\n0.122\n\n\n-0.494\n\n\n0.621\n\n\n0.741\n\n\n1.196\n\n\n\n\nWEALTHTHighest tertile\n\n\n0.773\n\n\n0.140\n\n\n-1.848\n\n\n0.065\n\n\n0.588\n\n\n1.016\n\n\n\n\nURBAN_BASETRUE\n\n\n1.266\n\n\n0.129\n\n\n1.824\n\n\n0.068\n\n\n0.983\n\n\n1.631\n\n\n\n\nPARTNERECONTRUE\n\n\n0.693\n\n\n0.107\n\n\n-3.420\n\n\n0.001\n\n\n0.562\n\n\n0.855\n\n\n***\n\n\nWORKWKTRUE\n\n\n0.638\n\n\n0.107\n\n\n-4.209\n\n\n0.000\n\n\n0.518\n\n\n0.787\n\n\n***\n\n\nCOUNTRY_BASEDRC (Kinshasa)\n\n\n0.739\n\n\n0.233\n\n\n-1.297\n\n\n0.195\n\n\n0.468\n\n\n1.167\n\n\n\n\nCOUNTRY_BASEKenya\n\n\n0.901\n\n\n0.149\n\n\n-0.704\n\n\n0.482\n\n\n0.673\n\n\n1.205\n\n\n\n\nCOUNTRY_BASENigeria (Lagos/Kano)\n\n\n0.918\n\n\n0.203\n\n\n-0.420\n\n\n0.675\n\n\n0.617\n\n\n1.367\n\n\n\n\nOnce you’ve got the model output in a tidy format, you can also pass it to a tidyverse aligned graphing function, like the popular dwplot from dotwhisker:\n\n\ntidy_pma_model %>% \n  mutate(\n    model = if_else(\n      y.level == 1, \n      \"Less Purchasing Power\",\n      \"More Purchasing Power\", \n    ),\n    sig = gtools::stars.pval(p.value)\n  ) %>%\n  relabel_predictors(c(\n    AGE_BASE = \"Age at baseline\",\n    RELIGIONMuslim = \"Muslim Head of Household\",\n    RELIGIONChristian = \"Christian Head of Household\",\n    `WEALTHTMiddle tertile` = \"Middle Wealth Tertile\",\n    `WEALTHTHighest tertile` = \"Highest Wealth Tertile\",\n    URBAN_BASETRUE = \"Urban Residence\",\n    `COUNTRY_BASEDRC (Kinshasa)` = \"DRC (Kinshasa)\",\n    COUNTRY_BASEKenya = \"Kenya\",\n    `COUNTRY_BASENigeria (Lagos/Kano)` = \"Nigeria (Lagos/Kano)\",\n    PARTNERECONTRUE = \"Reliant on partner\",\n    WORKWKTRUE = \"Work outside home\"\n  )) %>% \n  dotwhisker::dwplot(\n    dodge_size = 0.3,\n    vline = geom_vline(xintercept = 1, colour = \"grey60\", linetype = 2)\n  ) + \n  guides(color = guide_legend(reverse = TRUE)) + \n  theme_pma(\n    title = \n      \"Which factors impact women's purchasing power during COVID-19?\", \n    subtitle = \n      'Odds ratios relative to \"no change\" (95% CI)',\n    legend.position = \"top\"\n  ) \n\n\n\n\nBecause we’ve exponentiated the coefficients produced by this model, each estimate shows how each covariate changes the odds of experiencing a given level of PURCHASE_PWR relative to the reference group of women who maintained the same level of autonomy since COVID-19 restrictions began. For example, an estimate of 1.0 would indicate equal odds, 2.0 would indicate double odds, and so on.\nGenerally, the effects of demographic factors in our model are pretty limited. For example, notice the proximity of both AGE_BASE estimates to 1.0. Each additional year of age at baseline is estimated to change the odds that a woman would experience “Less” purchasing power by just 0.001 - or 0.1%; this is not a statistically significant difference from 1.0, as you’ll notice that the 95% confidence interval includes values both above and below “equal odds”. The “More” estimate for AGE_BASE tells a similar story: while we estimate that each year of age at baseline contributes to a 0.9% drop in the odds that a woman would gain “More” purchasing power, this difference is not sufficiently far enough from “equal odds” to indicate statistical significance. Perhaps surprisingly, neither of the variables AGE_BASE, WEALTHT, or URBAN_BASE are significant predictors for any outcome of PURCHASE_PWR.\nOn the other hand, we see some evidence that RELIGION may have a more meaningful impact, particularly for women living with a Muslim head of household. The estimated odds that such a woman would lose purchasing power are just 60.2% of the estimated odds that she would maintain the same level. Meanwhile, her estimated odds for gaining power are 152.5% - or about one and a half times higher than - her odds of maintaining the same level. The former estimate includes a 95% confidence interval that includes no values above 1.0, but the interval for the latter estimate just barely overlaps with 1.0. Hence, only the estimate for “Less” purchasing power meets the formal requirement for statistical significance.\nCompared to demographic factors, the economic factors we’ve selected are much more powerful predictors for all levels of PURCHASE_PWR. Controlling for household wealth via WEALTHT, women who indicated on the COVID-19 survey that they were currently economically reliant on their husband/partner were both likely to have lost purchasing power, and unlikely to have gained it. Meanwhile, women who indicated on the baseline survey that they recently worked outside the home were somewhat likely to lose purchasing power, while they were unlikely to gain more. Three of the four estimates from these variables were statistically significant even at the 99.9% confidence threshold.\nHow might these conclusions change if we hadn’t bothered to include CVQWEIGHT and EAID_CV? Without CVQWEIGHT, the point estimates for each of our covariates would shift in a direction reflecting the sample biases in the COVID-19 survey. Sample cluster information specified by EAID_CV doesn’t change those point estimates, but it does impact the estimated standard error for each point estimate; if we had omitted EAID_CV, we would have generally underestimated standard error, leading to confidence intervals that are too narrow. Fortunately, the new svyVGAM package made it incredibly easy to incorporate both survey design elements into our multinomial model.\nWrap-up\nThe four samples we’ve been examining throughout this summer-long series on PMA COVID-19 data are just one part of a larger panel study you’ll find on the IPUMS PMA website in the months and years ahead. You’ll find information about samples from other countries and plans for additional follow-up surveys on the PMA survey methodology page.\nHere at the PMA Data Analysis Hub, we plan to change gears in the coming weeks as we begin a new series on PMA Nutrition Data. However, we’ll continue to focus on COVID-19 data when new surveys become available. For updates, keep an eye on the New Data tag or look for us on Twitter.\n\n\n\n",
    "preview": "posts/2021-08-15-covid-analysis/images/alluvial.png",
    "last_modified": "2021-11-15T14:34:54-06:00",
    "input_file": {},
    "preview_width": 1422,
    "preview_height": 714
  },
  {
    "path": "posts/2021-08-07-pma-intro/",
    "title": "New Online Course: Introduction to IPUMS PMA Data Analysis",
    "description": "Explore IPUMS PMA data with R right in your browser, or download Stata code for offline practice",
    "author": [],
    "date": "2021-08-07",
    "categories": [],
    "contents": "\nToday, we’re proud to announce the launch of a brand new feature on the IPUMS PMA Data Analysis Hub: our very first online course!\nWhether you’re a beginner-level data analyst exploring statistical software for the first time, or if you’re a more advanced analyst looking for a guided tour of IPUMS PMA data, we think our new Introduction to IPUMS PMA Data Analysis is a great place to start. Over time, we’ll add courses on more advanced topics in the new Courses menu in the navigation bar at the top of your screen.\n\n\n\n\nClick here to get started\nWe expect most people will probably need about an hour to complete the Introduction course, but you’re welcome to work at your own pace. You can also leave and resume your progress at any time.\nUpon completion, we offer a downloadable certificate signed by the IPUMS PMA team, plus a script containing all of the code you mastered during the course.\nRequirements\nIPUMS PMA data are free to use, and all of our online courses will be free, as well. To get started, you just need to register and agree to our Terms of Use.\nWe’ve created exercises for both R and Stata users, so you’ll need to choose a preferred language at the start of each course.\nR users: you’ll find an embedded R console where you can practice coding exercises right in your browser. We’ve pre-installed all of the packages necessary to complete each exercise, and you’ll receive interactive feedback to help you identify any mistakes along the way.\nStata users: you’ll need access to a copy of the Stata program. We’ll provide example code that you’ll run offline, and you’ll need to use the output from Stata to answer multiple choice questions in our “guided practice” section.\n\nWe’re able to offer interactive practice and feedback to R users thanks to the amazing learnr and gradethis packages for R.\nIPUMS PMA courses are hosted by Qualtrics, so you may need to deactive your browser’s ad blocker in order see course content. If you have any difficulties, please contact us!\n\n\n\n",
    "preview": "posts/2021-08-07-pma-intro/../../images/new_course.png",
    "last_modified": "2021-08-16T13:19:00-05:00",
    "input_file": {},
    "preview_width": 2999,
    "preview_height": 1687
  },
  {
    "path": "posts/2021-08-01-covid-batches/",
    "title": "Three ways to visualize binary survey data",
    "description": "You can compare levels of trust and efficacy for 13 information sources in 4 countries with data from the new PMA COVID-19 survey. Let's talk data visualization options.",
    "author": [
      {
        "name": "Tayler Nelson",
        "url": {}
      },
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-08-01",
    "categories": [
      "COVID-19",
      "Data Visualization",
      "ggplot2",
      "fmsb",
      "Bar Charts",
      "Lollipop Charts",
      "Radar Charts"
    ],
    "contents": "\n\nContents\nSetup\nLogical variables\nUsing dplyr::across\n\nPre-processing\nSummary Table\nPlot theme\n\nBar Charts\nLollipop Chart\nRadar Chart\nWrap-up\n\nIn our last post, we discussed some of the strategies you might use to compare items from the PMA COVID-19 questionnaire that use a Likert-type response scale. Another type of question used in the COVID-19 survey - and all PMA surveys - follows the “select all that apply” format. Responses from this type of question appear as a series of related binary (“yes” or “no”) variables on the IPUMS PMA website.\nIn this post, we’ll go over some of the strategies you might use to process and synthesize data from a large number of related variables, like those created from “select all that apply” style questions. First, we’ll review one of our favorite tools for manipulating lots of variables at once: the across function from dplyr. Then, we’ll explore our data in three different layouts: the faceted bar chart, the lollipop chart, and the beautiful (if controversial) radar chart.\nAs we’ll see, each of these three layouts makes it possible to compare multiple questions posed with the same list of “select all that apply” response options. In our case, we’ll look at two questions that are each related to sources of information about COVID-19. Because both questions feature the same list of 13 possible information sources, we’ll be working with 26 variables in total:\n202. How did you learn about Coronavirus (COVID-19)? Select all that apply:\n\n[] Newspaper\n[] Radio\n[] Television\n[] Poster / billboard\n[] Town crier\n[] Phone message\n[] Family\n[] Friends / neighbors\n[] Community/religious leaders\n[] Social media (Twitter, Facebook, WhatsApp)\n[] Health personnel\n[] Messages from government or authorities\n[] School\n[] None of the above\n[] No response\n204. Which of these sources do you trust for accurate information about\nCoronavirus (COVID-19)? Select all that apply:\n\n[] Newspaper\n[] Radio\n[] Television\n[] Poster / billboard\n[] Town crier\n[] Phone message\n[] Family\n[] Friends / neighbors\n[] Community/religious leaders\n[] Social media (Twitter, Facebook, WhatsApp)\n[] Health personnel\n[] Messages from government or authorities\n[] School\n[] None of the above\n[] No response\nSetup\nBefore we think about how to visualize the responses to both of these questions, we’ll need to change a few things about the way they’re coded in each of the 26 corresponding variables. Suppose we’ve downloaded an extract from the IPUMS PMA website (all four samples, Female Respondents only) and we’ve put it in the “data” folder in our working directory. We’ll load a few packages (which you’ll need to install if you haven’t done so before), and we’ll load the extract into R:\n\n\nlibrary(ipumsr)\nlibrary(tidyverse)\nlibrary(srvyr)\nlibrary(showtext)\nlibrary(gtsummary)\nlibrary(fmsb)\n\ncovid <- read_ipums_micro(\n  ddi = \"data/pma_00032.xml\",\n  dat = \"data/pma_00032.dat.gz\"\n)\n\n\n\nYou’ll find that all of the variables derived from the response options for the first question (“How did you learn about Coronavirus (COVID-19)?”) share the same prefix CVLEARN, and all of the variables derived from the response options from the second question (“Which of these sources do you trust for accurate information about Coronavirus (COVID-19)?”) share the same prefix CVTRUST. For example, you’ll find the responses for “Newspaper” recorded in CVLEARNNEWS and CVTRUSTNEWS.\n\nHarmonized data from IPUMS will feature patterned variable names wherever possible. This makes it easy to apply a function over several related variables.\nTaking a look at a variable like CVTRUSTNEWS, you’ll notice that these variables contain several different top-codes associated with different reasons for non-response:\n\n\ncovid %>% count(CVTRUSTNEWS)\n\n\n# A tibble: 5 x 2\n                  CVTRUSTNEWS     n\n                    <int+lbl> <int>\n1  0 [No]                      7071\n2  1 [Yes]                     4886\n3  2 [None of the above]        208\n4 98 [No response or missing]    10\n5 99 [NIU (not in universe)]      9\n\nThe top-codes 98 and 99 should be familiar to regular IPUMS data users: they represent women who selected the “No response” option, and women who were ineligible to respond (see the Universe tab). The code 2 represents women who trust “None of the above” sources, including “Newspaper”.\nImportantly, the number of women who selected codes 2, 98, and 99 will be the same for all variables derived from the same “select all that apply” question. While there are some analytic contexts where you might want to identify women who selected “None of the above”, this information could be collapsed with “No” responses in our plots. The simplest way to visualize data from these variables is to first transform them into simple binary indicators taking the logical values TRUE, FALSE, or NA.\nLogical variables\nIn previous posts, we’ve suggested changing labelled variables into factors for most data visualization and analysis applications. That’s because IPUMS value labels won’t appear in graphics or model summary output by default; you’ll instead see the numeric codes associated with each response (0, 1, 2, 98, 99, etc).\nBinary variables are different: it’s not always necessary to plot both the “Yes” and “No” responses, and this is particularly true where the non-response frequency is low. Look at this stacked bar chart that includes all of the available response categories, including non-response options:\n\n\n\nThe non-response options barely appear on the plot at all: they comprise less than 1% of the respondents for each sample, so their presence in the legend adds unnecessary clutter. Meanwhile, the options “No” and “None of the above” could be collapsed into a single response; however, because we’ll be plotting 26 variables like this one, we’ll find that plotting only the “Yes” responses will provide much needed visual space to the final chart. In other words, we’ll rely on the reader to infer “No” responses through the use of negative space.\nBecause we plan to plot only the proportion of “Yes” responses in each variable, we won’t need value labels displayed in a legend. This makes it unnecessary to convert our variables into factors. Instead, we’ll convert our variables into logical objects that take the values TRUE, FALSE, or NA. As we’ll see, this makes it easy to pre-filter only the cases where our variables of interest were coded 1 for “Yes”.\nAnother reason you should avoid creating binary factors is that factors are not meaningful in certain mathematical contexts. For example, we might want to exploit the mean of a binary variable as a handy way to generate the proportion of women who responded “Yes”. This won’t work if you create a factor:\n\n\nmy_factor <- factor(c(1, 0, 1))\nmean(my_factor)\n\n\n[1] NA\n\nLogical objects behave like integers in mathematical evaluation: every TRUE value is automatically treated as the value 1, and every FALSE value is automatically treated as the value 0.\n\n\nmy_logical <- c(TRUE, FALSE, TRUE)\nmean(my_logical)\n\n\n[1] 0.6666667\n\nConverting labelled integer variables into logical variables is simple, but you must first remove value labels with the function zap_labels(). Here, we’ll also want to collapse code 2 together with code 0, and we’ll adopt the value NA for non-response codes 98 and 99. If we were working with just one variable (like CVTRUSTNEWS), we could combine all of these steps together in a single mutate() function like this:\n\n\nrecoded <- covid %>% \n  mutate(\n    CVTRUSTNEWS = CVTRUSTNEWS %>% \n      lbl_collapse(~if_else(.val == 2, 0L, .val)) %>%\n      lbl_na_if(~.val %in% 98:99) %>% \n      zap_labels() %>% \n      as.logical()\n  )\n\n\n\n\n\n\n\n© 2017 (MPL 2.0)\n\n\nrecoded %>% count(CVTRUSTNEWS)\n\n\n# A tibble: 3 x 2\n  CVTRUSTNEWS     n\n  <lgl>       <int>\n1 FALSE        7279\n2 TRUE         4886\n3 NA             19\n\nHowever, we’ll want to apply the same changes to all of the 26 variables we’ve discussed. It would be tedious to manually recode 26 variables in series, and it would also be difficult to make changes if we needed to do so later. Instead, we’ll change all of our variables in one step using the across() function from dplyr.\nUsing dplyr::across\nYou can use across() inside mutate() to edit variables “in place”, which means that it is unnecessary to assign every recoded variable to its old name with a line like CVTRUSTNEWS = CVTRUSTNEWS. The first argument in across() specifies the variable(s) to be modified, and the second argument specifies how you’ll modify them (an optional third argument .names can be used to create new variable names, preserving the original variables under their old names if desired). For example, this code is equivalent to the code shown above:\n\n\nrecoded <- covid %>% \n  mutate(across(\n    CVLEARNNEWS,\n    ~.x %>% \n      lbl_collapse(~if_else(.val == 2, 0L, .val)) %>%\n      lbl_na_if(~.val %in% 98:99) %>% \n      zap_labels() %>% \n      as.logical()\n  ))\n\n\n\nYou can use anonymous functions throughout the tidyverse using the symbol ~ and the pronoun .x.\nHere .x represents each of the variables selected for manipulation with across().\nOf course, the entire purpose of across() is to apply a function to multiple variables. You can use tidy selection to identify variables, which means you provide variable names in a vector without quotation marks (e.g. c(CVTRUSTNEWS, CVTRUSTTV, CVTRUSTRADIO)), or you can use the following selection helpers:\neverything() Matches all variables\nlast_col() Select the last variable, possibly with an offset\nstarts_with() Starts with a prefix\nends_with() Ends with a suffix\ncontains() Contains a literal string\nmatches() Matches a regular expression\nnum_range() Matches a numerical range like x01, x02, x03\n\n\n\n\n© 2018 RStudio (CC0 1.0)\nWe’ll use the “or” operator | to manipulate any variable that starts_with(\"CVTRUST\") or starts_with(\"CVLEARN\"):\n\n\ncovid <- covid %>% \n  mutate(across(\n    starts_with(\"CVTRUST\") | starts_with(\"CVLEARN\"),\n    ~.x %>% \n      lbl_collapse(~if_else(.val == 2, 0L, .val)) %>%\n      lbl_na_if(~.val %in% 98:99) %>% \n      zap_labels() %>% \n      as.logical()\n  ))\n\n\n\nPre-processing\nNow that we’ve recoded all 26 variables as logical objects, we’ll take two additional pre-processing steps to prepare our data for visualization:\nAs discussed in our last post, we’ll pre-calculate the summary statistics we’d like to display in our plots. We’ll store this information in a table called summary.\nWe’ll build a custom theme for ggplot2 that uses colors, fonts, and layout conventions matching the overall design of this blog. We’ll call this theme_pma(), and we’ll pipe it into the code used to create our charts below.\nSummary Table\nWith a little practice, most of us can visualize the final chart we want to create. Visualizing the intermediate data format we’ll need to build that chart can be much harder!\nParticularly for beginners, we think it’s important to start with a list of all the dimensions you think you might want to display in your plot. Then, plan to construct a summary table that includes one column for each dimension. Any column containing text should be formatted exactly as you want it to be shown in your final plot. For example,\nOur data come from two “select all that apply” questions with an identicial list of response options. Let’s list each response option in a column called SOURCE (as in “source of information”), and we’ll identify the original question with “CVTRUST” or “CVLEARN” in a second column called QUESTION.\nWe’ll include the third column called COUNTRY so that we can break down responses by country (we’ll also note the sub-national region for samples that are not nationally representative).\nFor each QUESTION, we’ll calculate a weighted population-level estimate for the proportion of women who trust / learned about COVID-19 from each SOURCE in each COUNTRY. We’ll store point-estimates in a column called EST, and we’ll store the upper and lower limits of a 95% confidence interval in UPP and LOW, respectively.\nLastly, we should create readable labels for each QUESTION in a column that we’ll call QTEXT.\nWe’ll use map_df() from the purrr package to build this table by iterating through all of the variables containing the prefixes “CVTRUST” and “CVLEARN”.\n\n\nsummary <- map_df(\n  # The first argument of `map_df()` is a vector we'd like the iterate\n  # through. \n  c(\"CVTRUST\", \"CVLEARN\"),\n  \n  # The second argument of `map_df()` is a function applied in each \n  # iteration. When you see `.x` below, it refers to the strings \"CVTRUST\" or\n  # \"CVLEARN\".\n  ~covid %>% \n    \n    # Convert COUNTRY to a factor with edited labels\n    mutate(\n      COUNTRY = COUNTRY %>% \n        as_factor() %>% \n        fct_recode(\n          `DRC (Kinshasa)` = \"Congo, Democratic Republic\",\n          `Nigeria (Lagos & Kano)` = \"Nigeria\"\n        ) %>% \n        fct_drop()\n    ) %>% \n    \n    # Specify survey design information with `CVQWEIGHT` and `EAID`\n    as_survey_design(weight = CVQWEIGHT, id = EAID) %>% \n    \n    # Estimate the proportion of TRUE responses for every source (by COUNTRY)\n    group_by(COUNTRY) %>% \n    summarise(across(\n      starts_with(.x),\n      function(var){100 * survey_mean(var, vartype = \"ci\", na.rm = T)}\n    )) %>% \n    \n    # Add the suffix \"_EST\" to each point estimate\n    # Ensure that all variable names use upper-case\n    rename_with(~paste0(.x, \"_EST\"), !contains(\"_\") & !COUNTRY) %>% \n    rename_with(~toupper(.x), everything()) %>% \n    \n    # Up to this point, each source has three columns: the point estimate,\n    # and the upper and lower-limits of the 95% confidence interval. We will \n    # use `pivot_longer` to store each source in a separate row. Each source\n    # label will appear in a new column called `SOURCE`.\n    pivot_longer(\n      !COUNTRY,\n      names_pattern = paste0(.x, \"(.*)_(.*)\"),\n      names_to = c(\"SOURCE\", \".value\"),\n      values_to = \"PCT\"\n    ) %>%\n    \n    # Relabel `SOURCE` with the labels we want to appear on our plots.\n    mutate(\n      SOURCE = SOURCE %>%\n        as_factor() %>%\n        fct_recode(\n          `Family` = \"FAM\",\n          `Radio` = \"RADIO\",\n          `TV` = \"TV\",\n          `Poster / Billboard` = \"POSTER\",\n          `Town Crier` = \"CRIER\",\n          `Phone Message` = \"PHONE\",\n          `Community Leaders` = \"LEADER\",\n          `Health Workers` = \"HW\",\n          `Social Media` = \"SOCMEDIA\",\n          `Friends / Neighbors` = \"FRIEND\",\n          `Government` = \"GOV\",\n          `School` = \"SCHOOL\",\n          `Newpaper` = \"NEWS\"\n        )\n    ) %>% \n  \n  # Finally, create `QUESTION` to hold the string represented by `.x`, and \n  # QTEXT to hold the text labels that we'll use to identify the \n  # original survey question on our plot\n  mutate(QUESTION = .x, .before = SOURCE) %>% \n  mutate(QTEXT = case_when(\n    QUESTION == \"CVTRUST\" ~ \"Which do you trust for COVID-19 information?\",\n    QUESTION == \"CVLEARN\" ~ \"How did you learn about COVID-19?\"\n  ))\n) \n\n\n\n\n\n\n\n© RStudio (CC0 1.0)\nThe resulting table has one row for each unique combination of COUNTRY, QUESTION, and SOURCE.\n\n\nsummary\n\n\n# A tibble: 104 x 7\n   COUNTRY    QUESTION SOURCE       EST   LOW   UPP QTEXT             \n   <fct>      <chr>    <fct>      <dbl> <dbl> <dbl> <chr>             \n 1 Burkina F… CVTRUST  Family      73.3  68.0  78.7 Which do you trus…\n 2 Burkina F… CVTRUST  Friends /…  67.6  62.2  73.0 Which do you trus…\n 3 Burkina F… CVTRUST  Government  76.8  69.4  84.2 Which do you trus…\n 4 Burkina F… CVTRUST  Health Wo…  89.0  85.5  92.5 Which do you trus…\n 5 Burkina F… CVTRUST  Community…  74.3  69.4  79.2 Which do you trus…\n 6 Burkina F… CVTRUST  Newpaper    45.6  39.0  52.3 Which do you trus…\n 7 Burkina F… CVTRUST  Phone Mes…  50.0  44.6  55.5 Which do you trus…\n 8 Burkina F… CVTRUST  Poster / …  54.6  48.6  60.5 Which do you trus…\n 9 Burkina F… CVTRUST  Radio       90.7  88.1  93.3 Which do you trus…\n10 Burkina F… CVTRUST  School      67.2  60.0  74.4 Which do you trus…\n11 Burkina F… CVTRUST  Social Me…  38.3  33.0  43.5 Which do you trus…\n12 Burkina F… CVTRUST  Town Crier  58.5  53.2  63.8 Which do you trus…\n13 Burkina F… CVTRUST  TV          72.7  67.2  78.3 Which do you trus…\n14 DRC (Kins… CVTRUST  Family      31.3  23.2  39.5 Which do you trus…\n15 DRC (Kins… CVTRUST  Friends /…  27.7  20.7  34.8 Which do you trus…\n# … with 89 more rows\n\nPlot theme\nA custom theme usually modifies one of the pre-made themes available in ggplot2. Here, we make some small changes to theme_minimal(), and we’ll list the modified theme together with a few additional ggplot2 functions that we’ll want to use in each plot. Our theme will also contain a custom font that we’ve downloaded and saved in the “fonts” subfolder of our project root directory.\n\n\nfont_add(\n  family = \"cabrito\", \n  regular = \"../../fonts/cabritosansnormregular-webfont.ttf\"\n)\nshowtext_auto()\n\ntheme_pma <- function(title, subtitle){\n  components <- list(\n    # We'll use `theme_minimal()` as a baseline\n    theme_minimal() %+replace% \n      theme(\n        \n        # Text elements\n        text = element_text(                    # text default\n          family = \"cabrito\", \n          size = 10\n        ), \n        plot.title = element_text(              # title override\n          size = 18, \n          color = \"#00263A\",\n          margin = margin(b = 5)\n        ), \n        plot.subtitle = element_text(           # subtitle override\n          size = 12,\n          margin = margin(b = 5)\n        ),\n        legend.title = element_blank(),         # legend title override\n        \n        # Grid elements\n        panel.grid.minor = element_blank(),     # strip minor gridlines\n        axis.ticks = element_blank(),           # strip axis ticks\n        \n        # Legend elements\n        legend.position = \"bottom\",\n      ),\n    \n    # We'll define custom colors for each COUNTRY  \n    scale_fill_manual(\n      values = alpha(\n        alpha = .85,   # .85 creates bars that are 15% transparent\n        colour = c(\n          \"#81A88D\",   # Green\n          \"#CCBA72\",   # Tan\n          \"#98579B\",   # PMA Pink\n          \"#00263A\"    # IPUMS Navy\n        )\n      )\n    ),\n    \n    # A similar color scheme will by used to color binary geom_points\n    scale_color_manual(\n      values = alpha(\n        alpha = .85,   # .85 creates bars that are 15% transparent\n        colour = c(\n          \"#98579B\",   # PMA Pink\n          \"#00263A\"    # IPUMS Navy\n        )\n      )\n    ),\n    \n    # Title and labels\n    labs(\n      title = toupper(title),\n      subtitle = \"Estimated percentage for populations of women age 15-49 in summer 2020\",\n      x = NULL,\n      y = NULL,\n      fill = NULL\n    ),\n    \n    # Flip coordinates and sort legend items\n    coord_flip(),\n    guides(fill = guide_legend(reverse = TRUE))\n    \n  )\n}\n\n\n\n\n\n\n\n© RStudio (CC0 1.0)\nBar Charts\nWe’re now ready to apply what we learned about bar charts in our last post to the binary variables we’ve summarized in summary. At first, you might consider plotting the variables from each QUESTION separately; to do this, you’d first need to use filter(QUESTION == \"CVLEARN\"). You should also consider how you’d like to order the 13 different sources of information in SOURCE: here, we use fct_reorder(SOURCE, EST, mean) to sort the levels of SOURCE according to the mean value of EST across the four samples.\nFirst, consider the responses to “How did you learn about COVID-19?”\n\n\nsummary %>% \n  filter(QUESTION == \"CVLEARN\") %>% \n  mutate(SOURCE = fct_reorder(SOURCE, EST, mean)) %>% \n  ggplot(aes(fill = COUNTRY, y = EST, x = SOURCE)) + \n  geom_bar(width = .6, position = position_dodge(0.6), stat = \"identity\") +\n  geom_errorbar(\n    aes(ymin = LOW, ymax = UPP),\n    width = 0.2,\n    position = position_dodge(0.6),\n    alpha = 0.5\n  ) +\n  theme_pma(\"How did you learn about COVID-19?\") \n\n\n\n\nWhen the PMA COVID-19 survey was fielded (summer 2020), we estimate that radio, television, friends / neighbors, and family were the most effective sources reaching women with information about COVID-19 across the four sampled countries. However, this plot also shows stark differences between countries: we can easily see that radio was comparatively less effective at reaching women in Kinshasa, whereas television was comparatively more effective. Meanwhile, television reached a much smaller share of the women in Burkina Faso compared to women in the other countries.\nAt the lower end of the spectrum, very few women in any country learned about COVID-19 from posters / billboards, town criers, or their school. Newspapers reached almost no women in Kinshasa or Burkina Faso, but they appeared to be more effective in Lagos / Kano and in Kenya. Perhaps most surprisingly, social media reached a only a modest number of women, although its role varies considerably between countries.\nLet’s now look at the responses to “Which of these sources do you trust for accurate information about COVID-19?”\n\n\nsummary %>% \n  filter(QUESTION == \"CVTRUST\") %>% \n  mutate(SOURCE = fct_reorder(SOURCE, EST, mean)) %>% \n  ggplot(aes(fill = COUNTRY, y = EST, x = SOURCE)) + \n  geom_bar(width = .6, position = position_dodge(0.6), stat = \"identity\") +\n  geom_errorbar(\n    aes(ymin = LOW, ymax = UPP),\n    width = 0.2,\n    position = position_dodge(0.6),\n    alpha = 0.5\n  ) +\n  theme_pma(\n    \"Which do you trust for COVID-19 information?\", \n    \"Estimated percentage of women aged 15-49 (summer 2020)\"\n  ) \n\n\n\n\nA clear strength of this layout is that we easily see the large differences in trust for each source between countries: women in Burkina Faso appear to hold much higher levels of trust in sources of information, overall. Taking into account the error bars produced by our 95% confidence interval, it’s also easy to observe certain similarities between women in Kenya and Kinshasa.\nWe’ve again sorted information sources using the mean value of EST across countries, making it easy to rank each source from “most trusted” to “least trusted” across countries. Radio and television were generally the most trusted sources of information, followed by health workers and government officials. The same sources that reached the smallest proportion of women in our last chart - posters / billboards, town criers, and schools - are also the least trusted sources here.\nOne important limitation of this layout is that - except for the top and bottom sources on each list - it’s difficult to keep track of the rank order of sources between charts. It’s also hard to tell whether trusted sources are also effective messengers of COVID-19 information, as the two charts are not yet aligned side-by-side. We’ll solve this problem by faceting the two charts together in a single display.\nHere, we’ll omit the filter() function and instead use facet_wrap(vars(QTEXT)) to build one plot for each QTEXT (the custom text labels we wrote for each question above).\n\n\nsummary %>% \n  mutate(SOURCE = fct_reorder(SOURCE, EST, mean)) %>% \n  ggplot(aes(fill = COUNTRY, y = EST, x = SOURCE)) + \n  geom_bar(width = .6, position = position_dodge(0.6), stat = \"identity\") +\n  geom_errorbar(\n    aes(ymin = LOW, ymax = UPP),\n    width = 0.2,\n    position = position_dodge(0.6),\n    alpha = 0.5\n  ) +\n  theme_pma(\n    \"Are trusted sources reaching\n    women with COVID-19 information?\"\n  ) + \n  facet_wrap(vars(QTEXT)) \n\n\n\n\nThis time, we’ve ranked the most trusted and effective sources according to the highest common mean value for both “CVLEARN” and “CVTRUST” across countries. Radio and TV both reach and are trusted by comparable proportions of women across countries. Beyond those two sources, we see get an overall sense that - as of summer 2020 - most sources fell short of reaching the share of women in each country who would trust them for accurate information about COVID-19.\nWe like a lot of things about this faceted bar chart: while it’s easy to compare sources within and across countries, we’re also able to directly compare responses to both survey questions at the same time. And, unlike the other charts we’ll explore below, it allows us to include error bars representing the 95% confidence interval for each population-level estimate.\nOn the other hand, this plot is not particularly concise: it uses a good deal of space, and it forces the reader to scan two grids when comparing the effectiveness and trust of any particular source.\nLollipop Chart\nIf the main point you’d like to emphasize with these data is the disparity between effectiveness and trust for each source, you might consider plotting both survey questions on the same grid using a lollipop chart. The lollipop chart is also useful in certain contexts where space is limited, as it minimizes the surface area of each geom.\nLet’s take a look at a lollipop chart where the estimated proportion of women who learned from each source is connected on a line with the proportion of women who trust each source. In the previous plot, we saw that levels of trust are generally higher than levels of effectiveness for each source, but we couldn’t easily identify exceptions to this trend; here we’ll draw a dashed line any time the proportion of women who trust a source is lower than the proportion of women who learned from the source. We’ll do this by adding a new column to summary that takes the value “dashed” in those cases, and “solid” otherwise.\n\n\nsummary <- summary %>% \n  group_by(COUNTRY, SOURCE) %>%  \n  mutate(DEFICIT = if_else(\n    any(QUESTION == \"CVTRUST\" & EST == min(EST)), \n    \"solid\", \n    \"dashed\"\n  )) %>% \n  ungroup() \n\n\n\nWe’ll create one “lollipop” for each source by linking two geom_point() markers for each source with a geom_line(), and we’ll use facet_wrap()to build one panel for each COUNTRY. To make this plot easier to interpret, we’ll use a pink circle to represent the our point estimate EST for “CVLEARN”, and a blue triangle to represent the our point estimate EST for “CVTRUST”. Then, wherever “CVTRUST” lags behind “CVLEARN”, we’ll use the column DEFICIT we added to summary to instruct geom_line to draw a “dashed” line instead of a “solid” one.\n\n\nsummary %>% \n  mutate(SOURCE = fct_reorder(SOURCE, EST, mean)) %>% \n  ggplot(aes(x = SOURCE, y = EST)) + \n  geom_point(aes(shape = QTEXT, color = QTEXT), size = 3) +\n  geom_line(aes(linetype = DEFICIT), alpha = .5, show.legend = FALSE) + \n  facet_wrap(vars(COUNTRY)) +\n  theme_pma(\n    \"Are trusted sources reaching\n    women with COVID-19 information?\"\n  ) + \n  theme(legend.position = \"top\")\n\n\n\n\nCompared to the faceted bar chart, this lollipop chart allows us to easily see the size of the gap between the proportion of women who trust each source and the number of women who had learned about COVID-19 from the source in summer 2020. We can also easily identify the sources in each country that are trusted by fewer women than had they had reached: as you might expect, these are typically “social” sources of information like friends / neighbors, family members, and social media.\nThis chart works pretty well for our data, where we only need to compare responses from two “select all that apply” questions. You might be wondering, though, what to do with three or more: in that case, the lollipop chart might not be your best option. Instead, you might consider overlaying several survey questions with semi-transparent layers in a radar chart.\nRadar Chart\nIn a radar chart configuration, each SOURCE will be plotted on a polar coordinate grid - much like the numbers on a clock. The value in EST will map the radial distance of each SOURCE from the center. Otherwise, we’ll use the same layout we adopted for the lollipop chart above: we’ll create one facet for each COUNTRY and we’ll plot both survey questions from QUESTION on the same grid.\nUnfortunately, radar charts are not natively supported in ggplot2, so we’ll need to rely on another package called fmsb that uses base R graphics and syntax conventions. We won’t get into the details here, but if you’d like to see more examples using the fmsb package, we recommend checking out the excellent R Graph Gallery.\n\nIf you’re interested in a ggplot2 approach to radar charts, check out the excellent ggradar package.\nIn short, we’ll need to transform summary once more by pivoting the EST value for each SOURCE into separate columns. Moreover, we’ll define a border_color and a fill_color for each of our survey questions; we’ll use blue for variables derived from “CVTRUST”, and we’ll use pink for those derived from “CVLEARN”.\n\n\nsummary <- summary %>% \n  mutate(SOURCE = str_replace(SOURCE, \" (?=[:alpha:])\", \"\\n\")) %>% \n  pivot_wider(\n    id_cols = c(COUNTRY, QUESTION),\n    names_from = SOURCE,\n    values_from = EST\n  ) %>% \n  mutate(\n    border_color = if_else(\n      QUESTION == \"CVTRUST\",\n      \"#00263A\", # d blue\n      \"#98579B\"  # d pink\n    ),\n    fill_color = if_else(\n      QUESTION == \"CVTRUST\",\n      \"#7A99AC50\", # l blue\n      \"#e8bce850\"  # l pink\n    )\n  )\n\nsummary\n\n\n# A tibble: 8 x 17\n  COUNTRY QUESTION Family `Friends /\\nNei… Government `Health\\nWorker…\n  <fct>   <chr>     <dbl>            <dbl>      <dbl>            <dbl>\n1 Burkin… CVTRUST    73.3             67.6      76.8             89.0 \n2 DRC (K… CVTRUST    31.3             27.7      37.7             47.4 \n3 Kenya   CVTRUST    42.0             40.1      61.2             57.8 \n4 Nigeri… CVTRUST    27.1             32.8      29.5             34.1 \n5 Burkin… CVLEARN    22.7             31.9       2.07            13.0 \n6 DRC (K… CVLEARN    16.6             40.5       5.38             8.24\n7 Kenya   CVLEARN    28.7             45.3      20.7             17.6 \n8 Nigeri… CVLEARN    32.6             53.0       8.66             8.68\n# … with 11 more variables: Community Leaders <dbl>, Newpaper <dbl>,\n#   Phone Message <dbl>, Poster / Billboard <dbl>, Radio <dbl>,\n#   School <dbl>, Social Media <dbl>, Town Crier <dbl>, TV <dbl>,\n#   border_color <chr>, fill_color <chr>\n\n\nWe’ve also inserted a line break symbol \\n wherever a SOURCE name contains a space followed by a letter. This will make the final plot a bit narrower and easier to read.\nThe easiest way to create a faceted plot with base R graphics is to create a custom layout() defined by a matrix containing an index number for each element you’ll place in a facet. We’ll create each of these elements in order:\nA title (in the “outer margin area” oma), and subtitle (in the facet numbered 1)\nFour radar charts: one for each country (in facets numbered 2 through 5)\nA legend (in the facet numbered 6)\nIf you’d like, you can preview the layout with layout.show().\n\n\nplot.new()\npar(\n  family = \"cabrito\",\n  oma = c(1, 1, 10, 1),\n  mar = c(2, 0, 2, 0)\n)\nlayout(\n  matrix(\n    c(1, 1, 2, 3, 4, 5, 6, 6), \n    ncol = 2, \n    byrow = TRUE\n  ),\n  heights = c(.5, 6, 6, 2)\n)\n\nlayout.show(6)\n\n\n\n\nNow, we’ll iterate through each facet one at a time. In order to build the radar charts, we’ll need to use radar_chart() in a for-loop, in which we’ll remove all rows from summary except for those containing a particular COUNTRY. Because radar_chart() requires additional rows containing the min and max values for each grid, we’ll add them with rbind(100, 0, .) within each loop.\n\n\n# Layout \nplot.new()\npar(\n  family = \"cabrito\",\n  oma = c(1, 1, 10, 1),\n  mar = c(2, 0, 2, 0)\n)\nlayout(\n  matrix(\n    c(1, 1, 2, 3, 4, 5, 6, 6),\n    ncol = 2,\n    byrow = TRUE\n  ),\n  heights = c(.5, 6, 6, 2)\n)\n\n# Title \npar(mar = c(0, 0, 0, 0))\nplot.new()\ntitle(\n  main = toupper(\"Are trusted sources reaching\n    women with COVID-19 information?\"),\n  outer = TRUE, \n  cex.main = 3.5,\n  col.main =  \"#00263A\"\n)\npar(mar = c(0, 0, 1, 0))\nmtext(\n  \"Estimated percentage for populations of women age 15-49 in summer 2020\",\n  cex = 1.25\n)\n\n# Loop through each Country\npar(mar = c(1, 0, 2, 0))\nfor(i in unique(summary$COUNTRY)){\n  dat <- summary %>% \n    filter(COUNTRY == i) %>% \n    rbind(100, 0, .) %>% \n    column_to_rownames(\"QUESTION\") %>% \n    select(-COUNTRY)\n  \n  border_color <- dat %>% \n    slice(3:4) %>% \n    pull(border_color)\n  \n  fill_color <- dat %>% \n    slice(3:4) %>% \n    pull(fill_color)\n  \n  radarchart(\n    dat %>% select(-border_color, -fill_color),\n    axistype = 1,\n    \n    # polygon\n    pcol = border_color, pfcol = fill_color, plwd = 4, plty = 1,\n    \n    # grid\n    cglcol=  \"grey\", cglty = 1, axislabcol = \"grey\", \n    caxislabels = seq(0,100,20), cglwd = 0.8,\n    \n    # labels\n    vlcex = 0.8,\n    \n    # title \n    title = i, cex.main = 2\n  )\n}\n\n# Legend \nplot.new()\npar(mar = c(0, 0, 0, 0))\nlegend(\n  \"top\",\n  legend = c(\"Which do you trust for COVID-19 information?\",\n             \"How did you learn about COVID-19?\"),\n  horiz = FALSE,\n  bty = \"n\",\n  pch = 20 ,\n  col = unique(summary$border_color),\n  text.col =  \"#00263A\",\n  cex = 2,\n  pt.cex = 4,\n  y.intersp = 1.5\n)\n\n\n\n\nThere are quite a few important caveats to the radar chart, but we think these are a bit more intuitive than the lollipop chart shown above. The perimeter of the blue “trust” shape is clearly larger than the perimeter of the pink “learn” shape in all four countries, and it’s easy to identify cases where a particular source has reached a larger share of the population than the share that trusts it.\nOn the other hand, the circular layout of the radar chart makes it nearly impossible to visualize which of the source is most trusted and effective. If the overall rank order of the response options is an important take-away for your readers, you should almost certainly choose the lollipop chart or the faceted bar chart instead.\nAn additional consideration here: we’ve highlighted the radar chart because it allows you to overlay the responses to each “select all that apply” question, rather than plotting them side-by-side. You should still use caution, however, not to overcrowd your chart with too many layers.\nWrap-up\nWhich chart do you like best for the “sources of information” data in the PMA COVID-19 survey? Let us know what you think in the comments below, or reach out to us at the brand new IPUMS Global Health Twitter account!\nJoin us again in two weeks, when we’ll be wrapping up our series on the PMA COVID-19 survey with a tutorial showing how to connect it with data from the 2020 baseline survey.\n\n\n\n",
    "preview": "posts/2021-08-01-covid-batches/images/preview2.png",
    "last_modified": "2021-09-01T10:30:40-05:00",
    "input_file": {},
    "preview_width": 1134,
    "preview_height": 872
  },
  {
    "path": "posts/2021-07-15-covid-likert/",
    "title": "Visualizing perceptions of risk from COVID-19",
    "description": "A guide to bar charts for Likert-type psychometric scales built with ggplot2.",
    "author": [
      {
        "name": "Saeun Park",
        "url": "http://www.linkedin.com/in/saeun-park"
      },
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-07-15",
    "categories": [
      "COVID-19",
      "Descriptive Analysis",
      "Data Visualization",
      "ggplot2"
    ],
    "contents": "\n\nContents\nSetup\nFeatured variable: COVIDCONCERN\n\nBasic Bar Charts\nPosition\nStat\n\nCustomization\nColor\nLabels and Fonts\n\nAdvanced Bar Charts\nDivergent Stacked Bar Chart\nFaceted Neutral / Non-response\nFaceted Question Series\n\nNext Steps\n\nAs we’ve mentioned throughout this series, one of the most important focus areas of the new PMA COVID-19 survey has to do with perceptions of risk expressed by women during the early months of the pandemic. Because all respondents to the COVID-19 survey are participants in a multi-year panel study examining broad topics in reproductive health, analysts will soon be able to link women’s attitudes and beliefs about COVID-19 during the summer of 2020 to longer-term health and family planning outcomes.\nIn this post, we’ll examine one of the most common data visualization tools used to explore attitudinal data: the bar chart. In the PMA COVID-19 survey, women are asked to rate their level of concern for several different types of risk associated with the pandemic. The survey uses a four-point scale for such questions, and it includes the following response options:\nNot concerned\nA little concerned\nConcerned\nVery concerned\nThis type of scale is common in psychometric research, particularly where analysts want to compare attitudes about a wide range of topics. You might notice that the responses follow a bi-polar format, where more neutral responses are organized at the center, and more extreme responses are listed on either side. This type for scale is sometimes called the Likert scale after the pioneering social psychologist, Rensis Likert.\nThe bar chart is typically used for Likert-type data because:\nit is ordinal (responses should be arranged from a low-level of concern to a high-level of concern)\nit is discrete (responses are restricted to a small number of pre-defined choices)\nrepeated use of the same scale allows us to align and compare levels of concern on multiple questions\nWe’ll discuss some of the many choices you’ll have to make about layout, and we’ll show how to implement them with the tidyverse package ggplot2.\n\nWe also recommend the packages likert and hh for Likert-type data.\nSetup\nYou’ll find the data featured in this post if you navigate to the new COVID-19 Unit of Analysis in the IPUMS PMA data extract system. Our examples feature data from all four samples (Female Respondents only).\nTo follow along, make sure to create an extract that includes these variables:\nCOUNTRY\nCVQWEIGHT\nEAID\nCOVIDCONCERN\nCOMMCOVIDWORRY\nPREGFEELNOW\nYou’ll also need to install the following packages as needed (current versions are recommended):\nipumsr (0.4.5)\ntidyverse (1.3.1)\nsrvyr (1.0.1)\nshowtext (0.9.2)\ngtsummary (1.4.1)\nLoad those packages and your data extract into R (be sure to change the file paths to match the location of your own extract):\n\n\nlibrary(ipumsr)\nlibrary(tidyverse)\nlibrary(srvyr)\nlibrary(showtext)\nlibrary(gtsummary)\n\ncovid <- read_ipums_micro(\n  ddi = \"data/pma_00032.xml\",\n  dat = \"data/pma_00032.dat.gz\"\n)\n\n\n\nFeatured variable: COVIDCONCERN\nTo start, let’s take a look at just one of the variables that uses the Likert-type scale shown above. In COVIDCONCERN, women who have not already been infected with COVID-19 are asked to rate their level of concern for becoming infected:\nHow concerned are you about getting infected yourself?\n(Read all options)\n\n  [] Very concerned\n  [] Concerned\n  [] A little concerned\n  [] Not concerned\n  [] I am currently / was infected with COVID-19\n  [] No response\n\nIn addition to the two non-response options shown on the questionnaire, women could also be NIU (not in universe) if they had already been infected.\nLet’s break down the responses to this question by COUNTRY. First, following the explanation in our last post, we’d strongly recommend transforming both variables into factor objects (this will ensure that their value labels are displayed in graphics output). We’ll also edit the COUNTRY labels for DRC and Nigeria, and we’ll describe the NIU cases for COVIDCONCERN as women who Never heard or read about COVID-19.\n\nReminder: only the Burkina Faso and Kenya samples are nationally representative. The DRC and Nigeria samples represent sub-national regions.\n\n\ncovid <- covid %>% \n  mutate(\n    across(where(is.labelled), ~as_factor(.x) %>% fct_drop), \n    COUNTRY = COUNTRY %>%\n      fct_recode(\n        `DRC (Kinshasa)` = \"Congo, Democratic Republic\",\n        `Nigeria (Lagos & Kano)` = \"Nigeria\"\n      ),\n    COVIDCONCERN = COVIDCONCERN %>% \n      fct_recode(\n        `Never heard or read about COVID-19` = \"NIU (not in universe)\"\n      )\n  )\n\n\n\nUsing the gtsummary package featured in our last post, you might preview the breakdown of COVIDCONCERN by COUNTRY in a table as follows:\n\n\ncovid %>% tbl_summary(by = COUNTRY, include = COVIDCONCERN) \n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#mtkigwzygg .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#mtkigwzygg .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#mtkigwzygg .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#mtkigwzygg .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#mtkigwzygg .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#mtkigwzygg .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#mtkigwzygg .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#mtkigwzygg .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#mtkigwzygg .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#mtkigwzygg .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#mtkigwzygg .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#mtkigwzygg .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#mtkigwzygg .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#mtkigwzygg .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#mtkigwzygg .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#mtkigwzygg .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#mtkigwzygg .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#mtkigwzygg .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#mtkigwzygg .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#mtkigwzygg .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#mtkigwzygg .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#mtkigwzygg .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#mtkigwzygg .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#mtkigwzygg .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#mtkigwzygg .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#mtkigwzygg .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#mtkigwzygg .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#mtkigwzygg .gt_left {\n  text-align: left;\n}\n\n#mtkigwzygg .gt_center {\n  text-align: center;\n}\n\n#mtkigwzygg .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#mtkigwzygg .gt_font_normal {\n  font-weight: normal;\n}\n\n#mtkigwzygg .gt_font_bold {\n  font-weight: bold;\n}\n\n#mtkigwzygg .gt_font_italic {\n  font-style: italic;\n}\n\n#mtkigwzygg .gt_super {\n  font-size: 65%;\n}\n\n#mtkigwzygg .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\nCharacteristic\n      Burkina Faso, N = 3,5281\n      DRC (Kinshasa), N = 1,3241\n      Kenya, N = 5,9861\n      Nigeria (Lagos & Kano), N = 1,3461\n    Concerned about getting infected\n\n\n\nNot concerned\n238 (6.7%)\n259 (20%)\n236 (3.9%)\n39 (2.9%)A little concerned\n365 (10%)\n161 (12%)\n171 (2.9%)\n30 (2.2%)Concerned\n752 (21%)\n210 (16%)\n759 (13%)\n170 (13%)Very concerned\n2,168 (61%)\n689 (52%)\n4,818 (80%)\n1,100 (82%)Currently / previously infected with COVID-19\n2 (<0.1%)\n1 (<0.1%)\n0 (0%)\n2 (0.1%)No response or missing\n0 (0%)\n2 (0.2%)\n2 (<0.1%)\n1 (<0.1%)Never heard or read about COVID-19\n3 (<0.1%)\n2 (0.2%)\n0 (0%)\n4 (0.3%)\n        \n          1\n          \n           \n          n (%)\n          \n      \n    \n\n\n\n\n\n© 2020 Daniel D. Sjoberg (MIT)\nNow we’re ready to begin arranging these summary data into a bar chart with ggplot2.\nBasic Bar Charts\nAs you might know, ggplot2 is part of the tidyverse family of packages. For regular readers of this blog, this means that you’ll be able to use the same grammar that you’re used to seeing elsewhere, but with one important difference: while you’ll be able to pipe functions to ggplot() with the familiar %>% operator, functions within the package use their own pipe-like operator +.\n\n\n\n\n© RStudio (CC0 1.0)\nThis + operator allows the user to assemble multiple layers of visual information onto the same plot. These layers are built from functions that start with the prefix geom_, because each layer is more-or-less defined by the geometric shapes that convey information about the data.\nWhile these geom_ functions are simple to use and combine, you’ll first need to define some common parameters with the function ggplot(). This function initializes a kind of “skeleton” plot - or canvas - onto which you’ll layer each geom_ function. Usually, you’ll identify variables here that you’ll want to map onto the x and y-axes, or onto the “fill” styles (e.g. colors and shadings) within your plot’s geometric shapes.\n\nLooking for a free introduction to ggplot2? Try ggplot2: Elegant Graphics for Data Analysis.\nWe’ll use the geom_bar() function after we define some basic parameters for our plot in ggplot():\n\n\ncovid %>% \n  ggplot(aes(x = COUNTRY, fill = COVIDCONCERN)) + \n  geom_bar()\n\n\n\n\n\nA stacked bar chart showing the frequency of each response per sample\nIn the above function, we initialize our plot with ggplot() and define its basic aesthetic qualities with aes(): we specify that we’ll plot each COUNTRY on the x-axis, and - in whatever geometric shapes we draw next - we’ll fill its segments with colors defined by COVIDCONCERN. Note that the ggplot() function doesn’t draw anything, itself. Instead, we pipe ggplot to geom_bar(), which is responsible for drawing and stacking the bars.\nBut what about the values that appeared on the y-axis? We didn’t specify anything in our data, but it seems like ggplot() automatically calculated the number of women in each country who selected each response. While this might be a useful default in some situations, here we’d much rather normalize these bars as a percentage of the total number of responses for each sample. We’ll do this by manipulating the position argument in geom_bar().\nPosition\nThe position argument in geom_bar() determines how the bars representing each response should be arranged on our plot. This argument can take one of several position adjustment functions, and its default behavior uses position_stack() to “stack” bars representing the frequency of each response. This kind of bar chart is known as a stacked bar chart.\nIf we want to normalize the size of our bars to the size of each sample, we can use position_fill() to stretch each stack of bars to an equal length. The result allows us to compare the proportion of responses across samples:\n\n\ncovid %>% \n  ggplot(aes(x = COUNTRY, fill = COVIDCONCERN)) + \n  geom_bar(position = position_fill())\n\n\n\n\n\nA stacked bar chart showing the proportion of each response per sample\nThis arrangement is helpful for comparing more extreme responses, but you may notice that it’s still a bit hard to compare the proportion of moderate responses in the middle of each stack. For this reason, you might consider using position_dodge() to transform our stacked bar chart into a grouped bar chart.\n\n\ncovid %>% \n  ggplot(aes(x = COUNTRY, fill = COVIDCONCERN)) + \n  geom_bar(position = position_dodge())\n\n\n\n\nA grouped bar chart showing the frequency of each response per sample\nNote: the bars for Kenya and Burkina Faso appear wider because they contain zero responses for some levels of COVIDCONCERN. Check out position_dodge2() for more control over the width of these bars.\nUnfortunately, when we switch position from position_fill() to position_dodge(), we’re no longer able to stretch each bar to a normalized length. Instead, we’ll need to pre-calculate the proportion of each response and pass it to geom_bar() via the stat argument.\nStat\nIn each of the above plots, we’ve relied on the default behavior of geom_bar() to calculate the frequency of each response and - when requested - to stretch each bar to a normalized length. There are many reasons why you might want to pass your own statistics to geom_bar(), and you can do so with the argument stat = \"identity\".\nFor example, we might create a table of summary statistics showing the proportion of responses to COVIDCONCERN by COUNTRY:\n\n\nconcern_tbl <- covid %>% \n  as_survey_design() %>%\n  group_by(COUNTRY, COVIDCONCERN) %>%\n  summarise(PERCENT = 100 * survey_mean(vartype = NULL))\n\nconcern_tbl\n\n\n# A tibble: 25 x 3\n# Groups:   COUNTRY [4]\n   COUNTRY        COVIDCONCERN                                 PERCENT\n   <fct>          <fct>                                          <dbl>\n 1 Burkina Faso   Not concerned                                 6.75  \n 2 Burkina Faso   A little concerned                           10.3   \n 3 Burkina Faso   Concerned                                    21.3   \n 4 Burkina Faso   Very concerned                               61.5   \n 5 Burkina Faso   Currently / previously infected with COVID-…  0.0567\n 6 Burkina Faso   Never heard or read about COVID-19            0.0850\n 7 DRC (Kinshasa) Not concerned                                19.6   \n 8 DRC (Kinshasa) A little concerned                           12.2   \n 9 DRC (Kinshasa) Concerned                                    15.9   \n10 DRC (Kinshasa) Very concerned                               52.0   \n# … with 15 more rows\n\nNow, if we pass our summary table concern_tbl to ggplot(), we’ll be able to map response percentages in the PERCENT column to the y-axis. In the geom_bar() function, we’ll use stat = \"identity\" to ensure that our pre-calculated statistics are displayed:\n\n\nconcern_tbl %>% \n  ggplot(aes(x = COUNTRY, fill = COVIDCONCERN, y = PERCENT)) + \n  geom_bar(\n    position = position_dodge(),\n    stat = \"identity\"\n  ) \n\n\n\n\n\nA grouped bar chart showing response percentages per sample\nYou might also consider pre-calculating statistics if you want to add layers of text or error bars to your plot. As we’ve discussed elsewhere, we love using as_survey_design() and survey_mean() from the srvyr package to generate population-level estimates with cluster-robust standard errors. Here, we’ll use CVQWEIGHT as a weighting variable and EAID as the identification number for each sample cluster, thus creating a population-level summary table called concern_pop:\n\n\nconcern_pop <- covid %>% \n  as_survey_design(weight = CVQWEIGHT, id = EAID) %>%\n  group_by(COUNTRY, COVIDCONCERN) %>%\n  summarise(PERCENT = 100 * survey_mean(vartype = \"ci\"))\n\nconcern_pop\n\n\n# A tibble: 25 x 5\n# Groups:   COUNTRY [4]\n   COUNTRY    COVIDCONCERN             PERCENT PERCENT_low PERCENT_upp\n   <fct>      <fct>                      <dbl>       <dbl>       <dbl>\n 1 Burkina F… Not concerned             6.18        3.70        8.66  \n 2 Burkina F… A little concerned        8.35        5.61       11.1   \n 3 Burkina F… Concerned                17.7        14.3        21.2   \n 4 Burkina F… Very concerned           67.7        62.1        73.3   \n 5 Burkina F… Currently / previously …  0.0251     -0.0103      0.0604\n 6 Burkina F… Never heard or read abo…  0.0484     -0.0149      0.112 \n 7 DRC (Kins… Not concerned            18.8        14.9        22.7   \n 8 DRC (Kins… A little concerned        9.77        7.71       11.8   \n 9 DRC (Kins… Concerned                17.0        13.0        21.1   \n10 DRC (Kins… Very concerned           54.0        48.5        59.5   \n# … with 15 more rows\n\nNote the addition of PERCENT_low and PERCENT_upp, representing the lower and upper bounds of a 95% confidence interval for each population-level estimate of PERCENT. We’ll use these in a new layer created by geom_errorbar():\n\n\nconcern_pop %>% \n  ggplot(aes(x = COUNTRY, fill = COVIDCONCERN, y = PERCENT)) + \n  geom_bar(\n    position =  position_dodge(),\n    stat = \"identity\"\n  ) + \n  geom_errorbar(\n    aes(ymin = PERCENT_low, ymax = PERCENT_upp),\n    width = 0.2,\n    position = position_dodge(width = 0.9)\n  )\n\n\n\n\n\nA grouped bar chart showing population-level estimates with 95% confidence intervals\nLikewise, pre-calculating statistics in a table like concern_pop makes it easy to access statistics by name in geom_text(). In this example, adding the text label for each value of PERCENT is redundant with the y-axis (not recommended), but you could also include text from any column in the pre-calculated table:\n\n\nconcern_pop %>% \n  ggplot(aes(x = COUNTRY, fill = COVIDCONCERN, y = PERCENT)) + \n  geom_bar(\n    position = \"dodge\",\n    stat = \"identity\"\n  ) + \n  geom_text(\n    aes(label = round(PERCENT, 0)),\n    position = position_dodge(0.9),\n    vjust = -0.5\n  )\n\n\n\n\n\nA grouped bar chart showing population-level estimates, annotated with text\nCustomization\nSo far, we’ve focused all of our attention on passing the correct statistics to geom_bar(). Unfortunately, this is only half the battle: our plots still aren’t very readable!\nYou may have noticed, for example, that the legend in each of our plots seems to take up about one third of the usable space. In a blog like ours - where many of you might be reading this post on a mobile phone - this layout is certainly not ideal. Instead, we’ll flip the x and y-axes, and then we’ll position the legend below the plot.\nFor example, let’s return to the stacked bar chart showing the population-level percentages for each response:\n\n\nconcern_pop %>% \n  ggplot(aes(x = COUNTRY, fill = COVIDCONCERN, y = PERCENT)) + \n  geom_bar(stat = \"identity\") + \n  coord_flip() + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFlip your bar chart into a horizontal orientation with coord_flip().\nThe function coord_flip() pivots our plot into a horizontal orientation, and another new function - theme() - allows us to move our legend. However, the legend now occupies too much horizontal space: one of the responses appears to be cut-off by the right-hand margin of the page.\nIt is possible to manipulate the layout of your legend with another function, guides(). For example, you might arrange the response codes into two separate columns:\n\n\nconcern_pop %>% \n  ggplot(aes(x = COUNTRY, fill = COVIDCONCERN, y = PERCENT)) + \n  geom_bar(stat = \"identity\") + \n  coord_flip() + \n  theme(legend.position = \"bottom\") + \n  guides(fill = guide_legend(ncol = 2))\n\n\n\n\nThe guides() function controls the layout of the legend with guide_legend().\nYou can also control the layout of the axes with guide_axis().\nHowever, in our particular case, it might make more sense to simply drop the three types of non-response completely. We might do so, in part, because the remaining responses are part of an ordinal set. If we restrict the plotted values only to those ordinal responses, we’ll also be able to add an ordinal color scheme to our plot making the relationship between each response much clearer.\nColor\nAn easy way to drop non-response options in our particular case is to filter only those four responses containing the word “concern” (upper or lower case). Then, when only the four ordinal responses remain, we’ll use scale_fill_brewer() to select an ordinal color scheme (“blues” by default). This time, we’ll use guides() to reverse the order of the responses in our legend:\n\n\nconcern_pop %>% \n  filter(grepl(\"concern\", COVIDCONCERN, ignore.case = TRUE)) %>% \n  ggplot(aes(x = COUNTRY, fill = COVIDCONCERN, y = PERCENT)) + \n  geom_bar(stat = \"identity\") + \n  coord_flip() + \n  theme(legend.position = \"bottom\") + \n  scale_fill_brewer() + \n  guides(fill = guide_legend(reverse = TRUE))\n\n\n\n\nNon-response categories have been dropped, and the remaining responses are represented by an ordinal color scheme\nNote that the bars no longer sum to 100%\nYou can choose from several color palettes with scale_fill_brewer(), or you can define your own colors using scale_fill_manual(), where you’ll assign a color to each response via a named character vector.\nFor example, here we’ll use some of the hex color codes you’ll see in the CSS throughout this blog:\n\n\nconcern_pop %>% \n  filter(grepl(\"concern\", COVIDCONCERN, ignore.case = TRUE)) %>% \n  ggplot(aes(x = COUNTRY, fill = COVIDCONCERN, y = PERCENT)) + \n  geom_bar(stat = \"identity\") + \n  coord_flip() + \n  theme(legend.position = \"bottom\") + \n  scale_fill_manual(\n    values = alpha(\n      colour = c(\n        \"Very concerned\" = \"#00263A\",        # IPUMS Navy\n        \"Concerned\" = \"#4E6C7D\",             # IPUMS Dark-Grey\n        \"A little concerned\" = \"#7A99AC\",    # IPUMS Blue-Grey\n        \"Not concerned\" = \"#F1F5F7\"          # IPUMS Light-Grey\n      )\n    )\n  ) + \n  guides(fill = guide_legend(reverse = TRUE))\n\n\n\n\n\nA user-defined ordinal color scheme\nLabels and Fonts\nThere are several ways to add text labels to a plot, but we find it easiest to define every label together in a single function, labs(). If you want to omit a particular label, you can simply set it to NULL:\n\n\nconcern_pop %>% \n  filter(grepl(\"concern\", COVIDCONCERN, ignore.case = TRUE)) %>% \n  ggplot(aes(x = COUNTRY, fill = COVIDCONCERN, y = PERCENT)) + \n  geom_bar(stat = \"identity\") + \n  coord_flip() + \n  theme(legend.position = \"bottom\") + \n  scale_fill_manual(\n    values = alpha(\n      colour = c(\n        \"Very concerned\" = \"#00263A\",  \n        \"Concerned\" = \"#4E6C7D\", \n        \"A little concerned\" = \"#7A99AC\", \n        \"Not concerned\" = \"#F1F5F7\"\n      )\n    )\n  ) + \n  guides(fill = guide_legend(reverse = TRUE)) + \n  labs(\n    title = \"CONCERN ABOUT GETTING INFECTED WITH COVID-19\",\n    subtitle = \"Estimated percentage for populations of women age 15-49 (summer 2020)\",\n    x = NULL,\n    y = NULL,\n    fill = NULL\n  ) \n\n\n\n\n\nAdded a title and subtitle. Removed axis and legend labels.\nAs you can see, the default label fonts for ggplot2 do not match the fonts used on our blog. If this is an important consideration, you can download a .ttf file for your preferred font from a repository like Google Fonts, and then load that file into R with font_add().\n\n\nfont_add(\n  family = \"cabrito\", \n  regular = \"fonts/cabritosansnormregular-webfont.ttf\"\n)\n\n\n\n\nWe saved our ttf file in the fonts sub-folder of our working directory.\nOnce you’ve loaded a font into R, you can make it accessible to ggplot2 for the remainder of your R session with the function showtext::showtext_auto().\n\n\nshowtext_auto()\n\n\n\nNow, we can build on our custom theme() by defining a general font family and size in text. We can also tweak specific details for the title and plot.subtitle:\n\n\nconcern_pop %>% \n  filter(grepl(\"concern\", COVIDCONCERN, ignore.case = TRUE)) %>% \n  ggplot(aes(x = COUNTRY, fill = COVIDCONCERN, y = PERCENT)) + \n  geom_bar(stat = \"identity\") + \n  coord_flip() + \n  scale_fill_manual(\n    values = alpha(\n      colour = c(\n        \"Very concerned\" = \"#00263A\",  \n        \"Concerned\" = \"#4E6C7D\", \n        \"A little concerned\" = \"#7A99AC\", \n        \"Not concerned\" = \"#F1F5F7\"\n      )\n    )\n  ) + \n  guides(fill = guide_legend(reverse = TRUE)) + \n  labs(\n    title = \"CONCERN ABOUT GETTING INFECTED WITH COVID-19\",\n    subtitle = \"Estimated percentage for populations of women age 15-49 (summer 2020)\",\n    x = NULL,\n    y = NULL,\n    fill = NULL\n  ) + \n  theme(\n    text = element_text(family = \"cabrito\", size = 10),\n    title = element_text(size = 14, color = \"#00263A\"),\n    plot.subtitle = element_text(size = 12),\n    legend.position = \"bottom\"\n  ) \n\n\n\n\n\n\n\n© Yixuan Qiu (Apache >= 2.0)\n\n\n\n\nCustom font family and sizes implemented with showtext\nAdvanced Bar Charts\nDivergent Stacked Bar Chart\nIn the previous section, we decided to drop the three types of non-response for COVIDCONCERN so that we could adopt an ordinal color scheme (each color corresponds with an ordinal level of concern). This improved the readability of our plot by making the relationship between response options more clear. However, this decision also came with a small cost: because our bars no longer represent 100% of each population, it’s a bit harder to compare the percentage of women represented by the responses on the right side of the plot (“Not concerned”).\nIn this case, you might consider the divergent stacked bar chart, where “positive” and “negative” levels of concern are plotted in opposite directions from an origin point on our x-axis. You might also consider this if you wanted to directly juxtapose the most extreme responses on our scale: “Very concerned” and “Not concerned”.\nNote that three of the responses on our scale reflect some level of concern about getting infected with COVID-19; we’ll plot these responses in the positive direction on our x-axis. The negative response - “Not concerned” - will be plotted in the negative direction if we multiply PERCENT by -1 for those cases. We’ll also give our negative response a secondary color (“PMA Pink”) and draw a vertical line at the origin to provide extra clarity. Finally, we’ll use a new function breaks() to fully customize the order of responses in our legend:\n\n\nconcern_pop <- concern_pop %>% \n  mutate(PERCENT = if_else(\n   COVIDCONCERN == \"Not concerned\",\n   -PERCENT,                                  # Multiply by -1\n   PERCENT\n  )) %>% \n  filter(grepl(\"concern\", COVIDCONCERN, ignore.case = T)) \n\nconcern_pop\n\n\n# A tibble: 16 x 5\n# Groups:   COUNTRY [4]\n   COUNTRY            COVIDCONCERN     PERCENT PERCENT_low PERCENT_upp\n   <fct>              <fct>              <dbl>       <dbl>       <dbl>\n 1 Burkina Faso       Not concerned      -6.18       3.70         8.66\n 2 Burkina Faso       A little concer…    8.35       5.61        11.1 \n 3 Burkina Faso       Concerned          17.7       14.3         21.2 \n 4 Burkina Faso       Very concerned     67.7       62.1         73.3 \n 5 DRC (Kinshasa)     Not concerned     -18.8       14.9         22.7 \n 6 DRC (Kinshasa)     A little concer…    9.77       7.71        11.8 \n 7 DRC (Kinshasa)     Concerned          17.0       13.0         21.1 \n 8 DRC (Kinshasa)     Very concerned     54.0       48.5         59.5 \n 9 Kenya              Not concerned      -4.84       2.44         7.23\n10 Kenya              A little concer…    3.45       2.12         4.79\n11 Kenya              Concerned          13.1       10.4         15.7 \n12 Kenya              Very concerned     78.6       74.6         82.7 \n13 Nigeria (Lagos & … Not concerned      -3.17       1.45         4.89\n14 Nigeria (Lagos & … A little concer…    2.04       0.899        3.19\n15 Nigeria (Lagos & … Concerned          14.1        6.18        22.0 \n16 Nigeria (Lagos & … Very concerned     80.3       72.6         87.9 \n\n\nThe PERCENT value for negative responses are multiplied by -1.\n\n\nconcern_pop %>% \n  ggplot(aes(x = PERCENT, y = COUNTRY, fill = COVIDCONCERN)) + \n  geom_bar(stat = \"identity\") +\n  \n  # draws a vertical line at 0 on the x axis\n  geom_vline(xintercept = 0) +\n  \n  # define fill colors (values) and the arrangement of the legend (breaks)\n  scale_fill_manual(\n    values = alpha(\n      colour = c(\n        \"Very concerned\" = \"#00263A\",           # PMA Pink\n        \"Concerned\" = \"#4E6C7D\",                # IPUMS Dark-Grey\n        \"A little concerned\" = \"#7A99AC\",       # IPUMS Blue-Grey\n        \"Not concerned\" = \"#98579B\"             # IPUMS Light-Grey\n      )\n    ),\n    breaks = c(\n      \"Not concerned\",\n      \"Very concerned\",  \n      \"Concerned\", \n      \"A little concerned\"\n    )\n  ) + \n  \n  # define labels (labs) and format them as desired (theme)\n  labs(\n    title = \"CONCERN ABOUT GETTING INFECTED WITH COVID-19\",\n    subtitle = \"Estimated percentage for populations of women age 15-49 (summer 2020)\",\n    x = NULL,\n    y = NULL,\n    fill = NULL\n  ) + \n  theme(\n    text = element_text(family = \"cabrito\", size = 10),\n    title = element_text(size = 14, color = \"#00263A\"),\n    plot.subtitle = element_text(size = 12),\n    legend.position = \"bottom\",\n  ) \n\n\n\n\n\nA divergent bar plot shows positive and negative responses oriented in opposite directions.\nWhile it is still difficult to compare women who are “Concerned” or “A little concerned”, this type of chart makes it easy to compare the most extreme response while also comparing the full set of negative response to the full set of positive responses.\n\nRead more about the debate surrounding divergent stacked bar charts here and here.\nA word of caution: data visualization experts disagree about what to do with middle / neutral responses. While it’s possible to distribute these responses in halves on the outside or in the middle of each bar stack, we much prefer a facet showing both neutral and non-response options to the side.\nFaceted Neutral / Non-response\nAll of the plots we’ve explored so far have contained a single panel, where both the x and y-axes are uninterrupted for the full width of the display. In some cases, you may want to facet multiple panels together.\nFor example, consider the variable PREGFEELNOW, in which women describe how they would feel if they became pregnant “now”. As we’ll see, this variable contains both a large number of middle / neutral responses (“Mixed happy and unhappy”) and a large number of non-responses (e.g. women who were pregnant at the time, or who simply did not respond). We’ll use a facet to show these responses in a separate panel alongside those who provided a positive or negative opinion.\nFirst, we’ll create a summary table and clean up the factor levels for clarity. As we’ve shown above, we’ll make PERCENT a negative value for negative responses (“Very unhappy” or “Sort of unhappy”):\n\n\npg_tbl <- covid %>% \n  as_survey_design(weight = CVQWEIGHT, id = EAID) %>%\n  group_by(COUNTRY, PREGFEELNOW) %>%\n  summarise(PERCENT = 100 * survey_mean(vartype = NULL)) %>% \n  mutate(\n    PREGFEELNOW = factor(\n      PREGFEELNOW, \n      levels = c(\n        \"Sort of unhappy\",\n        \"Very unhappy\", \n        \"Sort of happy\",      \n        \"Very happy\",\n        \"No response or missing\",\n        \"NIU (not in universe)\",\n        \"Mixed happy and unhappy\"\n      )\n    ) %>% fct_recode(`Currently Pregnant` = \"NIU (not in universe)\"),\n    PERCENT = if_else(\n      PREGFEELNOW %in% c(\"Very unhappy\", \"Sort of unhappy\"), \n      -PERCENT, \n      PERCENT\n    )\n  )\n\npg_tbl\n\n\n# A tibble: 28 x 3\n# Groups:   COUNTRY [4]\n   COUNTRY        PREGFEELNOW             PERCENT\n   <fct>          <fct>                     <dbl>\n 1 Burkina Faso   Very unhappy            -29.1  \n 2 Burkina Faso   Sort of unhappy         -11.1  \n 3 Burkina Faso   Mixed happy and unhappy   5.65 \n 4 Burkina Faso   Sort of happy            15.0  \n 5 Burkina Faso   Very happy               30.8  \n 6 Burkina Faso   No response or missing    0.333\n 7 Burkina Faso   Currently Pregnant        7.99 \n 8 DRC (Kinshasa) Very unhappy            -44.6  \n 9 DRC (Kinshasa) Sort of unhappy         -11.2  \n10 DRC (Kinshasa) Mixed happy and unhappy   5.90 \n# … with 18 more rows\n\nNext, we’ll create a new column that indicates whether we want each response to appear in the second panel in our faceted display. Let’s call this column ASIDE:\n\n\npg_tbl <- pg_tbl %>% \n  mutate(ASIDE = PREGFEELNOW %in% c(\n      \"Mixed happy and unhappy\",\n      \"No response or missing\",\n      \"Currently Pregnant\" \n  ))\n\npg_tbl\n\n\n# A tibble: 28 x 4\n# Groups:   COUNTRY [4]\n   COUNTRY        PREGFEELNOW             PERCENT ASIDE\n   <fct>          <fct>                     <dbl> <lgl>\n 1 Burkina Faso   Very unhappy            -29.1   FALSE\n 2 Burkina Faso   Sort of unhappy         -11.1   FALSE\n 3 Burkina Faso   Mixed happy and unhappy   5.65  TRUE \n 4 Burkina Faso   Sort of happy            15.0   FALSE\n 5 Burkina Faso   Very happy               30.8   FALSE\n 6 Burkina Faso   No response or missing    0.333 TRUE \n 7 Burkina Faso   Currently Pregnant        7.99  TRUE \n 8 DRC (Kinshasa) Very unhappy            -44.6   FALSE\n 9 DRC (Kinshasa) Sort of unhappy         -11.2   FALSE\n10 DRC (Kinshasa) Mixed happy and unhappy   5.90  TRUE \n# … with 18 more rows\n\nOur plot will look similar to the divergent bar chart we made in the previous section, but will now add a new function facet_grid() that divides pg_tbl into separate panels defined by ASIDE:\n\n\npg_tbl %>% \n  ggplot(aes(x = COUNTRY, fill = PREGFEELNOW, y = PERCENT)) + \n  geom_bar(stat = \"identity\") + \n  coord_flip() + \n  \n  # facet_grid() can distribute facets in rows and/or columns\n  facet_grid(\n    cols = vars(ASIDE), # here, we choose columns defined by ASIDE\n    scales = \"free\",    # \"free\" scales allows for independent facet scales  \n    space = \"free\"      # \"free\" space allows for independent facet widths\n  ) + \n  \n  # we define fill colors (values) and the arrangement of the legend (breaks)\n  scale_fill_manual(\n    values = alpha(c(\n      \"Very unhappy\" =  \"#98579B\",   \n      \"Sort of unhappy\" = \"#e8bce8\",                  \n      \"Mixed happy and unhappy\" =  \"#969696\",         \n      \"Sort of happy\" =  \"#7A99AC\",            \n      \"Very happy\" = \"#00263A\",     \n      \"Currently Pregnant\" = \"#cccccc\", \n      \"No response or missing\" = \"#F1F5F7\"\n    )),\n    breaks = c(\n      \"Sort of unhappy\",\n      \"Very unhappy\", \n      \"Very happy\",\n      \"Sort of happy\", \n      \"Mixed happy and unhappy\",\n      \"Currently Pregnant\",\n      \"No response or missing\"\n    )\n  ) + \n  guides(fill = guide_legend(nrow = 2, byrow = T)) + \n  \n  # In theme(), we control labeling for each facet:\n  theme(\n    text = element_text(family = \"cabrito\", size = 10),\n    title = element_text(size = 14, color = \"#00263A\"),\n    plot.subtitle = element_text(size = 12),\n    legend.position = \"bottom\",\n    strip.text = element_blank(), # leaves facet labels blank\n    strip.background = element_blank() # removes background for facet labels\n  ) + \n  \n  # All other labels are defined in labs() \n  labs(\n    title = \"IF YOU GOT PREGNANT NOW, HOW WOULD YOU FEEL?\",\n    subtitle = \"Estimated percentage for populations of women age 15-49 (summer 2020)\",\n    x = NULL,\n    y = NULL,\n    fill = NULL\n  ) \n\n\n\n\n\nA faceted divergent bar chart\nFaceted Question Series\nAnother reason you might want to use facets is to align responses to questions that use a common response scale. For example, the variable COMMCOVIDWORRY uses the same response options shown in COVIDCONCERN, and it reflects each woman’s level of concern for the spread of COVID-19 in her community. If we align two bar charts for COVIDCONCERN and COMMCOVIDWORRY with facet_grid(), we’ll be able to easily compare women’s concerns for personal and communal health.\nFirst, we’ll use pivot_longer to organize responses to COVIDCONCERN and COMMCOVIDWORRY in separate rows. Then, we’ll pre-calculate our summary statistics in a table called covid_pop.\n\n\ncovid_pop <- covid %>% \n  select(COUNTRY, CVQWEIGHT, EAID, COVIDCONCERN, COMMCOVIDWORRY) %>% \n  pivot_longer(\n    c(COVIDCONCERN, COMMCOVIDWORRY),\n    names_to = \"QUESTION\",\n    values_to = \"RESPONSE\"\n  ) %>% \n  as_survey_design(weight = CVQWEIGHT, id = EAID) %>%\n  group_by(COUNTRY, QUESTION, RESPONSE) %>% \n  summarise(PERCENT = survey_mean(vartype = NULL)) %>% \n  filter(grepl(\"concern\", RESPONSE, ignore.case = T)) \n\ncovid_pop\n\n\n# A tibble: 32 x 4\n# Groups:   COUNTRY, QUESTION [8]\n   COUNTRY        QUESTION       RESPONSE           PERCENT\n   <fct>          <chr>          <fct>                <dbl>\n 1 Burkina Faso   COMMCOVIDWORRY Not concerned       0.0422\n 2 Burkina Faso   COMMCOVIDWORRY A little concerned  0.0741\n 3 Burkina Faso   COMMCOVIDWORRY Concerned           0.215 \n 4 Burkina Faso   COMMCOVIDWORRY Very concerned      0.668 \n 5 Burkina Faso   COVIDCONCERN   Not concerned       0.0618\n 6 Burkina Faso   COVIDCONCERN   A little concerned  0.0835\n 7 Burkina Faso   COVIDCONCERN   Concerned           0.177 \n 8 Burkina Faso   COVIDCONCERN   Very concerned      0.677 \n 9 DRC (Kinshasa) COMMCOVIDWORRY Not concerned       0.161 \n10 DRC (Kinshasa) COMMCOVIDWORRY A little concerned  0.0943\n# … with 22 more rows\n\n\nTo keep things simple, we’ll again drop all of the non-response options by filtering only values in RESPONSE containing the word “concern”.\nThis time, we’ll build separate facets for each QUESTION. We’ll also arrange facets in the direction perpendicular to the direction of the bars (i.e. in rows).\n\n\ncovid_pop %>% \n  ggplot(aes(x = COUNTRY, y = PERCENT, fill = RESPONSE)) + \n  geom_bar(stat = \"identity\") + \n  geom_vline(xintercept = 0) +\n  coord_flip() + \n  \n  # This time, we'll add labels to each facet with labeller()\n  facet_grid(\n    rows = vars(QUESTION), \n    scales = \"free\",\n    space = \"free\",\n    labeller = labeller(QUESTION = c(\n      COMMCOVIDWORRY = \"Getting infected\",\n      COVIDCONCERN = \"Spread in community\"\n    ))\n  ) + \n  \n  # Define fill colors (values) and legend orientation\n  scale_fill_manual(\n    values = alpha(colour = c(\n      \"Very concerned\" = \"#00263A\",        # IPUMS Navy\n      \"Concerned\" = \"#4E6C7D\",             # IPUMS Dark-Grey\n      \"A little concerned\" = \"#7A99AC\",    # IPUMS Blue-Grey\n      \"Not concerned\" = \"#F1F5F7\"          # IPUMS Light-Grey\n    ))\n  ) + \n  guides(fill = guide_legend(reverse = TRUE)) + \n  \n  # We'll format the labels defined above in strip.text.y\n  # We also increase the panel.spacing by 1 \"line\"\n  theme(\n    text = element_text(family = \"cabrito\", size = 10),\n    title = element_text(size = 14, color = \"#00263A\"),\n    plot.subtitle = element_text(size = 12),\n    legend.position = \"bottom\",\n    strip.background = element_blank(),\n    strip.text.y = element_text(size = 12, angle = 0),\n    panel.spacing = unit(1, \"lines\")\n  ) + \n  \n  # All other labels are defined in labs() \n  labs(\n    title = \"COVID-19 CONCERNS: PERSONAL VS COMMUNAL\",\n    subtitle = \"Estimated percentage for populations of women age 15-49 (summer 2020)\",\n    x = NULL,\n    y = NULL,\n    fill = NULL\n  ) \n\n\n\n\nNext Steps\nOf course, bar charts are only one of the many ways you might choose to visualize Likert-type data from the PMA COVID-19 survey. We think faceted bar charts are a great way to compare data from several questions that use the same response scale, or to showcase the different types of non-response you’ll find in the top-codes used throughout all of the IPUMS PMA data series.\nThe customization options afforded by ggplot2 are incredibly powerful, but they can also be overwhelming! We’ll practice using tools from ggplot2 again in our next post, where we’ll be thinking about ways to visualize larger batches of related variables.\n\n\n\n",
    "preview": "posts/2021-07-15-covid-likert/images/faceted.png",
    "last_modified": "2021-08-18T14:28:20-05:00",
    "input_file": {},
    "preview_width": 1388,
    "preview_height": 682
  },
  {
    "path": "posts/2021-07-01-covid-tables/",
    "title": "Making Tables with PMA COVID-19 Data",
    "description": "Showcasing the gtsummary package for sample descriptive statistics, weighted population estimates, and model summary output.",
    "author": [
      {
        "name": "Shelby Rutzick",
        "url": "https://www.linkedin.com/in/shelby-rutzick/"
      },
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-07-01",
    "categories": [
      "COVID-19",
      "gtsummary",
      "srvyr",
      "purrr",
      "Descriptive Analysis",
      "Data Manipulation"
    ],
    "contents": "\n\nContents\nSetup\nDescriptive Statistics Table\nDisplay value labels\nCustomize variable labels\nChange default statistics\nCustomize headers & footnotes\n\nSurvey Weights\nModel Summary Table\nResults from one model\nResults from several models\n\nOutput Options\nNext Steps\n\nEarlier this spring, IPUMS PMA released a harmonized version of the PMA COVID-19 survey, which comes from a telephone interview with reproductive aged women in four countries who are all participants in an ongoing panel study related to family planning and reproductive health. We’re excited to feature this urgent resource all summer long here on the Data Analysis Hub; you’ll find find future posts related to PMA COVID-19 data and our last post - an introduction to the available data - if you follow along here.\nAs always, one of our main goals on this blog is to introduce tools that make it easy for anyone to explore new data and develop new ideas for research projects. Today, we’ll be diving into a topic that has probably frustrated everyone who has ever presented or published statistical findings at one time or another: making publication-ready tables.\nIf you talk to students or colleagues who use R, you might be surprised to learn that many of us don’t actually use statistical software to make the tables you see when you read an academic article. In reality, plenty of us just use R to make a model, and then we copy and paste the output into a table we make by hand with Microsoft Word! This can save lots of time, and it’s a perfectly reasonable solution if you know that you can make exactly what you want with the tools that Word provides.\nYou might consider making tables with R if you’ve ever found yourself:\nmanually inserting information to several rows or columns that could be calculated by R (like significance symbols *, **, and ***)\nendlessly tweaking row height, column width, or fonts\ncorrecting copy / paste errors or typos made while transferring information between R and Word\nrevisiting work you’ve already done to format a table for one publication, only to change it all for a different publication, a presentation, or a new file format (PDF, HTML, etc)\n\n\n\n\n© 2020 Daniel D. Sjoberg (MIT)\nIn this post, we’ll show you how to get up and running with flexible, easy-to-make tables using gtsummary, an R package that builds on the same tidyverse conventions we’ve featured elsewhere on this blog. There are a lot of different packages available to help make tables with R, but - as we’ll see - we love gtsummary because we think it allows users to maximize choice of style and output formats, all while minimizing the amount of code necessary to implement those choices.\n\nIf gtsummary doesn’t fit your needs, we also recommend gt (which does much of the heavy-lifting for gtsummary), kableExtra, flextable, and huxtable for different contexts.\nSetup\nIn our last post, we explained that you’ll find all of the PMA COVID-19 survey data if you select the new COVID-19 Unit of Analysis in the IPUMS PMA data extract system.\nIn this post, we’ll work with a data extract containing the following variables. You’ll be able to follow along with our coding examples if you create and download an extract containing all four samples (Female Respondents only) and these variables:\nAGE\nMARSTAT\nEDUCATTGEN\nURBAN\nHLTHCAREDIFFFEAR\nYou’ll also need to install these R packages if you don’t haven’t done so before (current versions are recommended):\nipumsr (0.4.5)\ntidyverse (1.3.1)\nsurvey (4.0)\nsrvyr (1.0.1)\ngtsummary (1.4.1)\nWhen you’ve finished downloading your data extract and installing all of these packages, load the packages and use read_ipums_micro() to load the data extract into R (make sure to change the file paths to match your own extract):\n\n\nlibrary(ipumsr)\nlibrary(tidyverse)\nlibrary(survey)\nlibrary(srvyr)\nlibrary(gtsummary)\n\ncovid <- read_ipums_micro(\n  ddi = \"data/pma_00032.xml\",\n  dat = \"data/pma_00032.dat.gz\"\n)\n\n\n\n\n\n\nAs a reminder: women who were interviewed for the PMA COVID-19 survey are participants in a ongoing panel study focused on core PMA topics in reproductive health. The baseline survey for this panel study was conducted just a few months prior to the COVID-19 survey (between November 2019 and February 2020), but data from the baseline survey are not included in the COVID-19 dataset you’ll download here. We will show how to locate and merge data from the baseline survey in an upcoming post in this series. The COVID-19 survey data are structured like all of the other cross-sectional survey datasets available from IPUMS PMA: each woman’s responses are stored in a single row.\n\n\ncovid\n\n\n# A tibble: 12,184 x 12\n          SAMPLE  COUNTRY  YEAR ROUND   EAID CONSENTFQ CVQWEIGHT   AGE\n       <int+lbl> <int+lb> <int> <dbl>  <dbl> <int+lbl>     <dbl> <int>\n 1 85411 [Burki… 1 [Burk…  2020     1 8.54e8   1 [Yes]     2.13     35\n 2 85411 [Burki… 1 [Burk…  2020     1 8.54e8   1 [Yes]     0.309    29\n 3 85411 [Burki… 1 [Burk…  2020     1 8.54e8   1 [Yes]     0.624    25\n 4 85411 [Burki… 1 [Burk…  2020     1 8.54e8   1 [Yes]     0.265    38\n 5 85411 [Burki… 1 [Burk…  2020     1 8.54e8   1 [Yes]     0.133    30\n 6 85411 [Burki… 1 [Burk…  2020     1 8.54e8   1 [Yes]     0.944    16\n 7 85411 [Burki… 1 [Burk…  2020     1 8.54e8   1 [Yes]     0.361    29\n 8 85411 [Burki… 1 [Burk…  2020     1 8.54e8   1 [Yes]     0.234    29\n 9 85411 [Burki… 1 [Burk…  2020     1 8.54e8   1 [Yes]     2.99     41\n10 85411 [Burki… 1 [Burk…  2020     1 8.54e8   1 [Yes]     0.194    27\n# … with 12,174 more rows, and 4 more variables: MARSTAT <int+lbl>,\n#   EDUCATTGEN <int+lbl>, URBAN <int+lbl>, HLTHCAREDIFFFEAR <int+lbl>\n\n\nEvery COVID-19 data extract will contain 7 preselected variables in addition to those you select, yourself.\nDescriptive Statistics Table\nThe great thing about gtsummary is that you can make a high quality table with just one line of code, but you can also customize any element of your table and easily apply custom styling (you can choose between several journal-specific themes or create your own). And, unlike many of other table-making packages for R, gtsummary supports printing directly to HTML, PDF, Word, and Rich Text Format.\nBecause gtsummary is designed with tidyverse users in-mind, you can pipe functions like dplyr::select directly into the function tbl_summary, which will then identify the object class for each variable and calculate default summary statistics accordingly. To demonstrate, we’ll select a few demographic variables, then break them down by COUNTRY in a basic call to tbl_summary():\n\n\ncovid %>% \n  select(AGE, MARSTAT, EDUCATTGEN, URBAN, COUNTRY) %>% \n  tbl_summary(by = COUNTRY)\n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#dsrccgmvvf .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#dsrccgmvvf .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#dsrccgmvvf .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#dsrccgmvvf .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#dsrccgmvvf .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#dsrccgmvvf .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#dsrccgmvvf .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#dsrccgmvvf .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#dsrccgmvvf .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#dsrccgmvvf .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#dsrccgmvvf .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#dsrccgmvvf .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#dsrccgmvvf .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#dsrccgmvvf .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#dsrccgmvvf .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#dsrccgmvvf .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#dsrccgmvvf .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#dsrccgmvvf .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#dsrccgmvvf .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#dsrccgmvvf .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#dsrccgmvvf .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#dsrccgmvvf .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#dsrccgmvvf .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#dsrccgmvvf .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#dsrccgmvvf .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#dsrccgmvvf .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#dsrccgmvvf .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#dsrccgmvvf .gt_left {\n  text-align: left;\n}\n\n#dsrccgmvvf .gt_center {\n  text-align: center;\n}\n\n#dsrccgmvvf .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#dsrccgmvvf .gt_font_normal {\n  font-weight: normal;\n}\n\n#dsrccgmvvf .gt_font_bold {\n  font-weight: bold;\n}\n\n#dsrccgmvvf .gt_font_italic {\n  font-style: italic;\n}\n\n#dsrccgmvvf .gt_super {\n  font-size: 65%;\n}\n\n#dsrccgmvvf .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\nCharacteristic\n      1, N = 3,5281\n      2, N = 1,3241\n      7, N = 5,9861\n      9, N = 1,3461\n    Age in female respondent questionnaire\n29 (22, 36)\n28 (22, 36)\n30 (24, 38)\n31 (24, 39)Marital status, female questionnaire\n\n\n\n10\n882 (25%)\n605 (46%)\n1,206 (20%)\n348 (26%)21\n2,187 (62%)\n450 (34%)\n3,717 (62%)\n855 (64%)22\n254 (7.2%)\n170 (13%)\n367 (6.1%)\n26 (1.9%)31\n126 (3.6%)\n82 (6.2%)\n493 (8.2%)\n77 (5.7%)32\n78 (2.2%)\n16 (1.2%)\n201 (3.4%)\n39 (2.9%)98\n1 (<0.1%)\n1 (<0.1%)\n2 (<0.1%)\n1 (<0.1%)Highest level of school attended, general (4 categories)\n\n\n\n1\n1,213 (34%)\n4 (0.3%)\n119 (2.0%)\n86 (6.4%)2\n702 (20%)\n40 (3.0%)\n2,598 (43%)\n134 (10.0%)3\n1,374 (39%)\n884 (67%)\n2,217 (37%)\n637 (47%)4\n239 (6.8%)\n395 (30%)\n1,052 (18%)\n486 (36%)98\n0 (0%)\n1 (<0.1%)\n0 (0%)\n3 (0.2%)Urban/rural status\n2,636 (75%)\n0 (NA%)\n2,289 (38%)\n1,182 (88%)Unknown\n0\n1,324\n0\n0\n        \n          1\n          \n           \n          Median (IQR); n (%)\n          \n      \n    \n\n\nThis table is not quite done yet, but notice that all of the fonts and other CSS style elements you see throughout this blog have been automatically applied to this table. Imagine how hard that would be if we made our table with Word or Excel!\n\nℹ Column(s) AGE, MARSTAT, EDUCATTGEN, URBAN, and COUNTRY are class \n'haven_labelled'. This is an intermediate datastructure not meant for analysis. \nConvert columns with `haven::as_factor()`,`labelled::to_factor()`, \n`labelled::unlabelled()`, and `unclass()`. 'haven_labelled' value labels are \nignored when columns are not converted. Failure to convert may have unintended \nconsequences or result in error.\n\n• https://haven.tidyverse.org/articles/semantics.html\n• https://larmarange.github.io/labelled/articles/intro_labelled.html#unlabelled\n\nDisplay value labels\nThis is a very helpful starting point, but it’s certainly not a finished product yet. The biggest issue is related to the alert message shown above: as we’ve discussed in previous posts, the categorical variables you’ll find in IPUMS data extracts are usually imported as haven_labelled objects, rather than the more common factor class of objects. In practice, this means that every response option from the questionnaire has both a value and a label:\n\n\ncovid %>% count(MARSTAT)\n\n\n# A tibble: 6 x 2\n                             MARSTAT     n\n                           <int+lbl> <int>\n1 10 [Never married]                  3041\n2 21 [Currently married]              7209\n3 22 [Currently living with partner]   817\n4 31 [Divorced or separated]           778\n5 32 [Widow or widower]                334\n6 98 [No response or missing]            5\n\n\nYou can access value labels with the ipumsr function ipums_val_labels.\nThe variable MARSTAT is a haven_labelled object where the value of each response is an integer (10, 21, 22, 31, 32, or 98), and the label describing each value is shown in square brackets to the right.\nWhen gtsummary warns you that\nThis is an intermediate datastructure not meant for analysis\n…it’s referring to the fact that labels are only an attribute of the variable. Attributes are metadata meant to assist the analyst running R in real-time, but they aren’t typically used by R in graphics or computational analysis.\nIn our table, gtsummary displays the numeric value for each response, rather than the much more readable labels. The easiest way to change this behavior is to coerce all of our categorical variables to the factor object class.\nWe recommend dividing this process into three steps:\nIdentify any labels in your dataset that represent non-response codes, like No response or missing shown for MARSTAT. We’ll want to exclude these from our table, so we’ll convert them to the generic missing value NA with ipumsr::lbl_na_if().\nIdentify any labelled variables that are not categorical. For example, the variable AGE in our dataset is labelled because the values 90 through 99 can represent either a respondent’s age in years (if not labelled) or a non-response code (if labelled). After converting non-response codes to NA in step 1, we’ll want to change the class of variables like AGE with ipumsr::zap_labels().\nCoerce all remaining labelled variables as factors with ipumsr::as_factor(), and remove any unused response options with fct_drop().\n\n\ncovid <- covid %>% \n  mutate(\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"Logical edit - missing\",\n        \"Not interviewed (female questionnaire)\",\n        \"Not interviewed (household questionnaire)\",\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    )),\n    across(AGE, zap_labels),\n    across(where(is.labelled), ~as_factor(.x) %>% fct_drop)\n  )\n\n\n\nCommon mistake: make sure to use the function as_factor() from ipumsr, and not the base R function as.factor(). The difference is that the former will use labels for each factor level, while the latter will use the original numeric values.\nCoercing categorical variables as factors will greatly improve the readability of our table. Because we’ve introduced NA values, we’ll add the argument missing = \"no\" to exclude them from our table:\n\n\ncovid %>% \n  select(AGE, MARSTAT, EDUCATTGEN, URBAN, COUNTRY) %>% \n  tbl_summary(by = COUNTRY, missing = \"no\")\n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#xzjnyzpawq .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#xzjnyzpawq .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#xzjnyzpawq .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#xzjnyzpawq .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#xzjnyzpawq .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#xzjnyzpawq .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#xzjnyzpawq .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#xzjnyzpawq .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#xzjnyzpawq .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#xzjnyzpawq .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#xzjnyzpawq .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#xzjnyzpawq .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#xzjnyzpawq .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#xzjnyzpawq .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#xzjnyzpawq .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#xzjnyzpawq .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#xzjnyzpawq .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#xzjnyzpawq .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#xzjnyzpawq .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#xzjnyzpawq .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#xzjnyzpawq .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#xzjnyzpawq .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#xzjnyzpawq .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#xzjnyzpawq .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#xzjnyzpawq .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#xzjnyzpawq .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#xzjnyzpawq .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#xzjnyzpawq .gt_left {\n  text-align: left;\n}\n\n#xzjnyzpawq .gt_center {\n  text-align: center;\n}\n\n#xzjnyzpawq .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#xzjnyzpawq .gt_font_normal {\n  font-weight: normal;\n}\n\n#xzjnyzpawq .gt_font_bold {\n  font-weight: bold;\n}\n\n#xzjnyzpawq .gt_font_italic {\n  font-style: italic;\n}\n\n#xzjnyzpawq .gt_super {\n  font-size: 65%;\n}\n\n#xzjnyzpawq .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\nCharacteristic\n      Burkina Faso, N = 3,5281\n      Congo, Democratic Republic, N = 1,3241\n      Kenya, N = 5,9861\n      Nigeria, N = 1,3461\n    Age in female respondent questionnaire\n29 (22, 36)\n28 (22, 36)\n30 (24, 38)\n31 (24, 39)Marital status, female questionnaire\n\n\n\nNever married\n882 (25%)\n605 (46%)\n1,206 (20%)\n348 (26%)Currently married\n2,187 (62%)\n450 (34%)\n3,717 (62%)\n855 (64%)Currently living with partner\n254 (7.2%)\n170 (13%)\n367 (6.1%)\n26 (1.9%)Divorced or separated\n126 (3.6%)\n82 (6.2%)\n493 (8.2%)\n77 (5.7%)Widow or widower\n78 (2.2%)\n16 (1.2%)\n201 (3.4%)\n39 (2.9%)Highest level of school attended, general (4 categories)\n\n\n\nNever attended\n1,213 (34%)\n4 (0.3%)\n119 (2.0%)\n86 (6.4%)Primary/Middle school\n702 (20%)\n40 (3.0%)\n2,598 (43%)\n134 (10.0%)Secondary/post-primary\n1,374 (39%)\n884 (67%)\n2,217 (37%)\n637 (47%)Tertiary/post-secondary\n239 (6.8%)\n395 (30%)\n1,052 (18%)\n486 (36%)Urban/rural status\n\n\n\nRural\n892 (25%)\n0 (NA%)\n3,697 (62%)\n164 (12%)Urban\n2,636 (75%)\n0 (NA%)\n2,289 (38%)\n1,182 (88%)\n        \n          1\n          \n           \n          Median (IQR); n (%)\n          \n      \n    \n\nCustomize variable labels\nYou may have noticed that gtsummary ignored the value labels for each of our haven_labelled variables before we converted them into factor variables, but it did find and use variable labels. For example, you see the variable label Age in female respondent questionnaire instead of the variable name AGE.\n\nYou can access variable labels with the ipumsr function ipums_var_label.\nWhile this is sometimes helpful behavior, we feel that the word “Age” would have been fine on its own. Likewise, we’d like to clean up the labels for MARSTAT, EDUCATTGEN, and URBAN to make them as concise as possible.\nYou can override the ipumsr value labels in your table without changing the underlying data. Just add them as a list of formulas via the label argument: the variable name goes on the left of ~, and a character string containing the desired label goes on the right.\nThere are a few supporting functions that allow you to stylize these labels. We’ll use italicize_labels() to print our new labels in italics.\n\n\ncovid %>% \n  select(AGE, MARSTAT, EDUCATTGEN, URBAN, COUNTRY) %>% \n  tbl_summary(\n    by = COUNTRY,\n    missing = \"no\",\n    label = list(\n      AGE ~ \"Age\",\n      MARSTAT ~ \"Marital status\",\n      EDUCATTGEN ~ \"Education\",\n      URBAN ~ \"Urban vs Rural\"\n    )\n  ) %>% \n  italicize_labels() \n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#jlzrstwmkm .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#jlzrstwmkm .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#jlzrstwmkm .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#jlzrstwmkm .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#jlzrstwmkm .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#jlzrstwmkm .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#jlzrstwmkm .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#jlzrstwmkm .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#jlzrstwmkm .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#jlzrstwmkm .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#jlzrstwmkm .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#jlzrstwmkm .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#jlzrstwmkm .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#jlzrstwmkm .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#jlzrstwmkm .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#jlzrstwmkm .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#jlzrstwmkm .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#jlzrstwmkm .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#jlzrstwmkm .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#jlzrstwmkm .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#jlzrstwmkm .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#jlzrstwmkm .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#jlzrstwmkm .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#jlzrstwmkm .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#jlzrstwmkm .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#jlzrstwmkm .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#jlzrstwmkm .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#jlzrstwmkm .gt_left {\n  text-align: left;\n}\n\n#jlzrstwmkm .gt_center {\n  text-align: center;\n}\n\n#jlzrstwmkm .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#jlzrstwmkm .gt_font_normal {\n  font-weight: normal;\n}\n\n#jlzrstwmkm .gt_font_bold {\n  font-weight: bold;\n}\n\n#jlzrstwmkm .gt_font_italic {\n  font-style: italic;\n}\n\n#jlzrstwmkm .gt_super {\n  font-size: 65%;\n}\n\n#jlzrstwmkm .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\nCharacteristic\n      Burkina Faso, N = 3,5281\n      Congo, Democratic Republic, N = 1,3241\n      Kenya, N = 5,9861\n      Nigeria, N = 1,3461\n    Age\n29 (22, 36)\n28 (22, 36)\n30 (24, 38)\n31 (24, 39)Marital status\n\n\n\nNever married\n882 (25%)\n605 (46%)\n1,206 (20%)\n348 (26%)Currently married\n2,187 (62%)\n450 (34%)\n3,717 (62%)\n855 (64%)Currently living with partner\n254 (7.2%)\n170 (13%)\n367 (6.1%)\n26 (1.9%)Divorced or separated\n126 (3.6%)\n82 (6.2%)\n493 (8.2%)\n77 (5.7%)Widow or widower\n78 (2.2%)\n16 (1.2%)\n201 (3.4%)\n39 (2.9%)Education\n\n\n\nNever attended\n1,213 (34%)\n4 (0.3%)\n119 (2.0%)\n86 (6.4%)Primary/Middle school\n702 (20%)\n40 (3.0%)\n2,598 (43%)\n134 (10.0%)Secondary/post-primary\n1,374 (39%)\n884 (67%)\n2,217 (37%)\n637 (47%)Tertiary/post-secondary\n239 (6.8%)\n395 (30%)\n1,052 (18%)\n486 (36%)Urban vs Rural\n\n\n\nRural\n892 (25%)\n0 (NA%)\n3,697 (62%)\n164 (12%)Urban\n2,636 (75%)\n0 (NA%)\n2,289 (38%)\n1,182 (88%)\n        \n          1\n          \n           \n          Median (IQR); n (%)\n          \n      \n    \n\nChange default statistics\nWhat about the statistics that tbl_summary() calculates? By default, tbl_summary() reports the median (and IQR) for integer variables like AGE, and it reports the frequency (and percentage) of each level for all of the factor variables we’ve created.\nYou can change the statistics calculated for one or more variables by name, or you can change them for all variables of a similar type (e.g. all_categorical). You’ll need to choose or define a custom function, then provide it to the stat argument as a character string between curly brackets like this:\n\"{mean}\"\nHere, we’ll demonstrate how to calculate the mean (and standard deviation) for AGE, and the percentage for all responses to all factors / categorical variables.\nYou may also decide to feature information about which statistics were calculated more prominently alongside the variable labels, rather than in the footer. Simply pipe your table to the function add_stat_label() as shown below:\n\n\ncovid %>% \n  select(AGE, MARSTAT, EDUCATTGEN, URBAN, COUNTRY) %>% \n  tbl_summary(\n    by = COUNTRY,\n    missing = \"no\",\n    label = list(\n      AGE ~ \"Age\",\n      MARSTAT ~ \"Marital status\",\n      EDUCATTGEN ~ \"Education\",\n      URBAN ~ \"Urban vs Rural\"\n    ),\n    stat = list(\n      AGE ~ \"{mean} ({sd})\",\n      all_categorical() ~\"{p}\"\n    )\n  )%>% \n  italicize_labels() %>% \n  add_stat_label()\n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#eqvdsbjivr .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#eqvdsbjivr .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#eqvdsbjivr .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#eqvdsbjivr .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#eqvdsbjivr .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#eqvdsbjivr .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#eqvdsbjivr .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#eqvdsbjivr .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#eqvdsbjivr .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#eqvdsbjivr .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#eqvdsbjivr .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#eqvdsbjivr .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#eqvdsbjivr .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#eqvdsbjivr .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#eqvdsbjivr .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#eqvdsbjivr .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#eqvdsbjivr .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#eqvdsbjivr .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#eqvdsbjivr .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#eqvdsbjivr .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#eqvdsbjivr .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#eqvdsbjivr .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#eqvdsbjivr .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#eqvdsbjivr .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#eqvdsbjivr .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#eqvdsbjivr .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#eqvdsbjivr .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#eqvdsbjivr .gt_left {\n  text-align: left;\n}\n\n#eqvdsbjivr .gt_center {\n  text-align: center;\n}\n\n#eqvdsbjivr .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#eqvdsbjivr .gt_font_normal {\n  font-weight: normal;\n}\n\n#eqvdsbjivr .gt_font_bold {\n  font-weight: bold;\n}\n\n#eqvdsbjivr .gt_font_italic {\n  font-style: italic;\n}\n\n#eqvdsbjivr .gt_super {\n  font-size: 65%;\n}\n\n#eqvdsbjivr .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\nCharacteristic\n      Burkina Faso, N = 3,528\n      Congo, Democratic Republic, N = 1,324\n      Kenya, N = 5,986\n      Nigeria, N = 1,346\n    Age, Mean (SD)\n30 (9)\n29 (9)\n31 (9)\n31 (9)Marital status, %\n\n\n\nNever married\n25\n46\n20\n26Currently married\n62\n34\n62\n64Currently living with partner\n7.2\n13\n6.1\n1.9Divorced or separated\n3.6\n6.2\n8.2\n5.7Widow or widower\n2.2\n1.2\n3.4\n2.9Education, %\n\n\n\nNever attended\n34\n0.3\n2.0\n6.4Primary/Middle school\n20\n3.0\n43\n10.0Secondary/post-primary\n39\n67\n37\n47Tertiary/post-secondary\n6.8\n30\n18\n36Urban vs Rural, %\n\n\n\nRural\n25\nNA\n62\n12Urban\n75\nNA\n38\n88\n\n\nMore information about functions for table statistics can be found here.\nCustomize headers & footnotes\nTable headers and footnotes can be customized with the functions modify_header() and modify_footnote(). We’ll remove the frequencies from our header, use the abbreviated label DR Congo, and remove the label Characteristic from the first column. Lastly, we’ll add a title to our table with modify_spanning_header().\n\n\ncovid %>% \n  select(AGE, MARSTAT, EDUCATTGEN, URBAN, COUNTRY) %>% \n  tbl_summary(\n    by = COUNTRY,\n    missing = \"no\",\n    stat = list(\n      AGE ~ \"{mean} ({sd})\",\n      all_categorical() ~\"{p}\"\n    ),\n    label = list(\n      AGE ~ \"Age\",\n      MARSTAT ~ \"Marital status\",\n      EDUCATTGEN ~ \"Education\",\n      URBAN ~ \"Urban vs Rural\"\n    )\n  ) %>% \n  add_stat_label() %>% \n  italicize_labels() %>% \n  modify_header(update = list(\n      label ~ \" \",\n      stat_1 ~ \"**Burkina Faso**\",\n      stat_2 ~ \"**DR Congo <br> (Kinshasa)**\",\n      stat_3 ~ \"**Kenya**\",\n      stat_4 ~ \"**Nigeria <br> (Lagos & Kano)**\"\n  )) %>% \n  modify_spanning_header(\n    everything() ~ \"## Sample Demographics\"\n  ) \n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#ngdrevjbzf .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#ngdrevjbzf .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#ngdrevjbzf .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#ngdrevjbzf .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#ngdrevjbzf .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#ngdrevjbzf .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#ngdrevjbzf .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#ngdrevjbzf .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#ngdrevjbzf .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#ngdrevjbzf .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#ngdrevjbzf .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#ngdrevjbzf .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#ngdrevjbzf .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#ngdrevjbzf .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#ngdrevjbzf .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#ngdrevjbzf .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#ngdrevjbzf .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#ngdrevjbzf .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ngdrevjbzf .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#ngdrevjbzf .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#ngdrevjbzf .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#ngdrevjbzf .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#ngdrevjbzf .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#ngdrevjbzf .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#ngdrevjbzf .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#ngdrevjbzf .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#ngdrevjbzf .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#ngdrevjbzf .gt_left {\n  text-align: left;\n}\n\n#ngdrevjbzf .gt_center {\n  text-align: center;\n}\n\n#ngdrevjbzf .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#ngdrevjbzf .gt_font_normal {\n  font-weight: normal;\n}\n\n#ngdrevjbzf .gt_font_bold {\n  font-weight: bold;\n}\n\n#ngdrevjbzf .gt_font_italic {\n  font-style: italic;\n}\n\n#ngdrevjbzf .gt_super {\n  font-size: 65%;\n}\n\n#ngdrevjbzf .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\n\n        Sample Demographics\n\n      \n    \n      Burkina Faso\n      DR Congo  (Kinshasa)\n      Kenya\n      Nigeria  (Lagos & Kano)\n    Age, Mean (SD)\n30 (9)\n29 (9)\n31 (9)\n31 (9)Marital status, %\n\n\n\nNever married\n25\n46\n20\n26Currently married\n62\n34\n62\n64Currently living with partner\n7.2\n13\n6.1\n1.9Divorced or separated\n3.6\n6.2\n8.2\n5.7Widow or widower\n2.2\n1.2\n3.4\n2.9Education, %\n\n\n\nNever attended\n34\n0.3\n2.0\n6.4Primary/Middle school\n20\n3.0\n43\n10.0Secondary/post-primary\n39\n67\n37\n47Tertiary/post-secondary\n6.8\n30\n18\n36Urban vs Rural, %\n\n\n\nRural\n25\nNA\n62\n12Urban\n75\nNA\n38\n88\n\nText provided to modify_header(), modify_footnote(), and modify_spanning_header() can be stylized with either markdown syntax (shown) or HTML.\nHeaders can span individuals columns (as shown in modify_header()), groups of columns, or everything() (as shown in modify_spanning_header()).\nTip: if you simply want to move the sample size from the header into its own row (rather than delete it, as we’ve done here), you could create a new factor variable N where every person gets the value 1 in the step after our select() function.\nSurvey Weights\nWe mentioned in our last post that the PMA COVID-19 survey comes with a new weighting variable, CVQWEIGHT, which is analogous to the variable FQWEIGHT found in other Household and Female samples. CVQWEIGHT can be used to estimate all of the statistics shown in our table for the broader population represented by each sample (note that two of the samples are not nationally representative):\n\nYou’ll find more detail about the construction of PMA COVID-19 survey weights here.\nBurkina Faso: nationally representative\nKenya: nationally representative\nDRC: Kinshasa only\nNigeria: Lagos and Kano only\nThe srvyr package includes several functions that make it easy to incorporate survey weights into a tidy workflow. Simply provide information about the survey design to srvyr::as_survey_design(), and then pipe this information to a survey analysis function.\nFor example, a tidy workflow calculating the mean AGE of women in each of the four samples might look like this:\n\n\ncovid %>% \n  group_by(COUNTRY) %>% \n  summarise(mean(AGE))\n\n\n# A tibble: 4 x 2\n  COUNTRY                    `mean(AGE)`\n  <fct>                            <dbl>\n1 Burkina Faso                      29.7\n2 Congo, Democratic Republic        29.5\n3 Kenya                             31.1\n4 Nigeria                           31.4\n\nYou can use CVQWEIGHT to estimate the mean AGE of reproductive age women in each of the four target populations like this:\n\n\ncovid %>% \n  as_survey_design(weight = CVQWEIGHT) %>%\n  group_by(COUNTRY) %>% \n  summarise(survey_mean(AGE))\n\n\n# A tibble: 4 x 3\n  COUNTRY                     coef `_se`\n  <fct>                      <dbl> <dbl>\n1 Burkina Faso                27.9 0.370\n2 Congo, Democratic Republic  28.9 0.327\n3 Kenya                       28.9 0.186\n4 Nigeria                     30.4 0.329\n\n\nNotice that all four samples skew a bit older compared to their target populations.\nHappily, gtsummary can read survey information from the function as_survey_design(). This saves us the trouble of calculating weighted statistics for each of the variables in our table; instead, we can create weighted statistics for our table with just one line of code if we swap tbl_summary for its companion function, tbl_svysummary():\n\n\ncovid %>% \n  as_survey_design(weight = CVQWEIGHT) %>% # CVQWEIGHT goes here\n  select(AGE, MARSTAT, EDUCATTGEN, URBAN, COUNTRY) %>% \n  tbl_svysummary( # use tbl_svysummary() in place of tbl_summary()\n    by = COUNTRY,\n    missing = \"no\",\n    stat = list(\n      AGE ~ \"{mean} ({sd})\",\n      all_categorical() ~\"{p}\"\n    ),\n    label = list(\n      AGE ~ \"Age\",\n      MARSTAT ~ \"Marital status\",\n      EDUCATTGEN ~ \"Education\",\n      URBAN ~ \"Urban vs Rural\"\n    )\n  ) %>% \n  italicize_labels() %>% \n  add_stat_label() %>% \n  modify_header(update = list(\n      label ~ \" \",\n      stat_1 ~ \"**Burkina Faso**\",\n      stat_2 ~ \"**DR Congo <br> (Kinshasa)**\",\n      stat_3 ~ \"**Kenya**\",\n      stat_4 ~ \"**Nigeria <br> (Lagos & Kano)**\"\n  )) %>% \n  modify_spanning_header(\n    everything() ~ \"## Weighted Population Estimates\"\n  )\n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#szjdcizczs .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#szjdcizczs .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#szjdcizczs .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#szjdcizczs .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#szjdcizczs .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#szjdcizczs .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#szjdcizczs .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#szjdcizczs .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#szjdcizczs .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#szjdcizczs .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#szjdcizczs .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#szjdcizczs .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#szjdcizczs .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#szjdcizczs .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#szjdcizczs .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#szjdcizczs .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#szjdcizczs .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#szjdcizczs .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#szjdcizczs .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#szjdcizczs .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#szjdcizczs .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#szjdcizczs .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#szjdcizczs .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#szjdcizczs .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#szjdcizczs .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#szjdcizczs .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#szjdcizczs .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#szjdcizczs .gt_left {\n  text-align: left;\n}\n\n#szjdcizczs .gt_center {\n  text-align: center;\n}\n\n#szjdcizczs .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#szjdcizczs .gt_font_normal {\n  font-weight: normal;\n}\n\n#szjdcizczs .gt_font_bold {\n  font-weight: bold;\n}\n\n#szjdcizczs .gt_font_italic {\n  font-style: italic;\n}\n\n#szjdcizczs .gt_super {\n  font-size: 65%;\n}\n\n#szjdcizczs .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\n\n        Weighted Population Estimates\n\n      \n    \n      Burkina Faso\n      DR Congo  (Kinshasa)\n      Kenya\n      Nigeria  (Lagos & Kano)\n    Age, Mean (SD)\n28 (9)\n29 (9)\n29 (9)\n30 (9)Marital status, %\n\n\n\nNever married\n22\n47\n30\n27Currently married\n69\n31\n54\n63Currently living with partner\n6.1\n13\n5.8\n2.4Divorced or separated\n1.9\n6.3\n7.3\n5.1Widow or widower\n1.4\n1.9\n2.8\n2.3Education, %\n\n\n\nNever attended\n55\n0.4\n1.9\n9.8Primary/Middle school\n19\n5.7\n46\n13Secondary/post-primary\n24\n72\n39\n46Tertiary/post-secondary\n1.8\n22\n13\n31Urban vs Rural, %\n\n\n\nRural\n79\nNA\n73\n18Urban\n21\nNA\n27\n82\n\n\n\n\n\n© Greg Freedman Ellis et al. (GPL 2 | GPL 3)\nWe’ve now made a weighted descriptive statistics table, and we’ve only changed two lines of code. As we’ll see, creating a summary table from a model that uses sample design information can be just as easy.\nModel Summary Table\nThe gtsummary package also contains a function designed to summarise and format output from regression models. For example, let’s build a simple logistic regression model for HLTHCAREDIFFFEAR, which indicates whether a woman experienced difficulty accessing healthcare because she was afraid of becoming infected with COVID-19. We’ll try modeling this outcome using the demographic variables that are available for all four samples: AGE, MARSTAT, and EDUCATTGEN (URBAN was not available for the DRC sample).\nFirst, we’ll recode HLTHCAREDIFFFEAR into a binary indicator that’s suitable for use in a logistic regression model. We can collapse responses “No” and “None of the above”, since the latter indicates that the woman experienced no difficulties accessing healthcare at all.\n\n\ncovid %>% count(HLTHCAREDIFFFEAR)\n\n\n# A tibble: 4 x 2\n  HLTHCAREDIFFFEAR      n\n  <fct>             <int>\n1 No                 1101\n2 Yes                3881\n3 None of the above  7114\n4 <NA>                 88\n\ncovid <- covid %>% \n  mutate(HLTHCAREDIFFFEAR = fct_collapse(HLTHCAREDIFFFEAR, No = c(\n    \"No\", \n    \"None of the above\"\n  ))) \n\ncovid %>% count(HLTHCAREDIFFFEAR)\n\n\n# A tibble: 3 x 2\n  HLTHCAREDIFFFEAR     n\n  <fct>            <int>\n1 No                8215\n2 Yes               3881\n3 <NA>                88\n\nResults from one model\nBecause IPUMS PMA samples are collected with geographic clusters - represented by EAID - we generally recommend specifying both a sample weight and a cluster identification with as_survey_design(). We’ll need to use a function that can build a general linear model using that survey design information, so we’ll use survey::svyglm(), rather than the base R function glm that might be more familiar. The output of a call to svyglm() is not formatted as a publication-ready table. Let’s see what happens when we build a model for our Burkina Faso sample:\n\n\ncovid %>% \n  filter(COUNTRY == \"Burkina Faso\") %>% \n  as_survey_design(weight = CVQWEIGHT, id = EAID) %>% \n  survey::svyglm(\n    formula = HLTHCAREDIFFFEAR ~ AGE + MARSTAT + EDUCATTGEN, \n    family = \"quasibinomial\"\n  ) \n\n\n1 - level Cluster Sampling design (with replacement)\nWith (164) clusters.\nCalled via srvyr\nSampling variables:\n - ids: EAID\n - weights: CVQWEIGHT\n\nCall:  svyglm(formula = HLTHCAREDIFFFEAR ~ AGE + MARSTAT + EDUCATTGEN, \n    design = ., family = \"quasibinomial\")\n\nCoefficients:\n                         (Intercept)  \n                             -0.7495  \n                                 AGE  \n                             -0.0128  \n            MARSTATCurrently married  \n                              0.5784  \nMARSTATCurrently living with partner  \n                             -0.3228  \n        MARSTATDivorced or separated  \n                              0.3920  \n             MARSTATWidow or widower  \n                              0.1639  \n     EDUCATTGENPrimary/Middle school  \n                             -0.0782  \n    EDUCATTGENSecondary/post-primary  \n                             -0.3467  \n   EDUCATTGENTertiary/post-secondary  \n                              0.1427  \n\nDegrees of Freedom: 3526 Total (i.e. Null);  155 Residual\n  (1 observation deleted due to missingness)\nNull Deviance:      4381 \nResidual Deviance: 4299     AIC: NA\n\n\nBecause our use of weights results in non-integer outcomes, we’ll also need to use the “quasibinomial” modeling distribution, rather than the more typical “binomial” distribution.\nThe function gtsummary::tbl_regression() will tidy this output into a much more reader-friendly format. We’ll assign labels to each of our variables using the same label argument we saw before, and we’ll also choose to exponentiate our regression coefficients so that the results will reflect odds ratios:\n\n\ncovid %>% \n  filter(COUNTRY == \"Burkina Faso\") %>% \n  as_survey_design(weight = CVQWEIGHT, id = EAID) %>% \n  survey::svyglm(\n    formula = HLTHCAREDIFFFEAR ~ AGE + MARSTAT + EDUCATTGEN, \n    family = \"quasibinomial\"\n  ) %>% \n  tbl_regression(\n    exponentiate = TRUE,\n    label = list(\n      AGE ~ \"Age\",\n      MARSTAT ~ \"Marital status\",\n      EDUCATTGEN ~ \"Education\"   \n    )\n  ) \n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#vajexzdwpf .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#vajexzdwpf .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#vajexzdwpf .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#vajexzdwpf .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#vajexzdwpf .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#vajexzdwpf .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#vajexzdwpf .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#vajexzdwpf .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#vajexzdwpf .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#vajexzdwpf .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#vajexzdwpf .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#vajexzdwpf .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#vajexzdwpf .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#vajexzdwpf .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#vajexzdwpf .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#vajexzdwpf .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#vajexzdwpf .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#vajexzdwpf .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vajexzdwpf .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#vajexzdwpf .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vajexzdwpf .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#vajexzdwpf .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#vajexzdwpf .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#vajexzdwpf .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#vajexzdwpf .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#vajexzdwpf .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#vajexzdwpf .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#vajexzdwpf .gt_left {\n  text-align: left;\n}\n\n#vajexzdwpf .gt_center {\n  text-align: center;\n}\n\n#vajexzdwpf .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#vajexzdwpf .gt_font_normal {\n  font-weight: normal;\n}\n\n#vajexzdwpf .gt_font_bold {\n  font-weight: bold;\n}\n\n#vajexzdwpf .gt_font_italic {\n  font-style: italic;\n}\n\n#vajexzdwpf .gt_super {\n  font-size: 65%;\n}\n\n#vajexzdwpf .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\nCharacteristic\n      OR1\n      95% CI1\n      p-value\n    Age\n0.99\n0.96, 1.01\n0.3Marital status\n\n\nNever married\n—\n—\nCurrently married\n1.78\n1.02, 3.11\n0.043Currently living with partner\n0.72\n0.26, 2.04\n0.5Divorced or separated\n1.48\n0.69, 3.16\n0.3Widow or widower\n1.18\n0.41, 3.39\n0.8Education\n\n\nNever attended\n—\n—\nPrimary/Middle school\n0.92\n0.61, 1.40\n0.7Secondary/post-primary\n0.71\n0.45, 1.12\n0.14Tertiary/post-secondary\n1.15\n0.66, 2.01\n0.6\n        \n          1\n          \n           \n          OR = Odds Ratio, CI = Confidence Interval\n          \n      \n    \n\nWe can also add conventional “stars” representing the significance of each coefficient with the add_significance_stars() function. Here, we could choose to display cluster-robust standard error estimates, but we’ll display 95% confidence intervals instead. We’ll also customize the header and add a title, using the same modify functions shown above.\n\n\ncovid %>% \n  filter(COUNTRY == \"Burkina Faso\") %>% \n  as_survey_design(weight = CVQWEIGHT, id = EAID) %>% \n  survey::svyglm(\n    formula = HLTHCAREDIFFFEAR ~ AGE + MARSTAT + EDUCATTGEN, \n    family = \"quasibinomial\"\n  ) %>% \n  tbl_regression(\n    exp = TRUE,\n    label = list(\n      AGE ~ \"Age\",\n      MARSTAT ~ \"Marital status\",\n      EDUCATTGEN ~ \"Education\"   \n    )\n  ) %>%\n  modify_footnote(everything() ~ NA, abbreviation = TRUE) %>%\n  add_significance_stars(hide_se = TRUE, hide_ci = FALSE) %>%\n  modify_header(update = list(\n    label ~ \" \" ,\n    estimate ~ '**Burkina Faso**',\n    ci ~ \"95% CI\"\n  )) %>% \n  modify_spanning_header(\n    everything() ~ \"## Odds Ratios obtained from Logistic Regression\"\n  ) \n\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#sxpcxfeabb .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#sxpcxfeabb .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#sxpcxfeabb .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#sxpcxfeabb .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#sxpcxfeabb .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#sxpcxfeabb .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#sxpcxfeabb .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#sxpcxfeabb .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#sxpcxfeabb .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#sxpcxfeabb .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#sxpcxfeabb .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#sxpcxfeabb .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#sxpcxfeabb .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#sxpcxfeabb .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#sxpcxfeabb .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#sxpcxfeabb .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#sxpcxfeabb .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#sxpcxfeabb .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#sxpcxfeabb .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#sxpcxfeabb .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#sxpcxfeabb .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#sxpcxfeabb .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#sxpcxfeabb .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#sxpcxfeabb .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#sxpcxfeabb .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#sxpcxfeabb .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#sxpcxfeabb .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#sxpcxfeabb .gt_left {\n  text-align: left;\n}\n\n#sxpcxfeabb .gt_center {\n  text-align: center;\n}\n\n#sxpcxfeabb .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#sxpcxfeabb .gt_font_normal {\n  font-weight: normal;\n}\n\n#sxpcxfeabb .gt_font_bold {\n  font-weight: bold;\n}\n\n#sxpcxfeabb .gt_font_italic {\n  font-style: italic;\n}\n\n#sxpcxfeabb .gt_super {\n  font-size: 65%;\n}\n\n#sxpcxfeabb .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\n\n        Odds Ratios obtained from Logistic Regression\n\n      \n    \n      Burkina Faso1\n      95% CI\n    Age\n0.99\n0.96, 1.01Marital status\n\nNever married\n—\n—Currently married\n1.78*\n1.02, 3.11Currently living with partner\n0.72\n0.26, 2.04Divorced or separated\n1.48\n0.69, 3.16Widow or widower\n1.18\n0.41, 3.39Education\n\nNever attended\n—\n—Primary/Middle school\n0.92\n0.61, 1.40Secondary/post-primary\n0.71\n0.45, 1.12Tertiary/post-secondary\n1.15\n0.66, 2.01\n        \n          1\n          \n           \n          *p<0.05; **p<0.01; ***p<0.001\n          \n      \n    \n\nResults from several models\nResearchers often report results from several models if, for example, they want to compare results including a range of different controls. You might also decide to model data from each of the COVID-19 samples separately in order to highlight important differences between their target populations. How would you merge results from four models into a single table?\nIf you repeat the same code shown above for each of the four samples, you’ll obtain four separate tables. We recommend using purrr::map() to store these tables in a list, which you can then pass to gtsummary::tbl_merge(). Here, we map over each of the factor levels in COUNTRY, using tbl_regression() to build a regression table for each. We pipe a list of four tables to tbl_merge(), and then add a header and title.\n\n\nlevels(covid$COUNTRY) %>% \n  map(~{\n    covid %>% \n      as_survey_design(weight = CVQWEIGHT) %>% \n      filter(COUNTRY == .x) %>% \n      survey::svyglm(\n        formula = HLTHCAREDIFFFEAR ~ AGE + MARSTAT + EDUCATTGEN, \n        family = \"quasibinomial\"\n      ) %>% \n      tbl_regression(\n        exp = TRUE,\n        label = list(\n          AGE ~ \"Age\",\n          MARSTAT ~ \"Marital status\",\n          EDUCATTGEN ~ \"Education\"   \n        )\n      ) %>% \n      italicize_labels() %>% \n      modify_footnote(everything() ~ NA, abbreviation = TRUE) %>%\n      add_significance_stars(hide_se = T, hide_ci = F) %>% \n      modify_header(update = list(ci ~ \"95% CI\"))\n  }) %>% \n  tbl_merge() %>% \n  modify_header(update = list(\n    label ~ \" \" ,\n    estimate_1 ~ '**Burkina Faso**',\n    estimate_2 ~ '**DR Congo <br> (Kinshasa)**',\n    estimate_3 ~ '**Kenya**',\n    estimate_4 ~ '**Nigeria <br> (Lagos & Kano)**'\n  )) %>% \n  modify_spanning_header(\n    everything() ~ \"## Odds Ratios obtained from Logistic Regression\"\n  ) \n\n\n\n\n\n\n\n© RStudio (CC0 1.0)\npurrr is included with library(tidyverse).\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#klfevxmdhs .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#klfevxmdhs .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#klfevxmdhs .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#klfevxmdhs .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#klfevxmdhs .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#klfevxmdhs .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#klfevxmdhs .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#klfevxmdhs .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#klfevxmdhs .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#klfevxmdhs .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#klfevxmdhs .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#klfevxmdhs .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#klfevxmdhs .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#klfevxmdhs .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#klfevxmdhs .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#klfevxmdhs .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#klfevxmdhs .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#klfevxmdhs .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#klfevxmdhs .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#klfevxmdhs .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#klfevxmdhs .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#klfevxmdhs .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#klfevxmdhs .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#klfevxmdhs .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#klfevxmdhs .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#klfevxmdhs .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#klfevxmdhs .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#klfevxmdhs .gt_left {\n  text-align: left;\n}\n\n#klfevxmdhs .gt_center {\n  text-align: center;\n}\n\n#klfevxmdhs .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#klfevxmdhs .gt_font_normal {\n  font-weight: normal;\n}\n\n#klfevxmdhs .gt_font_bold {\n  font-weight: bold;\n}\n\n#klfevxmdhs .gt_font_italic {\n  font-style: italic;\n}\n\n#klfevxmdhs .gt_super {\n  font-size: 65%;\n}\n\n#klfevxmdhs .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 65%;\n}\n\n        Odds Ratios obtained from Logistic Regression\n\n      \n    \n      Burkina Faso1\n      95% CI\n      DR Congo  (Kinshasa)1\n      95% CI\n      Kenya1\n      95% CI\n      Nigeria  (Lagos & Kano)1\n      95% CI\n    Age\n0.99\n0.97, 1.01\n0.97*\n0.95, 1.00\n0.98***\n0.97, 0.99\n0.99\n0.97, 1.01Marital status\n\n\n\n\n\n\n\nNever married\n—\n—\n—\n—\n—\n—\n—\n—Currently married\n1.78\n0.92, 3.45\n1.16\n0.73, 1.83\n1.21\n0.97, 1.52\n1.64\n0.95, 2.83Currently living with partner\n0.72\n0.32, 1.62\n0.55\n0.27, 1.11\n0.81\n0.59, 1.11\n2.81\n0.94, 8.39Divorced or separated\n1.48\n0.50, 4.35\n1.94\n0.95, 3.97\n1.26\n0.92, 1.73\n1.10\n0.49, 2.49Widow or widower\n1.18\n0.38, 3.62\n0.70\n0.08, 6.09\n1.09\n0.72, 1.65\n0.39\n0.10, 1.56Education\n\n\n\n\n\n\n\nNever attended\n—\n—\n—\n—\n—\n—\n—\n—Primary/Middle school\n0.92\n0.58, 1.47\n0.39\n0.02, 6.04\n1.50\n0.95, 2.38\n0.85\n0.33, 2.17Secondary/post-primary\n0.71\n0.43, 1.16\n0.99\n0.10, 9.65\n1.31\n0.82, 2.09\n0.80\n0.37, 1.73Tertiary/post-secondary\n1.15\n0.71, 1.88\n1.29\n0.13, 12.7\n1.47\n0.91, 2.37\n0.78\n0.35, 1.74\n        \n          1\n          \n           \n          *p<0.05; **p<0.01; ***p<0.001\n          \n      \n    \n\nOutput Options\nSo now you know that you can make a great-looking table in R that renders nicely in HTML (you’re reading this post on a web page, after all). But what if you want to export your table to a Word document, PDF, or some other format?\nFor Word documents, try piping your table to the function gtsummary::as_flex_table() like this, and then copy / paste the result directly into Word. (You may need to install the package flextable first).\n\n\ncovid %>% \n  select(AGE, MARSTAT, EDUCATTGEN, URBAN, COUNTRY) %>% \n  tbl_summary(by = COUNTRY) %>% \n  as_flex_table()\n\n\n\nSkip the copy and paste step! Did you know that you can create Word documents in R with RMarkdown?\nSeveral printers work well for PDF output, including the very popular kable_extra. If you’re making a PDF with RMarkdown, for example, you could pipe your table into gtsummary::as_kable_extra(). (You may need to install the package kable_extra first).\n\n\ncovid %>% \n  select(AGE, MARSTAT, EDUCATTGEN, URBAN, COUNTRY) %>% \n  tbl_summary(by = COUNTRY) %>% \n  as_kable_extra()\n\n\n\nYou’ll find more information about output options here.\nNext Steps\nWe hope you found these steps helpful in using R to create tables for descriptive statistics, survey weights, and model summary output. As always, feel free to reach out to us with any questions. In our next post, we will show how to make likert-style stacked bar charts with R using the PMA COVID-19 survey data. Check back here in two weeks for the next post in this series!\n\n\n\n",
    "preview": "posts/2021-07-01-covid-tables/images/gtsummary_wide.png",
    "last_modified": "2021-08-18T14:28:20-05:00",
    "input_file": {},
    "preview_width": 483,
    "preview_height": 230
  },
  {
    "path": "posts/2021-06-15-covid-discovery/",
    "title": "New PMA COVID-19 Survey Data",
    "description": "A new panel study promises insights into the impact of COVID-19 on family planning and reproductive health.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-06-15",
    "categories": [
      "COVID-19",
      "Panel Data",
      "Data Discovery",
      "New Data"
    ],
    "contents": "\n\nContents\nSample Design\nTopics\nHealthcare Access\nCOVID-related Experience\nCOVID Information Sources\nCOVID Knowledge\nCOVID Prevention\nPerceptions Around COVID\n\nNext Steps\n\nThe COVID-19 pandemic has strained healthcare systems across the globe, and researchers are already beginning to examine the short-term impacts of service disruption on family planning and reproductive health.1 This spring, IPUMS PMA released COVID-19 survey data collected from reproductive age women between May and August 2020 in these countries:\nBurkina Faso\nDemocratic Republic of Congo (DRC)\nKenya\nNigeria\n\n\n\nThese women are participants in an ongoing panel study focused on core PMA topics in reproductive health. The baseline survey data for this study have already been released, and we will demonstrate how to link records between the baseline survey and the COVID-19 follow-up in an upcoming post in this series. Subsequent waves of the panel study will help to show how women’s backgrounds and levels of knowledge, perceptions, and experiences with COVID-19 shape long-term family planning outcomes.\nClick here for more information on the COVID-19 Survey design, and to learn how it fits with the ongoing panel study.\nIn this post, we’ll cover the contents of the PMA COVID-19 survey. If you’re a registered IPUMS PMA user, you can obtain COVID-19 survey data by navigating to the new COVID-19 “Unit of Analysis.”\n\n\n\nClick here for help creating, downloading, and importing an IPUMS PMA data extract into R.\nSample Design\nThe PMA COVID-19 survey is a follow-up telephone survey administered to women who participated in an in-person baseline survey for a broader panel study. This baseline survey was collected between November 2019 and Februrary 2020 - prior to the appearance of COVID-19 in most countries.\nWhen the outbreak of COVID-19 grew into a global pandemic in the spring of 2020, PMA representatives partnered with the Ministries of Health in DRC, Kenya, Burkina Faso, and Nigeria to design a shorter - approximately 30 minute - survey responding directly to the effect of COVID-19 on women and their households.\n\nSeveral countries participating in the new PMA panel study had not completed baseline sample collection by March 2020 (Uganda, India, Niger, Cote d’Ivoire).\nWomen were selected for the baseline survey if they were age 15-49 and resided in a household screened at random from a sample cluster represented by EAID. All women surveyed at baseline where eligible to participate in the COVID-19 follow-up, provided that they 1) agreed to the interview, and 2) owned or had access to a telephone.\nA COVID-19 module was incorporated into their baseline surveys in late 2020, but these data have not yet been released.\nYou’ll find survey weights adjusted for the probability that a given woman had access to a telephone recorded in the new variable CVQWEIGHT. This weight is normalized for the target population of each sample (note that two of the samples are not nationally representative):\nBurkina Faso: nationally representative\nKenya: nationally representative\nDRC: Kinshasa only\nNigeria: Lagos and Kano only\nYou’ll find more detail about the construction of PMA COVID-19 survey weights here. For information about response rates for each sample, check out sample-specific Dataset Notes.\nTopics\nThe COVID-19 survey included a number of questions that you’ll also find in the baseline survey and in future rounds of the panel study. These include topics like fertility preferences, current or recent use of family planning, and core demographic information. You might use these variables, for example, to see if women who were using a particular contraceptive method at the time of the baseline survey had stopped using that method during the first few months of the COVID-19 outbreak.\nVariables from the remainder of the COVID-19 questionnaire are organized on the IPUMS PMA website under 6 topic headings:\nHealthcare Access\nCOVID-related Experience\nCOVID Information Sources\nCOVID Knowledge\nCOVID Prevention\nPerceptions Around COVID\n\n\n\nHealthcare Access\nAll sampled women report whether they have needed to visit a health facility since COVID-19 restrictions began - including family planning visits - in CVFACVISIT. Additionally, all sampled women report whether they experienced any of the following difficulties accessing healthcare services during the same time period (select all that apply, or none):\nfacility closed / no appointment available\nnot affordable\npartner does not approve\nno available transportation\ngovernment restrictions on movement\nfear of being infected with COVID-19 at healthcare facilities\nFinally, women who did visit a healthcare facility since COVID-19 restrictions began report whether they successfully accessed needed services in HCACCESS.\nCOVID-related Experience\nIn addition to their own experiences accessing healthcare during the outbreak, women who confirmed that they had heard or read about COVID-19 were also asked to report the impact of the virus on their communities and in their households.\nSpecifically, these women were asked to estimate whether most, some, few, or no people in their community had been infected, and whether any close relatives or friends had been infected. They were also asked to rate their level on concern about the spread of COVID-19 in their community.\nYou’ll find several measures related to household-level impacts, including indicators for whether anyone in the woman’s household experienced food insecurity, and whether the the household had experienced income loss. Related questions measure changes in married / partnered women’s autonomy during the outbreak, including whether they became more or less reliant on their partner for basic needs (if at all), and whether they or their partner now makes decisions about household purchases.\nCOVID Information Sources\nWomen who confirmed that they had heard or read about COVID-19 were also asked about several different sources of information about COVID-19. For each source of information, women were asked both:\nwhether they had learned about COVID-19 from the source, and\nwhether they trust the source for accurate information about COVID-19\n13 sources of information were listed (select all that apply, or none):\nNewspaper\nRadio\nTelevision\nPoster / billboard\nTown crier\nPhone message\nFamily\nFriends / neighbors\nCommunity/religious leaders\nSocial media (Twitter, Facebook, WhatsApp)\nHealth personnel\nMessages from government or authorities\nSchool\n\nNotably, all four samples used the same list of information sources.\nYou’ll also find variables in this topic heading related to awareness, trust, and use of an emergency number or call center for reporting suspected cases of COVID-19.\nCOVID Knowledge\nWomen who confirmed that they had heard or read about COVID-19 were asked to identify common symptoms of COVID-19 from this list (select all that apply, or none):\nFever\nCough\nShortness of breath/difficulty breathing\nChest pain\nSore throat\nRunny or stuffy nose\nMuscle or body aches\nHeadaches\nFatigue (tiredness)\nDiarrhea\nLoss of taste\nLoss of smell\nRash\nDizziness\nSneezing\nOther\nThese women were also asked whether any of the following actions could reduce the risk of being infected (available responses are “yes,” “no,” or “do not know” for each action):\nWashing hands with soap and water frequently\nWashing hands with hand sanitizer frequently\nAvoiding any close contact (2 meters) with people when you go out\nStaying in your home\nGetting vaccinated\nTraditional practices\nWearing something that covers your mouth and nose when you go out (a mask)\nAvoiding shaking hands with others\nCoughing/sneezing into your elbow or tissue\nPrayer\nCOVID Prevention\nWomen who confirmed that they had heard or read about COVID-19 were asked if they had personally taken any action to prevent becoming infected. If so, they were asked which of the following actions they had personally taken (select all that apply):\nWashing hands with soap and water frequently\nWashing hands with hand sanitizer frequently\nAvoiding any close contact (2 meters) with people when you go out\nStaying in your home\nGetting vaccinated\nTraditional practices\nWearing something that covers your mouth and nose when you go out (a mask)\nAvoiding shaking hands with others\nCoughing/sneezing into your elbow or tissue\nPrayer\nOther\nWomen who confirmed that they had heard or read about COVID-19 were also asked if they were able to avoid contact with people outside of their own household. If not, they were asked if any of the following reasons explained why they might not be able to avoid contact (select all that apply, or none):\nMy work or way of earning money requires me to leave the house\nI need to visit the market\nI need to visit the water source / well\nMy studies require me to leave the household\nI need to attend funerals in the community\nI need to attend religious services\nI need to visit my family/relatives\nTo seek out health care\nPerceptions Around COVID\nWomen who confirmed that they had heard or read about COVID-19 were asked several questions about their overall level of concern about COVID-19, including how concerned they were about getting infected, whether they were worried about the impact of COVID-19 on their household’s finances in the future, and whether they would conceal information about a family member’s COVID-19 infection.\nThese women were also asked whether each of the following statements are true about COVID-19 (available responses are “yes,” “no,” or “don’t know” for each statement):\nSome people cannot be infected with Coronavirus (COVID-19)\nMost people experience mild or no symptoms\nMost people develop serious illness requiring hospitalization\nPeople can be infected and not have symptoms\nOnly people with symptoms are contagious\nYou can become infected by shaking hands with someone who is infected\nYou can become infected by close contact with infected people even if you are not touching\nPeople of all ages can become infected\nCoronavirus (COVID-19) is mostly a risk to rich people\nNext Steps\nFor the next two months, we’ll be taking a deep dive into the PMA COVID-19 survey data. Along the way, we’ll showcase several examples of R code you can use to create publication-ready tables and data visualizations, and we’ll explore some of the research questions you might answer by linking COVID-19 data to the baseline survey. Check back here for a new post every two weeks!\n\n\n\nFerreira-Filho, Edson Santos, Nilson Roberto de Melo, Isabel Cristina Esposito Sorpreso, Luis Bahamondes, Ricardo Dos Santos Simões, José Maria Soares-Júnior, and Edmund Chada Baracat. 2020. “Contraception and Reproductive Planning During the COVID-19 Pandemic.” Expert Review of Clinical Pharmacology 13 (6): 615–22. http://dx.doi.org/10.1080/17512433.2020.1782738.\n\n\nSenderowicz, Leigh, and Jenny Higgins. 2020. “Reproductive Autonomy Is Nonnegotiable, Even in the Time of COVID-19.” Perspectives on Sexual and Reproductive Health 52 (2): 81–85. http://dx.doi.org/10.1363/psrh.12152.\n\n\nTemmerman, Marleen. 2021. “Family Planning in COVID-19 Times: Access for All.” The Lancet. Global Health 9 (6): e728–29. http://dx.doi.org/10.1016/S2214-109X(21)00231-X.\n\n\nFor discussion, see (Senderowicz and Higgins 2020), (Temmerman 2021), and (Ferreira-Filho et al. 2020).↩︎\n",
    "preview": "posts/2021-06-15-covid-discovery/../../images/new_data.png",
    "last_modified": "2021-10-18T16:11:16-05:00",
    "input_file": {},
    "preview_width": 2555,
    "preview_height": 1437
  },
  {
    "path": "posts/2021-05-24-migration-data-analysis/",
    "title": "Visualizing migration patterns over time",
    "description": "How to visualize patterns in migration data using alluvial plots, line plots, and density plots.",
    "author": [
      {
        "name": "Nina Brooks",
        "url": "http://www.ninarbrooks.com/"
      }
    ],
    "date": "2021-06-02",
    "categories": [
      "Migration",
      "Data Analysis",
      "Data Visualization",
      "Descriptive Analysis",
      "ggalluvial",
      "ggridges"
    ],
    "contents": "\n\nContents\nSet Up\nVisualizing Migration: Alluvial Plots\nWhy people migrate\nWhen people migrate\n\nMigration is an incredibly important, global demographic process. Yet, studying migration is often challenging due to data limitations. As we showed in a recent post, several new PMA samples include data about each respondent’s complete migration history, organized in chronological order. They include information on when respondents move (their age), the district or region they moved from, whether the place they moved from was a city, a town, peri-urban, or rural, and reasons why they moved.\n\nMake sure to check out our Data Discovery post on these migration variables for a lot more detail on what information is collected and the unique data structure for the migration variables!\nThis data opens up the opportunity to examine many interesting questions about migration such as:\nIs migration primarily from rural to urban places?\nWhy do people migrate?\nWhen do people migrate?\nIn this post, we’ll walk through three descriptive analyses that address aspects of these questions using the 2019 Kenya sample migration data from the last post in this series. Descriptive, exploratory work like this is an essential first step to any good analysis – and visualizing the data can help illuminate patterns across different dimensions. First, we’ll demonstrate how to visualize flows of migration across the urban-rural spectrum. Then, we’ll dig into the reasons why people moved to previous residences. Finally, we’ll explore how these reasons vary according to age at the time of migration.\nSet Up\nWe’ll be working with the same Kenya 2019 data extract we created for the previous post in this series (female respondents only). It contains all of the variables shown on the migration topic page.\nIn addition to loading the two standard packages we always use (ipumsr and tidyverse), we also load the ggalluvial package that we’ll use to make an alluvial plot and the ggridges package that we’ll use to make an overlapping ridgeline (aka density) plot. If this is the first time you’re using ggalluvial and ggridges, make sure to install them first using install.packages(c(\"ggalluvial\", \"ggridges\")).\nRecall that the migration data are stored in wide format, with many variables to capture information about each move for every single individual. We’ll quickly run the code from the last post that uses pivot_longer() to convert this into the much more useful long format. We’ll also replace the special codes as NA and create an ID that represents a short identification number for each person.\n\n\nlibrary(ipumsr)\nlibrary(ggalluvial)\nlibrary(ggridges)\nlibrary(tidyverse)\n\ndat <- read_ipums_micro(\n  ddi = \"data/pma_00016.xml\",\n  data = \"data/pma_00016.dat.gz\"\n)\n\ndat <- dat %>% \n  mutate(across(everything(), ~{\n    lbl_na_if(.x, ~.lbl %in% c(\n      \"No response or missing\",\n      \"NIU (not in universe)\"\n    ))\n  })) %>% \n  rowid_to_column(\"ID\") %>% \n  select(ID, starts_with(\"PLACE\"), -PLACELIVENUM) %>% \n  pivot_longer(\n    cols = starts_with(\"PLACE\"), \n    names_pattern = \"PLACE([0-9]*)(.*)\",\n    names_to = c(\"PLACE\", \".value\"),\n    values_drop_na = TRUE\n  ) %>%\n  mutate(\n    UR = as_factor(UR),\n    PLACE = as.numeric(PLACE)\n  )\n\n\n\n\nRemember: change these file paths to match the download location for your own data extract!\nNow that the data are in a long format, each row of the data represents one place in a respondent’s migration history. For example, notice that person ID == 20 occupies two rows, one for each of the two places she listed:\n\n\ndat\n\n\n# A tibble: 5,251 x 24\n      ID PLACE    COUNTRY  DISTRICTKE MOVEAGE UR    YCHILDEDU YCOHABIT\n   <int> <dbl>  <int+lbl>   <int+lbl> <int+l> <fct> <int+lbl> <int+lb>\n 1     2     1 404 [Keny…  6 [Nairob…      16 Rural    0 [No]   0 [No]\n 2    11     1 404 [Keny… 10 [Kakame…      17 Rural    0 [No]   0 [No]\n 3    12     1 404 [Keny… 10 [Kakame…      16 City…    0 [No]   0 [No]\n 4    14     1 404 [Keny…  7 [Nandi]       29 Rural    0 [No]   0 [No]\n 5    14     2 404 [Keny… 32 [Migori]      19 City…    0 [No]   0 [No]\n 6    16     1 404 [Keny…  6 [Nairob…      21 Rural    0 [No]   0 [No]\n 7    19     1 404 [Keny… 33 [Mombas…      31 Peri…    0 [No]   0 [No]\n 8    20     1 404 [Keny…  8 [Nyamir…      19 City…    0 [No]   0 [No]\n 9    20     2 404 [Keny…  8 [Nyamir…      21 Peri…    0 [No]   0 [No]\n10    21     1 404 [Keny…  6 [Nairob…      34 Peri…    0 [No]   0 [No]\n# … with 5,241 more rows, and 16 more variables: YCONFLICT <int+lbl>,\n#   YDIVORCE <int+lbl>, YFARM <int+lbl>, YHLTHACCESS <int+lbl>,\n#   YHLTHPROB <int+lbl>, YJOBSEARCH <int+lbl>, YOTHER <int+lbl>,\n#   YOTHERSOCIAL <int+lbl>, YPOSTMAR <int+lbl>,\n#   YSCHOOLATTEND <int+lbl>, YSCHOOLDONE <int+lbl>,\n#   YSICKREL <int+lbl>, YSPOUSEJOB <int+lbl>, YWKCHANGE <int+lbl>,\n#   YWKNONSEAS <int+lbl>, YWKSEASON <int+lbl>\n\nVisualizing Migration: Alluvial Plots\nAlluvial plots are a useful way to represent flows of data according by categorical variables. A “classic” alluvial plot maps flows of passengers on the Titanic according to various characteristics and whether or not they survived.\n\n\n\nFigure 1: Figure from https://cran.r-project.org/web/packages/ggalluvial/vignettes/ggalluvial.html\n\n\n\nAlluvial plots are particularly useful for visualizing flows over time – this means we can map the characteristics of respondents across different moves and places they’ve lived! This can be really informative for migration data.\nOne topic migration researchers are often interested in studying is how people move across the urban-rural spectrum. The PMA migration module includes a variable that classifies each residence respondents previously lived in as urban, peri-urban, or rural. Here we can see that individual 2 previously lived in a rural location. Individual 14 lived in two previous residences: the most recent (PLACE == 1) was a rural location, and prior to that (PLACE == 2) she lived in a city or town.\n\nRemember, PMA considers moves in the migration data only if respondents lived in at least one other location for six months or more after the age of 15 or after her first marriage if married before the age of 15.\n\n\ndat %>%\n  select(ID, PLACE, UR)\n\n\n# A tibble: 5,251 x 3\n      ID PLACE UR        \n   <int> <dbl> <fct>     \n 1     2     1 Rural     \n 2    11     1 Rural     \n 3    12     1 City/town \n 4    14     1 Rural     \n 5    14     2 City/town \n 6    16     1 Rural     \n 7    19     1 Peri-urban\n 8    20     1 City/town \n 9    20     2 Peri-urban\n10    21     1 Peri-urban\n# … with 5,241 more rows\n\nOne way to visualize this data is to make a bar plot that shows the number of respondents in each residence category across all seven previous residences that PMA collects information about.\n\n\ndat %>%\n  ggplot(aes(x = factor(PLACE), fill = UR)) +\n    geom_bar(stat = \"count\") +\n    scale_fill_viridis_d()\n\n\n\n\nThis bar plot shows the distribution of living in an urban/peri-urban/rural location over different residences across the sample, but it doesn’t tell us anything about the flows. For example, we might want to know if people are moving from rural to urban places and vice versa. This is where an alluvial plot can add a lot of value! Another thing that is very apparent from this plot is that very few people have lived in more than three previous residences, so going forward we’ll restrict the sample to people who have lived in at least three previous locations.\nThe ggalluvial package makes it easy to generate alluvial plots using the ggplot2 grammar of graphics. There are a few key elements to an alluvial plot:\nAxes: axes are the dimensions represented by the vertical bars. In this example, the axes are the places respondents have lived.\nStrata: strata are the groups or categories each axis is divided into. In this example, each axis has the same strata (city/town, peri-urban, rural). But in the titanic example above, each axis represents a different categorical variable (class, sex, age) with different values.\nAlluvia: alluvia are the flows between categories across axes. The width or thickness of the alluvia depends on the size of that group.\n\n\n\n\n© RStudio (CC0 1.0)\nggplot2 is included with library(tidyverse).\nWe’ll specify these elements using the standard ggplot2 syntax. First, we’ll count the total number of places each respondent has lived so that we can restrict the alluvial plot to respondents with at least three previous residences and then look at the flows between different categories of residence for the three most recent places. When making the alluvial plot, we’ll add the axes using + geom_stratum(), the alluvia using + geom_flow(), and fill in the colors according to the UR variable.\n\n\ndat <- dat %>%\n  group_by(ID) %>%\n  mutate(TOTAL_PLACES = max(PLACE)) %>%\n  ungroup() \n\n\n# alluvial plot\ndat %>%\n  filter(TOTAL_PLACES >= 3 & PLACE < 4) %>%\n  ggplot(aes(x = PLACE, \n             stratum = UR, \n             alluvium = ID, \n             fill = UR)) +\n  geom_stratum() + \n  geom_flow() + \n  scale_fill_viridis_d() +\n  theme_minimal()\n\n\n\n\nWe can see quite a bit more detail about the types of places people are moving to and from with this alluvial plot compared to the bar plot. For example, we can see that most people who lived in a city/town moved to another urban location from place 3 to 2 to 1. Rural residents followed a similar pattern. In contrast, we see the most movement across categories from people whose 3rd most recent residence was peri-urban. Despite the fact that most urban migrants move to other urban locations and most rural migrants move to other rural locations, this plot also makes it clear there is a fair amount of rural to urban/peri-urban migration, and even urban to rural migration! Finally, we can also see there is a small number of people for whom we’re missing data on their oldest (PLACE == 3) location.\nIt was pretty simple to create this alluvial plot off-the-shelf, but we can do a bit of work to improve the clarity and presentation of this plot.\n\n\ndat %>%\n  filter(TOTAL_PLACES >= 3 & PLACE < 4) %>%\n  mutate(PLACE = PLACE %>% as_factor %>% fct_recode(\n    \"Most recent place\" = \"1\",\n    \"2nd most recent place\" = \"2\",\n    \"3rd most recent place\" = \"3\"\n  )) %>%\n  ggplot(aes(x = PLACE, \n             stratum = UR, \n             alluvium = ID, \n             fill = UR)) +\n  scale_x_discrete(expand = c(.1, .1)) +\n  geom_flow() + \n  geom_stratum(alpha = .5) + # increases the transparency of the axes' colors \n  scale_fill_viridis_d() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        axis.title.y = element_text(angle = 0, hjust = 0)) + \n  labs(title = \"Migration from Place 3 to Place 1: Kenya 2019 Sample\",\n       subtitle = \"Flows by residence category\",\n       x = NULL,\n       fill = NULL,\n       y = \"Number of\\nPeople\") + # the \\n adds a line break\n  geom_segment(aes( # adds an arrow to indicate that time is moving from right to left\n    x = 0.75, xend = 3.16,\n    y = 0, yend = 0),\n    arrow = arrow(length=unit(0.30,\"cm\"), \n                  ends=\"first\", \n                  type = \"closed\"))\n\n\n\n\nWhy people migrate\nWe might also be interested in seeing how the reasons people migrate change from move to move. The PMA surveys asked why respondents moved to each previous place they lived, allowing people to select multiple responses. In fact, there are 19 possible answers for respondents to choose from. To make this a bit more manageable, let’s summarize the reasons people moved to their previous three locations and identify the most common reasons:\n\nUnfortunately, this information is not available for the respondent’s migration to their current place of residence.\n\n\ndat %>% \n  filter(TOTAL_PLACES >= 3 & PLACE < 4) %>%\n  group_by(PLACE) %>%\n  summarise(\n    across(starts_with(\"Y\"),\n           ~100*mean(.x))) %>%\n  pivot_longer( # pivot_longer to sort by reason\n    cols = starts_with(\"Y\"),\n    names_to = \"REASON\") %>%\n  pivot_wider(\n    names_from = \"PLACE\" # pivot wider to easily see each of the 3 places as columns\n  ) %>%\n  arrange(-`1`, -`2`, -`3`)\n\n\n# A tibble: 18 x 4\n   REASON           `1`    `2`    `3`\n   <chr>          <dbl>  <dbl>  <dbl>\n 1 YSCHOOLATTEND 21.0   21.6   21.6  \n 2 YPOSTMAR      19.3   13.5   14.9  \n 3 YJOBSEARCH    17.2   19.5   18.4  \n 4 YOTHERSOCIAL  12.9   18.7   17.0  \n 5 YOTHER        10.1   10.1   11.2  \n 6 YCONFLICT      8.91   8.05   8.91 \n 7 YWKSEASON      7.76   8.91   9.20 \n 8 YWKNONSEAS     7.18   4.60   4.31 \n 9 YSCHOOLDONE    5.17   2.87   4.89 \n10 YSPOUSEJOB     4.02   2.87   4.02 \n11 YFARM          3.45   3.45   5.75 \n12 YWKCHANGE      3.16   2.30   3.74 \n13 YHLTHACCESS    2.59   2.30   3.16 \n14 YCHILDEDU      1.72   1.72   2.59 \n15 YCOHABIT       1.15   2.59   2.30 \n16 YSICKREL       1.15   2.30   0.575\n17 YHLTHPROB      0.862  1.72   2.01 \n18 YDIVORCE       0.862  0.862  1.15 \n\nWe can see that across the three locations, the most common reasons to move are: to attend school (YSCHOOLATTEND), to join a spouse after marriage (YPOSTMAR), to look for a job (YJOBSEARCH), other social reasons (YOTHERSOCIAL), other (YOTHER), because of family or village conflict (YCONFLICT), and for seasonal work (YWKSEASON). Because it will be difficult to see much with so many categories, we’ll aggregate all the less common reasons into YOTHER.\nSince each reason is stored as a different binary variable, we’ll first pivot_longer() again to create a variable called REASON that store the reason for migrating and a binary variable, VALUE, that equals 1 if the individual selected this as a reason for migrating. Then, we’ll use the very handy forcats::fct_lump_n() from the tidyverse to lump together less common responses into a single “other” category.\nAdditionally, since individuals can select multiple reasons, this variable is not well-suited to an alluvial plot, so we’ll look at how the proportion of respondents who selected each reason changes from each migration.\n\n\nmig_reasons <- dat %>% \n  filter(TOTAL_PLACES >= 3 & PLACE < 4) %>%\n  pivot_longer( # pivot_longer to sort by reason\n    cols = starts_with(\"Y\"),\n    names_to = \"REASON\",\n    values_to = \"VALUE\"\n  ) %>% \n  mutate(REASON = REASON %>%\n           as_factor %>%\n           fct_lump_n(n = 8, w = VALUE, other = \"YOTHER\") %>% \n           fct_relevel(sort)\n  ) %>%\n  group_by(ID, PLACE, REASON, VALUE) %>%\n  slice(1) %>% # to get rid of duplicate OTHER rows\n  group_by(PLACE, REASON) %>% \n  summarise(PROP = mean(VALUE)) %>% \n  ungroup()\n\n\n\n\n\n\n\n© RStudio (CC0 1.0)\nforcats is included with library(tidyverse).\n\n\nmig_reasons %>%\n  ggplot(aes(x = PLACE, \n             y = PROP,\n             color = REASON,\n             group = REASON)) +\n  geom_line() +\n  geom_point() +\n  scale_color_viridis_d() +\n  theme_minimal()\n\n\n\n\n\nNote that because respondents can select multiple reasons for moving to each previous residence, the proportions of all reasons to migrate to each previous residence will not add up to 1.\nAlthough many of the reasons have a similar proportion of responses across migrations, we can see that the proportion of people moving to join a spouse after marriage and moving for non-seasonal work increased from the least recent place to most recent place, while the proportion moving for seasonal work and other social reasons decreased. The most common reason for migrating – to attend school – remains pretty constant across moves for these respondents.\nTo make things a little easier to read, we’ll rename the reasons for migrating and tidy up the plot labels.\n\n\nmig_reasons %>%\n  mutate(\n    PLACE = PLACE %>% \n      as_factor %>% \n      fct_recode(\n        \"Most recent place\" = \"1\",\n        \"2nd most recent place\" = \"2\",\n        \"3rd most recent place\" = \"3\"\n      ),\n    REASON = REASON %>% \n      fct_recode(\n        \"Family or village conflict\" = \"YCONFLICT\",\n        \"To look for a job\" = \"YJOBSEARCH\",\n        \"Other\" = \"YOTHER\",\n        \"Other social reasons\" = \"YOTHERSOCIAL\",\n        \"To join spouse after marriage\" = \"YPOSTMAR\",\n        \"To attend school\" = \"YSCHOOLATTEND\",\n        \"For work (non-seasonal)\" = \"YWKNONSEAS\",\n        \"For seasonal work\" = \"YWKSEASON\"\n      )\n  )  %>%\n  ggplot(aes(x = PLACE, \n             y = PROP,\n             color = REASON,\n             group = REASON)) +\n  geom_line() +\n  geom_point() +\n  scale_color_viridis_d() +\n  scale_x_discrete(expand = c(.1, .1)) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  theme_minimal() +\n  labs(color = NULL,\n       x = NULL,\n       y = NULL,\n       title = \"Reasons for Migrating from Place 3 to Place 1\",\n       subtitle = \"% of respondents with three moves\") +\n  theme(legend.position = \"bottom\",\n        axis.title.y = element_text(angle = 0, hjust = 0))\n\n\n\n\nWhen people migrate\nThe reasons people migrate likely differ by age and is of interest to researchers. To look at how the reasons for migration might differ by age, we’ll again look at the three most recent moves, where we have the most data. To visualize how the reasons for migration differ by age, we’ll use ggridges to make an overlapping density plot that shows the distribution of ages for each reason. We’ll use the same code from before to aggregate the less common reasons into an OTHER category and pivot_longer() to store all the reasons in a single variable.\n\nYou should see a warning that says “Removed 6 rows containing non-finite values (stat_density_ridges).” This is because MOVEAGE is missing for 6 moves.\n\n\ndat %>% \n  filter(TOTAL_PLACES >= 3 & PLACE < 4) %>%\n  pivot_longer( # pivot_longer to sort by reason\n    cols = starts_with(\"Y\"),\n    names_to = \"REASON\",\n    values_to = \"VALUE\"\n  ) %>% \n  mutate(REASON = REASON %>%\n           as_factor %>%\n           fct_lump_n(n = 8, w = VALUE, other = \"YOTHER\") %>% \n           fct_relevel(sort) %>% \n           fct_recode(\n             \"Family or village conflict\" = \"YCONFLICT\",\n             \"To look for a job\" = \"YJOBSEARCH\",\n             \"Other\" = \"YOTHER\",\n             \"Other social reasons\" = \"YOTHERSOCIAL\",\n             \"To join spouse after marriage\" = \"YPOSTMAR\",\n             \"To attend school\" = \"YSCHOOLATTEND\",\n             \"For work (non-seasonal)\" = \"YWKNONSEAS\",\n             \"For seasonal work\" = \"YWKSEASON\"\n           )\n  ) %>% \n  group_by(ID, PLACE, REASON, VALUE) %>%\n  slice(1)  %>% # to get rid of duplicate OTHER rows \n  ungroup %>% \n  filter(VALUE == 1) %>% \n  ggplot(aes(x = MOVEAGE,  y = REASON, fill = REASON)) +\n    geom_density_ridges() +\n    scale_fill_viridis_d() +\n    theme_minimal() +\n    theme(legend.position = \"none\") +\n    labs(y = NULL,\n         x = \"Age at move\",\n         title = \"Reasons for Migrating from Place 3 to Place 1\",\n         subtitle = \"Distribution by age at move\")\n\n\nWarning: Removed 6 rows containing non-finite values\n(stat_density_ridges).\n\n\nThis figure shows that people tend to migrate to attend school at younger ages than most of the other reasons, which is unsurprising. Comparing the distributions by age for migrating for seasonal and non-seasonal work, it appears more young people migrate for seasonal work. In contrast, the age distributions for moving to join a spouse after marriage, to look for a job, and for non-seasonal work are all pretty similar – suggesting individuals move for these reasons at similar ages. Interestingly, there are masses at the young end of the distribution for family or village conflict, other social reasons, and other. Something further investigation could dig into more!\nWe hope this helps generate ideas for how to visualize the PMA migration data! Let us know what migration questions you’re interested in researching!\n\n\n\n",
    "preview": "posts/2021-05-24-migration-data-analysis/images/migration_alluvial.png",
    "last_modified": "2021-08-18T14:28:20-05:00",
    "input_file": {},
    "preview_width": 936,
    "preview_height": 574
  },
  {
    "path": "posts/2021-05-15-paa-2021/",
    "title": "Making the Contraceptive Calendar Data Work For You",
    "description": "R and Stata code with video from an event held at the Population Association of America 2021 Annual Meeting.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-05-15",
    "categories": [
      "PMA Publications",
      "PAA 2021",
      "Contraceptive Calendar",
      "Stata"
    ],
    "contents": "\n\nContents\nBreakout Session: R Users\nBreakout Session: Stata Users\nDownload Links\n\nOn May 4th, PMA and IPUMS PMA co-hosted a Population Association of America 2021 virtual data workshop showcasing the new PMA contraceptive calendar data available for these samples:\nBurkina Faso 2020\nCongo (DR), Kinshasa 2019\nCongo (DR), Kongo Central 2019\nKenya 2019\nNigeria, Kano 2019\nNigeria, Lagos 2019\nThese data represent contraceptive use, pregnancy, pregnancy termination, and birth information recalled by female respondents for each of several months preceding the PMA interview. Women sampled in Burkina Faso and Democratic Republic of the Congo were each asked to recall monthly information for up to 24 months, while women sampled from Kenya and Nigeria were asked to recall monthly information for up to 36 months. Their responses are recorded in a single comma delimited string, where information about each month is represented by one of the following codes:\nB = Birth\nP = Pregnant\nT = Pregnancy ended\n0 = No family planning method used\n1 = Female Sterilization\n2 = Male Sterilization\n3 = Implant\n4 = IUD\n5 = Injectables\n7 = Pill\n8 = Emergency Contraception\n9 = Male Condom\n10 = Female Condom\n11 = Diaphragm\n12 = Foam / Jelly\n13 = Standard Days / Cycle beads\n14 = LAM\n30 = Rhythm method\n31 = Withdrawal\n39 = Other traditional methods\nIn this video, PMA and IPUMS PMA explain the background behind contraceptive calendar data and show some of the ways you might consider using it in longitudinal analysis. We also give a conceptual overview of the steps both R and Stata users should take to reshape the data into a long format. After the overview, R and Stata users split into separate breakout sessions to work with a hands-on coding example using data from the Kenya 2019 sample; this example shows how to build a Kaplan-Meier survival curve for cohorts of women who were using the same family planning method in the first month of the contraceptive calendar.\n\nCheck out our recent post on migration recall data for information and practice with longitudinal data structures. The same 2019 and 2020 samples that contain contraceptive calendar data also include migration recall data.\n \nDownload Powerpoint slides here. A transcript of the chat from this session (including typed responses from the Q&A) is also available here (participant names are redacted).\nBreakout Session: R Users\nR users can load a fixed-width IPUMS PMA data extract with help from the ipumsr package (if you’re new to this blog, check out detailed instructions here). We also use packages from tidyverse to reformat the data, as well as survival and ggfortify for specific survival analysis functions.\n\n\nlibrary(ipumsr)\nlibrary(tidyverse)\nlibrary(survival)\nlibrary(ggfortify)\noptions(tibble.print_min = 20, tibble.min_extra_cols = 5)\n\ndat <- read_ipums_micro(\n  ddi = \"data/pma_00001.xml\",\n  data = \"data/pma_00001.dat.gz\"\n) \n\n\n\n\nThis code looks for my data file and DDI codebook in a folder called “data” inside of my R working directory. Make sure to change these paths as needed!\nWhen you open any IPUMS PMA data extract from the Household and Female Survey, you’ll find the data organized with one respondent per row. Here, there are 9,549 rows each representing one female respondent (all other household members have been excluded):\n\n\ndat \n\n\n# A tibble: 9,549 x 17\n         SAMPLE COUNTRY  YEAR HHID  PERSONID ELIGIBLE   EAID CONSENTFQ\n      <int+lbl> <int+l> <int> <chr> <chr>    <int+lb>  <dbl> <int+lbl>\n 1 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 2 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 3 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 4 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 5 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 6 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 7 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 8 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 9 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n10 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n11 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n12 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n13 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n14 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n15 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n16 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n17 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n18 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n19 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n20 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n# … with 9,529 more rows, and 9 more variables: FQINSTID <chr>,\n#   CONSENTHQ <int+lbl>, FQWEIGHT <dbl>, STRATA <int+lbl>,\n#   SUBNATIONAL <int+lbl>, AGE <int+lbl>, BIRTHEVENT <int+lbl>,\n#   WORKYR <int+lbl>, CALENDARKE <chr+lbl>\n\nFor the purpose of this exercise only we create a short identifying number for each respondent called ID. Then, we select only the variables ID and CALENDARKE (dropping all of the other variables pre-selected for every IPUMS PMA extract).\n\nIn practice, you should use the variable PERSONID to track unique respondents; we use ID instead here only because it fits better on the screen.\n\n\ndat <- dat %>% \n  rowid_to_column(\"ID\") %>% \n  select(ID, CALENDARKE)\n\ndat\n\n\n# A tibble: 9,549 x 2\n      ID CALENDARKE                                                   \n   <int> <chr+lbl>                                                    \n 1     1 0,0,B,P,P,P,P,P,P,P,P,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,3,3,3,…\n 2     2 ,7,7,7,7,7,7,7,7,0,B,P,P,P,P,P,P,P,P,0,0,0,0,0,9,9,9,9,9,9,9…\n 3     3 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,…\n 4     4 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,…\n 5     5 ,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,B,P,P,P,P,P,P,P,P,0,0,0,0,0…\n 6     6 5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,…\n 7     7 5,5,5,5,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,B,P,P,P,P,P,…\n 8     8 P,P,P,P,P,P,P,P,0,0,0,0,0,0,0,14,14,14,14,14,14,14,B,P,P,P,P…\n 9     9 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,…\n10    10 ,P,P,P,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9…\n11    11 ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0…\n12    12 ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0…\n13    13 ,P,P,P,P,P,P,P,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9…\n14    14 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,…\n15    15 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14…\n16    16 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,…\n17    17 ,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5…\n18    18 ,5,5,5,5,5,5,5,5,5,5,5,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7…\n19    19 0,0,0,0,0,0,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,…\n20    20 ,P,P,P,P,P,P,P,5,5,5,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0…\n# … with 9,529 more rows\n\nAs you can see, CALENDARKE includes the response codes shown above, each separated by a comma. Each string contains 36 codes: these represent the 36 months from January 2017 through December 2019 (the last month in which Kenya 2019 samples were collected). The left-most code represents the most recent available month.\nSome strings begin with a comma (i.e. the most recent month is blank). These are individuals who were interviewed in November 2019, rather than December. When we split the string into 36 columns, we must shift these individuals to the right, leaving a blank value in the left-most column (December 2019). For example, notice the blank value that appears in the new column cal_ke36 for the person ID == 2.\n\n\ndat <- dat %>% \n  separate(\n    col = CALENDARKE,\n    into = paste0(\"cal_ke\", 36:1),\n    fill = \"left\"\n  ) \n\ndat\n\n\n# A tibble: 9,549 x 37\n      ID cal_ke36 cal_ke35 cal_ke34 cal_ke33 cal_ke32 cal_ke31\n   <int> <chr>    <chr>    <chr>    <chr>    <chr>    <chr>   \n 1     1 \"0\"      0        B        P        P        P       \n 2     2 \"\"       7        7        7        7        7       \n 3     3 \"0\"      0        0        0        0        0       \n 4     4 \"0\"      0        0        0        0        0       \n 5     5 \"\"       5        5        5        5        5       \n 6     6 \"5\"      5        5        5        5        5       \n 7     7 \"5\"      5        5        5        5        5       \n 8     8 \"P\"      P        P        P        P        P       \n 9     9 \"0\"      0        0        0        0        0       \n10    10 \"\"       P        P        P        9        9       \n11    11 \"\"       0        0        0        0        0       \n12    12 \"\"       0        0        0        0        0       \n13    13 \"\"       P        P        P        P        P       \n14    14 \"0\"      0        0        0        0        0       \n15    15 \"0\"      0        0        0        0        0       \n16    16 \"3\"      3        3        3        3        3       \n17    17 \"\"       5        5        5        5        5       \n18    18 \"\"       5        5        5        5        5       \n19    19 \"0\"      0        0        0        0        0       \n20    20 \"\"       P        P        P        P        P       \n# … with 9,529 more rows, and 30 more variables: cal_ke30 <chr>,\n#   cal_ke29 <chr>, cal_ke28 <chr>, cal_ke27 <chr>, cal_ke26 <chr>,\n#   cal_ke25 <chr>, cal_ke24 <chr>, cal_ke23 <chr>, cal_ke22 <chr>,\n#   cal_ke21 <chr>, cal_ke20 <chr>, cal_ke19 <chr>, cal_ke18 <chr>,\n#   cal_ke17 <chr>, cal_ke16 <chr>, cal_ke15 <chr>, cal_ke14 <chr>,\n#   cal_ke13 <chr>, cal_ke12 <chr>, cal_ke11 <chr>, cal_ke10 <chr>,\n#   cal_ke9 <chr>, cal_ke8 <chr>, cal_ke7 <chr>, cal_ke6 <chr>,\n#   cal_ke5 <chr>, cal_ke4 <chr>, cal_ke3 <chr>, cal_ke2 <chr>,\n#   cal_ke1 <chr>\n\nLet’s now pivot the data from wide to long format so that we’ll be able to mark time in a new column called MONTH. The argument names_pattern pulls the number from each variable starting with cal_ke, which we then put in the new column MONTH.\n\n\noptions(tibble.print_min = 40)\n\ndat <- dat %>% \n  pivot_longer(\n    starts_with(\"cal_ke\"),\n    names_pattern = \"cal_ke(.*)\",\n    names_to = \"MONTH\",\n    values_to = \"FP\"\n  ) \n\ndat\n\n\n# A tibble: 343,764 x 3\n      ID MONTH FP   \n   <int> <chr> <chr>\n 1     1 36    \"0\"  \n 2     1 35    \"0\"  \n 3     1 34    \"B\"  \n 4     1 33    \"P\"  \n 5     1 32    \"P\"  \n 6     1 31    \"P\"  \n 7     1 30    \"P\"  \n 8     1 29    \"P\"  \n 9     1 28    \"P\"  \n10     1 27    \"P\"  \n11     1 26    \"P\"  \n12     1 25    \"0\"  \n13     1 24    \"0\"  \n14     1 23    \"0\"  \n15     1 22    \"0\"  \n16     1 21    \"0\"  \n17     1 20    \"0\"  \n18     1 19    \"0\"  \n19     1 18    \"0\"  \n20     1 17    \"0\"  \n21     1 16    \"0\"  \n22     1 15    \"0\"  \n23     1 14    \"0\"  \n24     1 13    \"3\"  \n25     1 12    \"3\"  \n26     1 11    \"3\"  \n27     1 10    \"3\"  \n28     1 9     \"3\"  \n29     1 8     \"3\"  \n30     1 7     \"3\"  \n31     1 6     \"3\"  \n32     1 5     \"3\"  \n33     1 4     \"3\"  \n34     1 3     \"3\"  \n35     1 2     \"3\"  \n36     1 1     \"3\"  \n37     2 36    \"\"   \n38     2 35    \"7\"  \n39     2 34    \"7\"  \n40     2 33    \"7\"  \n# … with 343,724 more rows\n\n\nNotice: each person ID now occupies 36 rows, so we increase the length of all printed dataframes to 40.\nWe’ve now created a variable FP containing the original CALENDARKE variable codes. This variable will be much easier to work with if we 1) convert it into a factor, and 2) replace missing values with NA (e.g. month 36 for individuals interviewed in November 2018). We’ll also coerce MONTH from a “character” to an “integer” class.\n\n\ndat <- dat %>%\n  mutate(\n    MONTH = as.integer(MONTH),\n    FP = FP %>%\n      na_if(\"\") %>%\n      fct_recode(\n        \"Birth\" = \"B\",\n        \"Pregnant\" = \"P\",\n        \"Pregnancy ended\" = \"T\",\n        \"No family planning method used\" = \"0\",\n        \"Female Sterilization\" = \"1\",\n        \"Male Sterilization\" = \"2\",\n        \"Implant\" = \"3\",\n        \"IUD\" = \"4\",\n        \"Injectables\" = \"5\",\n        \"Pill\" = \"7\",\n        \"Emergency Contraception\" = \"8\",\n        \"Male Condom\" = \"9\",\n        \"Female Condom\" = \"10\",\n        \"Diaphragm\" = \"11\",\n        \"Foam / Jelly\" = \"12\",\n        \"Standard Days / Cycle beads\" = \"13\",\n        \"LAM\" = \"14\",\n        \"Rhythm method\" = \"30\",\n        \"Withdrawal\" = \"31\",\n        \"Other traditional methods\" = \"39\"\n      )\n  )\n\ndat\n\n\n# A tibble: 343,764 x 3\n      ID MONTH FP                            \n   <int> <int> <fct>                         \n 1     1    36 No family planning method used\n 2     1    35 No family planning method used\n 3     1    34 Birth                         \n 4     1    33 Pregnant                      \n 5     1    32 Pregnant                      \n 6     1    31 Pregnant                      \n 7     1    30 Pregnant                      \n 8     1    29 Pregnant                      \n 9     1    28 Pregnant                      \n10     1    27 Pregnant                      \n11     1    26 Pregnant                      \n12     1    25 No family planning method used\n13     1    24 No family planning method used\n14     1    23 No family planning method used\n15     1    22 No family planning method used\n16     1    21 No family planning method used\n17     1    20 No family planning method used\n18     1    19 No family planning method used\n19     1    18 No family planning method used\n20     1    17 No family planning method used\n21     1    16 No family planning method used\n22     1    15 No family planning method used\n23     1    14 No family planning method used\n24     1    13 Implant                       \n25     1    12 Implant                       \n26     1    11 Implant                       \n27     1    10 Implant                       \n28     1     9 Implant                       \n29     1     8 Implant                       \n30     1     7 Implant                       \n31     1     6 Implant                       \n32     1     5 Implant                       \n33     1     4 Implant                       \n34     1     3 Implant                       \n35     1     2 Implant                       \n36     1     1 Implant                       \n37     2    36 <NA>                          \n38     2    35 Pill                          \n39     2    34 Pill                          \n40     2    33 Pill                          \n# … with 343,724 more rows\n\nWe’re now ready to begin our analysis. To keep our example simple, our survival curves will show the duration of continuously used family planning methods for cohorts of women who were using the same method in January 2017. These survival curves will estimate the probability that an individual “survives” - or continues using - a given method at each of 36 months, assuming that she used it in month 1. We’ll exclude the duration of use after any break (for example, if a woman stopped using family planning to become pregnant, but then started again afterward).\nLet’s begin with women who were using the contraceptive pill in January 2017. Remove all other women, saving those who remain as a sub-sample called pills:\n\n\npills <- dat %>% \n  group_by(ID) %>% \n  mutate(use_m1 = case_when(FP == \"Pill\" & MONTH == 1 ~ TRUE) %>% any()) %>% \n  filter(use_m1)\n\npills \n\n\n# A tibble: 10,332 x 4\n# Groups:   ID [287]\n      ID MONTH FP          use_m1\n   <int> <int> <fct>       <lgl> \n 1    18    36 <NA>        TRUE  \n 2    18    35 Injectables TRUE  \n 3    18    34 Injectables TRUE  \n 4    18    33 Injectables TRUE  \n 5    18    32 Injectables TRUE  \n 6    18    31 Injectables TRUE  \n 7    18    30 Injectables TRUE  \n 8    18    29 Injectables TRUE  \n 9    18    28 Injectables TRUE  \n10    18    27 Injectables TRUE  \n11    18    26 Injectables TRUE  \n12    18    25 Injectables TRUE  \n13    18    24 Pill        TRUE  \n14    18    23 Pill        TRUE  \n15    18    22 Pill        TRUE  \n16    18    21 Pill        TRUE  \n17    18    20 Pill        TRUE  \n18    18    19 Pill        TRUE  \n19    18    18 Pill        TRUE  \n20    18    17 Pill        TRUE  \n21    18    16 Pill        TRUE  \n22    18    15 Pill        TRUE  \n23    18    14 Pill        TRUE  \n24    18    13 Pill        TRUE  \n25    18    12 Pill        TRUE  \n26    18    11 Pill        TRUE  \n27    18    10 Pill        TRUE  \n28    18     9 Pill        TRUE  \n29    18     8 Pill        TRUE  \n30    18     7 Pill        TRUE  \n31    18     6 Pill        TRUE  \n32    18     5 Pill        TRUE  \n33    18     4 Pill        TRUE  \n34    18     3 Pill        TRUE  \n35    18     2 Pill        TRUE  \n36    18     1 Pill        TRUE  \n37    39    36 Pill        TRUE  \n38    39    35 Pill        TRUE  \n39    39    34 Pill        TRUE  \n40    39    33 Pill        TRUE  \n# … with 10,292 more rows\n\nThe next several steps will help us remove every record for each woman except for the last recorded month in which she used the pill. For those whose last month is month 36, we will say she “survived” the full observation period.\nTo avoid re-entry cases (returning to use of the pill), we’ll find the earliest month that a woman in this cohort was not using the pill. The month prior to this will be her last_month of using the pill. For the first person in this cohort ID == 18, for example, the last_month of use should be MONTH == 24.\n\n\npills <- pills %>% \n  mutate(\n    non_use_month = case_when(FP != \"Pill\" | is.na(FP) ~ MONTH),\n    last_month = ifelse(\n      all(is.na(non_use_month)),\n      36,\n      min(non_use_month, na.rm = T) - 1\n    )\n  ) \n\npills\n\n\n# A tibble: 10,332 x 6\n# Groups:   ID [287]\n      ID MONTH FP          use_m1 non_use_month last_month\n   <int> <int> <fct>       <lgl>          <int>      <dbl>\n 1    18    36 <NA>        TRUE              36         24\n 2    18    35 Injectables TRUE              35         24\n 3    18    34 Injectables TRUE              34         24\n 4    18    33 Injectables TRUE              33         24\n 5    18    32 Injectables TRUE              32         24\n 6    18    31 Injectables TRUE              31         24\n 7    18    30 Injectables TRUE              30         24\n 8    18    29 Injectables TRUE              29         24\n 9    18    28 Injectables TRUE              28         24\n10    18    27 Injectables TRUE              27         24\n11    18    26 Injectables TRUE              26         24\n12    18    25 Injectables TRUE              25         24\n13    18    24 Pill        TRUE              NA         24\n14    18    23 Pill        TRUE              NA         24\n15    18    22 Pill        TRUE              NA         24\n16    18    21 Pill        TRUE              NA         24\n17    18    20 Pill        TRUE              NA         24\n18    18    19 Pill        TRUE              NA         24\n19    18    18 Pill        TRUE              NA         24\n20    18    17 Pill        TRUE              NA         24\n21    18    16 Pill        TRUE              NA         24\n22    18    15 Pill        TRUE              NA         24\n23    18    14 Pill        TRUE              NA         24\n24    18    13 Pill        TRUE              NA         24\n25    18    12 Pill        TRUE              NA         24\n26    18    11 Pill        TRUE              NA         24\n27    18    10 Pill        TRUE              NA         24\n28    18     9 Pill        TRUE              NA         24\n29    18     8 Pill        TRUE              NA         24\n30    18     7 Pill        TRUE              NA         24\n31    18     6 Pill        TRUE              NA         24\n32    18     5 Pill        TRUE              NA         24\n33    18     4 Pill        TRUE              NA         24\n34    18     3 Pill        TRUE              NA         24\n35    18     2 Pill        TRUE              NA         24\n36    18     1 Pill        TRUE              NA         24\n37    39    36 Pill        TRUE              NA         36\n38    39    35 Pill        TRUE              NA         36\n39    39    34 Pill        TRUE              NA         36\n40    39    33 Pill        TRUE              NA         36\n# … with 10,292 more rows\n\nWe must now identify whether the last_month represents cessation or right-censoring. Remember that a large number of women in our sample have missing values in the 36th month: they are right-censored at month 35 if they had been continuously using the pill until that time, so we cannot say that they ceased using at month 35!\nTo make this easier, we’ll create a logical variable right_censored that simply indicates whether each person is missing a value for MONTH == 36.\n\n\npills <- pills %>% \n  mutate(right_censored = ifelse(MONTH == 36 & is.na(FP), T, F) %>% any())\n\npills \n\n\n# A tibble: 10,332 x 7\n# Groups:   ID [287]\n      ID MONTH FP       use_m1 non_use_month last_month right_censored\n   <int> <int> <fct>    <lgl>          <int>      <dbl> <lgl>         \n 1    18    36 <NA>     TRUE              36         24 TRUE          \n 2    18    35 Injecta… TRUE              35         24 TRUE          \n 3    18    34 Injecta… TRUE              34         24 TRUE          \n 4    18    33 Injecta… TRUE              33         24 TRUE          \n 5    18    32 Injecta… TRUE              32         24 TRUE          \n 6    18    31 Injecta… TRUE              31         24 TRUE          \n 7    18    30 Injecta… TRUE              30         24 TRUE          \n 8    18    29 Injecta… TRUE              29         24 TRUE          \n 9    18    28 Injecta… TRUE              28         24 TRUE          \n10    18    27 Injecta… TRUE              27         24 TRUE          \n11    18    26 Injecta… TRUE              26         24 TRUE          \n12    18    25 Injecta… TRUE              25         24 TRUE          \n13    18    24 Pill     TRUE              NA         24 TRUE          \n14    18    23 Pill     TRUE              NA         24 TRUE          \n15    18    22 Pill     TRUE              NA         24 TRUE          \n16    18    21 Pill     TRUE              NA         24 TRUE          \n17    18    20 Pill     TRUE              NA         24 TRUE          \n18    18    19 Pill     TRUE              NA         24 TRUE          \n19    18    18 Pill     TRUE              NA         24 TRUE          \n20    18    17 Pill     TRUE              NA         24 TRUE          \n21    18    16 Pill     TRUE              NA         24 TRUE          \n22    18    15 Pill     TRUE              NA         24 TRUE          \n23    18    14 Pill     TRUE              NA         24 TRUE          \n24    18    13 Pill     TRUE              NA         24 TRUE          \n25    18    12 Pill     TRUE              NA         24 TRUE          \n26    18    11 Pill     TRUE              NA         24 TRUE          \n27    18    10 Pill     TRUE              NA         24 TRUE          \n28    18     9 Pill     TRUE              NA         24 TRUE          \n29    18     8 Pill     TRUE              NA         24 TRUE          \n30    18     7 Pill     TRUE              NA         24 TRUE          \n31    18     6 Pill     TRUE              NA         24 TRUE          \n32    18     5 Pill     TRUE              NA         24 TRUE          \n33    18     4 Pill     TRUE              NA         24 TRUE          \n34    18     3 Pill     TRUE              NA         24 TRUE          \n35    18     2 Pill     TRUE              NA         24 TRUE          \n36    18     1 Pill     TRUE              NA         24 TRUE          \n37    39    36 Pill     TRUE              NA         36 FALSE         \n38    39    35 Pill     TRUE              NA         36 FALSE         \n39    39    34 Pill     TRUE              NA         36 FALSE         \n40    39    33 Pill     TRUE              NA         36 FALSE         \n# … with 10,292 more rows\n\nWe’ll create another logical variable ceased to indicate whether each woman actually stopped using the pill at her last_month. If not (either because last_month is 36, or she is right-censored and last_month is 35), it will take the value FALSE.\n\n\npills <- pills %>% \n  mutate(\n    ceased = case_when(\n      right_censored & last_month == 35 ~ F,\n      last_month == 36 ~ F,\n      last_month < 36 ~ T\n    )\n  ) %>% \n  select(ID, MONTH, FP, last_month, ceased)\n\npills\n\n\n# A tibble: 10,332 x 5\n# Groups:   ID [287]\n      ID MONTH FP          last_month ceased\n   <int> <int> <fct>            <dbl> <lgl> \n 1    18    36 <NA>                24 TRUE  \n 2    18    35 Injectables         24 TRUE  \n 3    18    34 Injectables         24 TRUE  \n 4    18    33 Injectables         24 TRUE  \n 5    18    32 Injectables         24 TRUE  \n 6    18    31 Injectables         24 TRUE  \n 7    18    30 Injectables         24 TRUE  \n 8    18    29 Injectables         24 TRUE  \n 9    18    28 Injectables         24 TRUE  \n10    18    27 Injectables         24 TRUE  \n11    18    26 Injectables         24 TRUE  \n12    18    25 Injectables         24 TRUE  \n13    18    24 Pill                24 TRUE  \n14    18    23 Pill                24 TRUE  \n15    18    22 Pill                24 TRUE  \n16    18    21 Pill                24 TRUE  \n17    18    20 Pill                24 TRUE  \n18    18    19 Pill                24 TRUE  \n19    18    18 Pill                24 TRUE  \n20    18    17 Pill                24 TRUE  \n21    18    16 Pill                24 TRUE  \n22    18    15 Pill                24 TRUE  \n23    18    14 Pill                24 TRUE  \n24    18    13 Pill                24 TRUE  \n25    18    12 Pill                24 TRUE  \n26    18    11 Pill                24 TRUE  \n27    18    10 Pill                24 TRUE  \n28    18     9 Pill                24 TRUE  \n29    18     8 Pill                24 TRUE  \n30    18     7 Pill                24 TRUE  \n31    18     6 Pill                24 TRUE  \n32    18     5 Pill                24 TRUE  \n33    18     4 Pill                24 TRUE  \n34    18     3 Pill                24 TRUE  \n35    18     2 Pill                24 TRUE  \n36    18     1 Pill                24 TRUE  \n37    39    36 Pill                36 FALSE \n38    39    35 Pill                36 FALSE \n39    39    34 Pill                36 FALSE \n40    39    33 Pill                36 FALSE \n# … with 10,292 more rows\n\n\nFor display purposes, we drop all variables except for ID, MONTH, FP, last_month, and ceased.\nRemove all rows except for the row containing each woman’s last_month. The result will be a data frame where each woman in the pills cohort occupies only one row, which 1) shows her last month of recorded use and 2) indicates whether we know that she actually stopped using the pill in her last month.\n\n\npills <- pills %>% filter(last_month == MONTH) \n\npills\n\n\n# A tibble: 287 x 5\n# Groups:   ID [287]\n      ID MONTH FP    last_month ceased\n   <int> <int> <fct>      <dbl> <lgl> \n 1    18    24 Pill          24 TRUE  \n 2    39    36 Pill          36 FALSE \n 3    44     5 Pill           5 TRUE  \n 4    59    36 Pill          36 FALSE \n 5    60     6 Pill           6 TRUE  \n 6    99    36 Pill          36 FALSE \n 7   188    36 Pill          36 FALSE \n 8   202    36 Pill          36 FALSE \n 9   218    35 Pill          35 FALSE \n10   233    12 Pill          12 TRUE  \n11   260     2 Pill           2 TRUE  \n12   290    35 Pill          35 FALSE \n13   298    20 Pill          20 TRUE  \n14   335    12 Pill          12 TRUE  \n15   340    11 Pill          11 TRUE  \n16   419    36 Pill          36 FALSE \n17   423    36 Pill          36 FALSE \n18   428     4 Pill           4 TRUE  \n19   460    16 Pill          16 TRUE  \n20   488    17 Pill          17 TRUE  \n21   551    32 Pill          32 TRUE  \n22   669    36 Pill          36 FALSE \n23   697    23 Pill          23 TRUE  \n24   707    26 Pill          26 TRUE  \n25   757    33 Pill          33 TRUE  \n26   767    23 Pill          23 TRUE  \n27   785    18 Pill          18 TRUE  \n28   836     1 Pill           1 TRUE  \n29   837    12 Pill          12 TRUE  \n30   850    35 Pill          35 FALSE \n31   930    36 Pill          36 FALSE \n32   943    13 Pill          13 TRUE  \n33   955     5 Pill           5 TRUE  \n34   977    36 Pill          36 FALSE \n35  1001    36 Pill          36 FALSE \n36  1037    36 Pill          36 FALSE \n37  1120     7 Pill           7 TRUE  \n38  1167    19 Pill          19 TRUE  \n39  1233    36 Pill          36 FALSE \n40  1271    32 Pill          32 TRUE  \n# … with 247 more rows\n\nLet’s now fit the Kaplan Meier estimator with survfit, which takes a survival object created by Surv. The function summary shows the survival probabilities at each month in a column called survival:\n\n\npills <- survfit(Surv(last_month, ceased) ~ 1, data = pills)\n\nsummary(pills)\n\n\nCall: survfit(formula = Surv(last_month, ceased) ~ 1, data = pills)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    287       4    0.986 0.00692        0.973        1.000\n    2    283       6    0.965 0.01082        0.944        0.987\n    3    277       7    0.941 0.01393        0.914        0.968\n    4    270      10    0.906 0.01723        0.873        0.940\n    5    260      14    0.857 0.02066        0.818        0.899\n    6    246       4    0.843 0.02146        0.802        0.886\n    7    242       7    0.819 0.02274        0.775        0.865\n    8    235       5    0.801 0.02355        0.757        0.849\n    9    230       3    0.791 0.02400        0.745        0.839\n   10    227       5    0.774 0.02471        0.727        0.823\n   11    222       8    0.746 0.02571        0.697        0.798\n   12    214      21    0.672 0.02770        0.620        0.729\n   13    193       9    0.641 0.02831        0.588        0.699\n   14    184       6    0.620 0.02865        0.567        0.679\n   15    178       2    0.613 0.02875        0.559        0.672\n   16    176       5    0.596 0.02897        0.542        0.655\n   17    171       8    0.568 0.02924        0.513        0.628\n   18    163       6    0.547 0.02938        0.492        0.608\n   19    157       7    0.523 0.02948        0.468        0.584\n   20    150       2    0.516 0.02950        0.461        0.577\n   21    148       2    0.509 0.02951        0.454        0.570\n   22    146       3    0.498 0.02951        0.444        0.560\n   23    143       6    0.477 0.02948        0.423        0.539\n   24    137       9    0.446 0.02934        0.392        0.507\n   25    128       3    0.436 0.02927        0.382        0.497\n   26    125       4    0.422 0.02915        0.368        0.483\n   27    121       1    0.418 0.02912        0.365        0.479\n   28    120       3    0.408 0.02901        0.355        0.469\n   29    117       2    0.401 0.02893        0.348        0.462\n   31    115       2    0.394 0.02884        0.341        0.455\n   32    113       3    0.383 0.02870        0.331        0.444\n   33    110       3    0.373 0.02854        0.321        0.433\n   34    107       3    0.362 0.02837        0.311        0.422\n\nWe can plot this with autoplot:\n\n\nautoplot(\n  pills,\n  main = \"Kaplan-Meier survival estimate: Pills\",\n  xlab = \"Months\",\n  ylab = \"Probability of Continuous Use\",\n  ylim = c(0, 1),\n  censor = F\n)\n\n\n\n\nFor additional examples using other family planning methods, download the R Markdown script from this breakout session. Video from the session is included below:\n \nBreakout Session: Stata Users\nIPUMS PMA extracts for Stata should be decompressed before use and loaded with an appropriate filepath:\n. clear\n. use \"[filepath]/pma_00001.dta\"\n. cd \"[filepath]\"\n. set more off\n\nRemember to set your own filepath and change the name of your file to match your own data extract!\nAs shown in the R example above, this dataset contains 9,549 rows where each row represents one respondent to the Female Questionnaire. You’ll find a unique identification number for each respondent in personid, and their comma-separated contraceptive calendar strings in calendarke. We’ll show these two variables for the first 3 respondents:\n. list personid calendarke in 1/3\n\n     +----------------------------------------------------------------------+\n  1. |                                           personid                   |\n     |                              404201900050442019002                   |\n     |----------------------------------------------------------------------|\n     | calendarke                                                           |\n     |   0,0,B,P,P,P,P,P,P,P,P,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,3,3,3,3,3,.. |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  2. |                                           personid                   |\n     |                              404201900009272019002                   |\n     |----------------------------------------------------------------------|\n     | calendarke                                                           |\n     |   ,7,7,7,7,7,7,7,7,0,B,P,P,P,P,P,P,P,P,0,0,0,0,0,9,9,9,9,9,9,9,9,9.. |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  3. |                                           personid                   |\n     |                              404201900099612019003                   |\n     |----------------------------------------------------------------------|\n     | calendarke                                                           |\n     |   0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,.. |\n     +----------------------------------------------------------------------+\nAs you can see, calendarke includes the same contraceptive calendar codes shown above, each separated by a comma. Each string contains 36 codes: these represent the 36 months from January 2017 through December 2019 (the last month in which Kenya 2019 samples were collected). The left-most code represents the most recent available month.\nNotice the second listed person (personid 404201900009272019002); their calendarke string begins with a comma rather than a response code. This indicates a person who was interviewed in November 2019, rather than December. Because December 2019 would have been a future month for such a person, their first value is blank.\nSplit the calendarke string into 36 separate columns with the split function:\n. split calendarke, p(,) gen(cal_ke)\nvariables created as string: \ncal_ke1   cal_ke7   cal_ke13  cal_ke19  cal_ke25  cal_ke31\ncal_ke2   cal_ke8   cal_ke14  cal_ke20  cal_ke26  cal_ke32\ncal_ke3   cal_ke9   cal_ke15  cal_ke21  cal_ke27  cal_ke33\ncal_ke4   cal_ke10  cal_ke16  cal_ke22  cal_ke28  cal_ke34\ncal_ke5   cal_ke11  cal_ke17  cal_ke23  cal_ke29  cal_ke35\ncal_ke6   cal_ke12  cal_ke18  cal_ke24  cal_ke30  cal_ke36\nThen, reshape the data from wide to long format as shown in the R example above. The index number for each month will pivot downward into a new column called month, and the woman’s response code for each month will pivot downward into an adjacent column called cal_ke:\n. reshape long cal_ke, i(personid) j(month)\n(note: j = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \n> 26 27 28 29 30 31 32 33 34 35 36)\n\nData                               wide   ->   long\n-----------------------------------------------------------------------------\nNumber of obs.                     9549   ->  343764\nNumber of variables                  52   ->      18\nj variable (36 values)                    ->   month\nxij variables:\n           cal_ke1 cal_ke2 ... cal_ke36   ->   cal_ke\n-----------------------------------------------------------------------------\nNotice that the dataset now has 343,764 rows, where each of our 9,549 respondents occupies 36 rows apiece (one for each month).\nBy default, Stata numbered each month in increasing order from left to right. As we’ve discussed, however, the contraceptive calendar data are chronologically organized from right to left (the most recent month is stored in the first month of the calendarke string). We suggest renumbering the cal_ke variables here so that cal_ke1 represents the first month, January 2017.\n. replace month = 37 - month\n(343,764 real changes made)\n. sort personid month\nYou might also find it useful to create century month codes cmc for each month, beginning with 1405 for January 2017.\n. gen cmc = month + 1404\nAs a final clean-up step, we create a numeric version of cal_ke called numcal_ke by changing the codes for pregnancy, pregnancy termination, and birth to 90, 91, and 92 respectively. We then provide labels to each of the values in numcal_ke.\n. gen numcal_ke = cal_ke\n(3,755 missing values generated)\n. replace numcal_ke = \"90\" if numcal_ke == \"P\"\n(24,364 real changes made)\n. replace numcal_ke = \"91\" if numcal_ke == \"T\"\n(341 real changes made)\n. replace numcal_ke = \"92\" if numcal_ke == \"B\"\n(2,934 real changes made)\n. destring numcal_ke, replace\nnumcal_ke: all characters numeric; replaced as byte\n(3755 missing values generated)\n\n. label define calendar 92 \"Birth\" 90 \"Pregnant\" 91 \"Pregnancy ended\" 0 \"No \n> family planning method used\" 1 \"Female Sterilization\" 2 \"Male Sterilization\" 3 \n> \"Implant\" 4 \"IUD\" 5 \"Injectables\" 7 \"Pill\" 8 \"Emergency Contraception\" 9 \"Male \n> Condom\" 10 \"Female Condom\" 11 \"Diaphragm\" 12 \"Foam / Jelly\" 13 \"Standard Days / \n> Cycle beads\" 14 \"LAM\" 30 \"Rhythm method\" 31 \"Withdrawal\" 39 \"Other traditional \n> methods\"\n\n. label values numcal_ke calendar\nFinally, we’re ready to begin our analysis. As with the R example above, we’ll create survival curves showing the duration of continuously used family planning methods for cohorts of women who were using the same method in January 2017. These curves will estimate the probability that an individual “survives” - or continues using - a given method at each of 36 months, assuming that she used it in month 1. We’ll exclude the duration of use after any break (for example, if a woman stopped using family planning to become pregnant, but then started again afterward).\nConsider the cohort of women who were all using the contraceptive pill in January 2017 (month 1). We’ll flag these cases and include all rows from those women in a sub-sample called pill_sample:\n. recode numcal_ke (7=1) (else=0), gen(pill)\n(162306 differences between numcal_ke and pill)\n. gen pill_temp = 0 \n. replace pill_temp = 1 if pill == 1 & month == 1\n(287 real changes made)\n. egen pill_sample = max(pill_temp), by(personid)\nThe function stset establishes the data in memory as “survival-time” data, where the variable month records time in months, the identification number for each person is provided by id(personid), and cessation of use (i.e. failure) is marked by the first instance where pill==0 for each person in the sub-sample.\n. stset month, id(personid) failure(pill==0)\n\n                id:  personid\n     failure event:  pill == 0\nobs. time interval:  (month[_n-1], month]\n exit on or before:  failure\n\n-----------------------------------------------------------------------------\n> -\n    343,764  total observations\n    328,001  observations begin on or after (first) failure\n-----------------------------------------------------------------------------\n> -\n     15,763  observations remaining, representing\n      9,549  subjects\n      9,463  failures in single-failure-per-subject data\n     15,763  total analysis time at risk and under observation\n                                                at risk from t =         0\n                                     earliest observed entry t =         0\n                                          last observed exit t =        36\nThe function sts list produces a similar table to the one produced by summary(pills) in the R example above. The column Survivor Function estimates the probability of “surviving” - or continuously using - the pill at each month shown in the column Time.\n.  sts list if pill_sample == 1\n\n         failure _d:  pill == 0\n   analysis time _t:  month\n                 id:  personid\n\n           Beg.          Net            Survivor      Std.\n  Time    Total   Fail   Lost           Function     Error     [95% Conf. Int.]\n-------------------------------------------------------------------------------\n     2      287      4      0             0.9861    0.0069     0.9633    0.9947\n     3      283      6      0             0.9652    0.0108     0.9362    0.9811\n     4      277      7      0             0.9408    0.0139     0.9064    0.9628\n     5      270     10      0             0.9059    0.0172     0.8658    0.9345\n     6      260     14      0             0.8571    0.0207     0.8111    0.8927\n     7      246      4      0             0.8432    0.0215     0.7957    0.8805\n     8      242      7      0             0.8188    0.0227     0.7692    0.8588\n     9      235      5      0             0.8014    0.0235     0.7504    0.8431\n    10      230      3      0             0.7909    0.0240     0.7392    0.8336\n    11      227      5      0             0.7735    0.0247     0.7206    0.8177\n    12      222      8      0             0.7456    0.0257     0.6911    0.7920\n    13      214     21      0             0.6725    0.0277     0.6149    0.7234\n    14      193      9      0             0.6411    0.0283     0.5827    0.6936\n    15      184      6      0             0.6202    0.0286     0.5614    0.6735\n    16      178      2      0             0.6132    0.0287     0.5543    0.6668\n    17      176      5      0             0.5958    0.0290     0.5366    0.6500\n    18      171      8      0             0.5679    0.0292     0.5085    0.6229\n    19      163      6      0             0.5470    0.0294     0.4876    0.6025\n    20      157      7      0             0.5226    0.0295     0.4633    0.5786\n    21      150      2      0             0.5157    0.0295     0.4564    0.5717\n    22      148      2      0             0.5087    0.0295     0.4495    0.5648\n    23      146      3      0             0.4983    0.0295     0.4391    0.5545\n    24      143      6      0             0.4774    0.0295     0.4185    0.5337\n    25      137      9      0             0.4460    0.0293     0.3878    0.5024\n    26      128      3      0             0.4355    0.0293     0.3776    0.4920\n    27      125      4      0             0.4216    0.0291     0.3641    0.4779\n    28      121      1      0             0.4181    0.0291     0.3607    0.4744\n    29      120      3      0             0.4077    0.0290     0.3506    0.4639\n    30      117      2      0             0.4007    0.0289     0.3438    0.4568\n    32      115      2      0             0.3937    0.0288     0.3371    0.4498\n    33      113      3      0             0.3833    0.0287     0.3271    0.4391\n    34      110      3      0             0.3728    0.0285     0.3170    0.4285\n    35      107      3      0             0.3624    0.0284     0.3070    0.4178\n    36      104     18     86             0.2997    0.0270     0.2477    0.3532\n-------------------------------------------------------------------------------\nA survival curve representing this table can be made with:\nsts graph if pill_sample == 1\n\n\n\n\nThis plot is similar to the one generated by autoplot in R, except that 1) it does not include a shaded 95% confidence interval, and 2) right-censored cases are not excluded, producing a sharp downward turn in month 36.\nFor additional examples using other family planning methods, download the Stata .do file from this breakout session here. Video from the session is included below:\n \nDownload Links\nPowerpoint Slides\nR Markdown Script\nStata .do File\nChat Transcript (participant names are redacted)\n\n\n\n",
    "preview": "posts/2021-05-15-paa-2021/images/featured.png",
    "last_modified": "2021-05-17T12:54:11-05:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 1800
  },
  {
    "path": "posts/2021-05-01-storymaps/",
    "title": "Visualizing PMA Data with StoryMaps",
    "description": "Five outstanding undergraduate research projects integrate dynamic data visualization with spatial analysis and narrative.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-05-03",
    "categories": [
      "PMA Publications",
      "StoryMaps",
      "Undergrads",
      "Teaching"
    ],
    "contents": "\n\nContents\nAttainment of Sex Preference in India\nIntimate Partner Violence and Body Weight\nHigher Probability that Women Report IPV\nEmpowered Women Raise Healthy Children\nWealth and Healthcare can Save Pregnant Women\n\nWhen great new research gets published with PMA data, we like to share it with you in a PMA Publications post. Sometimes we’ll take a deep dive into the R code you can use to reproduce great analysis, but today we’ll take a look at a different tool that undergraduate students are using to learn about IPUMS PMA and other IPUMS Global Health data at the University of Minnesota.\n\nHave a recent publication using PMA data that you’d like to feature in a PMA Publications post? Please let us know!\nThis semester, students in the Global Health Survey Analysis course used an amazing tool called StoryMaps to develop interactive narratives exploring different topics related to family planning. StoryMaps have been used in both the undergraduate and graduate curriculum throughout the College of Liberal Arts and beyond - we encourage you to check out the full gallery of student projects here!\nThanks to course professors Elizabeth Boyle and Kathryn Grace for sharing this great work!\nAttainment of Sex Preference in India\nAuthor: Lara Rae Erdmann\n\n\n\n\nIntimate Partner Violence and Body Weight\nAuthor: Jaclyn Willems\n\n\n\n\nHigher Probability that Women Report IPV\nAuthor: Peyton Retterath\n\n\n\n\nEmpowered Women Raise Healthy Children\nAuthor: Kassandra Fate\n\n\n\n\nWealth and Healthcare can Save Pregnant Women\nAuthor: Hana al’Absi\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-05-01-storymaps/images/featured.png",
    "last_modified": "2021-11-15T14:34:54-06:00",
    "input_file": {},
    "preview_width": 2190,
    "preview_height": 1254
  },
  {
    "path": "posts/2021-04-15-migration-discovery/",
    "title": "Formatting Migration Recall Data for Longitudinal Analysis",
    "description": "Use tidyr::pivot_longer to reshape wide data into a long format.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-04-15",
    "categories": [
      "Migration",
      "Data Discovery",
      "Data Manipulation",
      "pivot_longer",
      "regex"
    ],
    "contents": "\n\nContents\nData Availability\nLongitudinal Data Structures\nPivot Longer into One Column\nPivot Longer into Multiple Columns\n\n\nMost of the data you’ll find at IPUMS PMA comes from cross-sectional surveys, where each respondent is interviewed only once. However, there are some items on the Female Questionnaire that ask respondents to recall past events. When these recall data are linked to a measure of time, the data can be restructured to simulate longitudinal data – or repeated observations on individuals over time. Once the data are in this structure, we can use a range of analytic tools to determine how the frequency or duration of past experiences explains current outcomes.\n\nCurrently, only the 2016 Ethiopia Maternal and Newborn Health survey contains data from follow-up interviews. Panel data related to contraceptive use is coming soon!\nOne place where you’ll find this type of data is on the migration variables page. In our last post, we saw that PMA samples from Ethiopia began including information about each respondent’s single most most recent migration experience beginning with the 2016 sample. More recently, a number of samples from other countries have collected data about each respondent’s complete migration history, organized in chronological order. These samples include:\nBurkina Faso 2020\nCongo (DR), Kinshasa 2019\nCongo (DR), Kongo Central 2019\nKenya 2019\nNigeria, Kano 2019\nNigeria, Lagos 2019\nIn this post, we’ll take a look at the available information in the migration data collected from these newer samples. As we’ll see, female respondents who indicate that they have migrated at least once receive the same set of questions for each place they have lived, resulting in a dataset that is exceptionally wide and cumbersome to use in most time-dependent applications. We’ll show how to reshape an example data extract into a much friendlier long format using the function tidyr::pivot_longer.\nData Availability\nThe samples listed above contain data from nearly identical survey questions. In the interview, a respondent is first asked how long they have lived in their current place of residence; if they indicate that they have not “always” lived in the same place, they are then asked how many places they’ve lived for more than six months after age 15 or their first marriage (whichever happened first).\n\nInterviewers were instructed to define a place as “a community, village, or neighborhood”.\nRespondents who list at least one such place are then asked to recall information about each place, starting with the place before their current residence. Information about the most recent place is represented by variables beginning with the prefix PLACE1. Information about the second most recent place is represented by the prefix PLACE2, and so forth.\nIn each of these samples, the same questions are repeated for each place until all of the respondent’s previous places of residence are fully enumerated. The available information about each place includes:\nits country\nits district or region\nthe respondent’s age when she moved to the place\nwhether the place was a city, a town, peri-urban, or rural (not available for Nigeria samples)\nAdditionally, the respondent could list multiple reasons for migrating to each place (note that this information is not available for the respondent’s migration to their current place of residence). Options include:\nlooking for a job\nseasonal work\nwork (non-seasonal)\nwant to change jobs\nfamily or village conflict\nto attend school\nmove after completed school\njoin spouse after marriage\nco-reside with boy/girlfriend\ndivorce/widowhood\nhospitalization/health problem\nbetter access to health service\ncaring for sick relative\nfollowed spouse to job\nbetter land for farming\nbetter education for children\nother social reasons\nother\nBecause the respondent can choose multiple reasons, we’ll find one binary indicator for each of these 18 reasons. As you might imagine, this results in a very wide dataset! Some respondents move as many as 11 times, resulting in 198 columns from just this one repeated multiple-response question.\nThe wide shape of these data is more than an inconvenience: most longitudinal analysis applications require easy access to the time interval between events. In their current format, each numbered PLACE variable represents a single migration event, but it’s difficult to tell how much time passed between any two migrations for a given person. To make this kind of analysis possible, we need to pivot the migration variables into a long format accompanied by a new variable showing the time interval between each migration.\nLongitudinal Data Structures\nLet’s take a look at the way migration history variables are currently formatted for one of the samples we discussed above. For this example, we’ve created a data extract containing all of the available migration data for the Kenya 2019 sample (female respondents only). We’ll load it and the following packages into R:\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\n\ndat <- read_ipums_micro(\n  ddi = \"data/pma_00022.xml\",\n  data = \"data/pma_00022.dat.gz\"\n)\n\n\n\n\nRemember: change these file paths to match the download location for your own data extract!\n\n\n\nLike all IPUMS PMA data extracts, this dataset reflects a cross-sectional survey design where every response from each person is stored in a single row. If you’re familiar with longitudinal data structures, you know that repeated observations from the same respondents are stored in separate rows, where each row represents a different moment in time. Why might this be the case?\nAs we’ll see in our own data, the values for repeated observations and the amount of time that passes between observations are related only by a common pattern in the names for each variable when they’re stored together in a wide format. In our case, the only mark of time between migrations is the respondent’s age. Consider the following respondents, who have each migrated at least twice:\n\n\ndat %>% \n  filter(PLACELIVENUM %in% 2:7) %>% \n  select(ends_with(\"DISTRICTKE\"), ends_with(\"MOVEAGE\")) %>% \n  relocate(starts_with(\"PLACE1\"), starts_with(\"PLACE2\"))\n\n\n# A tibble: 1,206 x 14\n  PLACE1DISTRICTKE PLACE1MOVEAGE PLACE2DISTRICTKE PLACE2MOVEAGE\n         <int+lbl>     <int+lbl>        <int+lbl>     <int+lbl>\n1  7 [Nandi]                  29    32 [Migori]              19\n2  8 [Nyamira]                19     8 [Nyamira]             21\n3  6 [Nairobi]                34     3 [Kiambu]              28\n4  6 [Nairobi]                25    35 [Nakuru]               0\n5 43 [Trans-Nzoia]            15    28 [Machakos]             0\n# … with 1,201 more rows, and 10 more variables:\n#   PLACE3DISTRICTKE <int+lbl>, PLACE4DISTRICTKE <int+lbl>,\n#   PLACE5DISTRICTKE <int+lbl>, PLACE6DISTRICTKE <int+lbl>,\n#   PLACE7DISTRICTKE <int+lbl>, PLACE3MOVEAGE <int+lbl>,\n#   PLACE4MOVEAGE <int+lbl>, PLACE5MOVEAGE <int+lbl>,\n#   PLACE6MOVEAGE <int+lbl>, PLACE7MOVEAGE <int+lbl>\n\nPLACE1DISTRICTKE shows the administrative district of the last place a respondent lived before their current place of residence, and PLACE1MOVEAGE shows her age when she moved there. PLACE2DISTRICTKE shows the district of the second most recent place she lived, and PLACE2MOVEAGE shows her age when she moved there. The same pattern would be repeated for all of the places a person might have lived (in this sample, some respondents migrated as many as 7 times).\n\nYou might notice that the respondent in the second row recalls her prior places of residence in reverse chronological order. This particular type of recall error is also easier to fix when we pivot_longer.\nSuppose you wanted to know something very simple about the relationship between these variables, such as the average age of female migrants arriving at each district in the sample. In this wide format, you would first have to find the mean PLACE1MOVEAGE for every district in PLACE1DISTRICTKE, then the mean PLACE2MOVEAGE for every district in PLACE2DISTRICTKE, and so forth for all 7 places. Then, you’d need to find the frequency weighted mean for each district in all 7 places. That’s quite a bit of extra work for just one simple statistic!\nImagine you wanted to model the effect of a time-dependent variable on an outcome of interest. For example, you might suppose that the number of times a female respondent gives birth could be related to the length of time she’s lived in a district where there are relatively few family planning services available. As you can see, we’d have a hard time building an appropriate model because the relevant data are currently strewn across 14 different variables. Instead, we’d much prefer two work with the data in a long format with only two variables: one representing DISTRICTKE and one representing MOVEAGE.\nPivot Longer into One Column\nFor the moment, let’s continue working just with the district for each place in an individual’s migration history. To keep things simple, we’ll create a dataset called district containing only the variables ending with the string DISTRICTKE.\n\n\ndistrict <- dat %>% select(ends_with(\"DISTRICTKE\")) \n\ndistrict\n\n\n# A tibble: 9,549 x 7\n   PLACE1DISTRICTKE PLACE2DISTRICTKE PLACE3DISTRICTKE PLACE4DISTRICTKE\n          <int+lbl>        <int+lbl>        <int+lbl>        <int+lbl>\n1 99 [NIU (not in … 99 [NIU (not in… 99 [NIU (not in… 99 [NIU (not in…\n2  6 [Nairobi]      99 [NIU (not in… 99 [NIU (not in… 99 [NIU (not in…\n3 99 [NIU (not in … 99 [NIU (not in… 99 [NIU (not in… 99 [NIU (not in…\n4 99 [NIU (not in … 99 [NIU (not in… 99 [NIU (not in… 99 [NIU (not in…\n5 99 [NIU (not in … 99 [NIU (not in… 99 [NIU (not in… 99 [NIU (not in…\n# … with 9,544 more rows, and 3 more variables:\n#   PLACE5DISTRICTKE <int+lbl>, PLACE6DISTRICTKE <int+lbl>,\n#   PLACE7DISTRICTKE <int+lbl>\n\nAs you can see, a lot of the information contained in district isn’t really necessary. Every row holds information on 7 places of residence, but most respondents migrated only to 1 or 2 places if they ever migrated at all. The best approach here is to tell R that labels like NIU (not in universe) and No response or missing each represent a type of missing data that we can recode as NA. We can do that with help from ipumsr::lbl_na_if and dplyr::across:\n\n\ndistrict <- district %>% \n  mutate(across(everything(), ~{\n    lbl_na_if(.x, ~.lbl %in% c(\n      \"No response or missing\",\n      \"NIU (not in universe)\"\n    ))\n  }))\n\ndistrict\n\n\n# A tibble: 9,549 x 7\n  PLACE1DISTRICTKE PLACE2DISTRICTKE PLACE3DISTRICTKE PLACE4DISTRICTKE\n         <int+lbl>        <int+lbl>        <int+lbl>        <int+lbl>\n1     NA                         NA               NA               NA\n2      6 [Nairobi]               NA               NA               NA\n3     NA                         NA               NA               NA\n4     NA                         NA               NA               NA\n5     NA                         NA               NA               NA\n# … with 9,544 more rows, and 3 more variables:\n#   PLACE5DISTRICTKE <int+lbl>, PLACE6DISTRICTKE <int+lbl>,\n#   PLACE7DISTRICTKE <int+lbl>\n\n\nSee this post for additional details on recoding variables with ipumsr and dplyr::across.\nIn order to keep track of individuals, let’s also add a column ID that represents a short identification number for each person:\n\n\ndistrict <- district %>% rowid_to_column(\"ID\")\ndistrict\n\n\n# A tibble: 9,549 x 8\n     ID PLACE1DISTRICTKE PLACE2DISTRICTKE PLACE3DISTRICTKE\n  <int>        <int+lbl>        <int+lbl>        <int+lbl>\n1     1     NA                         NA               NA\n2     2      6 [Nairobi]               NA               NA\n3     3     NA                         NA               NA\n4     4     NA                         NA               NA\n5     5     NA                         NA               NA\n# … with 9,544 more rows, and 4 more variables:\n#   PLACE4DISTRICTKE <int+lbl>, PLACE5DISTRICTKE <int+lbl>,\n#   PLACE6DISTRICTKE <int+lbl>, PLACE7DISTRICTKE <int+lbl>\n\nNow, we’re ready to use the function tidyr::pivot_longer to reshape district. The simplest way to use this function is to specify a group of columns with the argument cols:\n\n\n\n\n\ndistrict %>% pivot_longer(cols = ends_with(\"DISTRICTKE\"))\n\n\n# A tibble: 66,843 x 3\n      ID name                    value\n   <int> <chr>               <int+lbl>\n 1     1 PLACE1DISTRICTKE NA          \n 2     1 PLACE2DISTRICTKE NA          \n 3     1 PLACE3DISTRICTKE NA          \n 4     1 PLACE4DISTRICTKE NA          \n 5     1 PLACE5DISTRICTKE NA          \n 6     1 PLACE6DISTRICTKE NA          \n 7     1 PLACE7DISTRICTKE NA          \n 8     2 PLACE1DISTRICTKE  6 [Nairobi]\n 9     2 PLACE2DISTRICTKE NA          \n10     2 PLACE3DISTRICTKE NA          \n# … with 66,833 more rows\n\n\n\n\n\n© 2018 RStudio (CC0 1.0)\ntidyr is included with library(tidyverse).\nBy default, the name of each column moves into a single column called name, and the value of each column moves into an adjascent column called value. We can manually change the names of these columns with the arguments names_to and values_to:\n\n\ndistrict %>% pivot_longer(\n  cols = ends_with(\"DISTRICTKE\"),\n  names_to = \"PLACE\",\n  values_to = \"DISTRICTKE\"\n)\n\n\n# A tibble: 66,843 x 3\n      ID PLACE              DISTRICTKE\n   <int> <chr>               <int+lbl>\n 1     1 PLACE1DISTRICTKE NA          \n 2     1 PLACE2DISTRICTKE NA          \n 3     1 PLACE3DISTRICTKE NA          \n 4     1 PLACE4DISTRICTKE NA          \n 5     1 PLACE5DISTRICTKE NA          \n 6     1 PLACE6DISTRICTKE NA          \n 7     1 PLACE7DISTRICTKE NA          \n 8     2 PLACE1DISTRICTKE  6 [Nairobi]\n 9     2 PLACE2DISTRICTKE NA          \n10     2 PLACE3DISTRICTKE NA          \n# … with 66,833 more rows\n\nEven more conveniently, we can generate these columns automatically if we identify a pattern in the original column names. This approach efficiently handles both the names of the new columns and the values stored in each column. Notice that we’ve manually specified the name of the column DISTRICTKE above; this is fine if we’re only pivoting one column, but we want to avoid manually writing a name for each new column when we start working with several variables at once. Also, notice the values that appear in the PLACE column; wouldn’t it be more convenient to extract the index number for each place, rather than the full names of the original columns?\nWe’ll use the additional argument names_pattern to solve both problems at once. Any string enclosed with parentheses () in names_pattern can be passed, in sequential order, to names_to. In this example, we specify a pattern where the string PLACE will be followed by a single-digit number ([0-9]) followed by the string (DISTRICTKE). The argument names_to places the single-digit number in a column called PLACE, and it places the string DISTRICTKE in a column that uses a pronoun .value to represent the contents of the string.\n\n\ndistrict %>% \n  pivot_longer(\n    cols = ends_with(\"DISTRICTKE\"), \n    names_pattern = \"PLACE([0-9])(DISTRICTKE)\",\n    names_to = c(\"PLACE\", \".value\")\n  ) \n\n\n# A tibble: 66,843 x 3\n      ID PLACE   DISTRICTKE\n   <int> <chr>    <int+lbl>\n 1     1 1     NA          \n 2     1 2     NA          \n 3     1 3     NA          \n 4     1 4     NA          \n 5     1 5     NA          \n 6     1 6     NA          \n 7     1 7     NA          \n 8     2 1      6 [Nairobi]\n 9     2 2     NA          \n10     2 3     NA          \n# … with 66,833 more rows\n\nWe can improve the scalability of this code just a little bit more by adding the wildcard . in names_pattern to represent “any character” and the operator * to represent “any number of times”. This allows us to write ([0-9]*) to find an integer of any length (in case some respondents move 10 places or more), and (.*) to find a string of any length afterward (this saves us the trouble of writing “DISTRICTKE”).\n\n\ndistrict %>% \n  pivot_longer(\n    cols = ends_with(\"DISTRICTKE\"), \n    names_pattern = \"PLACE([0-9]*)(.*)\",\n    names_to = c(\"PLACE\", \".value\")\n  ) \n\n\n# A tibble: 66,843 x 3\n      ID PLACE   DISTRICTKE\n   <int> <chr>    <int+lbl>\n 1     1 1     NA          \n 2     1 2     NA          \n 3     1 3     NA          \n 4     1 4     NA          \n 5     1 5     NA          \n 6     1 6     NA          \n 7     1 7     NA          \n 8     2 1      6 [Nairobi]\n 9     2 2     NA          \n10     2 3     NA          \n# … with 66,833 more rows\n\nNotice that there are now 66,843 rows in district: that’s 7 rows for 7 places per respondent. Adding the argument values_drop_NA = TRUE drops placeholder values for respondents who lived in fewer than 7 places:\n\n\ndistrict %>% \n  pivot_longer(\n    cols = ends_with(\"DISTRICTKE\"), \n    names_pattern = \"PLACE([0-9]*)(.*)\",\n    names_to = c(\"PLACE\", \".value\"),\n    values_drop_na = TRUE\n  ) \n\n\n# A tibble: 5,165 x 3\n      ID PLACE    DISTRICTKE\n   <int> <chr>     <int+lbl>\n 1     2 1      6 [Nairobi] \n 2    11 1     10 [Kakamega]\n 3    12 1     10 [Kakamega]\n 4    14 1      7 [Nandi]   \n 5    14 2     32 [Migori]  \n 6    16 1      6 [Nairobi] \n 7    19 1     33 [Mombasa] \n 8    20 1      8 [Nyamira] \n 9    20 2      8 [Nyamira] \n10    21 1      6 [Nairobi] \n# … with 5,155 more rows\n\nThis step causes any respondent who has never migrated from a place they lived for 6 months or more after age 15 / first marriage to be filtered out of the data. Here, we see the first 10 rows from all of the remaining female respondents. Individuals 14 and 20 lived in two such places: individual 14 first lived in the district Migori, then moved to Nandi, and finally moved to her current residence (not shown). Individual 20 first lived in Nyamira, then moved to another place also in Nyamira, and finally moved to her current residence (not shown). All of the other displayed respondents lived in exactly one such place. Next, we’ll add the age at which each of the women moved to each location.\n\nRemember: these migration history variables contain information about each place a person has lived prior to their current residence. You’ll find information on the woman’s current district of residence in either GEOKE (for Kenya samples) or SUBNATIONAL (for all samples, including Kenya).\nPivot Longer into Multiple Columns\nNow that we know how to use wildcard operators in pivot_longer, we’ll be able to start pivoting multiple columns at once. Let’s start by adding the respondent’s age when they moved to each place. Using the same processing steps we used to make district, we’ll create a new dataset called age.\n\n\nage <- dat %>% \n  select(ends_with(\"DISTRICTKE\"), ends_with(\"MOVEAGE\")) %>% \n  mutate(across(everything(), ~{\n    lbl_na_if(.x, ~.lbl %in% c(\n      \"No response or missing\",\n      \"NIU (not in universe)\"\n    ))\n  })) %>% \n  rowid_to_column(\"ID\")\n\n\n\n\n\nage %>% relocate(ID, starts_with(\"PLACE1\"), starts_with(\"PLACE2\"))\n\n\n# A tibble: 9,549 x 15\n      ID PLACE1DISTRICTKE PLACE1MOVEAGE PLACE2DISTRICTKE PLACE2MOVEAGE\n   <int>        <int+lbl>     <int+lbl>        <int+lbl>     <int+lbl>\n 1     1     NA                      NA               NA            NA\n 2     2      6 [Nairobi]            16               NA            NA\n 3     3     NA                      NA               NA            NA\n 4     4     NA                      NA               NA            NA\n 5     5     NA                      NA               NA            NA\n 6     6     NA                      NA               NA            NA\n 7     7     NA                      NA               NA            NA\n 8     8     NA                      NA               NA            NA\n 9     9     NA                      NA               NA            NA\n10    10     NA                      NA               NA            NA\n# … with 9,539 more rows, and 10 more variables:\n#   PLACE3DISTRICTKE <int+lbl>, PLACE4DISTRICTKE <int+lbl>,\n#   PLACE5DISTRICTKE <int+lbl>, PLACE6DISTRICTKE <int+lbl>,\n#   PLACE7DISTRICTKE <int+lbl>, PLACE3MOVEAGE <int+lbl>,\n#   PLACE4MOVEAGE <int+lbl>, PLACE5MOVEAGE <int+lbl>,\n#   PLACE6MOVEAGE <int+lbl>, PLACE7MOVEAGE <int+lbl>\n\n\nWe’re using relocate here just so that we can display the DISTRICTKE and MOVEAGE variables side-by-side. It doesn’t change anything else about the structure of the data!\nBecause we’re using the wildcard pattern (.*), the function will treat the string MOVEEAGE the same way it treats DISTRICTKE. We only need to add the new columns to cols:\n\n\nage <- age %>% \n  pivot_longer(\n    cols = c(ends_with(\"DISTRICTKE\"), ends_with(\"MOVEAGE\")), \n    names_pattern = \"PLACE([0-9]*)(.*)\",\n    names_to = c(\"PLACE\", \".value\"),\n    values_drop_na = TRUE\n  ) \n\nage\n\n\n# A tibble: 5,246 x 4\n      ID PLACE    DISTRICTKE   MOVEAGE\n   <int> <chr>     <int+lbl> <int+lbl>\n 1     2 1      6 [Nairobi]         16\n 2    11 1     10 [Kakamega]        17\n 3    12 1     10 [Kakamega]        16\n 4    14 1      7 [Nandi]           29\n 5    14 2     32 [Migori]          19\n 6    16 1      6 [Nairobi]         21\n 7    19 1     33 [Mombasa]         31\n 8    20 1      8 [Nyamira]         19\n 9    20 2      8 [Nyamira]         21\n10    21 1      6 [Nairobi]         34\n# … with 5,236 more rows\n\nThe advantages we’ve gained with a longer data format are starting to become clear! Suppose you wanted to know the average age of migrants arriving at each of Kenya’s administrative districts in this sample. You could find this information easily with just one summarise function:\n\n\nage %>% \n  group_by(DISTRICTKE) %>% \n  summarise(MEAN_AGE = mean(MOVEAGE, na.rm = T))\n\n\n# A tibble: 48 x 2\n      DISTRICTKE MEAN_AGE\n       <int+lbl>    <dbl>\n 1  1 [Bungoma]      18.5\n 2  2 [Kericho]      18.0\n 3  3 [Kiambu]       20.7\n 4  4 [Kilifi]       17.1\n 5  5 [Kitui]        19.3\n 6  6 [Nairobi]      20.0\n 7  7 [Nandi]        18.4\n 8  8 [Nyamira]      17.9\n 9  9 [Siaya]        16.1\n10 10 [Kakamega]     17.1\n# … with 38 more rows\n\nLet’s now pivot all of the migration history columns in our original dataset dat. This time, we’ll specify that all of the desired cols start with the same prefix PLACE (but we’ll drop the column PLACELIVENUM, since it contains the string “PLACE” we’re using in names_pattern):\n\n\ndat <- dat %>% \n  mutate(across(everything(), ~{\n    lbl_na_if(.x, ~.lbl %in% c(\n      \"No response or missing\",\n      \"NIU (not in universe)\"\n    ))\n  })) %>% \n  rowid_to_column(\"ID\") %>% \n  select(ID, starts_with(\"PLACE\"), -PLACELIVENUM) %>% \n  pivot_longer(\n    cols = starts_with(\"PLACE\"), \n    names_pattern = \"PLACE([0-9]*)(.*)\",\n    names_to = c(\"PLACE\", \".value\"),\n    values_drop_na = TRUE\n  )\n\ndat\n\n\n# A tibble: 5,251 x 24\n      ID PLACE COUNTRY  DISTRICTKE MOVEAGE       UR YCHILDEDU YCOHABIT\n   <int> <chr>   <int>   <int+lbl> <int+l> <int+lb> <int+lbl> <int+lb>\n 1     2 1         404  6 [Nairob…      16 30 [Rur…    0 [No]   0 [No]\n 2    11 1         404 10 [Kakame…      17 30 [Rur…    0 [No]   0 [No]\n 3    12 1         404 10 [Kakame…      16 10 [Cit…    0 [No]   0 [No]\n 4    14 1         404  7 [Nandi]       29 30 [Rur…    0 [No]   0 [No]\n 5    14 2         404 32 [Migori]      19 10 [Cit…    0 [No]   0 [No]\n 6    16 1         404  6 [Nairob…      21 30 [Rur…    0 [No]   0 [No]\n 7    19 1         404 33 [Mombas…      31 20 [Per…    0 [No]   0 [No]\n 8    20 1         404  8 [Nyamir…      19 10 [Cit…    0 [No]   0 [No]\n 9    20 2         404  8 [Nyamir…      21 20 [Per…    0 [No]   0 [No]\n10    21 1         404  6 [Nairob…      34 20 [Per…    0 [No]   0 [No]\n# … with 5,241 more rows, and 16 more variables: YCONFLICT <int+lbl>,\n#   YDIVORCE <int+lbl>, YFARM <int+lbl>, YHLTHACCESS <int+lbl>,\n#   YHLTHPROB <int+lbl>, YJOBSEARCH <int+lbl>, YOTHER <int+lbl>,\n#   YOTHERSOCIAL <int+lbl>, YPOSTMAR <int+lbl>,\n#   YSCHOOLATTEND <int+lbl>, …\n\nWe’re left with a very manageable 24 migration history variables. Among these, all of the variables starting with Y indicate a possible reason “why” a respondent migrated to a particular PLACE. The simplest way to work with these Y variables is to use tidy selection functions, like starts_with(\"Y\"). For example, suppose you wanted to know the percentage of all migrations in the sample that happened for all of the available reasons:\n\n\ndat %>% \n  summarise(across(starts_with(\"Y\"), ~100*mean(.x))) %>% \n  glimpse()\n\n\nRows: 1\nColumns: 18\n$ YCHILDEDU     <dbl> 1.980575\n$ YCOHABIT      <dbl> 2.418587\n$ YCONFLICT     <dbl> 8.931632\n$ YDIVORCE      <dbl> 0.8188916\n$ YFARM         <dbl> 4.05637\n$ YHLTHACCESS   <dbl> 1.866311\n$ YHLTHPROB     <dbl> 0.9331556\n$ YJOBSEARCH    <dbl> 16.85393\n$ YOTHER        <dbl> 9.502952\n$ YOTHERSOCIAL  <dbl> 12.53095\n$ YPOSTMAR      <dbl> 24.01447\n$ YSCHOOLATTEND <dbl> 22.73853\n$ YSCHOOLDONE   <dbl> 4.361074\n$ YSICKREL      <dbl> 1.371167\n$ YSPOUSEJOB    <dbl> 2.704247\n$ YWKCHANGE     <dbl> 1.885355\n$ YWKNONSEAS    <dbl> 3.77071\n$ YWKSEASON     <dbl> 6.189297\n\nNow that we’ve reshaped our migration recall data from a wide format to a long format, obtaining this summary data is a snap. And, as we’ll see in an upcoming migration Data Analysis post, using these data in longitudinal analysis can be just as easy.\n\n\n\n\n",
    "preview": "posts/2021-04-15-migration-discovery/images/tidyr_wide.png",
    "last_modified": "2021-08-18T14:28:20-05:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 540
  },
  {
    "path": "posts/2021-04-01-et-internal-migration/",
    "title": "Unmet need for family planning after internal migration",
    "description": "Summary and source code from a recent article using data from Ethiopia.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-04-02",
    "categories": [
      "Migration",
      "PMA Publications",
      "svyglm",
      "bootstraps"
    ],
    "contents": "\n\nContents\nMotivation\nData\nDependent variable\nKey independent variable\nCovariates\nSub-sample\nReference groups\n\nRegression Model\nInterpretation\nPredicted Probabilities\n\n\n\n\n\nWhen great new research gets published with PMA data on a topic we’re covering here on the Data Analysis Hub, we’ll cover the highlights and explore some source code in a PMA Publications post.\n\nHave a recent publication using PMA data that you’d like to feature in a PMA Publications post? Please let us know!\nAs part of our new series on women’s migration experiences and their impact on family planning, let’s dig into a paper from University of Minnesota researchers Emily Groene and Devon Kristiansen (2021) published in the journal Population, Space and Place.\nMotivation\nAs we’ll see throughout this series, migration can be associated with major changes in an individual’s fertility intentions and family planning access, and it can either increase or decrease the likelihood of experiencing unmet need for family planning under different circumstances. Groene & Kristiansen focus their attention on the particular circumstances around rural-to-urban internal (within-country) migration, which is one of the prevailing modes of migration throughout the countries surveyed by PMA (McAuliffe and Ruhs 2017).\nConsider all of the potential changes a person might experience when moving from a rural to an urban area: Groene & Kristiansen outline literature that suggests quite a few ways that these changes might impact fertility behavior. Some are likely to increase demand for family planning, for example:\nIncreased availability and access to health services and long-acting contraceptives (Skiles et al. 2015)\nNew opportunities for employment, education, and greater wealth that can delay or limit plans for additional births (Schultz 1994)\nDiminished incentives for larger family sizes tied to rural culture and livelihoods (Abebe 2007)\nAcculturation, or adoption of destination cultural roles and values (Kohler 2000)\nOn the other hand, several offsetting factors may push to maintain or even decrease demand for family planning:\nSpousal separation tied to seasonal migration for employment (Sevoyan and Agadjanian 2013)\nFamily planning preferences established prior to migration (Kulu 2005)\nSelection into destinations where familiar cultural roles and values are prevalent (Courgeau 1989)\nEven when we focus our analysis on rural-urban internal migrants, it’s very hard to predict how these and other factors might react to determine the family planning needs for any given person. From a policy perspective, where planning is needed to identify and address unmet need for family planning services on a larger scale, Groene & Kristiansen offer important insights into the ways that migration experiences are tied to a particular place. Using female respondents from the Ethiopia 2017 and 2018 samples, they compare unmet need among rural-urban internal migrants to the unmet need experienced by non-migrants in both rural and urban settings. They find that migrants are less likely to experience unmet need compared to non-migrants, controlling for a number of demographic factors.\nUnmet need is the difference between an individual’s reproductive intentions and contraceptive behavior.\nIn this post, we’ll show how to recreate their analysis using an IPUMS PMA data extract in R.\nData\nThe Ethiopia 2017 and 2018 samples were among the first PMA samples to include questions related to women’s most recent migration experience, and about the region where they were born. Their responses are included in variables listed in the migration variable group:\nLIVEINREGION - How long living continuously in current region\nLIVEINREGIONYRS - Number of years continuously living in current region\nLASTREGION - Region/country of residence before current region\nLASTUR - Urban/rural status of residence before current region\nBIRTHREGIONET - Region of woman’s birth, Ethiopia\nBIRTHUR - Urban/rural status of region of woman’s birth\nMIGMAINRSN - The main reason why moved to current place of residence\nMIGPREKID - Gave birth before moved to current region\nMIGPREKIDNUM - Number of sons/daughters before moving to current region\n\nA 2013 sample of women from Kinshasa, DRC were also given questions related to their recent migration history, but these data have not been made available for public use. See Anglewicz et al. (2017).\nGroene & Kristiansen use the variable LIVEINREGION to determine whether a woman has always lived in the same place and, if not, they use BIRTHUR together with URBAN to identify those who ultimately moved from a rural place to an urban place. We’ve created a data extract containing these and all of the other variables discussed below (female respondents only); we’ll start by loading it and the following packages in R:\n\n\nlibrary(ipumsr)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(survey)\nlibrary(srvyr)\n\ndat <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00019.xml\",\n  data = \"data/pma_00019.dat.gz\"\n)\n\n\n\nIf you’re a registered user at pma.ipums.org, you can recreate the authors’ data extract by selecting the variables mentioned in this post.\nSee our guide for help importing IPUMS data extracts into R.\nWe’ll first label the various non-response values in this dataset with the value NA using ipumsr::lbl_na_if applied to all variables with dplyr::across:\n\n\ndat <- dat %>% \n  mutate(\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\",\n        \"NIU (not in universe) or missing\"\n      )\n    ))\n  )\n\n\n\nEthiopia samples are stratified by region (GEOET) and urban status (URBAN), resulting in 21 sampling strata from which primary sampling units (EAID) are selected. The authors exclude women from any STRATA where fewer than 200 women were sampled across both sample years:\n\nPMA uses a multistage, stratified cluster sample design. For more information, see PMA’s sample design memo.\n\n\ndat %>% count(STRATA) \n\n\n# A tibble: 21 × 2\n                                  STRATA     n\n                               <int+lbl> <int>\n 1 23101 [Addis Ababa - urban, Ethiopia]  1833\n 2 23102 [Amhara - urban, Ethiopia]       1005\n 3 23103 [Amhara - rural, Ethiopia]       1651\n 4 23104 [Oromiya - urban, Ethiopia]      1216\n 5 23105 [Oromiya - rural, Ethiopia]      2275\n 6 23106 [Other, Ethiopia]                 839\n 7 23107 [SNNP - urban, Ethiopia]         1904\n 8 23108 [SNNP - rural, Ethiopia]         1241\n 9 23109 [Tigray - urban, Ethiopia]       1455\n10 23110 [Tigray - rural, Ethiopia]        810\n11 23111 [Dire Dawa - urban, Ethiopia]      22\n12 23112 [Dire Dawa - rural, Ethiopia]      29\n13 23113 [Afar - urban, Ethiopia]           32\n14 23114 [Afar - rural, Ethiopia]          218\n15 23115 [Somali - urban, Ethiopia]         73\n16 23116 [Somali - rural, Ethiopia]        123\n17 23117 [Gambella - urban, Ethiopia]       34\n18 23118 [Gambella - rural, Ethiopia]       23\n19 23119 [Harari - urban, Ethiopia]         29\n20 23120 [Harari - rural, Ethiopia]         26\n21 23121 [BG - rural, Ethiopia]            172\n\nNote that this will drop women from STRATA numbered 23111-23113 and 23115-23121:\n\n\ndat <- dat %>% \n  group_by(STRATA) %>% \n  mutate(STRATA_N = n()) %>% \n  ungroup() %>% \n  filter(STRATA_N > 200)\n\n\n\nDependent variable\nNow, consider the dependent variable UNMETYN, which is a constructed variable indicating whether each respondent currently has an unmet need for family planning. All respondents to the female questionnaire are included in the universe for UNMETYN, so women who are not able to become pregnant or are not sexually active are determined to have “no unmet need.”\nUNMETYN is a recoded binary indicator from UNMETNEED, which contains additional details on types of unmet need.\nWithin the combined sample of female respondents from both years, about 12% of women demonstrated unmet need for family planning:\n\n\ndat %>% summarize(mean_UNMETYN = mean(UNMETYN, na.rm = T)) \n\n\n# A tibble: 1 × 1\n  mean_UNMETYN\n         <dbl>\n1        0.120\n\nWe’ll use the survey package - and its tidy companion srvyr - to specify PMA sample design in our population estimates. The function srvyr::survey_mean uses information about the survey design (given by srvyr::as_survey_design) to estimate that an average woman aged 15-49 in Ethiopia has about a 15% chance of experiencing unmet need for family planning, with a 95% confidence interval ranging between 13.5% and 16.7%:\n\n\n\n\n© Greg Freedman Ellis et al. (GPL 2 | GPL 3)\n\n\ndat %>% \n  as_survey_design(\n    id = EAID,\n    nest = T,\n    weight = FQWEIGHT,\n    strata = STRATA\n  ) %>% \n  summarize(pop_UNMETYN = survey_mean(UNMETYN, vartype = \"ci\", na.rm = T))\n\n\n# A tibble: 1 × 3\n  pop_UNMETYN pop_UNMETYN_low pop_UNMETYN_upp\n        <dbl>           <dbl>           <dbl>\n1       0.151           0.135           0.167\n\nsrvyr brings parts of dplyr syntax to survey analysis, using the survey package.\nKey independent variable\nIn order to conduct a three-way comparison between rural-urban migrants, rural non-migrants, and urban non-migrants, the authors construct a variable we’ll call MIGRANT_DIR.\nThe first component of MIGRANT_DIR evaluates whether each woman ever migrated from her place of birth. Using LIVEINREGION, any woman reporting that she has “always” lived in her current region is not a migrant, and any woman who has lived in her current region for a number of “months or years” is a migrant (note: we cannot determine the migration history for all of the remaining cases, so they will be excluded from further analysis).\n\n\ndat %>% count(LIVEINREGION)\n\n\n# A tibble: 4 × 2\n             LIVEINREGION     n\n                <int+lbl> <int>\n1 10 [Always]             11777\n2 20 [Currently visiting]   124\n3 30 [Months or years]     2539\n4 NA                          7\n\nMigrants who were born in a rural place !BIRTHUR and now live in an urban place URBAN meet the definition for rural-urban migrant. Non-migrants are classified by their current URBAN status only. All other women are implicitly given the value NA and then filtered out of the dataset.\n\nWe implicitly assign NA to any cases that aren’t specified by the logical statements inside case_when().\n\n\ndat <- dat %>% \n   mutate(\n     across(c(BIRTHUR, URBAN), ~.x %>% zap_labels),\n     MIGRANT = case_when(\n       LIVEINREGION == 30 ~ T, \n       LIVEINREGION == 10 ~ F\n     ),\n     MIGRANT_DIR = case_when(\n       MIGRANT & !BIRTHUR & URBAN ~ \"rural to urban\",\n       !MIGRANT & !URBAN ~ \"nonmigrant - rural\",\n       !MIGRANT & URBAN ~ \"nonmigrant - urban\"\n     ) \n   ) %>% \n  filter(!is.na(MIGRANT_DIR))\n\n\n\n\nRemember: BIRTHUR and URBAN are labeled integers. Our use of zap_labels allows R to ignore their assigned labels and, instead, treat them as logicals where 1 == TRUE and 0 == FALSE.\n\n\ndat %>% count(MIGRANT_DIR)\n\n\n# A tibble: 3 × 2\n  MIGRANT_DIR            n\n  <chr>              <int>\n1 nonmigrant - rural  6437\n2 nonmigrant - urban  5340\n3 rural to urban      1421\n\nYou may notice that we’ve created MIGRANT_DIR as a string, or a character object. We’ll coerce it as a factor later so we can easily use each of the three classifications in a logistic regression model.\nCovariates\nThe authors control for a number of covariates in addition to MIGRANT_DIR. The following covariates are recoded versions of existing PMA variables:\nBIRTHS: number of children ever born CHEB (2017), or the woman’s total number of birth events BIRTHEVENT (2018, e.g. birth of twins is a single event)\nPARTNERED: recoded MARSTAT, indicating if the woman is either currently married or living with a partner\nRELGEN: recoded RELIGION as “muslim,” “christian,” or “other”\n\n\ndat <- dat %>% \n  mutate(\n    across(c(BIRTHEVENT, CHEB), ~.x %>% zap_labels),\n    BIRTHS = case_when(\n      YEAR == 2018 ~ BIRTHEVENT, \n      T ~ CHEB\n    ),\n    PARTNERED = case_when(\n      MARSTAT %in% 21:22 ~ T, \n      !is.na(MARSTAT) ~ F\n    ),\n    RELGEN = case_when(\n      RELIGION == 100 ~ \"muslim\",\n      RELIGION >= 200 & RELIGION < 300 ~ \"christian\",\n      T ~ \"other\"\n    )\n  )\n\n\n\nThe remaining covariates are used without further modification:\nAGE: the woman’s age (years)\nWEALTHQ: wealth quintile\nEDUCATTGEN: education level (general)\nHCVISITY: whether the woman visited a health facility in the last 12 months\nSUBNATIONAL: subnational region\nYEAR: survey year (2017 or 2018)\nSub-sample\nAs discussed above, the authors exclude any female respondents from small STRATA (n < 200) and those who are neither rural-urban migrants nor non-migrants. Additionally, they remove rural-urban migrants who moved to Ethiopia from another country. Women can indicate this information in two places: they may either list a foreign country in LASTREGION or indicate “abroad” as their region of birth in BIRTHREGIONET.\n\n\ndat %>% count(LASTREGION)\n\n\n# A tibble: 18 × 2\n                   LASTREGION     n\n                    <int+lbl> <int>\n 1 101 [Tigray]                  50\n 2 102 [Afar]                     7\n 3 103 [Amhara]                 399\n 4 104 [Oromia]                 360\n 5 105 [Ethiopia Somali]          1\n 6 106 [Benishangul Gumuz]        4\n 7 107 [SNNPR]                  390\n 8 108 [Gambella]                 4\n 9 109 [Harari]                   7\n10 110 [Addis Ababa]            116\n11 111 [Dire Dawa]                7\n12 202 [Saudi Arabia]            33\n13 204 [Beirut]                   5\n14 205 [United Arab Emirates]     9\n15 206 [Sudan]                    5\n16 210 [Lebanon]                  1\n17 300 [Other]                   23\n18  NA                        11777\n\ndat %>% count(BIRTHREGIONET)\n\n\n# A tibble: 12 × 2\n                                               BIRTHREGIONET     n\n                                                   <int+lbl> <int>\n 1  1 [Tigray Region]                                         2110\n 2  2 [Afar Region]                                            430\n 3  3 [Amhara Region]                                         2911\n 4  4 [Oromia Region]                                         3472\n 5  5 [Somali Region]                                          166\n 6  6 [Benishangul-Gumuz Region]                               148\n 7  7 [Southern Nations, Nationalities, and Peoples' Region]  3057\n 8  8 [Gambella Region]                                         26\n 9  9 [Harari Region]                                           35\n10 10 [Addis Ababa (city)]                                     814\n11 11 [Dire Dawa (city)]                                        27\n12 12 [Abroad]                                                   2\n\ndat <- dat %>% \n  mutate(\n    EXTERNAL = case_when(\n      LASTREGION %in% 200:900 | BIRTHREGIONET == 12 ~ T,\n      T ~ F\n    )\n  ) %>% \n  filter(!EXTERNAL)\n\n\n\nThe authors also exclude women whose PARTNERED status (i.e. sexual activity) cannot be determined, and women who indicate that they are either “infertile” in FERTPREF or “menopausal / hysterectomy” in TIMEMENSTRUATE. Women who are not at risk of pregnancy for these reasons cannot have unmet need, so they are removed from the sample.\n\n\ndat %>% count(PARTNERED)\n\n\n# A tibble: 3 × 2\n  PARTNERED     n\n  <lgl>     <int>\n1 FALSE      5513\n2 TRUE       7606\n3 NA            2\n\ndat %>% count(FERTPREF)\n\n\n# A tibble: 4 × 2\n                 FERTPREF     n\n                <int+lbl> <int>\n1  1 [Have another child]  8983\n2  2 [No more children]    2702\n3  3 [Infertile]            225\n4 NA                       1211\n\ndat %>% count(TIMEMENSTRUATE)\n\n\n# A tibble: 8 × 2\n                TIMEMENSTRUATE     n\n                     <int+lbl> <int>\n1  1 [Days]                     3113\n2  2 [Weeks]                    4111\n3  3 [Months]                   2955\n4  4 [Years]                    1276\n5  5 [Menopausal/hysterectomy]    85\n6  6 [Before last birth]        1231\n7  7 [Never menstruated]         317\n8 NA                              33\n\ndat <- dat %>% \n  mutate(\n    INFERTILE = case_when(\n      FERTPREF == 3 | TIMEMENSTRUATE == 5 ~ T,\n      T ~ F\n    )\n  ) %>% \n  filter(!is.na(PARTNERED), !INFERTILE)\n\n\n\nLastly, they exclude women with missing values on any of the remaining covariates.\n\n\ndat <- dat %>%\n  filter(\n    !if_any(c(UNMETYN, EDUCATTGEN, HCVISITY, BIRTHS, MCP), is.na),\n  )\n\n\n\nFrom 15,010 female respondents included in the original extract, this sub-sampling procedure leaves us with 12,630 remaing cases.\n\n\ndat %>% summarize(n = n())\n\n\n# A tibble: 1 × 1\n      n\n  <int>\n1 12630\n\nReference groups\nAs a final processing step, we’ll coerce each of our categorical variables (including MIGRANT_DIR) as factors. All but one of these is a labelled integer object where we’ll use the response with the lowest value as a reference group; because we created MIGRANT_DIR as a character object, we’ll specify its reference group manually:\n\n\ndat <- dat %>% \n  mutate(\n    across(\n      c(\n        MIGRANT_DIR, \n        RELGEN, \n        WEALTHQ, \n        EDUCATTGEN, \n        HCVISITY, \n        SUBNATIONAL, \n        PARTNERED,\n        YEAR\n      ), \n      ~as_factor(.) %>% droplevels()\n    ),\n    MIGRANT_DIR = fct_relevel(MIGRANT_DIR, \"nonmigrant - urban\")\n  )\n\n\n\nRegression Model\nFinally, we’re ready to build a regression model for UNMETYN using MIGRANT_DIR and the covariates discussed above!\nRecall that the function srvyr::survey_mean estimated that 15.1% of all women aged 15-49 in Ethiopia experience unmet need for family planning. This estimate used all of the women in our original sample prior to the sub-sampling procedure we just discussed. Now that we’ve created a sub-sample from the original dataset, let’s see how the population estimate has changed:\n\n\ndat %>% \n  as_survey_design(\n    id = EAID,\n    nest = T,\n    weight = FQWEIGHT,\n    strata = STRATA\n  ) %>% \n  summarize(pop_UNMETYN = survey_mean(UNMETYN, vartype = \"ci\", na.rm = T))\n\n\n# A tibble: 1 × 3\n  pop_UNMETYN pop_UNMETYN_low pop_UNMETYN_upp\n        <dbl>           <dbl>           <dbl>\n1       0.157           0.140           0.174\n\nNow that we’ve removed some cases (notably, all women who are infertile), the estimated population mean is close, but somewhat higher at 15.7%.\nGroene & Kristiansen build a multilevel logistic regression model for UNMETYN that breaks down this full-population estimate for each of the sub-groups represented by our independent variables. We’ll report the exponentiated coefficient estimates for each variable, which means that we’ll need to interpret each estimate as a change in the odds that a woman will experience UNMETYN relative to a woman in a reference group.\nWe’ll build the authors’ model m1 using the function survey::svyglm, which - like survey_mean - uses information about the sample design provided by as_survey_design to generate cluster-robust standard error estimates:\n\n\nm1 <- dat %>% \n  as_survey_design(\n    id = EAID,\n    nest = T,\n    weight = FQWEIGHT,\n    strata = STRATA\n  ) %>% \n  svyglm(\n    UNMETYN ~  \n      AGE + \n      MIGRANT_DIR +\n      RELGEN + \n      WEALTHQ +\n      EDUCATTGEN + \n      BIRTHS + \n      HCVISITY + \n      SUBNATIONAL +\n      PARTNERED + \n      YEAR,\n    design = .,\n    family = \"quasibinomial\",\n  ) \n\n\n\nWe tell svyglm to fit a logistic regression model with family = \"quasibinomial\".\nWhy “quasi” binomial? A simple binomial distribution yields the same point estimates and standard errors, but generates a warning because our use of sample weights produces a non-integer count of women with unmet need.\nTo simplify the output a bit, we’ll show a tidy table with just the term, point estimate, the 95% confidence interval, and the p-value (each rounded to two decimal places):\n\n\nm1 %>% \n  tidy(exp = T, conf.int = T) %>% \n  select(term, estimate, conf.low, conf.high, p.value) %>% \n  mutate(across(where(is.numeric), ~round(.x, 2))) \n\n\n# A tibble: 27 × 5\n   term                            estimate conf.low conf.high p.value\n   <chr>                              <dbl>    <dbl>     <dbl>   <dbl>\n 1 (Intercept)                         0.13     0.06      0.27    0   \n 2 AGE                                 0.96     0.94      0.97    0   \n 3 MIGRANT_DIRnonmigrant - rural       1.15     0.84      1.55    0.38\n 4 MIGRANT_DIRrural to urban           0.75     0.59      0.96    0.02\n 5 RELGENother                         0.88     0.58      1.34    0.54\n 6 RELGENchristian                     0.59     0.48      0.72    0   \n 7 WEALTHQLower quintile               0.99     0.79      1.25    0.96\n 8 WEALTHQMiddle quintile              1.06     0.84      1.33    0.63\n 9 WEALTHQHigher quintile              0.82     0.64      1.05    0.11\n10 WEALTHQHighest quintile             0.84     0.56      1.27    0.42\n11 EDUCATTGENPrimary/Middle school     1.05     0.87      1.26    0.59\n12 EDUCATTGENSecondary/post-prima…     0.76     0.55      1.05    0.1 \n13 EDUCATTGENTertiary/post-second…     1        0.68      1.49    0.98\n14 BIRTHS                              1.24     1.18      1.31    0   \n15 HCVISITYYes                         0.85     0.71      1.01    0.06\n16 SUBNATIONALAfar, Ethiopia           0.23     0.12      0.44    0   \n17 SUBNATIONALAmhara, Ethiopia         0.55     0.44      0.69    0   \n18 SUBNATIONALOromiya, Ethiopia        0.91     0.73      1.14    0.41\n19 SUBNATIONALSomali, Ethiopia         0.74     0.33      1.7     0.48\n20 SUBNATIONALBenishangul-Gumuz, …     0.55     0.28      1.08    0.08\n21 SUBNATIONALSNNP, Ethiopia           0.9      0.68      1.2     0.48\n22 SUBNATIONALGambella, Ethiopia       0.86     0.15      4.9     0.86\n23 SUBNATIONALHarari, Ethiopia         2.06     1.69      2.52    0   \n24 SUBNATIONALAddis Ababa, Ethiop…     0.98     0.69      1.4     0.93\n25 SUBNATIONALDire Dawa, Ethiopia      0.77     0.47      1.27    0.3 \n26 PARTNEREDTRUE                       6.69     4.75      9.43    0   \n27 YEAR2018                            0.88     0.74      1.05    0.15\n\nInterpretation\nControlling for all of the covariates we’ve discussed, the authors find that rural-urban internal migrants are less likely than both urban and rural non-migrants to experience unmet need for family planning!\nHow do we identify this finding in the model output? Notice that the estimated odds of experiencing UNMETYN for rural to urban migrants is 0.75, and that the associated 95% confidence interval ranges from 0.59 to 0.95: this represents the migrants’ odds compared to urban non-migrants. If the 95% confidence interval included the value 1.0, we would say that there’s more than a 5% chance that the migrants’ odds could be equal to the odds experienced by urban non-migrants. Because it does not include 1.0, we instead say that there’s a statistically significant difference between the two groups (at the 5% confidence threshold).\nBecause the authors selected urban non-migrants as a reference group, our model output shows the relationship between rural non-migrants and rural-urban migrants a bit less clearly. Although the point estimate for rural to urban migrants (0.75) is lower than the point estimate for nonmigrant - rural (1.15), their respective confidence intervals overlap. In order to see that they actually are statistically different, we’d need to run the model again with a different reference group.\nPredicted Probabilities\n\n\n\nWhile the output from our logistic regression model helps show the relative difference between groups, we’re not yet able to predict the absolute risk of UNMETYN for each group. Recall that, before building our model, we calculated that the average unmet need for all women in Ethiopia (excluding external migrants, infertile women, etc) was about 15.7%. We’ll now estimate the average unmet need experienced by all women in Ethiopia sorted into the three groups represented by MIGRANT_DIR.\nThe predict function allows us to make a prediction about each woman’s likelihood of experiencing unmet need according to the model m1. When we tell predict to return type = \"response\", it gives us the predicted probability that each woman should have UNMETYN.\n\n\ntibble(predicted = predict(m1, type = \"response\"))\n\n\n# A tibble: 12,630 × 1\n   predicted  \n   <svystat>  \n 1 0.072659141\n 2 0.338704987\n 3 0.033291206\n 4 0.376765517\n 5 0.025819303\n 6 0.528723102\n 7 0.009790659\n 8 0.088323159\n 9 0.044348246\n10 0.027093896\n# … with 12,620 more rows\n\nIf we wanted to compare each individual’s predicted probability to the value they actually do have for UNMETYN, we could attach our prediction back to our dataset. Remember that the original UNMETYN variable is binary, whereas the predictions are continuous probabilities that range from 0 to 1. Here, we hope to see that women whose predicted probability exceeds 0.50 have UNMETYN == 1, while those whose predicted probability is less than 0.50 have UNMETYN == 0:\n\n\ntibble(predicted = predict(m1, type = \"response\")) %>% \n  bind_cols(dat) %>% \n  select(predicted, UNMETYN) \n\n\n# A tibble: 12,630 × 2\n   predicted             UNMETYN\n   <svystat>           <int+lbl>\n 1 0.072659141 0 [No unmet need]\n 2 0.338704987 0 [No unmet need]\n 3 0.033291206 0 [No unmet need]\n 4 0.376765517 0 [No unmet need]\n 5 0.025819303 0 [No unmet need]\n 6 0.528723102 1 [Unmet need]   \n 7 0.009790659 0 [No unmet need]\n 8 0.088323159 0 [No unmet need]\n 9 0.044348246 0 [No unmet need]\n10 0.027093896 0 [No unmet need]\n# … with 12,620 more rows\n\nWe can also use predict to calculate predicted probabilities for hypothetical samples. For instance, the authors provide the predicted probabilities for a hypothetical sample of women that is completely identical to the real sample, except that they all share the same value for MIGRANT_DIR (all other variables are kept at their originial values). The mean predicted probability derived from this kind of hypothetical sample is known as a predictive margin.\nWhile the point estimates for each group in MIGRANT_DIR are easy to calculate with predict, the confidence intervals for those estimates are a bit harder to obtain. Here, we’ll use rsample::bootstraps to generate 100 replicates of our sample. This will allow us to rebuild our model 100 times:\n\n\nset.seed(1) # This ensures reproducible bootstrap sampling\n\nboots_dat <- dat %>% \n  rsample::bootstraps(100, EAID) %>% \n  transmute(\n    id = parse_number(id),\n    splits = map(splits, as_tibble),\n    model = map(splits, ~{\n      .x %>% \n        as_survey_design(\n          id = EAID,\n          nest = T,\n          weight = FQWEIGHT,\n          strata = STRATA\n        ) %>% \n        svyglm(\n          UNMETYN ~  \n            AGE + \n            MIGRANT_DIR +\n            RELGEN + \n            WEALTHQ +\n            EDUCATTGEN + \n            BIRTHS + \n            HCVISITY + \n            SUBNATIONAL +\n            PARTNERED + \n            YEAR,\n          design = .,\n          family = \"quasibinomial\",\n        ) \n    })\n  )\n\nboots_dat\n\n\n# A tibble: 100 × 3\n      id splits                 model   \n   <dbl> <list>                 <list>  \n 1     1 <tibble [12,630 × 38]> <svyglm>\n 2     2 <tibble [12,630 × 38]> <svyglm>\n 3     3 <tibble [12,630 × 38]> <svyglm>\n 4     4 <tibble [12,630 × 38]> <svyglm>\n 5     5 <tibble [12,630 × 38]> <svyglm>\n 6     6 <tibble [12,630 × 38]> <svyglm>\n 7     7 <tibble [12,630 × 38]> <svyglm>\n 8     8 <tibble [12,630 × 38]> <svyglm>\n 9     9 <tibble [12,630 × 38]> <svyglm>\n10    10 <tibble [12,630 × 38]> <svyglm>\n# … with 90 more rows\n\n\n\n\n\n© 2018 RStudio (CC0 1.0)\nrsample is included with library(tidymodels).\nNotice that each row of boots_dat contains a completely resampled version of dat contained in each row of the column splits. The column model contains the output from a model that’s uniquely fitted to the resampled data in splits.\nNext, we’ll use predict separately for each row in boots_dat. Because we generate three new rows each time - one prediction for each group in MIGRANT_DIR - the resulting data frame has 300 rows.\n\n\nboots_dat <- boots_dat %>% \n  rowwise() %>%\n  mutate(nested_predictions = list(map_df(levels(dat$MIGRANT_DIR), ~{\n    predict(\n      model, \n      type = \"response\", \n      newdata = splits %>% mutate(MIGRANT_DIR = .x)) %>% \n      tibble() %>% \n      bind_cols(splits) %>% \n      as_survey_design(\n        id = EAID,\n        nest = T,\n        weight = FQWEIGHT,\n        strata = STRATA\n      ) %>% \n      summarise(predicted = survey_mean(., vartype = \"ci\")) %>% \n      mutate(MIGRANT_DIR = .x) %>% \n      select(MIGRANT_DIR, predicted) \n  }))) %>% \n  unnest(nested_predictions) \n\nboots_dat\n\n\n# A tibble: 300 × 5\n      id splits                 model    MIGRANT_DIR        predicted\n   <dbl> <list>                 <list>   <chr>                  <dbl>\n 1     1 <tibble [12,630 × 38]> <svyglm> nonmigrant - urban    0.151 \n 2     1 <tibble [12,630 × 38]> <svyglm> nonmigrant - rural    0.163 \n 3     1 <tibble [12,630 × 38]> <svyglm> rural to urban        0.112 \n 4     2 <tibble [12,630 × 38]> <svyglm> nonmigrant - urban    0.149 \n 5     2 <tibble [12,630 × 38]> <svyglm> nonmigrant - rural    0.155 \n 6     2 <tibble [12,630 × 38]> <svyglm> rural to urban        0.0840\n 7     3 <tibble [12,630 × 38]> <svyglm> nonmigrant - urban    0.142 \n 8     3 <tibble [12,630 × 38]> <svyglm> nonmigrant - rural    0.155 \n 9     3 <tibble [12,630 × 38]> <svyglm> rural to urban        0.133 \n10     4 <tibble [12,630 × 38]> <svyglm> nonmigrant - urban    0.133 \n# … with 290 more rows\n\nFinally, we’ll calculate:\nthe predicted probability for each group from the mean of 100 bootstrap predictions\nthe standard error of each group’s predicted probability from the standard deviation of 100 bootstrap predictions\nthe 95% confidence interval from the product of each group’s standard error and qnrom(0.975)\n\n\ngroup_predictions <- boots_dat %>% \n  group_by(MIGRANT_DIR) %>% \n  summarise(\n    mean = mean(predicted),\n    se = sd(predicted),\n    lower = mean - se*qnorm(0.975),\n    upper = mean + se*qnorm(0.975)\n  )\n\ngroup_predictions\n\n\n# A tibble: 3 × 5\n  MIGRANT_DIR         mean      se  lower upper\n  <chr>              <dbl>   <dbl>  <dbl> <dbl>\n1 nonmigrant - rural 0.161 0.00568 0.149  0.172\n2 nonmigrant - urban 0.143 0.0125  0.119  0.168\n3 rural to urban     0.117 0.0171  0.0831 0.150\n\nAnd here are those intervals plotted with geom_errorbarh and geom_point:\n\n\nggplot(group_predictions) +\n  geom_errorbarh(\n    color = \"#A2269C\", \n    aes(height = .2, xmin = lower, xmax = upper, y = MIGRANT_DIR)\n  ) + \n  geom_point(\n    color = \"#A2269C\", \n    aes(x = mean, y = MIGRANT_DIR)\n  ) +\n  geom_text(\n    nudge_y = 0.2,\n    aes(label = round(mean, 3), x = mean, y = MIGRANT_DIR)\n  ) + \n  scale_x_continuous(breaks = seq(.08, .18, by = .02)) +\n  theme_minimal() + \n  labs(\n    subtitle = \"95% Confidence Interval\",\n    title = \"Predicted Probability of Unmet Need for Family Planning\",\n    y = \"\",\n    x = \"\"\n  ) + \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 10)\n  ) \n\n\n\n\nAs you can see, the confidence intervals for each group in MIGRANT_DIR overlap quite a bit. However, the probability that a rural-urban internal migrant will experience unmet need for family planning seems to be generally lower than the other groups: we show a point-estimate of just 11.7% for migrants compared to 14.3% and 16.1% respectively for urban and rural non-migrants.\nTo learn more about the conceptual reasons why rural-urban internal migrants in Ethiopia might experience less unmet need for family planning compared to non-migrants, be sure to checkout out Groene & Kristiansen’s full article published at Populations, Space and Place! And, for more information about migration data available in other PMA samples, stay tuned for upcoming posts in this series.\n\n\n\nAbebe, Tatek. 2007. “Changing Livelihoods, Changing Childhoods: Patterns of Children’s Work in Rural Southern Ethiopia.” Children’s Geographies 5 (1-2): 77–93. https://doi.org/10.1080/14733280601108205.\n\n\nAnglewicz, Philip, Jamaica Corker, and Patrick Kayembe. 2017. “The Fertility of Internal Migrants to Kinshasa.” Genus 73 (1): 4. http://dx.doi.org/10.1186/s41118-017-0020-8.\n\n\nCourgeau, D. 1989. “Family Formation and Urbanization.” Population. English Selection 44 (1): 123–46. https://www.ncbi.nlm.nih.gov/pubmed/12157901.\n\n\nGroene, Emily A, and Devon Kristiansen. 2021. “Unmet Need for Family Planning After Internal Migration: Analysis of Ethiopia 2017–2018 PMA Survey Data.” Population, Space and Place 27 (1). https://onlinelibrary.wiley.com/doi/10.1002/psp.2376.\n\n\nKohler, Hans-Peter. 2000. “Social Interactions and Fluctuations in Birth Rates.” Population Studies 54 (2): 223–37. https://doi.org/10.1080/713779084.\n\n\nKulu, Hill. 2005. “Migration and Fertility: Competing Hypotheses Re-Examined.” European Journal of Population/Revue Européenne de Démographie 21 (1): 51–87. https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/content/pdf/10.1007/s10680-005-3581-8.pdf&casa_token=ofnVP7_3oy4AAAAA:nOmsfJTITgKjtiNcXj6u9GsVD9yHCkWsqmAtwTs6aG3vrQaL9DlhaOwcxFQMYweYNSt1mAtGNNonsZ-9.\n\n\nMcAuliffe, Marie, and Martin Ruhs. 2017. “World Migration Report 2018.” International Office of Migration, Geneva. https://publications.iom.int/fr/system/files/pdf/wmr_2018_en_chapter7.pdf.\n\n\nSchultz, T Paul. 1994. “Human Capital, Family Planning, and Their Effects on Population Growth.” The American Economic Review 84 (2): 255–60. http://www.jstor.org/stable/2117839.\n\n\nSevoyan, Arusyak, and Victor Agadjanian. 2013. “Contraception and Abortion in a Low-Fertility Setting: The Role of Seasonal Migration.” International Perspectives on Sexual and Reproductive Health 39 (3): 124–32. http://dx.doi.org/10.1363/3912413.\n\n\nSkiles, Martha Priedeman, Marc Cunningham, Andrew Inglis, Becky Wilkes, Ben Hatch, Ariella Bock, and Janine Barden-O’Fallon. 2015. “The Effect of Access to Contraceptive Services on Injectable Use and Demand for Family Planning in Malawi.” International Perspectives on Sexual and Reproductive Health 41 (1): 20–30. http://dx.doi.org/10.1363/4102015.\n\n\n\n\n",
    "preview": "posts/2021-04-01-et-internal-migration/images/pred_prob_h4.png",
    "last_modified": "2021-10-18T16:11:16-05:00",
    "input_file": {},
    "preview_width": 2683,
    "preview_height": 1200
  },
  {
    "path": "posts/2021-02-09-march-2021-data-release/",
    "title": "New SDP Data Available Spring 2021",
    "description": "Get details on new variables related to labor & delivery services, antenatal care, vaccinations, facility shipment schedules, and more!",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [
      "New Data",
      "Data Discovery"
    ],
    "contents": "\n\nContents\nLabor and Delivery\nAntenatal Care\nVaccinations\nFacility Stockouts & Expected Shipments\nNational Health Programs\n\n\n\n\nWe’re excited to announce the release of several new Service Delivery Point samples this month over at pma.ipums.org! As always, you’ll find the new data harmonized with older samples wherever the new surveys repeat questions you’ve seen before. The new samples also contain a big batch of new variables derived from questions that were posed for the very first time in PMA surveys, so we’d like to introduce a few of the highlights here.\n\n\n\nThe new samples included in this release represent data collected from:\nBurkina Faso 2020\nCongo (DR) 2019\nEthiopia 2019\nKenya 2019\nNigeria 2019\nUganda 2019\nLabor and Delivery\nWe’ve added a variable group in the Other Health Services topic offering more than 60 new variables related to Labor and Delivery. Many of these are currently available only for the Ethiopia 2019 sample, which piloted the new questions.\nFor example, you’ll find new variables about delivery personnel, including those showing whether a facility has a skilled birth attendant or a provider able to perform C-section delivery available 24 hours per day. Other variables describe the infrastructure available for labor and delivery, including the number of delivery rooms and beds, labor rooms, maternity waiting rooms, and newborn resuscitation tables. You’ll also find a number of variables describing the environment inside of a facility’s delivery rooms, including whether they are private, heated, and whether they have several specific delivery guidelines and protocols posted in the room.\nSeveral labor and delivery statistics are also provided for the month preceding the interview. These include the total number of facility deliveries, cesarian deliveries, stillbirths (both fresh and macerated), and neonatal deaths (reported separately for those occurring within 24 hours and one week). Other variables report whether certain services were provided any time within 3 months preceding the interview, including:\ninstrument / assisted delivery\ncaesarean section\nparenteral antibiotics for infections\nparenteral anticonvulsants for high blood pressure (Diazepam, Magnesium Sulfate, or other) or hypertension (Hydralazine, Nifedipine, Methyldopa, or other)\nparenteal / oral uterotonics for hemorrhage (Ergometrine, Misoprostol tablets, Oxytocin, or other)\ncortisteroids for fetal lung maturation\nmanual placenta removal\npartographs to monitor labor\nnewborn resuscitation\nblood transfusion\npostpartum implant insertion\npostpartum IUD insertion\npostpartum tubal ligation\nLastly, a number of variables indicate whether a particular service is typically provided at a facility. These include,\nneonatal intensive care, and whether it was available on the day of the interview\nreferrals for outgoing newborns and pregnant, laboring, and postpartum women; policies on referrals made from other facilities\nbreastfeeding assistance, newborn skin contact, and family planning discussions with new mothers prior to discharge\nAntenatal Care\nAs with Labor and Delivery, a new Antenatal Care group contains a number of variables that are currently available only for the Ethiopia 2019 sample, such as:\nwhether antenatal care was available on the day of the interview\nwhether trained staff are available to use ultrasound, and whether they were available on the day of the interview\nthe total number of rooms for antenatal care, and whether they are private\nwhether a number of different procedures are typically provided as a routine part of antenatal care (for example: blood pressure, weight, HIV testing, and several blood / urine tests)\nBeyond Ethiopia 2019, several of the other new samples included questions related to topics that are normally discussed with patients during an antenatal visit:\nimmediate and exclusive breastfeeding\nreturn to fertility after pregnancy\nhealthy timing and spacing of pregnancies\nfamily planning methods available to use while breastfeeding\nuse of the lactational amenorrhea method (LAM) for family planning, and plans for a transition to other methods\ninterest in a postpartum IUD or other long-acting family planning methods\nIn earlier survey rounds, PMA questionnaires included questions on whether these topics were discussed with a new mother after birth. The new samples differentiate between whether these topics were covered before the mother left the facility after delivery (e.g DISPPSPACE) or at a postnatal care visit later on (e.g. DISPNCSPACE).\nVaccinations\nThe Ethiopia 2019 sample also includes some of the first PMA variables related to vaccination. You’ll find indicators for whether a facility typically provides immunization services, whether those services were provided on the day of the interview, and whether a woman visiting the facility for her child’s immunization would be offered family planning services or counseling during the visit.\nThe availability of the following vaccines are also provided:\nBCG\nIPV and oral polio\nMeasles\nPCV\nPentavalent\nRota\nTetanus toxoid\nVitamin A\n\nEach of these may be “observed” by the interviewer, or else “reported” without observation.\nFacility Stockouts & Expected Shipments\nPMA has included variables related to contraceptive stockouts for many samples dating back to 2014. Four of the new samples dig deeper into the reasons why facilities experience stockouts, and also report the expected delivery time for methods that were out of stock on the day of the interview.\nFor each of the following methods, the expected delivery time is reported by two variables: a numeric value and a unit (e.g. days, weeks, months) that defines the value.\ndiaphragms\nDepo Provera\nemergency contraception\nfemale condoms\nfoam / jelly\nimplants\nIUDs\nmale condoms\npills\nSayana Press\nStandard Days/Cycle Beads\n\n\n\nIt’s recommended that users construct their own derived variables for expected delivery times using whichever unit of time they prefer. For example, notice that the values for expected delivery of Depo Provera DEPOVAL are reported either in weeks or months in DEPOUNIT:\n\n\ndat %>% count(DEPOVAL, DEPOUNIT)\n\n\n# A tibble: 18 x 3\n                      DEPOVAL                    DEPOUNIT     n\n                    <int+lbl>                   <int+lbl> <int>\n 1  0                          1 [Weeks]                      3\n 2  1                          1 [Weeks]                     55\n 3  1                          2 [Months]                    29\n 4  2                          1 [Weeks]                     16\n 5  2                          2 [Months]                     9\n 6  3                          1 [Weeks]                      8\n 7  3                          2 [Months]                     9\n 8  4                          1 [Weeks]                      2\n 9  4                          2 [Months]                     1\n10  6                          2 [Months]                     1\n11  7                          1 [Weeks]                      1\n12 12                          2 [Months]                     1\n13 14                          1 [Weeks]                      1\n14 30                          2 [Months]                     5\n15 60                          2 [Months]                     1\n16 99 [NIU (not in universe)] 97 [Don't know]                92\n17 99 [NIU (not in universe)] 98 [No response or missing]     1\n18 99 [NIU (not in universe)] 99 [NIU (not in universe)]   1492\n\nSuppose you wanted to create a derived variable called DEPO_WKS that simply reports the expected delivery time of Depo Provera in weeks. For any value that’s currently reported in months (DEPOUNIT == 2), you might decide to multiply the value in DEPOVAL by 4. Don’t forget to handle non-response values (e.g. 97, 98, 99) separately!\n\n\ndat %>% \n  mutate(DEPO_WKS = case_when(\n      DEPOUNIT == 2 ~ DEPOVAL * 4, \n      DEPOUNIT > 90 ~ NA_real_, \n      T ~ as.double(DEPOVAL) \n    )) %>% \n  count(DEPO_WKS)\n\n\n# A tibble: 15 x 2\n   DEPO_WKS     n\n      <dbl> <int>\n 1        0     3\n 2        1    55\n 3        2    16\n 4        3     8\n 5        4    31\n 6        7     1\n 7        8     9\n 8       12     9\n 9       14     1\n10       16     1\n11       24     1\n12       48     1\n13      120     5\n14      240     1\n15       NA  1585\n\nDEPOVAL is an integer, but R coerces it into a double when you apply multiplication (what if multiplication creates non-integer values?). This is why we tell R to use the double class NA_real_ if DEPOUNIT > 90, and then to use as.double(DEPOVAL) if neither DEPOUNIT == 2 nor DEPOUNIT > 90. All of the values created by case_when have to be in the same class!\nFor facilities that were currently out of stock of a method that they normally provide, these new samples include variables explaining why the method was out of stock. With Depo Provera, for example, we can now see that a majority of stockouts across samples are caused by shipments that were ordered, but did not arrive:\n\n\ndat %>% count(OUTWHYDEPO)\n\n\n# A tibble: 8 x 2\n                                         OUTWHYDEPO     n\n                                          <int+lbl> <int>\n1  1 [Did not place order for shipment]                23\n2  2 [Ordered but did not receive shipment]           154\n3  3 [Did not order right quantities]                   8\n4  4 [Ordered but did not receive right quantities]    17\n5  5 [Unexpected increase in consumption]               5\n6  9 [Other]                                           27\n7 97 [Don't know]                                       1\n8 99 [NIU (not in universe)]                         1492\n\nNational Health Programs\nLastly, we’ve created a new variable group related to participation in national health programs. While we may see more data in upcoming samples, these variables currently describe facility participation in programs provided by the National Hospital Insurance Fund (NHIF) for Kenya. Specifically, you’ll find an indicator for whether a facility in the Kenya 2019 sample provides family planning methods / services covered by NHIF and, if so, whether it provides each of these:\nEdu Afya\nLinda Mama (number of enrolled adolescents and adult women)\nStandard Program\nSuper Program\nother program\n\n\n\n",
    "preview": "posts/2021-02-09-march-2021-data-release/../../images/new_data.png",
    "last_modified": "2021-08-18T14:28:20-05:00",
    "input_file": {},
    "preview_width": 2555,
    "preview_height": 1437
  },
  {
    "path": "posts/2021-02-19-analyzing-the-individual-in-context/",
    "title": "Putting It All Together: Analyzing the Individual in Context",
    "description": "Analyzing women's contraceptive use while considering service delivery point and spatial contextual factors.",
    "author": [
      {
        "name": "Nina Brooks",
        "url": "http://www.ninarbrooks.com/"
      }
    ],
    "date": "2021-03-02",
    "categories": [
      "Individuals in Context",
      "Service Delivery Points",
      "Data Analysis",
      "survey",
      "dotwhisker"
    ],
    "contents": "\n\nContents\nSetup: Load Packages and Data\nRecoding covariates\nRegression Models\nIndividual factors: model with glm\nIndividual factors: model with svyglm\nAvailabillity: modeling with SDP variables\nAccessibility: modeling with external spatial variables\n\n\n\n\n\nThroughout our series on Individuals in Context, we’ve been looking at PMA Service Delivery Point (SDP) data as a resource for understanding the health services environment experienced by women surveyed in PMA Household and Female samples. We created summary variables that capture the SDPs that provide services within the same enumeration areas PMA uses to construct samples of individuals. We’ve also shown how to complement SDP data with additional information about women’s lived environment collected from external geospatial data sources.\nIn this final post, we’ll bring everything together and demonstrate the kind of analysis you might want to do with the contextual data we’ve collected in this series. Specifically, we’ll analyze women’s current contraceptive use, FPCURRUSE, taking into account:\nFPCURRUSE indicates whether a woman is currently using any method of family planning, or doing something to delay or avoid pregnancy.\nindividual factors about each woman collected in the Household and Female survey\navailability factors related to the supply of family planning services provided by SDPs in each woman’s enumeration area\naccessibility factors in each woman’s enumeration area - including measures of population density and transportation infrastructure - that we collected from external data sources\nThe availability of both detailed individual data on family planning and supply-side (service delivery) factors is one of the unique advantages of the PMA data.\nSetup: Load Packages and Data\nWe’ll load the packages tidyverse and ipumsr, as usual. Additionally, we’ll load tidymodels, which helps apply tidyverse principles to the models we’ll be building, and a few other packages we’ll discuss below.\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\nlibrary(tidymodels)\nlibrary(survey)\nlibrary(dotwhisker)\n\n\n\nWe’ll be using both of the Burkina Faso datasets we created in earlier posts in this series:\nbf_merged contains a handful of variables from each sampled woman merged with summary variables about the SDPs that serve her enumeration area (created in this post).\nint contains population and road density variables for each enumeration area (created in this post).\n\nRemember, to use the GPS data you must request access directly from our partners at pmadata.org. The version of int we’re using in this post is based on the real GPS locations but the GPS data itself is not included.\nAs a reminder, let’s take a glimpse at the variables we’ve currently got in each:\n\n\n\n\n\nglimpse(bf_merged)\n\n\nRows: 6,944\nColumns: 10\n$ EAID                <dbl+lbl> 7003, 7003, 7003, 7003, 7003, 7003, …\n$ SAMPLE              <int+lbl> 85405, 85405, 85405, 85405, 85405, 8…\n$ N_SDP               <int> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ NUM_METHODS_PROV    <int> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, …\n$ NUM_METHODS_INSTOCK <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8…\n$ NUM_METHODS_OUT3MO  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ MEAN_OUTDAY         <dbl> NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, …\n$ PERSONID            <chr> \"0700300000019732017504\", \"0700300000019…\n$ URBAN               <int+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FPCURRUSE           <int+lbl> 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, …\n\nThe variables N_SDP, NUM_METHODS_PROV, NUM_METHODS_INSTOCK, NUM_METHODS_OUT3MO, MEAN_OUTDAY, and URBAN all describe the the enumeration area (EAID) where a woman identified by PERSONID resides. The only other variable from the Household and Female questionnaire, itself, is FPCURRUSE. We’ll add more variables describing each woman in a moment.\n\n\n\n\n\nglimpse(int)\n\n\nRows: 83\nColumns: 7\n$ EAID        <dbl> 7003, 7006, 7009, 7016, 7026, 7042, 7048, 7056, …\n$ ROAD_LENGTH <dbl> 30.29857, 28.87695, 24.08644, 41.92500, 67.67416…\n$ PMACC       <chr> \"BF\", \"BF\", \"BF\", \"BF\", \"BF\", \"BF\", \"BF\", \"BF\", …\n$ PMAYEAR     <dbl> 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, …\n$ REGION      <chr> \"5. centre-nord\", \"5. centre-nord\", \"8. est\", \"1…\n$ DATUM       <chr> \"WGS84\", \"WGS84\", \"WGS84\", \"WGS84\", \"WGS84\", \"WG…\n$ POP_DENSITY <dbl> 74.04364, 51.75554, 15.66303, 91.25882, 416.2320…\n\nWe’ll be using ROAD_LENGTH and POP_DENSITY, but first we’ll need to merge int to bf_merged by matching up the EAID for each woman:\n\n\nbf_merged <- left_join(bf_merged, int, by = \"EAID\")\n\n\n\nLet’s now introduce some new variables obtained from each woman’s responses to the Household and Female questionnaire. We’ll merge a new data extract with the following variables collected from the Burkina Faso 2017 and 2018 surveys (female respondents only):\nAGE - Age (in years)\nMARSTAT - Marital status\nEDUCATTGEN - Highest level of school attended, general (4 categories)\nWEALTHQ - Wealth score quintile\nBIRTHEVENT - Number of birth events\n\nFor a refresher on accessing and importing PMA data in R, check out our post Import IPUMS PMA Data Into R.\nFollowing the practice we used when we made bf_merged, we’ll simply handle all of the different non-response codes in this new extract by recoding them as NA. Then, we’ll merge the extract to bf_merged by matching up each person by PERSONID:\n\n\nbf_merged <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00018.xml\",\n  data_file = \"data/pma_00018.dat.gz\") %>% \n  select(PERSONID, AGE, MARSTAT, EDUCATTGEN, WEALTHQ, BIRTHEVENT, STRATA) %>% \n  mutate(across(everything(), ~lbl_na_if(\n    .x,\n    ~.lbl %in% c(\n      \"Not interviewed (female questionnaire)\",\n      \"Not interviewed (household questionnaire)\",\n      \"Don't know\",\n      \"No response or missing\",\n      \"NIU (not in universe)\"\n    )\n  ))) %>% \n  right_join(bf_merged, by = \"PERSONID\")\n\n\n\nRecoding covariates\nAll five of the new variables we’ve introduced are loaded into R as members of both the integer and the haven_labelled class of objects. But really, only AGE and BIRTHEVENT should be treated like continuous measures in our analysis. For MARSTAT, EDUCATTGEN, and WEALTHQ, the integer values associated with each response are arbitrary; we’re much more interested in the labels associated with these numeric values because each of these three variables reflects a categorical measurement.\n\n\nbf_merged %>% \n  select(MARSTAT, EDUCATTGEN, WEALTHQ) %>% \n  map(class)\n\n\n$MARSTAT\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"integer\"       \n\n$EDUCATTGEN\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"integer\"       \n\n$WEALTHQ\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"integer\"       \n\nAs you might know, the normal way to handle categorical variables in a regression model is to create a binary dummy variable associated with each response, and R normally performs this task automatically when it encounters a variable that’s a member of the factor class.\nIf we want, we can simply coerce these variables as factors. When we do this and then use the factor in a regression model, R will select the lowest numbered response as a “reference group” and create binary dummy variables for the other responses. This makes sense with WEALTHQ, where we’d interpret the coefficient for each wealth quintile as an effect relative to being in the lowest quintile.\n\n\nbf_merged %>% count(WEALTHQ)\n\n\n# A tibble: 6 × 2\n                WEALTHQ     n\n              <int+lbl> <int>\n1  1 [Lowest quintile]   1200\n2  2 [Lower quintile]    1031\n3  3 [Middle quintile]    984\n4  4 [Higher quintile]   1253\n5  5 [Highest quintile]  2474\n6 NA                        2\n\nbf_merged <- bf_merged %>% \n  mutate(WEALTHQ = as_factor(WEALTHQ)) \n\nbf_merged %>% count(WEALTHQ)\n\n\n# A tibble: 6 × 2\n  WEALTHQ              n\n  <fct>            <int>\n1 Lowest quintile   1200\n2 Lower quintile    1031\n3 Middle quintile    984\n4 Higher quintile   1253\n5 Highest quintile  2474\n6 <NA>                 2\n\nAlternatively, we might decide to make our own binary dummy variables. This makes sense when we might want to collapse several responses into one larger category, as with MARSTAT: here, for the purpose of analyzing FPCURRUSE, we probably only care about whether the woman is partnered (the reasons why she might not be partnered are less meaningful).\n\n\nbf_merged %>% count(MARSTAT)\n\n\n# A tibble: 5 × 2\n                             MARSTAT     n\n                           <int+lbl> <int>\n1 10 [Never married]                  1876\n2 21 [Currently married]              4307\n3 22 [Currently living with partner]   410\n4 31 [Divorced or separated]           163\n5 32 [Widow or widower]                188\n\nbf_merged <- bf_merged %>%\n  mutate(MARSTAT = lbl_relabel(\n      MARSTAT,\n      lbl(1, \"partnered\") ~ .val %in% 21:22,\n      lbl(0, \"unpartnered\") ~ .val %in% c(10, 31, 32)\n  )) \n\nbf_merged %>% count(MARSTAT)\n\n\n# A tibble: 2 × 2\n          MARSTAT     n\n        <dbl+lbl> <int>\n1 0 [unpartnered]  2227\n2 1 [partnered]    4717\n\nAnother reason to consider recoding categorical variables: what if one response option dominates a huge proportion of the responses in your data? Is it worth sacrificing additional degrees of freedom to accommodate dummy variables that could otherwise be merged together? This is the case with EDUCATTGEN, where over half of the responses are “never attended.” We’ll create a single, simplified binary variable where the responses are “some schooling” or “no schooling.”\n\n\nbf_merged %>% count(EDUCATTGEN)\n\n\n# A tibble: 5 × 2\n                    EDUCATTGEN     n\n                     <int+lbl> <int>\n1  1 [Never attended]           3605\n2  2 [Primary/Middle school]    1212\n3  3 [Secondary/post-primary]   1893\n4  4 [Tertiary/post-secondary]   231\n5 NA                               3\n\nbf_merged <- bf_merged %>% \n  mutate(EDUCATTGEN = lbl_relabel(\n      EDUCATTGEN,\n      lbl(1, \"some schooling\") ~ .val %in% 2:4,\n      lbl(0, \"no school\") ~ .val == 1\n  )) \n\nbf_merged %>% count(EDUCATTGEN)\n\n\n# A tibble: 3 × 2\n           EDUCATTGEN     n\n            <dbl+lbl> <int>\n1  0 [no school]       3605\n2  1 [some schooling]  3336\n3 NA                      3\n\nThe last thing we’ll do here is coerce SAMPLE as a factor so that we can control for arbitrary differences caused by selection into the two samples (recall that our dataset contains two samples from Burkina Faso 2017 and 2018). Because the women in each SAMPLE were surveyed in two different years, this essentially operates like a survey-year fixed effect.\n\n\nbf_merged <- bf_merged %>% \n  mutate(SAMPLE = as.factor(SAMPLE)) \n\nbf_merged %>% count(SAMPLE)\n\n\n# A tibble: 2 × 2\n  SAMPLE     n\n  <fct>  <int>\n1 85405   3556\n2 85408   3388\n\nRegression Models\nWe’re now ready to examine the relative effects individual factors on FPCURRUSE compared to the availability and accessibility of family planning services in each woman’s enumeration area. Let’s begin with a simple model containing the factors we added to the dataset above.\nIndividual factors: model with glm\nMost R users probably use the generalized linear model function glm for this purpose. To keep our demonstration as simple as possible, we’ll fit a model using the Ordinary Least-Squares (OLS) method that glm adopts by default. We’ll use the tidymodels function broom::tidy to clean up the output for our model’s coefficient estimates.\n\nRecall that FPCURRUSE is a binary response (“yes” or “no”), so you might consider fitting a logit model by adding the argument family = 'binomial' to glm().\n\n\nm1 <- glm(\n  FPCURRUSE ~\n    AGE + \n    MARSTAT + \n    EDUCATTGEN + \n    BIRTHEVENT  + \n    WEALTHQ + \n    SAMPLE,\n  data = bf_merged\n)\n\ntidy(m1)\n\n\n# A tibble: 10 × 5\n   term                    estimate std.error statistic  p.value\n   <chr>                      <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)              0.0421   0.0260       1.62  1.06e- 1\n 2 AGE                     -0.00280  0.000948    -2.95  3.17e- 3\n 3 MARSTAT                  0.201    0.0145      13.9   5.21e-43\n 4 EDUCATTGEN               0.159    0.0143      11.2   9.42e-29\n 5 BIRTHEVENT               0.0289   0.00390      7.40  1.48e-13\n 6 WEALTHQLower quintile    0.0319   0.0204       1.56  1.18e- 1\n 7 WEALTHQMiddle quintile   0.0222   0.0208       1.07  2.85e- 1\n 8 WEALTHQHigher quintile   0.0884   0.0198       4.46  8.25e- 6\n 9 WEALTHQHighest quintile  0.166    0.0191       8.67  5.24e-18\n10 SAMPLE85408              0.00515  0.0114       0.453 6.51e- 1\n\nBecause the outcome (FPCURRUSE) is binary, this linear regression is a linear probability model and the coefficients on each term should be interpreted as a percentage point change in the probability of current family planning use.\nFor each of the binary dummy variables we created, the coefficient estimate shows how much the probability FPCURRUSE == \"yes\" increases if the value of the dummy variable is 1. For example, in MARSTAT the value 1 represents “partnered” women, while the value 0 represented “unpartnered” women. The coefficient on MARSTAT is 0.201, meaning our model predicts that being partnered is associated with an increase in the expected probability of family planning use by 0.201.\nIs this a meaningful difference? Consider that the mean of FPCURRUSE is 0.34: this is the probability you might use to guess a woman’s likelihood for using family planning if we didn’t have access to any other variables. Relative to that, an increase of 0.201 is pretty substantive.\nWhat about the other coefficients? We also see a large increase in the probability of family planning use for women who have “some schooling,” and a smaller increase for those who have more children.\nNotice what happened with WEALTHQ, the variable we coerced as a factor above. As expected, R created a binary dummy variable from each response option except the reference group, which is the “lowest quintile.” It’s important to remember that each of these dummy variables represents the effect a being in a particular quintile relative to the “lowest quintile.” These results show that family planning use increases with wealth, which is expected (although the effects don’t become large or statistically significant until we get to the “high” and “highest” income quintiles).\nIndividual factors: model with svyglm\nThere is one problem with the model we created above: as we’ve discussed, PMA samples households randomly within the same enumeration area, and it’s likely that households located together will share many common features. This violates one of the basic assumptions of OLS regression, where we expect modeling errors to be uncorrelated (Cameron and Miller 2015). To address this, we’ll need to use a model that allows us to specify the complexities of PMA survey design. A common approach uses the survey package developed by Thomas Lumley.1\n\nWe’ll use the package svyglm to specify PMA survey design whenever we create analytic models on the PMA Data Analysis Hub!\nLumley’s modeling function survey::svyglm is similar to glm, except that it takes a special design argument where glm takes a data argument. We use the function survey::svydesign to specify the data, the cluster ids from EAID, and the sampling strata STRATA (if we were using the sample weights from FQWEIGHT, we could do that here, too):\n\n\nm2 <- svyglm(\n  FPCURRUSE ~\n    AGE + \n    MARSTAT + \n    EDUCATTGEN + \n    BIRTHEVENT  + \n    WEALTHQ + \n    SAMPLE,\n  design = svydesign(\n    ids = ~EAID,\n    strata = ~STRATA,\n    data = bf_merged\n  )\n)\n\ntidy(m2)\n\n\n# A tibble: 10 × 5\n   term                    estimate std.error statistic  p.value\n   <chr>                      <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)              0.0421    0.0366      1.15  2.54e- 1\n 2 AGE                     -0.00280   0.00114    -2.45  1.65e- 2\n 3 MARSTAT                  0.201     0.0157     12.8   3.19e-20\n 4 EDUCATTGEN               0.159     0.0167      9.53  2.13e-14\n 5 BIRTHEVENT               0.0289    0.00405     7.13  6.41e-10\n 6 WEALTHQLower quintile    0.0319    0.0244      1.31  1.95e- 1\n 7 WEALTHQMiddle quintile   0.0222    0.0282      0.788 4.33e- 1\n 8 WEALTHQHigher quintile   0.0884    0.0302      2.92  4.62e- 3\n 9 WEALTHQHighest quintile  0.166     0.0283      5.86  1.30e- 7\n10 SAMPLE85408              0.00515   0.0145      0.356 7.23e- 1\n\nTo see how this impacts our model estimates, let’s visualize the confidence interval for each coefficient with dotwhisker::dwplot. We’ll use the same function a few times here, and we’ll want to repeat the same visual elements each time, so we’ll just wrap everything together in a custom function we’re calling pma_dwplot():\n\n\npma_dwplot <- function(...){\n  dwplot(\n    bind_rows(...),\n    dodge_size = 0.8,\n    vline = geom_vline(xintercept = 0, colour = \"grey60\", linetype = 2)) +\n    scale_color_viridis_d(option = \"plasma\", end = .7) +\n    theme_minimal()\n}\n\npma_dwplot(\n  tidy(m1) %>% mutate(model = \"glm\"),\n  tidy(m2) %>% mutate(model = \"svyglm\")\n)\n\n\n\n\nWe can see from this plot that the confidence intervals obtained from svyglm are wider than those we got from glm, but the point estimates for each coefficient are unchanged. We also added a dashed line at 0 to make it really easy to see when coefficients are statistically insignificant at the 5% level (if so, the “whiskers” of a 95% confidence interval will cross 0).\nAvailabillity: modeling with SDP variables\nWhile these individual factors are important, we should also expect the availability and accessibility of family planning services to partially determine their use (Bongaarts 2011). Back in an earlier post, we observed that the women in our sample appeared to be 5% more likely to use family planning if they lived in an enumeration area where no SDPs reported a recent contraceptive stockout, compared to women living in areas where at least one SDP did experience a recent stockout. Now we’ll see if that difference is statistically significant, controlling for other factors.\n\nFor our purposes, a “recent stockout” is a stockout of any contraceptive method normally provided by an SDP if the stockout occurred within 3 months prior to the survey.\nFirst, we’ll create a binary variable STOCKOUT indicating whether each woman lives in an enumeration area where at least one SDP reported a recent stockout:\n\n\nbf_merged <- bf_merged %>%\n  mutate(STOCKOUT = case_when(\n    NUM_METHODS_OUT3MO > 0 ~ 1,\n    NUM_METHODS_OUT3MO == 0 ~ 0\n  ))\n\nbf_merged %>% count(STOCKOUT)\n\n\n# A tibble: 3 × 2\n  STOCKOUT     n\n     <dbl> <int>\n1        0  4561\n2        1  1725\n3       NA   658\n\nNext, we’ll add STOCKOUT to our previous model, along with NUM_METHODS_PROV (the number of methods available from at least one SDP serving the woman’s enumeration area) and N_SDP (the number of sampled SDPs serving the woman’s enumeration area).\n\n\nm3 <- svyglm(\n  FPCURRUSE ~\n    AGE + \n    MARSTAT + \n    EDUCATTGEN + \n    BIRTHEVENT  + \n    WEALTHQ + \n    SAMPLE + \n    STOCKOUT + \n    NUM_METHODS_PROV + \n    N_SDP,\n  design = svydesign(\n    ids = ~EAID,\n    strata = ~STRATA,\n    data = bf_merged\n  )\n)\n\ntidy(m3)\n\n\n# A tibble: 13 × 5\n   term                     estimate std.error statistic  p.value\n   <chr>                       <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)              0.113      0.105      1.08   2.85e- 1\n 2 AGE                     -0.00249    0.00114   -2.19   3.23e- 2\n 3 MARSTAT                  0.201      0.0161    12.5    1.94e-19\n 4 EDUCATTGEN               0.168      0.0169     9.93   6.35e-15\n 5 BIRTHEVENT               0.0272     0.00414    6.57   7.77e- 9\n 6 WEALTHQLower quintile    0.0380     0.0244     1.56   1.24e- 1\n 7 WEALTHQMiddle quintile   0.0211     0.0287     0.736  4.64e- 1\n 8 WEALTHQHigher quintile   0.0879     0.0300     2.93   4.58e- 3\n 9 WEALTHQHighest quintile  0.162      0.0270     6.02   7.55e- 8\n10 SAMPLE85408              0.000951   0.0175     0.0542 9.57e- 1\n11 STOCKOUT                -0.0444     0.0206    -2.16   3.44e- 2\n12 NUM_METHODS_PROV        -0.00199    0.0104    -0.192  8.48e- 1\n13 N_SDP                   -0.0165     0.0120    -1.38   1.72e- 1\n\nIndeed, women living in an enumeration area where we’re aware of recent stockouts are less likely to be currently using family planning! The effect isn’t quite as large as some of the individual level factors we’ve examined, but it is statistically significant (p < 0.05).\nDoes the introduction of SDP variables change our estimates for the individual factors we examined previously? A new dwplot seems to show little difference:\n\n\npma_dwplot(\n  tidy(m2) %>% mutate(model = \"Individual-only\"),\n  tidy(m3) %>% mutate(model = \"SDP + Individual\")\n)\n\n\n\n\nAccessibility: modeling with external spatial variables\nAvailability of family planning methods (or lack thereof) is not the same as accessibility. The variables we created in our last post using external geospatial data allow us to explore some factors related to accessibility, which is what we’ll add now. We’ll complement these external variables with URBAN, indicating whether the woman lives in an urban area.\n\n\nm4 <- svyglm(\n  FPCURRUSE ~\n    AGE + \n    MARSTAT + \n    EDUCATTGEN + \n    BIRTHEVENT  + \n    WEALTHQ + \n    SAMPLE + \n    STOCKOUT + \n    NUM_METHODS_PROV + \n    N_SDP + \n    POP_DENSITY + \n    ROAD_LENGTH + \n    URBAN,\n  design = svydesign(\n    ids = ~EAID,\n    strata = ~STRATA,\n    data = bf_merged\n  )\n)\n\ntidy(m4) \n\n\n# A tibble: 16 × 5\n   term                       estimate  std.error statistic  p.value\n   <chr>                         <dbl>      <dbl>     <dbl>    <dbl>\n 1 (Intercept)              0.0833     0.117         0.712  4.79e- 1\n 2 AGE                     -0.00303    0.00112      -2.71   8.61e- 3\n 3 MARSTAT                  0.210      0.0157       13.4    1.97e-20\n 4 EDUCATTGEN               0.162      0.0174        9.30   1.28e-13\n 5 BIRTHEVENT               0.0284     0.00417       6.82   3.41e- 9\n 6 WEALTHQLower quintile    0.0343     0.0249        1.38   1.72e- 1\n 7 WEALTHQMiddle quintile   0.0124     0.0291        0.426  6.72e- 1\n 8 WEALTHQHigher quintile   0.0405     0.0312        1.30   1.99e- 1\n 9 WEALTHQHighest quintile  0.0734     0.0384        1.91   6.07e- 2\n10 SAMPLE85408              0.00133    0.0179        0.0744 9.41e- 1\n11 STOCKOUT                -0.0483     0.0211       -2.29   2.53e- 2\n12 NUM_METHODS_PROV        -0.00225    0.0104       -0.216  8.30e- 1\n13 N_SDP                   -0.0158     0.0124       -1.27   2.09e- 1\n14 POP_DENSITY             -0.00000144 0.00000522   -0.276  7.83e- 1\n15 ROAD_LENGTH              0.00110    0.00144       0.760  4.50e- 1\n16 URBAN                    0.0773     0.0370        2.09   4.03e- 2\n\npma_dwplot(\n  tidy(m2) %>% mutate(model = \"Individual-only\"),\n  tidy(m3) %>% mutate(model = \"SDP + Individual\"),\n  tidy(m4) %>% mutate(model = \"All\")\n) \n\n\n\n\nThis figure with all three models reveals that marital status, education, number of births, and living in an enumeration area that faced recent stockouts are all significantly associated with current family planning use. However, it’s pretty difficult to compare the effects across all the variables. The coefficients on age, population density, and road length are particularly hard to examine and compare. dotwhisker includes a very handy function that re-scales continuous variables on the right-hand side of your regression model to make them more comparable to binary predictors. Specifically, dotwhisker::by_2sd() re-scales continuous input variables by 2 standard deviations following Gelman (2008).2\nWhile we’re adding some last touches to make the plot more readable, we’ll also provide a title, clearer variable names on the Y axis, a tighter scale on the X axis, and a caption at the bottom.\n\n\nlist(\n  tidy(m2) %>% mutate(model = \"Individual-only\"),\n  tidy(m3) %>% mutate(model = \"SDP + Individual\"),\n  tidy(m4) %>% mutate(model = \"All\")\n) %>% \n  map(~by_2sd(.x, bf_merged)) %>% \n  bind_rows() %>% \n  relabel_predictors(\n    c(\n      AGE = \"Age\",  \n      MARSTAT = \"Married\", \n      EDUCATTGEN = \"Some Schooling\", \n      BIRTHEVENT = \"No. of Children\",\n      `WEALTHQLower quintile` = \"Lower Wealth Quintile\", \n      `WEALTHQMiddle quintile` = \"Middle Wealth Quintile\",\n      `WEALTHQHigher quintile` = \"Higher Wealth Quintile\", \n      `WEALTHQHighest quintile` = \"Highest Wealth Quintile\", \n      STOCKOUT = \"Recent Stockout\", \n      NUM_METHODS_PROV = \"No. of FP Methods\",\n      N_SDP = \"No. of SDP Providers\",\n      POP_DENSITY = \"Population Density (w/i 10 km)\",\n      ROAD_LENGTH = \"Road length (w/i 10 km)\",\n      URBAN = \"Lives in Urban EA\",\n      SAMPLE85408 = \"2018 Sample\"\n    )\n  ) %>% \n  dwplot(\n    dodge_size = 0.8,\n    vline = geom_vline(xintercept = 0, colour = \"grey60\", linetype = 2)\n  ) + \n  scale_color_viridis_d(option = \"plasma\", end = .7) +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient\",\n    color = NULL,\n    title = \"Marital Status Is the Strongest Predictor of Family Planning Use\",\n    subtitle = \"Impact of Individual and Contextual Factors on Family Planning Use\",\n    caption = \"Source: IPUMS PMA (Burkina Faso 2017-2018), DIVA-GIS (road length), and WorldPop (population density)\"\n  ) +\n  scale_x_continuous(limits = c(-0.1, 0.3)) + # to make space for the legend\n  theme(\n    legend.position = c(0.8, 0.2),\n    title = element_text(size = 8),\n    legend.text = element_text(size = 8),\n    plot.caption = element_text(hjust = 0), #left align the caption\n    legend.background = element_rect(colour = \"grey80\")\n  )\n\n\n\n\nNow that we’ve re-scaled the continuous input variables by two standard deviations, we can much more easily see the relationship between age and family planning use. Across all models a one-year increase in a woman’s age is associated with a five percentage point lower expected probability of using family planning. This effect is statistically significant at the 5% level in all three models as well.\nThe relationships we observed with marital status, education, and number of children are quite stable across all the models – even as we added variables representing the service environment and broader context of contraceptive availability the coefficients did not meaningfully change.\nThis is in pretty striking contrast to what happens to the wealth quintile variables. When we included only woman and SDP characteristics, being in either the higher and highest wealth quintiles was associated with large and statistically significant increases in the probability of using family planning. But as we added geospatial variables and the URBAN variable in particular, the coefficients become smaller and the confidence intervals become wider. This indicates that there was likely omitted variable bias because wealth is correlated with living in an urban area but when we excluded URBAN the wealth quintile variables were capturing some of this relationship with family planning use.\nEven though this analysis was relatively simple, it was quite informative about different drivers of family planning use. You could easily extend this analysis to include other factors that influence family planning use, incorporate fixed or random effects, or take advantage of the multiple years of survey data!\nAs always, let us know if you have any questions on this post or if you’re working any fertility related analyses and you have a question that we can help address with the blog!\n\n\n\nBongaarts, John. 2011. “Can Family Planning Programs Reduce High Desired Family Size in Sub-Saharan Africa?” International Perspectives on Sexual and Reproductive Health 37 (4): 209–16. https://doi.org/10.1363/3720911.\n\n\nCameron, A. Colin, and Douglas L. Miller. 2015. “A Practitioner’s Guide to Cluster-Robust Inference.” Journal of Human Resources 50 (2): 317–72.\n\n\nGelman, Andrew. 2008. “Scaling Regression Inputs by Dividing by Two Standard Deviations.” Statistics in Medicine 27 (15): 2865–73. https://doi.org/10.1002/sim.3107.\n\n\nLumley, Thomas. 2011. Complex Surveys: A Guide to Analysis Using R. Wiley Series in Survey Methodology. John Wiley & Sons.\n\n\nWe highly recommend Lumley’s (2011) book, Complex Surveys: A Guide to Analysis Using R.↩︎\nWe recommend checking out the full paper, but the short explanation is that with binary predictors you are comparing a value of 0 to a value of 1 when interpreting coefficients. A 1-unit change in a binary predictor is equivalent to a 2 standard deviation change because the standard deviation of a binary variable with equal probabilities is 0.5.↩︎\n",
    "preview": "posts/2021-02-19-analyzing-the-individual-in-context/images/results.png",
    "last_modified": "2021-10-18T16:11:16-05:00",
    "input_file": {},
    "preview_width": 1950,
    "preview_height": 1199
  },
  {
    "path": "posts/2021-02-04-merging-external-spatial-data/",
    "title": "Merging external spatial data",
    "description": "How to integrate external spatial data with PMA data.",
    "author": [
      {
        "name": "Nina Brooks",
        "url": "http://www.ninarbrooks.com/"
      }
    ],
    "date": "2021-02-15",
    "categories": [
      "Individuals in Context",
      "Service Delivery Points",
      "Data Manipulation",
      "join",
      "sf",
      "raster",
      "Spatial"
    ],
    "contents": "\n\nContents\nData\nSetup: Load packages and data\nPopulation Density: working with raster data\nRoad Networks: Working with vector data\n\n\n\n\nOur last post showed how to read, merge, and map the PMA GPS data - and how mapping can shed light on interesting spatial variation. A big advantage of the PMA GPS data is that you can also merge in other sources of spatial data, which opens up enormous opportunities for analyzing how contextual and environmental factors affect topics of interest in the PMA data. In this post, we’ll show how to merge in two different types of spatial data and construct variables of interest.\nData\nWe’ll be using toy PMA GPS data for this post. To use real PMA GPS data you must request access directly from our partners at pmadata.org. The toy data we’ll use here contains randomly sampled locations within Burkina Faso which have no actual relationship to the EAs in the PMA data. This means none of the interpretations of spatial patterns will hold, but all the code will run.\nWe will also be introducing two different spatial datasets that represent different kinds of spatial data. The first is population density data from WorldPop.1 If you want to download the data from the WorldPop site, we’re using the “Unconstrained individual countries 2000-2020 (1 km resolution)” data from 2017 for Burkina Faso. This is raster data, which means the data are stored as a grid of values which are rendered on a map as pixels. You can think of this as a matrix that is spatially referenced – that is each pixel represents a specific area of land on the Earth. Lots of spatial data are stored as rasters including climate data (e.g., temperature and rainfall), elevation, and satellite images. Note that the raster data is saved as a .tiff (which is a common way of storing raster data). The resolution of the raster maps to the area that each pixel represents in the real world. The population density is 1 km resolution, which means that each pixel represents a 1 km by 1 km square on the ground. The figure below shows the impact of different spatial resolutions for the same raster data.2\n\n\n\nFigure 1: Source: NEON\n\n\n\n\nThere are tons of resources on earth data science in R. We recommend the resources by Earth Lab and NEON by NSF. This post is an excellent introduction to working with rasters in R!\nPopulation density is also conceptually important to the SDP data on contraceptive supply that we’ve been examining through this series of posts. Population density may provide a more nuanced characterization of urbanization than the URBAN variable. Additionally, density may be correlated with longer wait times at clinics, which may also impact contraceptive use at the individual level.\nThe second spatial dataset we’ll introduce is data on road networks in Burkina Faso from the Digital Chart of the World and made publicly available by DIVA-GIS, an excellent source for publicly available spatial datasets. Road networks serve as a proxy for accessibility to health clinics – an important component of the contraceptive service environment – that may be more nuanced than the binary urban/rural distinction. To download the road data, go to DIVA-GIS Data and select Burkina Faso from the Country dropdown and Roads from the Subject dropdown. The road data is called vector data and is stored in a shapefile (.shp). Vector data is used to represent real world features and are three basic types: points, lines, and polygons. The road data we’re using in this post is an example of vector line data.\n\nRemember the administrative boundaries we used in the previous post were polygons and the GPS points for the PMA enumeration areas were points. Both are vector data!\nSetup: Load packages and data\nWe’ll be using many of the packages from the last few posts, as well as a new package for specifically working with raster data – appropriately called raster – and one called units, which enables easy conversion between objects of different units. Make sure to install the raster and units packages first and then load everything we’ll be using today:\n\n\nlibrary(sf) # primary spatial package\nlibrary(raster) # for working with raster data\nlibrary(viridis) # for color palettes\nlibrary(units) # to easily convert between units\nlibrary(tidyverse)\n\n\n\nLet’s start by reading in the raster using raster::raster() and check out the meta-data.\n\n\npop_density <- raster(\"bfa_pd_2017_1km.tif\")\npop_density\n\n\nclass      : RasterLayer \ndimensions : 682, 951, 648582  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -5.517917, 2.407083, 9.407917, 15.09125  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : /Users/Matt/R/pma-data-hub/_posts/2021-02-04-merging-external-spatial-data/bfa_pd_2017_1km.tif \nnames      : bfa_pd_2017_1km \nvalues     : 0.281988, 8820.016  (min, max)\n\nBecause rasters are essentially just matrices, you can think of the dimensions in the same way. At a spatial resolution of 1 km, this raster covers all of Burkina Faso with 648,582 cells. The resolution describes the size of the cells (the length of one side of each square cell). You may be wondering why this is showing up as 0.00833 when the data has a spatial scale of 1 km by 1 km. This is because the units that the resolution is reported in depend on the coordinate reference system of the data. More on this in a moment.\nThe extent (or spatial extent) refers to the geographic area that the raster covers. The values are in the same coordinate reference system as the raster. The coordinate reference system or crs is the next piece of meta-data we have. “A coordinate reference system (CRS) is a coordinate-based local, regional or global system used to locate geographical entities.”3 The crs for this raster is +proj=longlat +datum=WGS84 +no_defs. The crs contains several pieces of information including the datum (WGS84) and the projection.4 The appropriate CRS to use for any given spatial task depends on what part of the world the data represent and what kind of spatial operations you’ll be performing. It’s really important to know what crs your data are in and make sure that all your spatial data are in the same  crs if you use more than one kind. Otherwise, they won’t line up on a map and any spatial analysis or processing you do will be incorrect.\nThe projection of this raster data is described as longlat, which actually is not a projection. A projection refers to how the Earth’s surface is flattened so it can be represented as a 2-dimensional raster grid. These data use a geographic coordinate system, simply the raw latitude and longitude coordinates, rather than a projected coordinate system, which would transform the coordinates into a 2-dimensional plane. Latitude and longitude locate positions on the Earth using angles, so the spacing of each line of latitude as you move north or south along the Earth is not uniform. The units of this reference system are in degrees (of latitude and longitude), so the 0.00833 resolution we saw above is reporting the spatial resolution in degrees, rather than meters or kilometers. This crs is not ideal for measuring distances because the distance covered by a single degree of latitude or longitude varies greatly across the Earth’s surface. This also means that the stated 1 km resolution is only nominal. At the equator, 0.00833 degrees is approximately equal to 1 km, but this distance, and the ground area represented by each pixel, will vary. Fortunately, Burkina Faso is relatively close to the equator, so the pixels will be quite close to 1 km by 1 km.\nThe last piece of meta-data to look at are the values – this is reporting the minimum and maximum values across all of the cells. Because these are population density data, it can be interpreted as the number of people in each pixel divided by the area of each pixel (which we know is 1 km2)\nNow that we’ve reviewed the raster attributes, let’s see what it looks like. We can use the basic plot function to do this.\n\n\nplot(pop_density)\n\n\n\n\nWe can see three locations stand out in terms of population density. First is Ouagadougou the capital of Burkina Faso and largest city, right in the center. Then we can see higher density around Bobo Dioulasso and Banfora in the southwest of the country, which are the second and third largest cities in the country.\nNext we’ll load the roads data using sf::st_read().\n\n\nroads <- st_read(\"BFA_roads/BFA_roads.shp\", quiet = TRUE)\nroads\n\n\nSimple feature collection with 1149 features and 5 fields\ngeometry type:  MULTILINESTRING\ndimension:      XY\nbbox:           xmin: -5.482261 ymin: 9.407643 xmax: 2.393089 ymax: 15.08071\ngeographic CRS: WGS 84\nFirst 10 features:\n       MED_DESCRI      RTT_DESCRI F_CODE_DES ISO   ISOCOUNTRY\n1  Without Median Secondary Route       Road BFA BURKINA FASO\n2  Without Median Secondary Route       Road BFA BURKINA FASO\n3  Without Median Secondary Route       Road BFA BURKINA FASO\n4  Without Median Secondary Route       Road BFA BURKINA FASO\n5  Without Median Secondary Route       Road BFA BURKINA FASO\n6  Without Median Secondary Route       Road BFA BURKINA FASO\n7  Without Median Secondary Route       Road BFA BURKINA FASO\n8  Without Median Secondary Route       Road BFA BURKINA FASO\n9  Without Median Secondary Route       Road BFA BURKINA FASO\n10 Without Median Secondary Route       Road BFA BURKINA FASO\n                         geometry\n1  MULTILINESTRING ((-0.720550...\n2  MULTILINESTRING ((-0.583273...\n3  MULTILINESTRING ((-0.397415...\n4  MULTILINESTRING ((-0.142728...\n5  MULTILINESTRING ((-0.403059...\n6  MULTILINESTRING ((-0.171111...\n7  MULTILINESTRING ((-0.116756...\n8  MULTILINESTRING ((0.0672155...\n9  MULTILINESTRING ((-1.245636...\n10 MULTILINESTRING ((-1.50246 ...\n\nThis sf object also contains meta-data (shown at the top). In terms of meta-data, the geometry type field tells us this data is a MULTILINESTRING object, which makes sense since these are roads. The bbox (short for bounding box), is the same information as the extent field for the raster data – it tells us the bounds of the geographic area that this spatial data covers. We see the geographic CRS which is the coordinate reference system of the data. For this roads dataset it is WGS84, which is the same as the population density raster data.\nThe roads data contains several variables: MED_DESCRI, RTT_DESCRI, F_CODE_DES, ISO, ISOCOUNTRY, and geometry. The first three variables provide some information about the types of roads in this data. ISO and ISOCOUNTRY simply provide country codes and names for the data. Finally, we see the geometry variable, which is the variable that contains the spatial information in an sf object.\nWe can also plot this roads data to see what it looks like.\n\n\nplot(roads)\n\n\n\n\nBy calling the basic plot function, we get a panel of plots of the road network, with one plot for each variable. We can see some variation in color MED_DESCRI and RTT_DESCRI, indicating that there multiple values for those variables. If we wanted just a single plot of the road network, we can get that by calling plot on the geometry variable:\n\n\nplot(roads$geometry)\n\n\n\n\nFinally, we’ll load the “toy” GPS data and convert it to an sf object. The option crs = 4326 means that we are creating this with the WGS84 coordinate reference system because 4326 is the EPSG code for WGS84.\n\nMost crs are assigned an “EPSG code”, which is a unique ID that can be used to identify a CRS.\n\n\ngps <- read_csv(\"bf_gps_fake.csv\") %>%\n  rename(EAID = EA_ID) %>% # rename to be consistent with other PMA data\n  st_as_sf(\n    coords = c(\"GPSLONG\", \"GPSLAT\"), \n    crs = 4326)\n\n\n\n\n\n\nPopulation Density: working with raster data\nWe want to construct a variable that captures the population density at each enumeration area in the data. We’ll use sf::st_buffer() to do this, which will construct a buffer circle around each GPS point. The PMA GPS data are randomly displaced to protect the privacy of respondents, so it’s imperative to consider this displacement when working with the GPS data to do spatial operations. Because the maximum displacement distance is 10 km, if we construct buffers with a radius of 10 km we can be 100% confident that the true locations of each GPS point fall within that buffer.\n\nUrban EAs are displaced from their true location up to 2 km. Rural EAs are displaced from their true location up to 5 km. Additionally, a random sample of 1% of rural EAs are displaced up to 10km.\n\n\nbuffers <- st_buffer(gps, dist = 10000)\n\n\nWarning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle\n= endCapStyle, : st_buffer does not correctly buffer longitude/\nlatitude data\ndist is assumed to be in decimal degrees (arc_degrees).\n\nggplot() +\n  geom_sf(data = buffers) +\n  geom_sf(data = gps)\n\n\n\n\nThis giant circle is certainly not what we would expect! What’s going on here? Earlier in this post we mentioned that the WGS84 crs is a geographic coordinate system that simply uses the latitude and longitude coordinates to identify locations and the units are in degrees, rather than meters or kilometers. This circle thus has a radius of 10,000 degrees and since the Earth only spans 360 degrees it is fully covered by this circle. As we mentioned, the WGS84 crs is not ideal for measuring distances. R alerted us of this problem with two warnings: st_buffer does not correctly buffer longitude/latitude data and dist is assumed to be in decimal degrees (arc_degrees). This is why it’s so important to pay attention to the crs of your data.\nTo properly construct a buffer circle around these GPS points, we need to transform the data to a different projection that uses meters or kilometers. And, because it’s essential that all of our data are in the same crs, we need to transform or reproject everything. For vector data, we can do this using sf::st_transform() and for raster data we’ll do this with raster::projectRaster(). For the transformation, we’re using a crs that is projected to meters and is appropriate to the local geography of Burkina Faso. You can read about it on the epsg.io site. After reprojecting, we’ll calculate the buffer again and plot it to make sure this looks right.\n\n\n# transform the GPS data\ngps_tr <- gps %>% st_transform(crs = 32630)\ngps_tr\n\n\nSimple feature collection with 83 features and 5 fields\ngeometry type:  POINT\ndimension:      XY\nbbox:           xmin: 260943.3 ymin: 1114669 xmax: 1025472 ymax: 1653511\nprojected CRS:  WGS 84 / UTM zone 30N\n# A tibble: 83 x 6\n   PMACC PMAYEAR REGION                EAID DATUM           geometry\n * <chr>   <dbl> <chr>                <dbl> <chr>        <POINT [m]>\n 1 BF       2017 5. centre-nord        7610 WGS84 (837531.4 1567675)\n 2 BF       2017 1. boucle-du-mouhoun  7820 WGS84 (491871.7 1488848)\n 3 BF       2017 3. centre             7271 WGS84   (982414 1349907)\n 4 BF       2017 3. centre             7799 WGS84   (739431 1352652)\n 5 BF       2017 8. est                7243 WGS84 (545866.2 1219668)\n 6 BF       2017 6. centre-ouest       7026 WGS84 (352638.7 1209502)\n 7 BF       2017 3. centre             7859 WGS84 (833822.1 1377527)\n 8 BF       2017 3. centre             7725 WGS84 (980025.8 1406727)\n 9 BF       2017 6. centre-ouest       7390 WGS84 (439876.7 1190609)\n10 BF       2017 11. plateau-central   7104 WGS84 (835483.2 1469280)\n# … with 73 more rows\n\n# reproject the raster data\npop_density_tr <- projectRaster(\n  pop_density, \n  crs = \"+proj=utm +zone=30 +datum=WGS84 +units=m +no_defs\"\n)\npop_density_tr\n\n\nclass      : RasterLayer \ndimensions : 699, 970, 678030  (nrow, ncol, ncell)\nresolution : 907, 922  (x, y)\nextent     : 218942.9, 1098733, 1035714, 1680192  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=30 +datum=WGS84 +units=m +no_defs \nsource     : memory\nnames      : bfa_pd_2017_1km \nvalues     : 0.9709167, 8775.492  (min, max)\n\n\nNote: the projectRaster function takes crs as a character string, rather than the EPSG code 32630. We’re using the PROJ.4 code shown in the “Export” menu on the epsg.io site.\n\n\n# calculate 10 km (10,000 meter) buffer circles\nbuffers_tr <- st_buffer(gps_tr, dist = 10000) # because the units are in meters\n\n# plot\nggplot() +\n  geom_sf(data = buffers_tr) +\n  geom_sf(data = gps_tr, color = \"red\")\n\n\n\n\nLooking at the meta-data for both the gps_tr and raster_tr objects, we can see they have the same new projected crs: UTM zone 30N. The raster_tr meta-data also includes information on the units (+units=m) confirming that distances are measured in meters. Turning to the plot, we can see the GPS coordinates marked in red and each has a circle around it.\nNow that we have correctly estimated 10 km buffer circles, we can calculate the average population density within each buffer using the raster::extract() command and specifying fun = mean. This produces an 83 x 1 vector of results, which means we have one population density value for each enumeration area. Printing the first 5 results shows there is some substantial variation in population density.\n\n\nbuffer_density <- raster::extract(\n  pop_density_tr, \n  buffers_tr, \n  fun = mean, \n  na.rm = TRUE,\n  cellnumbers = TRUE\n)\ndim(buffer_density)\n\n\n[1] 83  1\n\nhead(buffer_density)\n\n\n          [,1]\n[1,]  35.58080\n[2,]  21.36878\n[3,]  17.56161\n[4,] 113.36298\n[5,]  31.73017\n[6,]  57.09591\n\nNote, that we don’t actually need to create the buffers first to extract the mean values of the raster. We can do it all in one step, shown below. Just make sure to use the gps_tr object instead of the buffer_tr object! But, we’ll use those buffers again with the road data.\n\n\nbuffer_density_alt <- raster::extract(\n  pop_density_tr, gps_tr, \n  buffer = 10000,\n  fun = mean, \n  na.rm = TRUE\n)\nhead(buffer_density_alt)\n\n\n[1]  35.58080  21.36878  17.56161 113.21781  31.73017  57.09591\n\nFinally, so we can merge everything together by EAID, let’s add the population density calculation directly to the gps_tr data. Note that the raster::extract() command preserves the order of the inputs, so we know the first row of the density calculation corresponds to the first row of the gps_tr data.\n\n\ngps_tr$pop_density <- raster::extract(\n  pop_density_tr, gps_tr, \n  buffer = 10000,\n  fun = mean, na.rm = TRUE\n)\n\n\n\nRoad Networks: Working with vector data\nBefore we do anything with the road data, let’s make sure to reproject it to match the rest of our data.\n\n\nroads_tr <- roads %>%\n  st_transform(crs = 32630)\n\n\n\nBecause enumeration areas with better access to roads may make it easier for women to reach local service delivery providers. We are going to calculate the total length of roads within each buffer as a proxy for this accessibility. Because each of these buffers was constructed with the same 10 km radius, they have the same area, which means the sum of road length can also be thought of as a road density measure.\nFirst, we need to identify which portions of the road fall into each buffer. We’ll use sf::st_intersection(), which returns a new sf object that contains observations from the first argument that touch (geographically) the second argument.\n\nNote that there is also an sf::intersects() command. This is different than the one we’re using because it returns a logical matrix that indicates whether each geometry pair intersects. See more on these types of operations in the sf vignette.\n\n\nint <- st_intersection(roads_tr, buffers_tr)\nint\n\n\nSimple feature collection with 238 features and 10 fields\ngeometry type:  LINESTRING\ndimension:      XY\nbbox:           xmin: 251238.7 ymin: 1104708 xmax: 1033002 ymax: 1658158\nprojected CRS:  WGS 84 / UTM zone 30N\nFirst 10 features:\n        MED_DESCRI      RTT_DESCRI F_CODE_DES ISO   ISOCOUNTRY PMACC\n240 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n241 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n268 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n711 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n640    With Median   Primary Route       Road BFA BURKINA FASO    BF\n649    With Median   Primary Route       Road BFA BURKINA FASO    BF\n728 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n958 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n959 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n964 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n    PMAYEAR               REGION EAID DATUM\n240    2017 1. boucle-du-mouhoun 7820 WGS84\n241    2017 1. boucle-du-mouhoun 7820 WGS84\n268    2017 1. boucle-du-mouhoun 7820 WGS84\n711    2017            3. centre 7271 WGS84\n640    2017            3. centre 7799 WGS84\n649    2017            3. centre 7799 WGS84\n728    2017            3. centre 7799 WGS84\n958    2017               8. est 7243 WGS84\n959    2017               8. est 7243 WGS84\n964    2017               8. est 7243 WGS84\n                          geometry\n240 LINESTRING (501051.3 149280...\n241 LINESTRING (481961.6 149016...\n268 LINESTRING (489661.7 148171...\n711 LINESTRING (981157 1359825,...\n640 LINESTRING (732417.5 135977...\n649 LINESTRING (746990.9 135527...\n728 LINESTRING (746990.9 135527...\n958 LINESTRING (553191.7 122647...\n959 LINESTRING (554462.3 122043...\n964 LINESTRING (535888.5 122030...\n\nThe returned object (int) is a data.frame with 238 observations (far fewer than the original 1149 in the roads_tr data). Note that it also contains all the variables from both roads_tr and buffers_tr, so this operates a bit like an inner_join, which means it only includes observations that are in both datasets. We can see the implications of this by making a quick map. The full road network is shown in gray, the buffer circles are in black and the roads that fall into the circles are highlighted in red. Based on this map, it looks like there are a few buffer circles that don’t contain any roads. We want to be sure we account for this.\n\n\n# plot intersection with buffers and road networks \nggplot() +\n  geom_sf(data = buffers_tr) +\n  geom_sf(data = roads_tr, color = \"grey\") +\n  geom_sf(data = int, color = \"red\")\n\n\n\n\nWe can merge in the full list of EAIDs to make sure we don’t miss this one (or any others) using sf:st_join(), which works like dplyr::left_join(). It’s important that when we do the join, the first argument is int, so that it will retain the LINESTRING geometry from this dataset, which we need to calculate the road length. Then, we’ll calculate the length of the road networks contained in each buffer. We can do this with sf::st_length(). Because many of the buffer circles contain multiple roads, we first need calculate the length of each road then we need to aggregate to get the length of all roads in a given enumeration area. We’ll convert from meters to km for greater readability. It’s important to note that any EAs with buffers that don’t contain any roads will not be in the int dataset, so we’ll do a dplyr::full_join() with gps_tr to make sure we get them all.\nBecause int and gps_tr are both sf objects, it’s not possible to do a standard join – you can only use sf::join() when you have two sf objects. That’s why we convert both to data.frames for the dplyr::full_join() and then back into an sf object. Finally, we’ll convert int back into an sf object, retaining the POINT geometry from gps_tr, and replace all NA road lengths as 0.\n\n\n# join, calculate length, & summarize\nint <- int %>%\n  mutate(road_length = st_length(geometry)) %>%\n  group_by(EAID) %>%\n  summarise(road_length = sum(road_length, na.rm = T)) %>%\n  mutate(road_length = set_units(road_length, \"km\")) %>%\n  as.data.frame() %>%\n  full_join(as.data.frame(gps_tr), by = \"EAID\") %>%\n  st_sf(sf_column_name = 'geometry.y') %>%\n  dplyr::select(-geometry.x) %>%\n  mutate(road_length = ifelse(is.na(road_length), 0, road_length))\n\nint\n\n\nSimple feature collection with 83 features and 7 fields\ngeometry type:  POINT\ndimension:      XY\nbbox:           xmin: 260943.3 ymin: 1114669 xmax: 1025472 ymax: 1653511\nprojected CRS:  WGS 84 / UTM zone 30N\nFirst 10 features:\n   EAID road_length PMACC PMAYEAR               REGION DATUM\n1  7003    22.41682    BF    2017       5. centre-nord WGS84\n2  7006    13.35579    BF    2017       5. centre-nord WGS84\n3  7009    20.45756    BF    2017               8. est WGS84\n4  7016    20.31970    BF    2017 1. boucle-du-mouhoun WGS84\n5  7026    18.99583    BF    2017      6. centre-ouest WGS84\n6  7042    14.52567    BF    2017               8. est WGS84\n7  7048    24.29931    BF    2017        4. centre-est WGS84\n8  7056    15.21114    BF    2017     9. hauts-bassins WGS84\n9  7082    44.15771    BF    2017          2. cascades WGS84\n10 7092    26.30945    BF    2017        7. centre-sud WGS84\n   pop_density               geometry.y\n1     29.55058 POINT (422741.8 1383385)\n2     29.98035 POINT (371721.6 1276582)\n3     37.77861 POINT (348264.9 1114669)\n4    154.07399 POINT (346019.4 1218739)\n5     57.09591 POINT (352638.7 1209502)\n6     63.08637 POINT (752580.5 1219236)\n7     74.19692 POINT (479619.4 1234717)\n8     35.07390 POINT (359915.1 1392593)\n9   2731.76393 POINT (669089.2 1375002)\n10   152.23504 POINT (559070.9 1346179)\n\nThe added benefit of the full_join() with gps_tr is that it brings in the pop_density variable we created earlier. So now everything is in one dataset!\nThis can now be merged into other PMA data, such as the individual level dataset bf_merged we worked with in the other posts in this module, and the variables can be used for analysis!\nAs always, let us know if you have any questions and if you’re doing anything exciting with the PMA spatial data!\nSpecial thanks to Tracy Kugler, Nicholas Nagle, and Jonathan Schroeder for excellent help with this post.\n\nLinard, C., Gilbert, M., Snow, R. W., Noor, A. M., & Tatem, A. J. (2012). Population distribution, settlement patterns and accessibility across Africa in 2010. PloS one, 7(2), e31743.↩︎\nNEON: https://www.neonscience.org/resources/learning-hub/tutorials/raster-res-extent-pixels-r↩︎\nWikipedia↩︎\nhttps://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/intro-to-coordinate-reference-systems/↩︎\n",
    "preview": "posts/2021-02-04-merging-external-spatial-data/images/road_map.png",
    "last_modified": "2021-05-14T09:23:13-05:00",
    "input_file": {},
    "preview_width": 936,
    "preview_height": 574
  },
  {
    "path": "posts/2021-01-29-mapping-sdp-variables/",
    "title": "Mapping Service Delivery Point Data",
    "description": "Map spatial variation in the service delivery environment across enumeration areas.",
    "author": [
      {
        "name": "Nina Brooks",
        "url": "http://www.ninarbrooks.com/"
      }
    ],
    "date": "2021-01-30",
    "categories": [
      "Individuals in Context",
      "Service Delivery Points",
      "Data Manipulation",
      "Mapping",
      "sf"
    ],
    "contents": "\n\nContents\nData\nSetup\nMerge and Map\nBasic Maps\nMerge GPS and SDP Data\nMap SDP data\n\nPutting it All Together\n\n\n\n\nIn our last post, we showed how PMA Service Delivery Point (SDP) data can be aggregated to the enumeration area they serve (captured by EASERVED) and linked to individual-level data from a PMA Household and Female survey. In this post, we’ll continue thinking about the spatial distribution of SDP summary data. We’ll first show how to merge our example data to a GPS dataset obtained from pmadata.org, and we’ll then use the new dataset to visualize a few of our variables on a map of Burkina Faso.\nData\nBuilding on the steps we’ve covered in the last two posts in this series, we’ll be working with an example dataset we’re calling bf_merged that contains records from female respondents to the 2017 and 2018 Burkina Faso Household and Female surveys merged with five variables we’ve created from the 2017 and 2018 SDP surveys. These five variables describe services provided within the enumeration area where each woman resides:\nNUM_METHODS_PROV - number of methods provided by at least one SDP\nNUM_METHODS_INSTOCK - number of methods in-stock with at least one SDP\nNUM_METHODS_OUT3MO - number of methods out of stock any time in the last 3 months with at least one SDP\nMEAN_OUTDAY - the mean length of a stockout for any family planning method (measured in days)\nN_SDP - number of SDPs\nThe remaining four variables in bf_merged were taken directly from a data extract containing only female respondents:\nPERSONID - unique identifer for each woman\nEAID - unique identifier for each woman’s enumeration area\nURBAN - whether each woman lives in an urban enumeration area\nFPCURRUSE - whether each woman is currently using any family planning method\nWe’ll also be working with toy PMA GPS datasets for Burkina Faso. PMA GPS data include one GPS coordinate per enumeration area. The Burkina Faso Round 5 and 6 surveys sampled the same enumeration areas, which means we can link the GPS data to both rounds. To use real PMA GPS data you must request access directly from our partners at pmadata.org. For the purpose of use in this post, we’ve created a “toy” GPS dataset: the toy data contains randomly sampled locations within Burkina Faso that have no actual relationship to the EAs in the PMA data.\nThe last dataset we’ll use in this post are the administrative boundaries for Burkina Faso. Shapefiles with administrative boundaries are widely available for download, but we’ll use the ones made available from IPUMS PMA.\nSetup\nMake sure you have all of the following packages installed. Once installed, load the packages we’ll be using today:\n\n\nlibrary(ipumsr)\nlibrary(sf) # primary spatial package\nlibrary(viridis) # for color palettes\nlibrary(tabulator) # for pipe-friendly tabs & cross-tabs\nlibrary(tidyverse)\n\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nIf you followed along with our last post, glimpse(bf_merged) should list the first few records for all of the variables we have so far:\n\n\n\n\n\nglimpse(bf_merged)\n\n\nRows: 6,944\nColumns: 10\n$ EAID                <dbl+lbl> 7003, 7003, 7003, 7003, 7003, 7003,…\n$ SAMPLE              <int+lbl> 85405, 85405, 85405, 85405, 85405, …\n$ N_SDP               <int> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ NUM_METHODS_PROV    <int> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ NUM_METHODS_INSTOCK <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, …\n$ NUM_METHODS_OUT3MO  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MEAN_OUTDAY         <dbl> NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN,…\n$ PERSONID            <chr> \"0700300000019732017504\", \"070030000001…\n$ URBAN               <int+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FPCURRUSE           <int+lbl> 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,…\n\nRemember that we merged the EA-level data into the individual-level data, but the GPS datasets provide coordinates for the enumeration area. So the first thing we’ll do is aggregate bf_merged to the EA-level, and assign the aggregated data to a new object called bf_ea.\n\n\nbf_ea <- bf_merged %>%\n  dplyr::select(-PERSONID) %>%\n  group_by(EAID, SAMPLE) %>%\n  summarise_all(mean, na.rm = T) %>%\n  filter(!is.na(N_SDP)) \n\n\n\n\nIn this example, we’ll exclude any EAs where no facilities in our SDP sample provide services with filter(!is.na(N_SDP))\nNow, let’s read in the GPS data from the data folder and see what the it contains.\n\n\ngps <- read_csv(\"bf_gps_fake.csv\")\ngps\n\n\n# A tibble: 83 x 7\n   PMACC PMAYEAR REGION               EA_ID DATUM GPSLAT GPSLONG\n   <chr>   <dbl> <chr>                <dbl> <chr>  <dbl>   <dbl>\n 1 BF       2017 5. centre-nord        7610 WGS84   14.2  0.126 \n 2 BF       2017 1. boucle-du-mouhoun  7820 WGS84   13.5 -3.08  \n 3 BF       2017 3. centre             7271 WGS84   12.2  1.43  \n 4 BF       2017 3. centre             7799 WGS84   12.2 -0.799 \n 5 BF       2017 8. est                7243 WGS84   11.0 -2.58  \n 6 BF       2017 6. centre-ouest       7026 WGS84   10.9 -4.35  \n 7 BF       2017 3. centre             7859 WGS84   12.4  0.0703\n 8 BF       2017 3. centre             7725 WGS84   12.7  1.42  \n 9 BF       2017 6. centre-ouest       7390 WGS84   10.8 -3.55  \n10 BF       2017 11. plateau-central   7104 WGS84   13.3  0.0957\n# … with 73 more rows\n\n\nIf you requested access to the actual GPS datasets, make sure to replace the bf_gps_fake.csv with the filename of the real data!\n\n\n\nThe gps data has 7 variables:\nPMACC: the country code\nPMAYEAR: the 4-digit year of data collection\nREGION: sub-national administrative division name\nEA_ID: the enumeration area ID (and how we’ll merge this data into other PMA datasets)\nGPSLAT: the displaced EA’s centroid latitude coordinate in decimal degrees\nGPSLONG: the displaced EA’s centroid longitude coordinate in decimal degrees\nDATUM: the coordinate reference system and geographic datum. This variable is always “WGS84” for the World Geodetic System 1984.\n\nNote that while the PMAYEAR variable is 2017 for all EAs, because the same EAs were sampled in the 2017 (Round 5) and 2018 (Round 6) surveys, we can link these coordinates to both samples.\nNote that the GPSLAT and GPSLONG are displaced coordinates of the EA centroid. This is because PMA randomly displaces the geographic coordinates to preserve the privacy of survey respondents. Coordinates are displaced randomly by both angle and distance. Urban EAs are displaced from their true location up to 2 km. Rural EAs are displaced from their true location up to 5 km. Additionally, a random sample of 1% of rural EAs are displaced up to 10km. The PMA GPS data come with documentation that explains the displacement in more detail. The primary spatial package we’ll use is simple features or sf. We’ll use sf::st_as_sf() to convert the GPS data to a spatial data object (known as a simple feature collection).\n\n\ngps <- gps %>%\n    rename(EAID = EA_ID) %>% # rename to be consistent with other PMA data\n    st_as_sf(\n      coords = c(\"GPSLONG\", \"GPSLAT\"), \n      crs = 4326) # 4326 is the coordinate reference system (CRS) identifier for WGS84\ngps\n\n\nSimple feature collection with 83 features and 5 fields\ngeometry type:  POINT\ndimension:      XY\nbbox:           xmin: -5.185229 ymin: 10.08082 xmax: 1.829187 ymax: 14.93619\ngeographic CRS: WGS 84\n# A tibble: 83 x 6\n   PMACC PMAYEAR REGION               EAID DATUM              geometry\n * <chr>   <dbl> <chr>               <dbl> <chr>           <POINT [°]>\n 1 BF       2017 5. centre-nord       7610 WGS84  (0.1263576 14.15999)\n 2 BF       2017 1. boucle-du-mouho…  7820 WGS84   (-3.075099 13.4676)\n 3 BF       2017 3. centre            7271 WGS84   (1.430382 12.17557)\n 4 BF       2017 3. centre            7799 WGS84 (-0.7991777 12.22721)\n 5 BF       2017 8. est               7243 WGS84  (-2.580105 11.03307)\n 6 BF       2017 6. centre-ouest      7026 WGS84  (-4.348525 10.93844)\n 7 BF       2017 3. centre            7859 WGS84 (0.07032293 12.44352)\n 8 BF       2017 3. centre            7725 WGS84   (1.417154 12.68821)\n 9 BF       2017 6. centre-ouest      7390 WGS84  (-3.549929 10.77006)\n10 BF       2017 11. plateau-central  7104 WGS84 (0.09573113 13.27184)\n# … with 73 more rows\n\n\n\n\nNow that gps is a simple features object, we’ve lost the GPSLAT and GPSLONG variables and gained a variable called geometry, which contains the spatial information for this data.\n\n\n\nThe last thing we need is the Burkina Faso shapefile, which are available from IPUMS PMA. You’ll need to download the shapefile (geobf.zip) from the IPUMS site and save it in your working directory to use it. We can use sf::st_read() to read the shapefile into R as an sf object. Note that here the geometry variable is a POLYGON, whereas in the gps data it is a POINT.\n\nNote that what we call a shapefile is actually a collection of many files. More on this in a future post! But for now, just know that you’ll need all the files that come in the zipped download and can refer to the collectively with “geobf.shp”.\n\n\nbf_shp <- st_read(\"geobf/geobf.shp\") \n\n\n\nMerge and Map\nNow that we have all our data, we’ll show you how to map variables… but before we do that, let’s do some basic, exploratory mapping.\nBasic Maps\nggplot2 has support for sf objects, which makes it really easy to map things using the ggplot2 system. ggplot2::geom_sf() will automatically identify what kind of spatial data you’re plotting and handle it appropriately. For example, let’s plot the gps data (which are points) and the administrative region (which are polygons).\nggplot2 is included when you load library(tidyverse)\n\n\n# Plot EA centroids\nggplot() +\n  geom_sf(data = gps)\n\n\n\n# Plot regions of Burkina Faso\nggplot() +\n  geom_sf(data = bf_shp)\n\n\n\n\nThe building-block approach of ggplot2 (“Grammar of Graphics”) also makes it really easy to layer different spatial features on the same map.\n\n\n# Plot regions of Burkina Faso & EA centroids on the same map\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = gps)\n\n\n\n\nMerge GPS and SDP Data\nTo map the EA-level variables constructed in the last post, we need to merge the bf_ea data and the gps data by EAID. First, let’s rename the EASEARVED variable to match the GPS data and then use a dplyr::right_join to merge in the SDP data. We need to use a right_join() because the sf object must be listed first in our join command to retain the sf class, but we want to ensure that all rows from bf_ea are preserved.\n\nRemember, the SDP data contains information from both 2017 and 2018, while the GPS data has a single observation per EA.\n\n\nbf_ea <- right_join(gps, bf_ea, by = \"EAID\")\n\n\n\nMap SDP data\nRemember, the bf_ea data contains information from 2017 & 2018 for the same EA, which can clog up the map depending on how we use this information. To start out, let’s use only the 2017 data and add information about the number of service delivery providers that serve a given EA (N_SDP). By passing N_SDP to the size aesthetic, we can more easily visualize how EAs vary in their access to service delivery providers.\n\n\nbf_ea2017 <- bf_ea %>%\n  filter(SAMPLE == 85405) # this sample corresponds to the 2017 wave\n\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = bf_ea2017, \n          aes(size = N_SDP),\n          alpha = 0.4) \n\n\n\n\nFrom the map, it looks like there may be a few locations where there EAs are both close together and served by many SDPs, which are likely in urban areas. For example, the capital of Burkina Faso, Ouagadougou, is in the center of the map where there are a number of EAs on top of each other. But, it’s a little hard to see the variation in size when there are so many values for N_SDP and so many EAs on top of each other. Let’s do two things to make this more readable. First, we’ll create smaller categories of the N_SDP variable, and second, we’ll map the URBAN variable to the color aesthetic.\n\n\nbf_ea2017 <- bf_ea2017 %>%\n  mutate(\n    N_SDP_CAT = case_when(\n      N_SDP <= 2 ~ 1,\n      N_SDP >2 & N_SDP <= 4 ~ 2,\n      N_SDP >4 ~ 3),\n    N_SDP_CAT = factor(N_SDP_CAT,\n                       levels = c(1, 2, 3),\n                       labels = c(\"Low\", \"Mid\", \"High\"),\n                       ordered = T), # needs to be an ORDERED factor to map to the size aesthetic\n    MEAN_OUTDAY = ifelse(is.na(MEAN_OUTDAY), 0, MEAN_OUTDAY),\n    URBAN = factor(URBAN, \n                   levels = c(0,1),\n                   labels = c(\"Rural\", \"Urban\"))\n  )\n\n\n# let's take a look at the distribution of this new categorical variable\nbf_ea2017 %>% \n  tab(URBAN, N_SDP_CAT)\n\n\nSimple feature collection with 5 features and 5 fields\ngeometry type:  MULTIPOINT\ndimension:      XY\nbbox:           xmin: -5.18865 ymin: 9.883331 xmax: 1.634087 ymax: 14.39679\ngeographic CRS: WGS 84\n# A tibble: 5 x 6\n  URBAN N_SDP_CAT     N                        geometry  prop cum_prop\n  <fct> <ord>     <int>                <MULTIPOINT [°]> <dbl>    <dbl>\n1 Rural Mid          29 ((-5.18865 11.54962), (-4.3401…  0.35     0.35\n2 Urban Mid          23 ((-5.178223 10.66001), (-4.781…  0.28     0.63\n3 Urban Low          15 ((-4.307564 11.18051), (-4.299…  0.18     0.81\n4 Rural Low          13 ((-4.969563 10.45619), (-4.804…  0.16     0.96\n5 Urban High          3 ((-4.262726 11.14746), (-1.528…  0.04     1   \n\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = bf_ea2017, \n          aes(size = N_SDP_CAT,\n              color = URBAN),\n          alpha = 0.4) +\n  scale_color_viridis_d() \n\n\n\n\nFrom this map we can see that urban areas are generally served by more SDPs – in fact, no rural EAs fall into the “High” category – although the difference is perhaps not as stark as one might have expected. But, what is the service environment like? Do urban areas have more stockouts than rural areas? Do SDPs in urban areas offer a greater selection of family planning methods? Did the service environment change between 2017 and 2018? Mapping can shed a lot of light on these questions.\nLet’s look at the NUM_METHODS_PROV variable created in the last post. This variable captures the number of family planning methods provided by at least one SDP that serves a given EA.\n\n\nbf_ea2017 %>%\n  tab(NUM_METHODS_PROV) %>%\n  arrange(NUM_METHODS_PROV)\n\n\nSimple feature collection with 5 features and 4 fields\ngeometry type:  MULTIPOINT\ndimension:      XY\nbbox:           xmin: -5.18865 ymin: 9.883331 xmax: 1.634087 ymax: 14.39679\ngeographic CRS: WGS 84\n# A tibble: 5 x 5\n  NUM_METHODS_PROV     N                       geometry  prop cum_prop\n             <dbl> <int>               <MULTIPOINT [°]> <dbl>    <dbl>\n1                8     2 ((-4.299839 11.18039), (-2.96…  0.02     1   \n2                9    13 ((-4.340111 11.8743), (-4.281…  0.16     0.92\n3               10    34 ((-5.18865 11.54962), (-4.969…  0.41     0.41\n4               11    29 ((-5.178223 10.66001), (-3.84…  0.35     0.76\n5               12     5 ((-2.757122 11.53829), (-2.26…  0.06     0.98\n\nSince there is not a large range of number of FP methods provided, let’s dichotomize this so we can map it to the shape aesthetic.\n\n\nbf_ea2017 <- bf_ea2017 %>%\n  mutate(\n    NUM_METHODS_CAT = case_when(\n      NUM_METHODS_PROV <= 9 ~ 1,\n      NUM_METHODS_PROV >9  ~ 2),\n    NUM_METHODS_CAT = factor(NUM_METHODS_CAT,\n                       levels = c(1, 2),\n                       labels = c(\"Low (<=9)\", \"High (>9)\"),\n                       ordered = T)\n  )\n\n\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = bf_ea2017, \n          aes(size = NUM_METHODS_CAT,\n              shape = URBAN,\n              color = NUM_METHODS_OUT3MO),\n          alpha = 0.4) +\n  scale_color_viridis_c() \n\n\n\n\nPutting it All Together\nNow we have a map that shows spatial variation in availability of different methods of family planning and prevalence of stock-outs, as well as demonstrates how these characteristic differ across urban vs. rural EAs. It’s super quick to make a basic map like this, but let’s clean up a few things to make it look nicer.\n\n\nggplot() +\n  geom_sf(data = bf_shp, fill = \"#f2f2f5\") +\n  geom_sf(data = bf_ea2017, \n          aes(size = NUM_METHODS_CAT,\n              shape = URBAN,\n              color = NUM_METHODS_OUT3MO),\n          alpha = 0.4) +\n  scale_color_viridis_c(direction = -1) + # reversing the direction makes the high #s stand out more\n  labs(title = \"Spatial Variation in Family Planning Service Environment\",\n       subtitle = \"Burkina Faso 2017\",\n       caption = \"Source: IPUMS PMA\",\n       shape = \"\",\n       size = \"Methods\\nProvided\",\n       color = \"Out of Stock\\n(Past 3 Months)\",\n       x = NULL,\n       y = NULL) +\n  theme_minimal() +\n  theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank())\n\n\n\n\n\n\n\nThis map suggests there is spatial correlation to the stockouts – with 2 regions responsible for the majority of EAs with stockouts. It also looks like these EAs tend to have more methods provided by the SDPs that serve them. Finally, let’s use both years of data and see if there is any temporal variation. To do this, we’ll use the original bf_ea dataset (instead of sdp2017) and re-create the same NUM_METHODS_CAT factor variable that dichotomizes the NUM_METHODS_PROV variable. Then, we’ll use facet_wrap() to make a multi-panel plot, with one panel per year.\n\n\nbf_ea <- bf_ea %>%\n  mutate(\n    NUM_METHODS_CAT = case_when(\n      NUM_METHODS_PROV <= 9 ~ 1,\n      NUM_METHODS_PROV >9  ~ 2),\n    NUM_METHODS_CAT = factor(NUM_METHODS_CAT,\n                       levels = c(1, 2),\n                       labels = c(\"Low (<=9)\", \"High (>9)\"),\n                       ordered = T),\n    YEAR = case_when(\n      SAMPLE == 85405 ~ 2017,\n      SAMPLE == 85408 ~ 2018\n    ),\n    URBAN = factor(URBAN, \n                   levels = c(0,1),\n                   labels = c(\"Urban\", \"Rural\"))\n  )\n\nggplot() +\n  geom_sf(data = bf_shp, fill = \"#f2f2f5\") +\n  geom_sf(data = bf_ea, \n          aes(size = NUM_METHODS_CAT,\n              shape = URBAN,\n              color = NUM_METHODS_OUT3MO),\n          alpha = 0.4) +\n  facet_wrap(~ YEAR) +\n  # reversing the direction makes the high #s stand out more\n  scale_color_viridis_c(direction = -1) + \n  guides(color = guide_colorbar(barheight = .75,\n                                barwidth = 4.5,\n                                label.position = \"top\",\n                                label.hjust = 0)) + \n  labs(title = \"Spatial Variation in Family Planning Service Environment\",\n       subtitle = \"Burkina Faso 2017-2018\",\n       caption = \"Source: IPUMS PMA\",\n       shape = \"\",\n       size = \"Methods\\nProvided\",\n       color = \"# Methods\\nOut of Stock\\n(Past 3 Months)\",\n       x = NULL,\n       y = NULL) +\n  theme_minimal() +\n  theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    legend.title = element_text(size = 8),\n    legend.position = \"bottom\") \n\n\n\n\nWith the 2018 data included, it looks like the service environment may have improved between 2017 and 2018 with fewer stockouts. However, it also looks the EAs that faced more stockouts in 2017 are not always the same as those facing stockouts in 2018. But, there is still a spatial pattern to the stockouts in 2018. It also looks like some EAs had fewer family planning methods available from SDPs in 2018 than in 2017, specifically in the western part of the country.\nFuture posts may explore other supply-side factors that could influence the SDPs (and look at how these change over time) or examine demand-side factors by merging in the individual-level data.\nAs always, let us know what kinds of questions about fertility and family planning you’re answering – especially if you’re doing anything spatial!\n\n\n\n",
    "preview": "posts/2021-01-29-mapping-sdp-variables/images/bf_fp_map.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 2100,
    "preview_height": 1350
  },
  {
    "path": "posts/2021-01-28-summarize-by-easerved/",
    "title": "Merging Service Delivery Point Data to Household & Female Records",
    "description": "Create aggregate measures for women living the areas served by SDPs",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-29",
    "categories": [
      "Individuals in Context",
      "Data Manipulation",
      "Service Delivery Points",
      "pivot_longer",
      "join"
    ],
    "contents": "\n\nContents\nReviewing SDP Sample Design\nSetup: Create and Load a Data Extract\nEAID and EASERVED\nPivot Longer: EASERVED in Rows\nSummarise by EASERVED and SAMPLE\nMerging to Household and Female Data\n\n\n\n\nWelcome to the third post in a series all about using PMA Service Delivery Point (SDP) data to better understand Individuals in Context. In our last post, we discussed a few of the variable groups related to contraceptive availability, and we showed how to use functions like dplyr::across to recode and summarize these variable groups in preparation for merging with Household and Female data.\nBefore we dive in, let’s quickly revisit the geographic sampling units - or enumeration areas - we’ll be using to link SDPs with their counterparts in the Household and Female data.\nReviewing SDP Sample Design\nRemember: the SDP sample design selects facilities meant to reflect the health service environment experienced by individuals included in Household and Female samples. If you were designing survey with this goal in mind, how would you select facilities?\nWell, you might target a sample of facilities located within the same geographic sampling units PMA used to define Household and Female samples from the same country in the same year. Presumably, the health services available to a woman living in enumeration area X would be captured pretty well if we surveyed a list of facilities also located in enumeration area X.\nBut what happens if a lot of women living in enumeration area X travel to enumeration area Y to receive family planning services? In that case, you’d want to know as much as possible about the service catchment areas for facilities in that country. Then, you could select facilities based on whether they provide services to enumeration area X, rather than relying simply to those that are located there.\nIn fact, PMA partners with government health agencies to obtain information about the service catchment area for all of the public-sector health facilities in each participating country. As a result, public SDPs are sampled if one of the enumeration areas used in a corresponding Household and Female sample appears in their service catchment area.\nBecause service catchment data are only available for public facilities, PMA uses a different method to select private-sector facilities. A private facility will be selected for a SDP sample only if it is located inside the boundaries of an enumeration area included in a corresponding Household and Female sample.\nSetup: Create and Load a Data Extract\nLet’s take a look at an example SDP dataset to see how all of this information gets reported. We’ll use the same data we highlighted in our last post, which includes facilities sampled from Burkina Faso in 2017 and 2018. First, load the following packages into R:\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\n\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nAgain in this post, we’ll be working with all of the available contraceptive services and stock variables ending with the suffixes PROV, OBS, OUT3MO, and OUTDAY. We’ll also add the variable group EASERVED, which - as we’ll see - stores information about the service catchment area for facilities where that information was available. Finally, we’ll add a few more variables that we’ll explore a bit later: AUTHORITY, FACILITYTYPE, and FACILITYTYPEGEN.\nWe’ll first load the data using ipumsr::read_ipums_micro:\n\n\nsdp <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00008.xml\",\n  data = \"data/pma_00008.dat.gz\") \n\n\n\n\nRemember: change these file paths to match your own data extract!\nThen, following the steps outlined in our last post, we’ll apply a couple of recoding functions from ipumsr.\n\n\nsdp <- sdp %>% \n  select(-EASERVED) %>% # error from extract system\n  mutate(\n    across(ends_with(\"OBS\"), ~lbl_relabel(\n      .x,\n      lbl(1, \"in stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    )),\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"Not interviewed (SDP questionnaire)\",\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    ))\n  )\n\n\n\nEAID and EASERVED\nFor the moment, let’s just take a look at the basic structure of our data, selecting only the variables FACILITYID, SAMPLE, AUTHORITY, CONSENTSQ, and EAID. For this preview, we’ll also arrange the data in ascending order of FACILITYID and SAMPLE:\nFACILITYID, SAMPLE, CONSENTSQ, and EAID are automatically included in every SDP data extract.\n\n\nsdp %>% \n  select(FACILITYID, SAMPLE, AUTHORITY, CONSENTSQ, EAID) %>% \n  arrange(FACILITYID, SAMPLE)\n\n\n# A tibble: 234 x 5\n   FACILITYID                      SAMPLE    AUTHORITY CONSENTSQ  EAID\n    <int+lbl>                   <int+lbl>    <int+lbl> <int+lbl> <dbl>\n 1       7006 85405 [Burkina Faso 2017 R… 1 [Governme…   1 [Yes]  7390\n 2       7006 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7390\n 3       7027 85405 [Burkina Faso 2017 R… 1 [Governme…   1 [Yes]  7332\n 4       7027 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7332\n 5       7029 85405 [Burkina Faso 2017 R… 4 [Private]    1 [Yes]  7111\n 6       7029 85408 [Burkina Faso 2018 R… 4 [Private]    0 [No]   7111\n 7       7036 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7412\n 8       7046 85408 [Burkina Faso 2018 R… 4 [Private]    1 [Yes]  7798\n 9       7048 85405 [Burkina Faso 2017 R… 1 [Governme…   1 [Yes]  7009\n10       7051 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7798\n# … with 224 more rows\n\nEach row in our data represents one facility from one sample. Notice that some - but not all - facilities appear once in sample 85405 (from 2017), and again in sample 85408 (from 2018).\nThe variable AUTHORITY shows the managing authority for each facility. Following the discussion above, we’ll expect to find information about the service catchment area for each facility where the managing authority is 1 - Government.\nAlso notice CONSENTSQ, which indicates whether a respondent at each facility consented to be interviewed. When you first obtain a data extract, you should expect most variables to be marked Not interviewed (SDP questionnaire) for facilities where CONSENTSQ shows 0 - No. However, we’ve already taken the extra step of marking all non-response values NA: we should now expect to see NA substituted for Not interviewed (SDP questionnaire).\nLastly, take particular note of the variable EAID: in SDP data, EAID shows the identification code associated with the enumeration area where a facility is located.\nWe’ll find information about the service catchment area for each facility in a different set of variables, each starting with with prefix EASERVED:\n\n\nsdp %>% \n  select(starts_with(\"EASERVED\")) \n\n\n# A tibble: 234 x 18\n   EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n   <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl>\n 1      7380      7323      7491      7605      7142      7279\n 2      7879      7516      7111      7554      7934      7791\n 3      7483        NA        NA        NA        NA        NA\n 4      7185        NA        NA        NA        NA        NA\n 5      7725      7859      7472      7175        NA        NA\n 6      7082        NA        NA        NA        NA        NA\n 7      7650        NA        NA        NA        NA        NA\n 8      7955        NA        NA        NA        NA        NA\n 9      7323        NA        NA        NA        NA        NA\n10      7774        NA        NA        NA        NA        NA\n# … with 224 more rows, and 12 more variables: EASERVED7 <int+lbl>,\n#   EASERVED8 <int+lbl>, EASERVED9 <int+lbl>, EASERVED10 <int+lbl>,\n#   EASERVED11 <int+lbl>, …\n\nYou’ll notice that our extract contains 18 EASERVED variables. Why 18? If you created your own data extract, you’ll remember that you only selected one variable called EASERVED: once you’ve selected samples, the IPUMS extract system automatically determines the correct number of EASERVED variables for your dataset based on the facility with the largest service catchment list.\n\nSome samples include facilities serving as many as 42 enumeration areas, requiring 42 EASERVED variables!\nAs we’ve discussed, PMA only receives service catchment information about public-sector facilities. In their case, each EASERVED variable contains an ID code for one of the enumeration areas in its service catchment list, or else it’s NA. We’ll look at these public-sector facilities first:\n\n\nsdp %>% count(AUTHORITY)\n\n\n# A tibble: 3 x 2\n        AUTHORITY     n\n*       <int+lbl> <int>\n1 1 [Government]    202\n2 3 [Faith-based]     3\n3 4 [Private]        29\n\nThe vast majority of SDPs in our sample are public-sector facilities. They comprise 202 of the 234 facilities in our sample.\n\n\nsdp %>% \n  filter(AUTHORITY == 1) %>% \n  select(starts_with(\"EASERVED\")) \n\n\n# A tibble: 202 x 18\n   EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n   <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl>\n 1      7380      7323      7491      7605      7142      7279\n 2      7879      7516      7111      7554      7934      7791\n 3      7483        NA        NA        NA        NA        NA\n 4      7185        NA        NA        NA        NA        NA\n 5      7725      7859      7472      7175        NA        NA\n 6      7082        NA        NA        NA        NA        NA\n 7      7650        NA        NA        NA        NA        NA\n 8      7955        NA        NA        NA        NA        NA\n 9      7323        NA        NA        NA        NA        NA\n10      7774        NA        NA        NA        NA        NA\n# … with 192 more rows, and 12 more variables: EASERVED7 <int+lbl>,\n#   EASERVED8 <int+lbl>, EASERVED9 <int+lbl>, EASERVED10 <int+lbl>,\n#   EASERVED11 <int+lbl>, …\n\nUsing two of the dplyr functions discussed in our last post - summarize and across - we’ll get a better sense of the catchment areas for our public-sector SDPs. Let’s see how many missing values exist for each of these EASERVED variables:\ndplyr is included when you load library(tidyverse)\n\n\nsdp %>% \n  filter(AUTHORITY == 1) %>% \n  summarise(across(starts_with(\"EASERVED\"), ~sum(is.na(.x))))\n\n\n# A tibble: 1 x 18\n  EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n      <int>     <int>     <int>     <int>     <int>     <int>\n1         0       156       173       181       190       192\n# … with 12 more variables: EASERVED7 <int>, EASERVED8 <int>,\n#   EASERVED9 <int>, EASERVED10 <int>, EASERVED11 <int>, …\n\nWe see that every public facility serves at least one enumeration area (there are no missing values for EASERVED1). However, there are 156 missing values for EASERVED2, which tells us that 156 public facilities only serve one enumeration area. Likewise: 173 facilities serve 2 enumeration areas or fewer, 181 serve 3 or fewer, and so forth.\nWhat about the 32 non-public facilities?\n\n\nsdp %>% \n  filter(AUTHORITY != 1) %>% \n  summarise(across(starts_with(\"EASERVED\"), ~sum(is.na(.x))))\n\n\n# A tibble: 1 x 18\n  EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n      <int>     <int>     <int>     <int>     <int>     <int>\n1         4        32        32        32        32        32\n# … with 12 more variables: EASERVED7 <int>, EASERVED8 <int>,\n#   EASERVED9 <int>, EASERVED10 <int>, EASERVED11 <int>, …\n\nPMA receives no information about the service catchment areas for these facilities, so - as you might expect - there are 32 missing values for EASERVED2 onward. Note, however, that there are only 4 missing values for EASERVED1: for non-public facilities, EASERVED1 usually contains that same enumeration area code shown in EAID (this is the enumeration area where the facility is, itself, located).\nThe exception to this rule comes from facilities where CONSENTSQ shows that no respondent provided consent to be interviewed. If we’d like, we can copy EAID to EASERVED1 for these facilities using dplyr::case_when:\n\n\nsdp <- sdp %>% \n  mutate(EASERVED1 = case_when(\n    is.na(EASERVED1) ~ EAID,\n    T ~ as.double(EASERVED1)\n  ))\n\n\n\n\nWe coerce EASERVED1 as a double, matching the class provided by EAID.\nNow, every SDP has at least one enumeration area included in the EASERVED group. This will be important in our next step, where we’ll see how to summarize the SDP data by groups of facilities serving the same enumeration area.\nPivot Longer: EASERVED in Rows\nNow that we’re familiar with EASERVED variables, let’s take a look at the kinds of summary statistics we might want to construct from variables related to contraceptive service availability. For example, consider EMRGPROV, which indicates whether a facility provides emergency contraceptives to clients.\nRemember that, right now, each row of our SDP dataset represents responses from one facility per sample. We’ll ultimately want to count the number of facilities providing emergency contraceptives to clients in each enumeration area, so we should use the tidyr function pivot_longer to reshape the data in a way that repeats each facility’s response to EMRGPROV once for every enumeration area that it serves.\ntidyr is included when you load library(tidyverse)\nTake, for example, the first 5 facilities in our dataset: for now, let’s just look at the first two EASERVED variables, along with each facility’s FACILITYID, EAID, and EMRGPROV response:\n\n\nsdp %>% \n  slice(1:5) %>% \n   select(FACILITYID, EASERVED1, EASERVED2, EMRGPROV)\n\n\n# A tibble: 5 x 4\n  FACILITYID EASERVED1 EASERVED2  EMRGPROV\n   <int+lbl>     <dbl> <int+lbl> <int+lbl>\n1       7250      7380      7323   0 [No] \n2       7399      7879      7516   0 [No] \n3       7506      7483        NA   0 [No] \n4       7982      7185        NA   0 [No] \n5       7065      7725      7859   1 [Yes]\n\nAmong these 5 facilities, only facility 7065 provides emergency contraceptives. This facility happens to provide services to 2 enumeration areas: 7725 and 7859. When we use pivot_longer, we’ll reshape the data to emphasize a different conclusion: our example shows two enumeration areas where individuals can access emergency contraceptives. We convey this information by placing each enumeration area from EASERVED1 or EASERVED2 in its own row:\n\n\nsdp %>% \n  slice(1:5) %>% \n  select(FACILITYID, EASERVED1, EASERVED2, EMRGPROV) %>% \n  pivot_longer(\n    cols = starts_with(\"EASERVED\"),\n    values_to = \"EASERVED\",\n    names_to = NULL\n  )\n\n\n# A tibble: 10 x 3\n   FACILITYID  EMRGPROV  EASERVED\n    <int+lbl> <int+lbl> <dbl+lbl>\n 1       7250   0 [No]       7380\n 2       7250   0 [No]       7323\n 3       7399   0 [No]       7879\n 4       7399   0 [No]       7516\n 5       7506   0 [No]       7483\n 6       7506   0 [No]         NA\n 7       7982   0 [No]       7185\n 8       7982   0 [No]         NA\n 9       7065   1 [Yes]      7725\n10       7065   1 [Yes]      7859\n\n\nHere, values_to gives the name of a new column where we store the values. If we wanted, we could use names_to to create another column storing the original variable names (EASERVED1 and EASERVED2) for each value.\nNow, we find that each of the values previously stored in EASERVED1 and EASERVED2 appear in a new column, EASERVED. Each facility occupies two rows: one for each of the enumeration areas that it serves.\nWhat about the rows where EASERVED contains NA? These rows are meaningless: we’re repeating each facility’s response to EMRGPROV twice to represent two enumeration areas, but facilities 7506 and 7982 only serve one enumeration area apiece. We should include the argument values_drop_na = T to drop these rows when we use pivot_longer():\n\n\nsdp %>% \n  slice(1:5) %>% \n  select(FACILITYID, EASERVED1, EASERVED2, EMRGPROV) %>% \n  pivot_longer(\n    cols = starts_with(\"EASERVED\"),\n    values_to = \"EASERVED\",\n    names_to = NULL,\n    values_drop_na = T\n  )\n\n\n# A tibble: 8 x 3\n  FACILITYID  EMRGPROV  EASERVED\n   <int+lbl> <int+lbl> <dbl+lbl>\n1       7250   0 [No]       7380\n2       7250   0 [No]       7323\n3       7399   0 [No]       7879\n4       7399   0 [No]       7516\n5       7506   0 [No]       7483\n6       7982   0 [No]       7185\n7       7065   1 [Yes]      7725\n8       7065   1 [Yes]      7859\n\nNow that we know how to pivot_longer, let’s apply the function to our full dataset:\n\n\nsdp <- sdp %>%\n  pivot_longer(\n    cols = starts_with(\"EASERVED\"),\n    values_to = \"EASERVED\",\n    values_drop_na = T,\n    names_to = NULL\n  ) %>%\n  distinct() # in case any facility listed the same EASERVED twice\n\n\n\nDropping each row where EASERVED is missing, we’re left with 372 rows where information about each SDP gets repeated once for every enumeration area that it serves. (Remember: our original dataset contained only 234 rows because SDPs occupied just one row apiece).\n\n\nsdp %>% select(FACILITYID, EASERVED, everything())\n\n\n# A tibble: 372 x 58\n   FACILITYID EASERVED      SAMPLE COUNTRY  YEAR ROUND  EAID CONSENTSQ\n    <int+lbl> <dbl+lb>   <int+lbl> <int+l> <int> <dbl> <dbl> <int+lbl>\n 1       7250     7380 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 2       7250     7323 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 3       7250     7491 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 4       7250     7605 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 5       7250     7142 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 6       7250     7279 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 7       7250     7370 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 8       7250     7725 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 9       7250     7811 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n10       7250     7859 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n# … with 362 more rows, and 50 more variables: STRATA <int+lbl>,\n#   FACILITYTYPE <int+lbl>, FACILITYTYPEGEN <int+lbl>,\n#   AUTHORITY <int+lbl>, CONPROV <int+lbl>, …\n\nSummarise by EASERVED and SAMPLE\nNow that we’ve reshaped our data, we’ll be able to create some simple summary statistics about each of the enumeration areas served by the facilities in our sample. First, let’s group_by(EASERVED, SAMPLE) and count() the number of facilities providing services to each enumeration area in each of our samples:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\", \n    N_SDP = n()\n  )\n\n\n# A tibble: 149 x 3\n# Groups:   EASERVED, SAMPLE [149]\n    EASERVED                            SAMPLE N_SDP\n   <dbl+lbl>                         <int+lbl> <int>\n 1      7003 85405 [Burkina Faso 2017 Round 5]     3\n 2      7003 85408 [Burkina Faso 2018 Round 6]     2\n 3      7006 85405 [Burkina Faso 2017 Round 5]     2\n 4      7006 85408 [Burkina Faso 2018 Round 6]     1\n 5      7009 85405 [Burkina Faso 2017 Round 5]     3\n 6      7009 85408 [Burkina Faso 2018 Round 6]     2\n 7      7016 85405 [Burkina Faso 2017 Round 5]     3\n 8      7016 85408 [Burkina Faso 2018 Round 6]     2\n 9      7026 85405 [Burkina Faso 2017 Round 5]     3\n10      7026 85408 [Burkina Faso 2018 Round 6]     3\n# … with 139 more rows\n\nContinuing with the variable EMRGPROV, we can now also count the number of sampled facilities providing emergency contraception to each EASERVED:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    N_EMRGPROV = sum(EMRGPROV)\n  )\n\n\n# A tibble: 149 x 3\n# Groups:   EASERVED, SAMPLE [149]\n    EASERVED                            SAMPLE N_EMRGPROV\n   <dbl+lbl>                         <int+lbl>      <int>\n 1      7003 85405 [Burkina Faso 2017 Round 5]          0\n 2      7003 85408 [Burkina Faso 2018 Round 6]          0\n 3      7006 85405 [Burkina Faso 2017 Round 5]          0\n 4      7006 85408 [Burkina Faso 2018 Round 6]          0\n 5      7009 85405 [Burkina Faso 2017 Round 5]          2\n 6      7009 85408 [Burkina Faso 2018 Round 6]          1\n 7      7016 85405 [Burkina Faso 2017 Round 5]          0\n 8      7016 85408 [Burkina Faso 2018 Round 6]          0\n 9      7026 85405 [Burkina Faso 2017 Round 5]          0\n10      7026 85408 [Burkina Faso 2018 Round 6]          0\n# … with 139 more rows\n\nWhat if we want to include a count of the facilities providing each of the different contraceptive methods in our data? Building on a technique showcased in our last post, we could use dplyr::across to iterate over all variables ending with the suffix PROV:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~sum(.x), .names = \"N_{.col}\")\n  ) \n\n\n# A tibble: 149 x 15\n# Groups:   EASERVED, SAMPLE [149]\n   EASERVED      SAMPLE N_CONPROV N_CYCBPROV N_DEPOPROV N_DIAPROV\n   <dbl+lb>   <int+lbl>     <int>      <int>      <int>     <int>\n 1     7003 85405 [Bur…         3          3          3         0\n 2     7003 85408 [Bur…         2          2          2         0\n 3     7006 85405 [Bur…         2          2          2         0\n 4     7006 85408 [Bur…         1          1          1         0\n 5     7009 85405 [Bur…         3          1          3         0\n 6     7009 85408 [Bur…         2          2          2         0\n 7     7016 85405 [Bur…         3          3          3         0\n 8     7016 85408 [Bur…         2          1          2         0\n 9     7026 85405 [Bur…         3          3          3         0\n10     7026 85408 [Bur…         3          3          3         0\n# … with 139 more rows, and 9 more variables: N_EMRGPROV <int>,\n#   N_FCPROV <int>, N_FSTPROV <int>, N_FJPROV <int>, N_IMPPROV <int>,\n#   …\n\nWe’ll reduce this information even further, creating a variable NUM_METHODS_PROV indicating the number of methods provided by at least one sampled facility:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~sum(.x), .names = \"N_{.col}\")\n  ) %>% \n  transmute(\n    EASERVED,\n    SAMPLE,\n    NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")) > 0, na.rm = T)\n  )\n\n\n# A tibble: 149 x 3\n# Groups:   EASERVED, SAMPLE [149]\n    EASERVED                            SAMPLE NUM_METHODS_PROV\n   <dbl+lbl>                         <int+lbl>            <int>\n 1      7003 85405 [Burkina Faso 2017 Round 5]               10\n 2      7003 85408 [Burkina Faso 2018 Round 6]                8\n 3      7006 85405 [Burkina Faso 2017 Round 5]               10\n 4      7006 85408 [Burkina Faso 2018 Round 6]                8\n 5      7009 85405 [Burkina Faso 2017 Round 5]                9\n 6      7009 85408 [Burkina Faso 2018 Round 6]               10\n 7      7016 85405 [Burkina Faso 2017 Round 5]                9\n 8      7016 85408 [Burkina Faso 2018 Round 6]                8\n 9      7026 85405 [Burkina Faso 2017 Round 5]               10\n10      7026 85408 [Burkina Faso 2018 Round 6]               10\n# … with 139 more rows\n\nIn our last post, we introduced 4 variable groups related to the availability of different contraceptive methods. We’ll now create a summary variable for each one, and then show how to attach our new variables to a Household and Female dataset:\nN_SDP - number of SDPs\nNUM_METHODS_PROV - number of methods provided by at least one SDP\nNUM_METHODS_INSTOCK - number of methods in-stock with at least one SDP\nNUM_METHODS_OUT3MO - number of methods out of stock in the last 3 months with at least one SDP\nMEAN_OUTDAY - the mean length of a stockout for all out of stock methods\n\n\nsdp <- sdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    N_SDP = n(),\n    across(ends_with(\"PROV\"), ~sum(.x, na.rm = T), .names = \"N_{.col}\"),\n    across(ends_with(\"OBS\"), ~sum(.x, na.rm = T), .names = \"N_{.col}\"),\n    across(ends_with(\"OUT3MO\"), ~sum(.x, na.rm = T), .names = \"N_{.col}\"),\n    across(ends_with(\"OUTDAY\"), ~mean(.x, na.rm = T), .names = \"N_{.col}\"),\n  ) %>% \n  transmute(\n    EASERVED,\n    SAMPLE,\n    N_SDP,\n    NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")) > 0, na.rm = T),\n    NUM_METHODS_INSTOCK = sum(c_across(ends_with(\"OBS\")) > 0, na.rm = T),\n    NUM_METHODS_OUT3MO = sum(c_across(ends_with(\"OUT3MO\")) > 0, na.rm = T),\n    MEAN_OUTDAY = mean(c_across(ends_with(\"OUTDAY\")), na.rm = T)\n  ) %>% \n  ungroup()\n\n\n\nMerging to Household and Female Data\nConsider the following female respondent dataset collected from Burkina Faso in 2017 and 2018. It contains a variable FPCURRUSE indicating whether the woman is currently using a method of family planning:\n\n\nhhf <- read_ipums_micro(\n  ddi = \"data/pma_00011.xml\",\n  data = \"data/pma_00011.dat.gz\"\n) %>% \n  select(PERSONID, EAID, URBAN, SAMPLE, FPCURRUSE) %>% \n  mutate(\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    ))\n  )\n\n\n\n\n\nhhf\n\n\n# A tibble: 6,944 x 5\n   PERSONID           EAID    URBAN                   SAMPLE FPCURRUSE\n   <chr>             <dbl> <int+lb>                <int+lbl> <int+lbl>\n 1 0762000000029022…  7620 1 [Urba… 85405 [Burkina Faso 201…  NA      \n 2 0735800000017142…  7358 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 3 0710400000020992…  7104 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 4 0704800000014092…  7048 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n 5 0715600000020782…  7156 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 6 0727900000021452…  7279 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n 7 0743100000024642…  7431 0 [Rura… 85405 [Burkina Faso 201…   1 [Yes]\n 8 0721200000025792…  7212 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 9 0704200000014542…  7042 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n10 0797200000013032…  7972 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n# … with 6,934 more rows\n\nYou’ll notice that each row represents one female respondent with a unique PERSONID (non-respondents and other household members have been removed beforehand). We’ve also got EAID, which represents the enumeration area where each respondent resides; the variable URBAN indicates whether the enumeration area is primarily “urban” or “rural”.\nThe variable SAMPLE contains the same values seen in our SDP data:\n85405 - Burkina Faso 2017 Round 5\n85408 - Burkina Faso 2018 Round 6\nWhen we merge, we’ll want to match each woman to both a SAMPLE and an EASERVED from the SDP data. We’ll rename EASERVED to match the variable EAID in the HHF data:\n\n\nbf_merged <- sdp %>% \n  rename(EAID = EASERVED) %>% \n  right_join(hhf, by = c(\"EAID\", \"SAMPLE\"))\n\n\n\nNow, each woman’s record contains all of the variables we created above summarizing the SDPs that serve her enumeration area. For example, for all sampled women living in EAID == 7003 in 2017, the value in NUM_METHODS_OUT3MO shows the number of family planning methods that were out of stock with any SDP serving the woman’s enumeration area within three months prior to the survey:\n\n\nbf_merged %>% \n  filter(EAID == 7003, SAMPLE == 85405) %>% \n  select(PERSONID, EAID, SAMPLE, NUM_METHODS_OUT3MO)\n\n\n# A tibble: 55 x 4\n   PERSONID             EAID                  SAMPLE NUM_METHODS_OUT3…\n   <chr>            <dbl+lb>               <int+lbl>             <int>\n 1 070030000001973…     7003 85405 [Burkina Faso 20…                 0\n 2 070030000001973…     7003 85405 [Burkina Faso 20…                 0\n 3 070030000002640…     7003 85405 [Burkina Faso 20…                 0\n 4 070030000001075…     7003 85405 [Burkina Faso 20…                 0\n 5 070030000001609…     7003 85405 [Burkina Faso 20…                 0\n 6 070030000000835…     7003 85405 [Burkina Faso 20…                 0\n 7 070030000001273…     7003 85405 [Burkina Faso 20…                 0\n 8 070030000000527…     7003 85405 [Burkina Faso 20…                 0\n 9 070030000002561…     7003 85405 [Burkina Faso 20…                 0\n10 070030000002391…     7003 85405 [Burkina Faso 20…                 0\n# … with 45 more rows\n\nYou’ll notice that 55 women were surveyed in EAID 7003 in 2017, and each one has the same value (0) for NUM_METHODS_OUT3MO.\nWe’ll dig deeper into the types of research questions that our new combined dataset can answer in our upcoming Data Analysis post. For now, take a look at the apparent relationship between FPCURRUSE and NUM_METHODS_OUT3MO for all of the women with non-missing responses for both variables:\n\n\nbf_merged %>% \n  filter(!is.na(FPCURRUSE) & !is.na(NUM_METHODS_OUT3MO)) %>% \n  group_by(NUM_METHODS_OUT3MO > 0) %>% \n  count(FPCURRUSE) %>% \n  mutate(pct = n/sum(n))\n\n\n# A tibble: 4 x 4\n# Groups:   NUM_METHODS_OUT3MO > 0 [2]\n  `NUM_METHODS_OUT3MO > 0` FPCURRUSE     n   pct\n  <lgl>                    <int+lbl> <int> <dbl>\n1 FALSE                      0 [No]   2721 0.648\n2 FALSE                      1 [Yes]  1475 0.352\n3 TRUE                       0 [No]   1124 0.700\n4 TRUE                       1 [Yes]   482 0.300\n\nNotably, among those respondents living in an enumeration area that experienced zero stockouts within the 3 months prior to the SDP survey, 35% indicated that they were actively using a family planning method. Compare that to the set of respondents living in an area where at least one method was out of stock during the same time period: only 30% were using a family planning method.\nWhile a 5% difference may or may not prove to be statistically significant under further analysis, it’s not entirely surprising that the reliable availability of contraceptive methods from service providers might influence the contraceptive prevalence rate among women in a given area.\nAs always, let us know what kinds of questions about fertility and family planning you’re answering with data merged from service providers!\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-14T09:23:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-28-across-sdp/",
    "title": "Recode and Summarize Variables from Multiple Response Questions",
    "description": "Use dplyr::across to summarize variables with a similar naming pattern.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-28",
    "categories": [
      "Individuals in Context",
      "Data Manipulation",
      "Service Delivery Points",
      "across",
      "ipumsr"
    ],
    "contents": "\n\nContents\nSDP Multiple Response Questions\nSetup: Load an Example Dataset into R\nRecoding Variables with ipumsr\nIntroducing dplyr::across\nSummarize Variable Groups by Facility\nSummarize Variable Groups by EAID\n\n\n\n\nIn our last post, we introduced PMA Service Delivery Point (SDP) data as an important resource for understanding the health services environment experienced by individuals sampled in PMA Household and Female data. For our second post in this Individuals in Context series, we’ll now take deeper dive into one of the important topics in SDP data: the range and availability of contraceptive methods provided at each facility.\nA common feature of the variables in this topic - and in many other topics - is that you’ll find several binary indicators constructed from the same multiple resposne item on the SDP questionnaire. We’ll see that IPUMS PMA uses a common naming convention to help users group these variables together for use in functions like dplyr::across.\nSDP Multiple Response Questions\nEvery SDP respondent receives a question associated with the variable FPOFFERED, which indicates whether to facility usually offers family planning services or products:\nDo you usually offer family planning services / products?\n\n  [] Yes\n  [] No\n  [] No response\nIf yes, they’ll then receive a multiple response-type question asking about the contraceptive methods provided to clients. The range of options provided on the questionnaire may vary across samples, but most look something like this:\nWhich of the following methods are provided to clients at this facility? \n\n  [] Female sterilization\n  [] Male sterilization\n  [] Implant\n  [] IUD\n  [] Injectables - Depo Provera\n  [] Injectables - Sayana Press\n  [] Pill\n  [] Emergency Contraception\n  [] Male Condom\n  [] Female Condom\n  [] Diaphragm\n  [] Foam/Jelly\n  [] Std. Days / Cycle beads\n  [] None of the above\n  [] No response\n\nIf the response to FPOFFERED was not “Yes”, this question will be skipped and marked “NIU (not in universe)”.\nThis is a multiple response question: each method in the list could be answered individually (Yes or No), or the respondent could reply None of the above or provide No response. The IPUMS PMA extract system generates one variable for each of the methods in the list:\nFSTPROV\nMSTPROV\nIMPPROV\nIUDPROV\nDEPOPROV\nSAYPROV\nPILLPROV\nEMRGPROV\nCONPROV\nFCPROV\nDIAPROV\nFJPROV\nCYCBPROV\nThe questionnaire continues for each one of the methods provided at a given facility. Next, it checks for the current availability of each of the provided methods:\nYou mentioned that you typically provide the [METHOD] at this facility,\ncan you show it to me? If no, probe: Is the [METHOD] out of stock today?\n\n  [] In-stock and observed\n  [] In-stock but not observed\n  [] Out of stock\n  [] No Response\nThe variables associated with each response end with the same suffix OBS:\nIMPOBS\nIUDOBS\nDEPOOBS\nSAYOBS\nPILLOBS\nEMRGOBS\nCONOBS\nFCOBS\nDIAOBS\nFJOBS\nCYCBOBS\nSterilization methods were not included in this question.\nNote: if a given method was not provided at a facility, it would be skipped and marked “NIU (not in universe)”.\nYou can always visit a variable’s Universe tab for details.\nIf a facility did have a particular method in-stock, it received a question asking whether supplies were unavailable any time in the previous three months:\nHas the [METHOD] been out of stock at any time in the last 3 months?\n\n  [] Yes \n  [] No \n  [] Don't know\n  [] No response\nThis question becomes a series of variables ending with the suffix OUT3MO:\nIMPOUT3MO\nIUDOUT3MO\nDEPOOUT3MO\nSAYOUT3MO\nPILLOUT3MO\nEMRGOUT3MO\nCONOUT3MO\nFCOUT3MO\nDIAOUT3MO\nFJOUT3MO\nCYCBOUT3MO\nAgain, sterilization methods were not included in this question.\nNote: if a given method was not in-stock at a facility where it’s normally provided, it would be skipped and marked “NIU (not in universe)”.\nOn the other hand, if a facility that normally provides a given method did not have supplies in-stock during the interview, it received a different question about the duration of the current stockout:\nHow many days has the [METHOD] been out of stock?\n\n  Number of days____\nThe resulting variables - each ending with the suffix OUTDAY - take an integer value representing the stockout duration in days (except where the value is a non-response code, see below):\nIMPOUTDAY\nIUDOUTDAY\nDEPOOUTDAY\nSAYOUTDAY\nPILLOUTDAY\nEMRGOUTDAY\nCONOUTDAY\nFCOUTDAY\nDIAOUTDAY\nFJOUTDAY\nCYCBOUTDAY\nAgain, sterilization methods were not included in this question.\nNote: if a given method was in-stock at a facility where it’s normally provided, it would be skipped and marked “NIU (not in universe)”.\nSetup: Load an Example Dataset into R\nAs you can see, we’re left with quite a few variables from just these 4 questions! That’s very useful if you’re interested in the availability of one method, in particular, but what if you want to get a picture of the full range of methods provided at a particular facility?\nFortunately, the repeated use of variable suffixes (PROV, OBS, OUT3MO, and OUTDAY) make these variables highly suitable for column-wise processing with dplyr::across.\nLet’s start with an example data extract containing all of the variables listed above, collected from just two samples:\nBurkina Faso - 2018 R6\nBurkina Faso - 2017 R5\nOnce you’ve downloaded an extract, open RStudio and load the packages tidyverse and ipumsr:\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\n\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nNext, use the file paths for your data extract to load it into R:\n\n\nsdp <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00008.xml\",\n  data = \"data/pma_00008.dat.gz\"\n)\n\n\n\n\nRemember: change these file paths to match your own extract!\nUsing dplyr::ends_with, we’ll select only FACILITYID, SAMPLE, EAID, and the variables using one of the four suffixes PROV, OBS, OUT3MO, or OUTDAY.\n\n\nsdp <- sdp %>%  \n  select(\n    FACILITYID,\n    SAMPLE, \n    EAID, \n    ends_with(\"PROV\"),\n    ends_with(\"OBS\"),\n    ends_with(\"OUT3MO\"),\n    ends_with(\"OUTDAY\")\n  )\n\n\n\nThat leaves us with 234 rows - each a facility from one of our two samples - and 49 variables:\n\n\nsdp\n\n\n# A tibble: 234 x 49\n   FACILITYID          SAMPLE  EAID  CONPROV CYCBPROV DEPOPROV DIAPROV\n    <int+lbl>       <int+lbl> <dbl> <int+lb> <int+lb> <int+lb> <int+l>\n 1       7250 85405 [Burkina…  7142  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 2       7399 85405 [Burkina…  7879  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 3       7506 85405 [Burkina…  7483  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 4       7982 85405 [Burkina…  7185  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 5       7065 85405 [Burkina…  7859  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 6       7729 85405 [Burkina…  7082  1 [Yes]  0 [No]   1 [Yes]  0 [No]\n 7       7490 85405 [Burkina…  7650  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 8       7311 85405 [Burkina…  7955  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 9       7524 85405 [Burkina…  7323  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n10       7932 85405 [Burkina…  7774  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n# … with 224 more rows, and 42 more variables: EMRGPROV <int+lbl>,\n#   FCPROV <int+lbl>, FSTPROV <int+lbl>, FJPROV <int+lbl>,\n#   IMPPROV <int+lbl>, IUDPROV <int+lbl>, MSTPROV <int+lbl>,\n#   PILLPROV <int+lbl>, SAYPROV <int+lbl>, CONOBS <int+lbl>,\n#   CYCBOBS <int+lbl>, DEPOOBS <int+lbl>, DIAOBS <int+lbl>,\n#   EMRGOBS <int+lbl>, FCOBS <int+lbl>, FJOBS <int+lbl>,\n#   IMPOBS <int+lbl>, IUDOBS <int+lbl>, PILLOBS <int+lbl>,\n#   SAYOBS <int+lbl>, CONOUT3MO <int+lbl>, CYCBOUT3MO <int+lbl>,\n#   DEPOOUT3MO <int+lbl>, DIAOUT3MO <int+lbl>, EMRGOUT3MO <int+lbl>,\n#   FCOUT3MO <int+lbl>, FJOUT3MO <int+lbl>, IMPOUT3MO <int+lbl>,\n#   IUDOUT3MO <int+lbl>, PILLOUT3MO <int+lbl>, SAYOUT3MO <int+lbl>,\n#   CONOUTDAY <int+lbl>, CYCBOUTDAY <int+lbl>, DEPOOUTDAY <int+lbl>,\n#   DIAOUTDAY <int+lbl>, EMRGOUTDAY <int+lbl>, FCOUTDAY <int+lbl>,\n#   FJOUTDAY <int+lbl>, IMPOUTDAY <int+lbl>, IUDOUTDAY <int+lbl>,\n#   PILLOUTDAY <int+lbl>, SAYOUTDAY <int+lbl>\n\nRecoding Variables with ipumsr\nA key feature to remember about IPUMS PMA extracts is that variables often have value labels, which are text labels assigned to the different values taken by a variable. When we load the extract into R with an ipumsr function, these variables are imported as labelled objects rather than the more common factor class of objects.\nMore information on the difference between factors and IPUMS labelled variables.\nAs a result, IPUMS data users need to take some unusual steps when recoding a variable or handling NA values. Happily, the ipumsr package provide a few functions (starting with the prefix lbl_) that make this process very easy.\nLet’s take a look at the variable CONOBS:\n\n\nsdp %>% count(CONOBS)\n\n\n# A tibble: 5 x 2\n                                    CONOBS     n\n                                 <int+lbl> <int>\n1  1 [In-stock and observed]                 204\n2  2 [In-stock but not observed]               3\n3  3 [Out of stock]                            5\n4 94 [Not interviewed (SDP questionnaire)]     4\n5 99 [NIU (not in universe)]                  18\n\nNotice that we have two values representing SDPs with male condoms “in-stock”: SDPs where the interviewer personally observed the condoms get 1, while those where condoms where reported in-stock - but not actually observed by the interviewer - get 2.\nDepending on your research question, the interviewer’s personal observation of each method may or may not be important. You might decide that you’d prefer to recode this variable into a simple binary measure that could be easily plugged into a regression model as a dummy variable later on. To do that, you could use the ipumsr function lbl_relabel:\n\n\nsdp %>% \n  mutate(CONOBS = lbl_relabel(\n      CONOBS,\n      lbl(1, \"In-stock\") ~ .val %in% 1:2,\n      lbl(0, \"Out of stock\") ~ .val == 3\n    )) %>% \n  count(CONOBS)\n\n\n# A tibble: 4 x 2\n                                    CONOBS     n\n                                 <dbl+lbl> <int>\n1  0 [Out of stock]                            5\n2  1 [In-stock]                              207\n3 94 [Not interviewed (SDP questionnaire)]     4\n4 99 [NIU (not in universe)]                  18\n\n\n\n\n\n© 2017 (MPL 2.0)\nThat collapses the values 1 and 2 together, and it moves the value 3 (“Out of stock”) to 0. However, we’ve still got a the values 94 and 99, which are each a different type of non-response. The easiest strategy here would be to recode any value larger than 90 as NA, and we could do that with another ipumsr function, lbl_na_if:\n\n\nsdp %>% \n  mutate(\n    CONOBS = lbl_relabel(\n      CONOBS,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    ),\n    CONOBS = lbl_na_if(\n      CONOBS,\n      ~.val > 90\n    )\n  ) %>% \n  count(CONOBS)\n\n\n# A tibble: 3 x 2\n             CONOBS     n\n          <dbl+lbl> <int>\n1  0 [out of stock]     5\n2  1 [in-stock]       207\n3 NA                   22\n\nThis works great for our example variable, CONOBS. Unfortunately, though, we can’t always rely on the rule ~.val > 90 to handle missing responses. For variables like CONOUTDAY, a value above 90 could be a valid response: what if a facility experienced a stockout lasting 94 days? For this reason, the non-response values for CONOUTDAY are padded with additional digits:\n\n\nsdp %>% count(CONOUTDAY)\n\n\n# A tibble: 7 x 2\n                                   CONOUTDAY     n\n                                   <int+lbl> <int>\n1    1                                           1\n2    3                                           1\n3   10                                           1\n4   15                                           1\n5   60                                           1\n6 9994 [Not interviewed (SDP questionnaire)]     4\n7 9999 [NIU (not in universe)]                 225\n\nWe could write a different lbl_na_if function for our OUTDAY variables, but ipumsr provides a much nicer workaround: we can specify non-response labels rather than values, as long as we make sure to use all of the different non-response labels appearing throughout our dataset:\n\n\nsdp %>% \n  mutate(\n    CONOBS = lbl_relabel(\n      CONOBS,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    ),\n    CONOBS = lbl_na_if(\n      CONOBS,\n      ~.lbl %in% c(\n        \"Not interviewed (SDP questionnaire)\",\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    )\n  ) %>% \n  count(CONOBS)\n\n\n# A tibble: 3 x 2\n             CONOBS     n\n          <dbl+lbl> <int>\n1  0 [out of stock]     5\n2  1 [in-stock]       207\n3 NA                   22\n\nNow, we’ll be able to recode all of our variables with the same pair of functions! To do that, we’ll first need to take a look at the column-wise workflow made available by dplyr::across.\nIntroducing dplyr::across\nWhile there are several ways to apply a function across a set of variables in R, the simplest method comes from a new addition to the dplyr package that’s loaded when you run library(tidyverse). The function dplyr::across takes two arguments: a function, and a selection of columns where you want that function to be applied.\ndplyr is included when you load library(tidyverse)\nRemember that we want collapse the values 1 - In-stock and observed and 2 - In-stock but not observed for all of the variables ending with OBS, not just CONOBS. Using across and a selection of variables ending with OBS, we’ll apply the same lbl_relabel function we used on CONOBS above:\n\n\nsdp %>% \n  mutate(\n    across(ends_with(\"OBS\"), ~lbl_relabel(\n      .x,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    )) \n  ) %>% \n  count(CONOBS)\n\n\n# A tibble: 4 x 2\n                                    CONOBS     n\n                                 <dbl+lbl> <int>\n1  0 [out of stock]                            5\n2  1 [in-stock]                              207\n3 94 [Not interviewed (SDP questionnaire)]     4\n4 99 [NIU (not in universe)]                  18\n\n\n\n\n\n© 2018 RStudio (CC0 1.0)\nHere, we stick lbl_relabel inside a lambda function with syntax from purrr: the ~ designates a tidy lambda function, which in turn uses .x as a kind of pronoun referencing each of the variables returned by ends_with(\"OBS\"). We’re showing that CONOBS still gets recoded as before, but so do all of the other variables in its group!\nWe’ll use across again with lbl_na_if, but this time we want to produce NA values for all of the variables in our dataset. In place of ends_with(\"OBS\"), we’ll use the selection function everything(). This will take care of all the recoding we want to do, so we’ll also reassign our data with sdp <- sdp:\n\n\nsdp <- sdp %>% \n  mutate(\n    across(ends_with(\"OBS\"), ~lbl_relabel(\n      .x,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    )),\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"Not interviewed (SDP questionnaire)\",\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    ))\n  )\n\n\n\nLet’s pick a few variables to check out work:\n\n\nsdp %>% count(CONOBS)\n\n\n# A tibble: 3 x 2\n             CONOBS     n\n          <dbl+lbl> <int>\n1  0 [out of stock]     5\n2  1 [in-stock]       207\n3 NA                   22\n\nsdp %>% count(IMPOBS)\n\n\n# A tibble: 3 x 2\n             IMPOBS     n\n          <dbl+lbl> <int>\n1  0 [out of stock]     3\n2  1 [in-stock]       204\n3 NA                   27\n\nsdp %>% count(CONOUTDAY)\n\n\n# A tibble: 6 x 2\n  CONOUTDAY     n\n  <int+lbl> <int>\n1         1     1\n2         3     1\n3        10     1\n4        15     1\n5        60     1\n6        NA   229\n\nsdp %>% count(IMPOUTDAY)\n\n\n# A tibble: 4 x 2\n  IMPOUTDAY     n\n  <int+lbl> <int>\n1         1     1\n2        14     1\n3        90     1\n4        NA   231\n\nSummarize Variable Groups by Facility\nEverything looks great! Now that we’ve finished reformatting the data, remember that our ultimate goal is to get some sense of the scope of methods available at a particular facility.\nWe’d like to use something like across again here, but this time we’ll only want to apply our function to a selection of columns within the same row (because each row of our dataset represents one facility). To do this, we’ll divide the dataset rowwise, and then use the related function c_across to apply a calculation across columns within each row.\nFor instance, suppose we want to create NUM_METHODS_PROV to show the total number of methods provided at each facility. Let’s look at the PROV variables for the first few facilities:\n\n\nsdp %>% select(ends_with(\"PROV\"))\n\n\n# A tibble: 234 x 13\n    CONPROV  CYCBPROV DEPOPROV DIAPROV EMRGPROV  FCPROV FSTPROV FJPROV\n   <int+lb> <int+lbl> <int+lb> <int+l> <int+lb> <int+l> <int+l> <int+>\n 1  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 2  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 3  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n 4  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n 5  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  1 [Yes] 1 [Yes] 1 [Yes] 0 [No]\n 6  1 [Yes]   0 [No]   1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 7  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n 8  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 9  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n10  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n# … with 224 more rows, and 5 more variables: IMPPROV <int+lbl>,\n#   IUDPROV <int+lbl>, MSTPROV <int+lbl>, PILLPROV <int+lbl>,\n#   SAYPROV <int+lbl>\n\nTo calculate NUM_METHODS_PROV, we can just find the sum of values across all of the PROV variables (thanks to our recoding work, the only possible values here are 1 for “yes”, or 0 for “no”). Notice that c_across takes only one argument: a selection function like ends_with(\"PROV\"). That’s because c_across works like the familiar concatenate function c() used to provide a vector of values to a function like sum(c(1,2,3)).\nFirst, use rowwise() to signal that we’ll only calculate the sum across variables in the same row. Then, use c_across() to find the sum() of PROV variables in each row:\n\n\nsdp %>% \n  rowwise() %>% \n  transmute(NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")), na.rm = T))\n\n\n# A tibble: 234 x 1\n# Rowwise: \n   NUM_METHODS_PROV\n              <int>\n 1               10\n 2               10\n 3                8\n 4                8\n 5               10\n 6                9\n 7                8\n 8               10\n 9                8\n10                8\n# … with 224 more rows\n\nWe can now create a summary variable for each of the four variable groups. Let’s create:\nNUM_METHODS_PROV - number of methods provided\nNUM_METHODS_INSTOCK - number of methods in-stock\nNUM_METHODS_OUT3MO - number of methods out of stock in the last 3 months\nMEAN_OUTDAY - the mean length of a stockout for all out of stock methods\n\n\nsdp %>% \n  rowwise() %>% \n  transmute(\n    NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")), na.rm = T),\n    NUM_METHODS_INSTOCK = sum(c_across(ends_with(\"OBS\")), na.rm = T),\n    NUM_METHODS_OUT3MO = sum(c_across(ends_with(\"OUT3MO\")), na.rm = T),\n    MEAN_OUTDAY = mean(c_across(ends_with(\"OUTDAY\")), na.rm = T)\n  )\n\n\n# A tibble: 234 x 4\n# Rowwise: \n   NUM_METHODS_PROV NUM_METHODS_INSTOCK NUM_METHODS_OUT3MO MEAN_OUTDAY\n              <int>               <dbl>              <int>       <dbl>\n 1               10                   8                  0         NaN\n 2               10                   7                  0           8\n 3                8                   8                  0         NaN\n 4                8                   8                  0         NaN\n 5               10                   9                  0         NaN\n 6                9                   7                  0         NaN\n 7                8                   7                  0         365\n 8               10                   8                  1         NaN\n 9                8                   8                  1         NaN\n10                8                   7                  0          30\n# … with 224 more rows\n\nMEAN_OUTDAY is NaN (not a number) if no methods were out of stock.\nSummarize Variable Groups by EAID\nIn our last post, we mentioned that the best use case for SDP data is to aggregate information collected from facilities working in the same geographic sampling units - or enumeration areas - used to select individuals for PMA Household and Female samples. In our next post, we’ll take a close look at the variable group EASERVED, which lists all of the enumeration area codes where a facility is known to provide health services. We’ll then introduce a strategy using tidyr::pivot_longer to summarize the full scope of services available to women living in a particular enumeration area.\nFor now, let’s simply consider all of the sampled facilities located in a particular enumeration area. That is, rather than calculating the number of methods provided by one facility NUM_METHODS_PROV, let’s create one variable for each method indicating whether the method was provided by at least one facility in a given enumeration area EAID in a given SAMPLE.\nFor instance, look at the number of facilities providing IUDs in enumeration area 7111 for the Burkina Faso sample collected in 2017:\n\n\nsdp %>% \n  filter(EAID == 7111, SAMPLE == 85405) %>% \n  select(EAID, SAMPLE, FACILITYID, PILLPROV)\n\n\n# A tibble: 4 x 4\n   EAID                            SAMPLE FACILITYID  PILLPROV\n  <dbl>                         <int+lbl>  <int+lbl> <int+lbl>\n1  7111 85405 [Burkina Faso 2017 Round 5]       7210   1 [Yes]\n2  7111 85405 [Burkina Faso 2017 Round 5]       7029   0 [No] \n3  7111 85405 [Burkina Faso 2017 Round 5]       7441   1 [Yes]\n4  7111 85405 [Burkina Faso 2017 Round 5]       7403   1 [Yes]\n\nWe want to use a summarize function to create a variable like ANY_PILLPROV, which should simply indicate whether any of these four facilities provide contraceptive pills. Three of them do provide pills, so we want ANY_PILLPROV to be TRUE.\n\n\nsdp %>% \n  filter(EAID == 7111, SAMPLE == 85405) %>% \n  summarize(ANY_PILLPROV = any(PILLPROV == 1))\n\n\n# A tibble: 1 x 1\n  ANY_PILLPROV\n  <lgl>       \n1 TRUE        \n\nNow that we’re familiar with across, we should be able to do the same thing to all PROV variables for this particular group of facilities. Let’s also introduce a naming convention where we glue the prefix ANY_ to the column name referenced by the pronoun .x:\n\n\nsdp %>% \n  filter(EAID == 7111, SAMPLE == 85405) %>% \n  summarize(across(ends_with(\"PROV\"), ~any(.x == 1), .names = \"ANY_{.col}\"))\n\n\n# A tibble: 1 x 13\n  ANY_CONPROV ANY_CYCBPROV ANY_DEPOPROV ANY_DIAPROV ANY_EMRGPROV\n  <lgl>       <lgl>        <lgl>        <lgl>       <lgl>       \n1 TRUE        TRUE         TRUE         FALSE       FALSE       \n# … with 8 more variables: ANY_FCPROV <lgl>, ANY_FSTPROV <lgl>,\n#   ANY_FJPROV <lgl>, ANY_IMPPROV <lgl>, ANY_IUDPROV <lgl>,\n#   ANY_MSTPROV <lgl>, ANY_PILLPROV <lgl>, ANY_SAYPROV <lgl>\n\n\nIt looks like none of the sampled facilities in enumeration area 7111 provided emergency contraception in 2017. This could be very important context for understanding the health services available to women sampled from that area!\nLet’s repeat the same procedure for every enumeration area in each of our samples. Rather than using a filter to select one EAID in one SAMPLE, we’ll use group_by to work with each EAID in each SAMPLE.\n\n\nsdp %>% \n  group_by(EAID, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~any(.x == 1), .names = \"ANY_{.col}\")\n  )\n\n\n# A tibble: 142 x 15\n# Groups:   EAID, SAMPLE [142]\n    EAID      SAMPLE ANY_CONPROV ANY_CYCBPROV ANY_DEPOPROV ANY_DIAPROV\n   <dbl>   <int+lbl> <lgl>       <lgl>        <lgl>        <lgl>      \n 1  7003 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n 2  7003 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 3  7006 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n 4  7006 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 5  7009 85405 [Bur… TRUE        FALSE        TRUE         FALSE      \n 6  7009 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 7  7016 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n 8  7016 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 9  7026 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n10  7042 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n# … with 132 more rows, and 9 more variables: ANY_EMRGPROV <lgl>,\n#   ANY_FCPROV <lgl>, ANY_FSTPROV <lgl>, ANY_FJPROV <lgl>,\n#   ANY_IMPPROV <lgl>, ANY_IUDPROV <lgl>, ANY_MSTPROV <lgl>,\n#   ANY_PILLPROV <lgl>, ANY_SAYPROV <lgl>\n\nThis is still quite a bit of information! Suppose we want to summarize it even further: let’s calculate NUM_METHODS_PROV again with our summary output. This time, NUM_METHODS_PROV will count the number of methods provided by at least one facility in each group.\n\n\nsdp %>% \n  group_by(EAID, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~any(.x == 1), .names = \"ANY_{.col}\")\n  ) %>% \n  transmute(NUM_METHODS_PROV= sum(c_across(ends_with(\"PROV\")), na.rm = T))\n\n\n# A tibble: 142 x 3\n# Groups:   EAID, SAMPLE [142]\n    EAID                            SAMPLE NUM_METHODS_PROV\n   <dbl>                         <int+lbl>            <int>\n 1  7003 85405 [Burkina Faso 2017 Round 5]                8\n 2  7003 85408 [Burkina Faso 2018 Round 6]                8\n 3  7006 85405 [Burkina Faso 2017 Round 5]                8\n 4  7006 85408 [Burkina Faso 2018 Round 6]                8\n 5  7009 85405 [Burkina Faso 2017 Round 5]                8\n 6  7009 85408 [Burkina Faso 2018 Round 6]               10\n 7  7016 85405 [Burkina Faso 2017 Round 5]                8\n 8  7016 85408 [Burkina Faso 2018 Round 6]                8\n 9  7026 85405 [Burkina Faso 2017 Round 5]                8\n10  7042 85405 [Burkina Faso 2017 Round 5]               10\n# … with 132 more rows\n\nThese summaries are exactly the type of SDP data we’d like to attach to a Household and Female dataset! Watch for our next post, where we’ll show how to create summaries by both EAID and EASERVED, and then match them to records from female respondents sampled from Burkina Faso in 2017 and 2018.\n\n\n\n",
    "preview": "posts/2021-01-28-across-sdp/images/logos.png",
    "last_modified": "2021-08-18T14:28:20-05:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 540
  },
  {
    "path": "posts/2021-01-26-sdp-data/",
    "title": "Service Delivery Point Data Explained",
    "description": "SDP samples are not nationally representative. Learn how to use them to describe the health service environment experienced by individuals.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-26",
    "categories": [
      "Individuals in Context",
      "Data Discovery",
      "Service Delivery Points"
    ],
    "contents": "\n\nContents\nWhat is an SDP?\nSurvey Topics\nSample Design\n\n\n\n\nWhen you visit pma.ipums.org and begin browsing data, you’ll notice that PMA data are available for several different units of analysis.\nYou can see which unit of analysis you’re currently browsing - or switch to a different unit of analysis - in this box:\n\n\n\nClick CHANGE, and you’ll see the different units of analysis that are available:\n\n\n\nThis Data Discovery post kicks off a series of posts all about the data available for the Family Planning - Service Delivery Point unit of analysis. As you’ll see, these data are meant to provide important context for the individuals included in the Family Planning - Person series: while SDP data are not nationally representative, they can help provide a rich portrait of the health service environment experienced by women and households.\nYou’ll find more blog posts about SDP data by following the Individuals in Context series. Look for upcoming posts about:\nWorking with variable groups created from multiple response questions\nMerging SDP summary data with Household and Female data\nMapping SDP Data with GPS Data from our partners at pmadata.org\nMerging SDP Data with spatial datasets from external sources\nAn example of the sort of spatial analysis you can perform with SDP data\nWhat is an SDP?\nA Service Delivery Point (SDP) is any type of facility that provides health services to a community: you’ll find a breakdown of the available facility types for each sample listed in FACILITYTYPE. Because countries may include regionally-specific facility types, we’ve integrated major groupings together in the variable FACILITYTYPEGEN. For example, you may find SDP data available from any of these general facility types:\nHospitals\nHealth Centers\nHealth Clinics\nOther Health Facilities\nPrivate Practices\nDispensaries\nPharmacies / Chemists / Drug Shops\nBoutiques / Shops\nOther\nPMA samples SDPs managed by governments, NGOs, faith-based organizations, private sector organizations, and a range of other institutions. You’ll find the managing authority for each SDP listed in AUTHORITY.\nSurvey Topics\nWhile all SDP surveys cover similar topics, individual questions may be posed somewhat differently - or not at all - for any given sample. That’s where IPUMS PMA comes in: we harmonize differences across samples and document the availability of every variable for each sample.\n\nYou’ll find the full text PDF of the original questionnaire administered to all SDPs in a particular sample here.\nIPUMS PMA also organizes SDP variables by topic. These topics currently include:\nFacility Characteristics\nGeneral Facility Characteristics\nGeography\nAreas Served\nStaffing\nMedical Equipment\nFunding\nManagement\nPerformance Feedback\nQuality of Care\nService Statistics\nMedical Records\nTransportation\n\nFamily Planning Services\nServices Provided\nContraceptive Stock\nReason for Stockout\nClients Served\nStock Supplier\nFees\nFacility Condition\n\nOther Health Services\nAbortion\nPost-abortion Care\nSTDs\nAntenatal Care\nLabor and Delivery\nPostpartum Care\nDelivery Medicines\nCommunity Health Workers\nVaccinations\nHealth Programs\nMedicines in Stock\nOther\n\nThese are listed in the TOPICS menu and are subject to growth & reorganization.\n\n\n\nAdditionally, there are a number of technical variables related to survey administration. For example, every SDP included in the sample frame receives a unique FACILITYID (this ID is preserved across survey rounds if a facility is surveyed more than once). However, some facilities never responded to the questionnaire if, for example, no individual respondent was present, competent, and available to be interviewed (see AVAILABLESQ); if no such person was available - or if such a person declined the interview - the variable CONSENTSQ will indicate that survey consent was never obtained. The variable RESULTSQ indicates whether the questionnaire was fully completed or, if not, it provides the reason.\nFor SDPs where CONSENTSQ is “No”, most variables will take the value “Not interviewed (SDP questionnaire)”.\nNote that the value “NIU (not in universe)” pertains to SDPs that were intentionally skipped because a question was deemed out-of-scope.\nYou may choose whether to include SDPs where RESULTSQ indicates that the questionnaire was not fully completed. Click CREATE DATA EXTRACT from you Data Cart:\n\n\n\nThen click CHANGE next to Sample Members:\n\n\n\nFinally, choose whether to include only “Facility Respondents” (those who fully completed the questionnaire), or “All Cases” instead:\n\n\n\nSample Design\nSo what conclusions can you draw from SDP data? First, it’s important to note that the SDP sample design is not nationally representative, and there are no sampling weights for SDP data.1 In other words, it is not possible to get a sense of the national health services profile in a particular country using SDP data.\nInstead, facilities were selected for the SDP survey using the same geographic enumeration areas used to select households for each Household and Female survey. To see how this works, let’s look at an example dataset collected from Burkina Faso in 2017, beginning with the set of female respondents to the Household questionnaire (other household members and female non-respondents have been excluded):\nRead more about the Household and Female sampling strategy.\n\n\nlibrary(tidyverse)\n\nbf17_hhf <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00011.xml\",\n  data = \"data/pma_00011.dat.gz\") %>% \n  filter(YEAR == 2017)\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nTo use this code, be sure to change both file paths to match your own extract!\nThe Dataset Notes for this sample describe a two-stage cluster design with urban-rural strata, producing a sample of women from 83 enumeration areas. If we count the number of unique values from EAID in our data, we see that there are 83 unique identification numbers - one for each enumeration area:\n\n\nn_distinct(bf17_hhf$EAID)\n\n\n[1] 83\n\nWe can also see how these enumeration areas are distributed throughout the 13 administrative regions of Burkina Faso. Note that we have more enumeration areas in the Central region (including the capital, Ouagadougou), and we have fewer enumeration areas in regions where the population is lower (Centre-Sud, Plateau-Central, Sud-Ouest, etc.):\n\n\nbf17_hhf %>% \n  group_by(GEOBF) %>% \n  summarize(.groups = \"keep\", n_EAID = n_distinct(EAID)) %>% \n  arrange(n_EAID)\n\n\n# A tibble: 13 x 2\n# Groups:   GEOBF [13]\n                    GEOBF n_EAID\n                <int+lbl>  <int>\n 1  7 [Centre-Sud]             2\n 2 11 [Plateau-Central]        3\n 3 13 [Sud-Ouest]              3\n 4  2 [Cascades]               4\n 5 12 [Sahel]                  4\n 6  5 [Centre-Nord]            5\n 7  4 [Centre-Est]             6\n 8 10 [Nord]                   6\n 9  1 [Boucle du Mouhoun]      7\n10  6 [Centre-Ouest]           7\n11  8 [Est]                    7\n12  9 [Hauts-Bassins]         11\n13  3 [Centre]                18\n\nAlthough the same number of households are randomly selected from within each enumeration area (typically 35), this concentration of enumeration areas within population-dense regions helps to ensure that the Household and Female data are nationally representative.\nLet’s now look at the sample of SDPs collected from Burkina Faso in that same year:\n\n\nbf17_sdp <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00008.xml\",\n  data = \"data/pma_00008.dat.gz\") %>% \n  filter(YEAR == 2017)\n\n\n\n\nRemember: to use this code, be sure to change both file paths to match your own extract!\nDataset Notes for the SDP sample explain that the same 83 enumeration areas used in the Household and Female Sample were used to select facilities for the SDP sample. Moreover, we can confirm that all of enumeration areas in the SDP data also appear in the HHF data:\n\n\nall(bf17_sdp$EAID %in% bf17_hhf$EAID)\n\n\n[1] TRUE\n\nBut is the reverse true? Is every enumeration area from the Household and Female Sample represented in the SDP data?\n\n\nall(bf17_hhf$EAID %in% bf17_sdp$EAID)\n\n\n[1] FALSE\n\nPerhaps surprisingly, the answer is no. To learn why, we have to dig a bit deeper into the SDP Dataset Notes. There, we see that a facility located within the physical boundaries of one of the 83 enumeration areas from the Household and Female Survey would have been included in the SDP sample. However, there may be enumeration areas - particularly in remote areas - where no facilities are located.\nFortunately, PMA also includes data about the service catchment area for some facilities.2 You can include this information by selecting the variable series EASERVED. If a given facility serves more than one enumeration area, EASERVED1 will contain the enumeration area ID code for the first enumeration area on its catchment list, EASERVED2 will contain the ID code for the second one, and so forth. If that same facility serves 5 enumeration areas, the variables EASERVED6, EASERVED7, and so forth would be “NIU (not in universe)”.\n\nThe IPUMS PMA extract system automatically determines the right maximum number of EASERVED variables by finding the facility with the largest service catchment list in your extract.\nWhat does this mean? As we’ll show in an upcoming post in this series, it’s possible to create a portrait of the health service environment provided to individuals sampled in the Household and Female surveys. This portrait extends beyond the list of facilities located in an individual’s geographic enumeration area, but users should take care to understand that the scope of facilities providing services to that enumeration area is somewhat limited by sample design.\n\nThe files do contain a weight variable for the sampling units EAWEIGHT, which is a probability weight representing the likelihood of an enumeration area being selected for sampling. The collectors of the original data do not recommend using EAWEIGHT to weight SDP variables.↩︎\nThis information is only available for SDPs where the managing authority listed in AUTHORITY is “government”.↩︎\n",
    "preview": "posts/2021-01-26-sdp-data/images/choose-unit.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 909,
    "preview_height": 632
  },
  {
    "path": "posts/2020-12-10-get-ipums-pma-data/",
    "title": "Import IPUMS PMA Data into R",
    "description": "How to download an IPUMS PMA data extract and start using it in R",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2020-11-10",
    "categories": [
      "R Tips",
      "R Packages",
      "Importing Data"
    ],
    "contents": "\n\nContents\nHow to access the data\nUser Guide\nYouTube Tutorials\n\nImporting the data\nFixed-width Data Format (dat)\nThe ipumsr package\n\n\nIPUMS PMA is the harmonized version of the multinational survey Performance Monitoring for Action (formerly known as Performance Monitoring and Accountability 2020 - PMA2020). IPUMS PMA lets researchers easily browse the contents of the survey series and craft customized microdata files they download for analysis.\nHow to access the data\nUser Guide\nVisit the IPUMS PMA data dissemination website to browse the available data, and then follow the posted user guide to get started with an extract of interest.\nNote: all users must register for a free account. See user guide for details.\nYouTube Tutorials\nVisit the IPUMS PMA YouTube page for a video playlist showing how to do things like:\nregister for a free IPUMS account\nselect from the available units of analysis\nbuild a data extract\nselect cases of interest\nuse the available survey weights\nImporting the data\nFixed-width Data Format (dat)\nOnce you have registered and finished selecting PMA samples and variables for your extract, click the “View Cart” button to begin checkout.\nReview the contents of your extract and clik the “Create Data Extract” button as shown:\n\n\n\nOn this final page be sure to change the data format to “.dat (fixed-width text)” if it is not selected by default:\n\n\n\nYou will receive an email when your extract request has been processed. Click the included link to find a download page like this one. You must download both the data file and the DDI codebook:\n\n\n\nThe ipumsr package\nThe R package ipumsr provides the tools you will need to import the data file and DDI codebook into R. You can install the package from CRAN with:\n\nClick here for more information on R packages.\ninstall.packages(\"ipumsr\")\nNote the location where your data file and DDI codebook were saved (in my case, they were saved in my local “Downloads” folder). Substitute your own paths into the function shown below:\n\n\ndat <- ipumsr::read_ipums_micro(\n  ddi = \"~/Downloads/pma_00001.xml\",\n  data_file = \"~/Downloads/pma_00001.dat.gz\"\n)\n\n\n\nYou’re done! The dataset is now accessible as the R object, dat.\n\n\n\n",
    "preview": "posts/2020-12-10-get-ipums-pma-data/images/create-data-extract.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 910,
    "preview_height": 790
  },
  {
    "path": "posts/2020-12-10-get-r-and-packages/",
    "title": "Getting Started with R",
    "description": "How to download R for free and install some of the R packages used on this blog",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2020-11-02",
    "categories": [
      "R Tips",
      "R Packages"
    ],
    "contents": "\n\nContents\nWhy analyze PMA data with R?\nGetting started with R\nOur favorite resources\n\nDo I really need statistical software?\nAre there alternatives?\n\nRStudio\nR packages\nEssentials\nipumsr\ntidyverse\nshiny\n\nWatch for updates here\n\n\nWhy analyze PMA data with R?\nLike all IPUMS data projects, IPUMS PMA data is available free of charge to users who agree to our terms of use. That’s because we believe that cost and institutional affiliation should not be barriers to answering pressing concerns around women’s health. (You can register here for a free IPUMS PMA user account.)\nIn fact, users can analyze IPUMS PMA data with any software they like! We’ve chosen to highlight R, in particular, because it is also free and popular with data analysts throughout the world. It’s available for Windows, MacOS, and a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux).\nNon-R users: IPUMS data extracts are available as CSV or fixed-width DAT with syntax files formatted for SPSS, Stata, and SAS.\nGetting started with R\nTo get a copy of R for yourself, visit the Comprehensive R Archive Network (CRAN) and choose the right download link for your operating system.\nIf you’re new to R (or want to refresh your skills), we recommend the excellent, free introductory text R for Data Science. It also introduces tidyverse conventions, which we use throughout this blog.\nOur favorite resources\nR for Data Science, for beginners\nAdvanced R, for a deeper dive\nRSpatial, for analysis with spatial data\nggplot2, for data visualization\nMastering Shiny, for interactive applications\nR Markdown: The Definitive Guide, for producing annotated code, word documents, presentations, web pages, and more\nR-bloggers, for regular news and tutorials\n\n\n\n\n© 2016 The R Foundation (CC-BY-SA 4.0)\nDo I really need statistical software?\nIf you’re new to data analysis, you might wonder exactly what you’re going to find in a toolkit like R.\nPlenty of people come to R after working with more common types of data analysis software, like Microsoft Excel or other spreadsheet programs. If you wanted to, you could absolutely download a CSV file from the IPUMS PMA extract system and open it in Excel. You would find individual respondents in rows and their responses for variables in columns, and you could make use of built-in spreadsheet functions to do things like:\nCalculate and visualize the distribution of a variable\nBuild pivot tables and graphs examining basic relationships between variables\nCreate new variables of your own that combine data from several variables\nHowever, you might also notice that a spreadsheet comes with certain limitations:\nThere is no variable “metadata”, including labels for the variables and each response option. For example, you might see that the responses to a certain variable include the numbers 0, 1, and 99 - what do these values actually mean?\nYou might find yourself repeating the same “point” and “click” procedure over and over. Or, maybe you’ve had to build a library of custom macro functions on your own to help automate those procedures.\nWhile you can perform arithmetic with built-in functions, there is little support for more advanced statistical procedures\nGraphics are limited within a set of pre-built templates\nMerging data from external sources (like spatial data) can be very tricky\nStatistical software is designed specifically to address these and other issues related to data cleaning and analysis. Learning a program like R takes a lot of practice, but doing so will almost certainly make your work much more efficient!\nAre there alternatives?\nYes! Many data analysts use proprietary statistical software like Stata, SAS, or SPSS. These tools are also powerful, and you may even find them easier to use than R.\n\nComing soon, we hope to include Stata code for many of the blog posts currently written in R.\nBeyond price, R has a few additional advantages that make it a particularly useful tool for working with PMA data:\nCommunity support: R users are particularly active on forums like Stack Overflow and R-bloggers. Groups like R-ladies even organize in-person meetups in cities around the world to help promote inclusion within the R community.\nCustomizability: Because R is open-source, you can change just about anything you like! With a little practice, you’ll be able to create functions and graphics that perfectly match your own needs.\nBeyond statistics: You can use R to build a website (like this one), manage and share a code repository on GitHub, scrape and compile a social media database, or automatically generate word documents, slide presentations, and more! There are practically endless ways to use functional programming in R that have nothing to do with statistics at all.\nIf you’re a beginner, learning R can be a daunting task. Keep at it! And never hesitate to ask questions.\nRStudio\nWe strongly recommend running R within RStudio, an integrated development environment (IDE) designed to make your experience with R much easier. Some of the reasons we use it, ourselves:\nIncludes a multi-pane window that puts your R console, source code, output, and help files all in one place\nSyntax highlighting and code completion\nSupport for R Projects, a crucial approach to organizing your work and sharing it with others\nIncludes RMarkdown, an R package that allows you write text-based documents with embedded snippets of code that can be passed directly to your R console\nComing soon: tools like the command palette, an improved package manager, and integrated citation management\nLike R, it is available at no cost for users on Windows, Mac, and Linux\n\nThis blog is, itself, an R Project with an individual R Markdown file for each page on the site. Look for a download button at the top of every post: you can download the original R Markdown file, open it in RStudio, and run all of the included code.\nR packages\nAn R package is a collection of functions created by other R users that you can download and install for yourself. Packages can be distributed in many ways, but all of the packages we highlight on this blog can be downloaded from CRAN (the same resource used to download “base” R). A package like ipumsr can be downloaded from CRAN by typing the following function into the R console:\n\nThis function saves package files in your default “library” location. If you’re using a Linux machine and don’t have root access, you’ll need to set up R to save packages to a location where you’re able to write files.\n\n\ninstall.packages(\"ipumsr\")\n\n\n\nPackages also come with help files detailing the purpose and possible inputs (or “arguments”) of each included function. Other included metadata explains what version of R you’ll need to use the package, and also whether the package borrows functions from any other packages that should also be installed (usually these are called “dependencies”).\nIn order to access the functions and help files for a package, you need to load it after installation with:\n\n\nlibrary(ipumsr)\n\n\n\nOn this blog, we will often show functions together with their package like this:\n\n\nipumsr::read_ipums_micro(\n  ddi = \"~/Downloads/pma_00001.xml\",\n  data_file = \"~/Downloads/pma_00001.dat.gz\"\n)\n\n\n\nThe function read_ipums_micro comes from the package ipumsr. It is not necessary for you to include the package each time you call a function (as long as you’ve already loaded the package with library()); we’re using this notation simply as a reminder (in case you want to consult the original package documentation).\n\nYou can use the package::function() notation if you ever want to access a function from a package without loading everything else in the package.\nEssentials\nHere are the packages you’ll need to install to reproduce the code on this blog:\nipumsr\nThe ipumsr package contains functions that make it easy to load IPUMS PMS data into R (mainly read_ipums_micro).\nIt also contains functions that will return variable metadata (like the variable descriptions you see while browsing for data on pma.ipums.org.\ntidyverse\nThe tidyverse package actually installs a family of related packages, including:\nggplot2, for data visualization\ndplyr, for data manipulation\ntidyr, for data tidying\nreadr, for data import\npurrr, for functional programming\ntibble, for tibbles (a modern re-imagining of data frames)\nstringr, for strings\nforcats, for factors\nThis blog uses tidyverse functions and syntax wherever possible because so-called “tidy” conventions are designed with the expressed purpose of making code and console output more human readable. Sometimes, human readability imposes a performance cost: in our experience, IPUMS PMA datasets are small enough that this is not an issue.\n\nFor larger datasets, we recommend exploring the package data.table.\nshiny\nInteractive graphics shown throughout this blog are built with the shiny package.\nWatch for updates here\nWe may add more package suggestions for future posts!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-18T14:28:20-05:00",
    "input_file": {}
  }
]
