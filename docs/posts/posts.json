[
  {
    "path": "posts/2021-02-04-merging-external-spatial-data/",
    "title": "Merging external spatial data",
    "description": "How to integrate external spatial data with PMA data.",
    "author": [
      {
        "name": "Nina Brooks",
        "url": "http://www.ninarbrooks.com/"
      }
    ],
    "date": "2021-02-04",
    "categories": [
      "Individuals in Context",
      "Service Delivery Points",
      "Data Manipulation",
      "sf",
      "raster",
      "Spatial"
    ],
    "contents": "\n\nContents\nData\nSetup: Load packages and data\nPopulation Density: working with raster data\nRoad Networks: Working with vector data\n\n\n\n Download this page as R code\n\n\nOur last post showed how to read, merge, and map the PMA GPS data - and how mapping can shed light on interesting spatial variation. A big advantage of the PMA GPS data is that you can also merge in other sources of spatial data, which opens up enormous opportunities for analyzing how contextual and environmental factors affect topics of interest in the PMA data. In this post, we’ll show how to merge in two different types of spatial data and construct variables of interest.\nData\nWe’ll be using the PMA GPS data for this post. To use the GPS data you must request access directly from our partners at pmadata.org. If you want to follow along with this post without accessing the GPS data, we have provided a “toy” GPS dataset in the data folder for this post. The toy data contains randomly sampled locations within Burkina Faso which have no actual relationship to the EAs in the PMA data. This means none of the interpretations of spatial patterns will hold, but all the code will run.\nWe will also be introducing two different spatial datasets that represent different kinds of spatial data. The first is population density data from WorldPop.1 If you want to download the data from the WorldPop site, we’re using the “Unconstrained individual countries 2000-2020 (1 km resolution)” data from 2017 for Burkina Faso. This is raster data, which means the data are stored as a grid of values which are rendered on a map as pixels. You can think of this as a matrix that is spatially referenced – that is each pixel represents a specific area of land on the Earth. Lots of spatial data are stored as rasters including climate data (e.g., temperature and rainfall), elevation, and satellite images. Note that the raster data is saved as a .tiff (which is a common way of storing raster data). The resolution of the raster maps to the area that each pixel represents in the real world. The population density is 1 km resolution, which means that each pixel represents a 1 km by 1 km square on the ground. The figure below shows the impact of different spatial resolutions for the same raster data.2\n\n\n\nFigure 1: Source: NEON\n\n\n\n\nThere are tons of resources on earth data science in R. We recommend the resources by Earth Lab and NEON by NSF. This post is an excellent introduction to working with rasters in R!\nPopulation density is also conceptually important to the SDP data on contraceptive supply that we’ve been examining through this series of posts. Population density may provide a more nuanced characterization of urbanization than the URBAN variable. Additionally, density may be correlated with longer wait times at clinics, which may also impact contraceptive use at the individual level.\nThe second spatial dataset we’ll introduce is data on road networks in Burkina Faso from the Digital Chart of the World and made publicly available by DIVA-GIS, an excellent source for publicly available spatial datasets. Road networks serve as a proxy for accessibility to health clinics – an important component of the contraceptive service environment – that may be more nuanced than the binary urban/rural distinction. To download the road data, go to DIVA-GIS Data and select Burkina Faso from the Country dropdown and Roads from the Subject dropdown. You can also access this directly from the data folder for this post. The road data is called vector data and is stored in a shapefile (.shp). Vector data is used to represent real world features and are three basic types: points, lines, and polygons. The road data we’re using in this post is an example of vector line data.\n\nRemember the administrative boundaries we used in the previous post were polygons and the GPS points for the PMA enumeration areas were points. Both are vector data!\nAll of the data we use in this post is included in the data folder when you click with Download button at the top of this page.\nSetup: Load packages and data\nWe’ll be using many of the packages from the last few posts, as well as a new package for specifically working with raster data – appropriately called raster and one called units, which enables easy conversion between objects of different units. Make sure to install the raster and units packages first and then load everything we’ll be using today:\n\n\nlibrary(sf) # primary spatial package\nlibrary(raster) # for working with raster data\nlibrary(viridis) # for color palettes\nlibrary(units) # to easily convert between units\nlibrary(tidyverse)\n\n\n\nLet’s start by reading in the raster using raster::raster() and check out the meta-data.\n\n\npop_density <- raster(\"data/bfa_pd_2017_1km.tif\")\npop_density\n\n\nclass      : RasterLayer \ndimensions : 682, 951, 648582  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -5.517917, 2.407083, 9.407917, 15.09125  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : /Users/Matt/R/pma-data-hub/_posts/2021-02-04-merging-external-spatial-data/data/bfa_pd_2017_1km.tif \nnames      : bfa_pd_2017_1km \nvalues     : 0.281988, 8820.016  (min, max)\n\nBecause rasters are essentially just matrices, you can think of the dimensions in the same way. At a spatial resolution of 1 km, this raster covers all of Burkina Faso with 648,582 cells. The resolution describes the size of the cells (the length of one side of each square cell). You may be wondering why this is showing up as 0.00833 when the data has a spatial scale of 1 km by 1 km. This is because the units that the resolution is reported in depend on the coordinate reference system of the data. More on this in a moment.\nThe extent (or spatial extent) refers to the geographic area that the raster covers. The values are in the same coordinate reference system as the raster. The coordinate reference system or crs is the next piece of meta-data we have. “A coordinate reference system (CRS) is a coordinate-based local, regional or global system used to locate geographical entities.”3 The crs for this raster is +proj=longlat +datum=WGS84 +no_defs. The crs contains several pieces of information including the datum (WGS84) and the projection.4 The appropriate CRS to use for any given spatial task depends on what part of the world the data represent and what kind of spatial operations you’ll be performing. It’s really important to know what crs your data are in and make sure that all your spatial data are in the same  crs if you use more than one kind. Otherwise, they won’t line up on a map and any spatial analysis or processing you do will be incorrect.\nThe projection of this raster data is described as longlat, which actually is not a projection. A projection refers to how the Earth’s surface is flattened so it can be represented as a 2-dimensional raster grid. These data use a geographic coordinate system, simply the raw latitude and longitude coordinates, rather than a projected coordinate system, which would transform the coordinates into a 2-dimensional plane. Latitude and longitude locate positions on the Earth using angles, so the spacing of each line of latitude as you move north or south along the Earth is not uniform. The units of this reference system are in degrees (of latitude and longitude), so the 0.00833 resolution we saw above is reporting the spatial resolution in degrees, rather than meters or kilometers. This crs is not ideal for measuring distances because the distance covered by a single degree of latitude or longitude varies greatly across the Earth’s surface. This also means that the stated 1 km resolution is only nominal. At the equator, 0.00833 degrees is approximately equal to 1 km, but this distance, and the ground area represented by each pixel, will vary. Fortunately, Burkina Faso is relatively close to the equator, so the pixels will be quite close to 1 km by 1 km.\nThe last piece of meta-data to look at are the values – this is reporting the minimum and maximum values across all of the cells. Because these are population density data, it can be interpreted as the number of people in each pixel divided by the area of each pixel (which we know is 1 km2)\nNow that we’ve reviewed the raster attributes, let’s see what it looks like. We can use the basic plot function to do this.\n\n\nplot(pop_density)\n\n\n\n\nWe can see three locations stand out in terms of population density. First is Ouagadougou the capital of Burkina Faso and largest city, right in the center. Then we can see higher density around Bobo Dioulasso and Banfora in the southwest of the country, which are the second and third largest cities in the country.\nNext we’ll load the roads data using sf::st_read().\n\n\nroads <- st_read(\"data/BFA_roads/BFA_roads.shp\", quiet = TRUE)\nroads\n\n\nSimple feature collection with 1149 features and 5 fields\ngeometry type:  MULTILINESTRING\ndimension:      XY\nbbox:           xmin: -5.482261 ymin: 9.407643 xmax: 2.393089 ymax: 15.08071\ngeographic CRS: WGS 84\nFirst 10 features:\n       MED_DESCRI      RTT_DESCRI F_CODE_DES ISO   ISOCOUNTRY\n1  Without Median Secondary Route       Road BFA BURKINA FASO\n2  Without Median Secondary Route       Road BFA BURKINA FASO\n3  Without Median Secondary Route       Road BFA BURKINA FASO\n4  Without Median Secondary Route       Road BFA BURKINA FASO\n5  Without Median Secondary Route       Road BFA BURKINA FASO\n6  Without Median Secondary Route       Road BFA BURKINA FASO\n7  Without Median Secondary Route       Road BFA BURKINA FASO\n8  Without Median Secondary Route       Road BFA BURKINA FASO\n9  Without Median Secondary Route       Road BFA BURKINA FASO\n10 Without Median Secondary Route       Road BFA BURKINA FASO\n                         geometry\n1  MULTILINESTRING ((-0.720550...\n2  MULTILINESTRING ((-0.583273...\n3  MULTILINESTRING ((-0.397415...\n4  MULTILINESTRING ((-0.142728...\n5  MULTILINESTRING ((-0.403059...\n6  MULTILINESTRING ((-0.171111...\n7  MULTILINESTRING ((-0.116756...\n8  MULTILINESTRING ((0.0672155...\n9  MULTILINESTRING ((-1.245636...\n10 MULTILINESTRING ((-1.50246 ...\n\nThis sf object also contains meta-data (shown at the top). In terms of meta-data, the geometry type field tells us this data is a MULTILINESTRING object, which makes sense since these are roads. The bbox (short for bounding box), is the same information as the extent field for the raster data – it tells us the bounds of the geographic area that this spatial data covers. We see the geographic CRS which is the coordinate reference system of the data. For this roads dataset it is WGS84, which is the same as the population density raster data.\nThe roads data contains several variables: MED_DESCRI, RTT_DESCRI, F_CODE_DES, ISO, ISOCOUNTRY, and geometry. The first three variables provide some information about the types of roads in this data. ISO and ISOCOUNTRY simply provide country codes and names for the data. Finally, we see the geometry variable, which is the variable that contains the spatial information in an sf object.\nWe can also plot this roads data to see what it looks like.\n\n\nplot(roads)\n\n\n\n\nBy calling the basic plot function, we get a panel of plots of the road network, with one plot for each variable. We can see some variation in color MED_DESCRI and RTT_DESCRI, indicating that there multiple values for those variables. If we wanted just a single plot of the road network, we can get that by calling plot on the geometry variable:\n\n\nplot(roads$geometry)\n\n\n\n\nFinally, we’ll load the “toy” GPS data and convert it to an sf object. The option crs = 4326 means that we are creating this with the WGS84 coordinate reference system because 4326 is the EPSG code for WGS84.\n\nMost crs are assigned an “EPSG code”, which is a unique ID that can be used to identify a CRS.\n\n\ngps <- read_csv(\"data/bf_gps_fake.csv\") %>%\n  rename(EAID = EA_ID) %>% # rename to be consistent with other PMA data\n  st_as_sf(\n    coords = c(\"GPSLONG\", \"GPSLAT\"), \n    crs = 4326)\n\n\n\n\n\n\nPopulation Density: working with raster data\nWe want to construct a variable that captures the population density at each enumeration area in the data. We’ll use sf::st_buffer() to do this, which will construct a buffer circle around each GPS point. The PMA GPS data are randomly displaced to protect the privacy of respondents, so it’s imperative to consider this displacement when working with the GPS data to do spatial operations. Because the maximum displacement distance is 10 km, if we construct buffers with a radius of 10 km we can be 100% confident that the true locations of each GPS point fall within that buffer.\n\nUrban EAs are displaced from their true location up to 2 km. Rural EAs are displaced from their true location up to 5 km. Additionally, a random sample of 1% of rural EAs are displaced up to 10km.\n\n\nbuffers <- st_buffer(gps, dist = 10000)\n\n\nWarning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle\n= endCapStyle, : st_buffer does not correctly buffer longitude/\nlatitude data\ndist is assumed to be in decimal degrees (arc_degrees).\n\nggplot() +\n  geom_sf(data = buffers) +\n  geom_sf(data = gps)\n\n\n\n\nThis giant circle is certainly not what we would expect! What’s going on here? Earlier in this post we mentioned that the WGS84 crs is a geographic coordinate system that simply uses the latitude and longitude coordinates to identify locations and the units are in degrees, rather than meters or kilometers. This circle thus has a radius of 10,000 degrees and since the Earth only spans 360 degrees it is fully covered by this circle. As we mentioned, the WGS84 crs is not ideal for measuring distances. R alerted us of this problem with two warnings: st_buffer does not correctly buffer longitude/latitude data and dist is assumed to be in decimal degrees (arc_degrees). This is why it’s so important to pay attention to the crs of your data.\nTo properly construct a buffer circle around these GPS points, we need to transform the data to a different projection that uses meters or kilometers. And, because it’s essential that all of our data are in the same crs, we need to transform or reproject everything. For vector data, we can do this using sf::st_transform() and for raster data we’ll do this with raster::projectRaster(). For the transformation, we’re using a crs that is projected to meters and is appropriate to the local geography of Burkina Faso. You can read about it on the epsg.io site. After reprojecting, we’ll calculate the buffer again and plot it to make sure this looks right.\n\n\n# transform the GPS data\ngps_tr <- gps %>% st_transform(crs = 32630)\ngps_tr\n\n\nSimple feature collection with 83 features and 5 fields\ngeometry type:  POINT\ndimension:      XY\nbbox:           xmin: 260943.3 ymin: 1114669 xmax: 1025472 ymax: 1653511\nprojected CRS:  WGS 84 / UTM zone 30N\n# A tibble: 83 x 6\n   PMACC PMAYEAR REGION                EAID DATUM           geometry\n * <chr>   <dbl> <chr>                <dbl> <chr>        <POINT [m]>\n 1 BF       2017 5. centre-nord        7610 WGS84 (837531.4 1567675)\n 2 BF       2017 1. boucle-du-mouhoun  7820 WGS84 (491871.7 1488848)\n 3 BF       2017 3. centre             7271 WGS84   (982414 1349907)\n 4 BF       2017 3. centre             7799 WGS84   (739431 1352652)\n 5 BF       2017 8. est                7243 WGS84 (545866.2 1219668)\n 6 BF       2017 6. centre-ouest       7026 WGS84 (352638.7 1209502)\n 7 BF       2017 3. centre             7859 WGS84 (833822.1 1377527)\n 8 BF       2017 3. centre             7725 WGS84 (980025.8 1406727)\n 9 BF       2017 6. centre-ouest       7390 WGS84 (439876.7 1190609)\n10 BF       2017 11. plateau-central   7104 WGS84 (835483.2 1469280)\n# … with 73 more rows\n\n# reproject the raster data\npop_density_tr <- projectRaster(\n  pop_density, \n  crs = \"+proj=utm +zone=30 +datum=WGS84 +units=m +no_defs\"\n)\npop_density_tr\n\n\nclass      : RasterLayer \ndimensions : 699, 970, 678030  (nrow, ncol, ncell)\nresolution : 907, 922  (x, y)\nextent     : 218942.9, 1098733, 1035714, 1680192  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=30 +datum=WGS84 +units=m +no_defs \nsource     : memory\nnames      : bfa_pd_2017_1km \nvalues     : 0.9709167, 8775.492  (min, max)\n\n\nNote: the projectRaster function takes crs as a character string, rather than the EPSG code 32630. We’re using the PROJ.4 code shown in the “Export” menu on the epsg.io site.\n\n\n# calculate 10 km (10,000 meter) buffer circles\nbuffers_tr <- st_buffer(gps_tr, dist = 10000) # because the units are in meters\n\n# plot\nggplot() +\n  geom_sf(data = buffers_tr) +\n  geom_sf(data = gps_tr, color = \"red\")\n\n\n\n\nLooking at the meta-data for both the gps_tr and raster_tr objects, we can see they have the same new projected crs: UTM zone 30N. The raster_tr meta-data also includes information on the units (+units=m) confirming that distances are measured in meters. Turning to the plot, we can see the GPS coordinates marked in red and each has a circle around it.\nNow that we have correctly estimated 10 km buffer circles, we can calculate the average population density within each buffer using the raster::extract() command and specifying fun = mean. This produces an 83 x 1 vector of results, which means we have one population density value for each enumeration area. Printing the first 5 results shows there is some substantial variation in population density.\n\n\nbuffer_density <- raster::extract(pop_density_tr, buffers_tr, \n                                  fun = mean, na.rm = TRUE,\n                                  cellnumbers = TRUE)\ndim(buffer_density)\n\n\n[1] 83  1\n\nhead(buffer_density)\n\n\n          [,1]\n[1,]  35.58080\n[2,]  21.36878\n[3,]  17.56161\n[4,] 113.36298\n[5,]  31.73017\n[6,]  57.09591\n\nNote, that we don’t actually need to create the buffers first to extract the mean values of the raster. We can do it all in one step, shown below. Just make sure to use the gps_tr object instead of the buffer_tr object! But, we’ll use those buffers again with the road data.\n\n\nbuffer_density_alt <- raster::extract(pop_density_tr, gps_tr, \n                                  buffer = 10000,\n                                  fun = mean, na.rm = TRUE)\nhead(buffer_density_alt)\n\n\n[1]  35.58080  21.36878  17.56161 113.21781  31.73017  57.09591\n\nFinally, so we can merge everything together by EAID, let’s add the population density calculation directly to the gps_tr data. Note that the raster::extract() command preserves the order of the inputs, so we know the first row of the density calculation corresponds to the first row of the gps_tr data.\n\n\ngps_tr$pop_density <- raster::extract(\n  pop_density_tr, gps_tr, \n  buffer = 10000,\n  fun = mean, na.rm = TRUE\n)\n\n\n\nRoad Networks: Working with vector data\nBefore we do anything with the road data, let’s make sure to reproject it to match the rest of our data.\n\n\nroads_tr <- roads %>%\n  st_transform(crs = 32630)\n\n\n\nBecause enumeration areas with better access to roads may make it easier for women to reach local service delivery providers. We are going to calculate the total length of roads within each buffer as a proxy for this accessibility. Because each of these buffers was constructed with the same 10 km radius, they have the same area, which means the sum of road length can also be thought of as a road density measure.\nFirst, we need to identify which portions of the road fall into each buffer. We’ll use sf::st_intersection(), which returns a new sf object that contains observations from the first argument that touch (geographically) the second argument.\n\nNote that there is also an sf::intersects() command. This is different than the one we’re using because it returns a logical matrix that indicates whether each geometry pair intersects. See more on these types of operations in the sf vignette.\n\n\nint <- st_intersection(roads_tr, buffers_tr)\nint\n\n\nSimple feature collection with 238 features and 10 fields\ngeometry type:  LINESTRING\ndimension:      XY\nbbox:           xmin: 251238.7 ymin: 1104708 xmax: 1033002 ymax: 1658158\nprojected CRS:  WGS 84 / UTM zone 30N\nFirst 10 features:\n        MED_DESCRI      RTT_DESCRI F_CODE_DES ISO   ISOCOUNTRY PMACC\n240 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n241 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n268 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n711 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n640    With Median   Primary Route       Road BFA BURKINA FASO    BF\n649    With Median   Primary Route       Road BFA BURKINA FASO    BF\n728 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n958 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n959 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n964 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n    PMAYEAR               REGION EAID DATUM\n240    2017 1. boucle-du-mouhoun 7820 WGS84\n241    2017 1. boucle-du-mouhoun 7820 WGS84\n268    2017 1. boucle-du-mouhoun 7820 WGS84\n711    2017            3. centre 7271 WGS84\n640    2017            3. centre 7799 WGS84\n649    2017            3. centre 7799 WGS84\n728    2017            3. centre 7799 WGS84\n958    2017               8. est 7243 WGS84\n959    2017               8. est 7243 WGS84\n964    2017               8. est 7243 WGS84\n                          geometry\n240 LINESTRING (501051.3 149280...\n241 LINESTRING (481961.6 149016...\n268 LINESTRING (489661.7 148171...\n711 LINESTRING (981157 1359825,...\n640 LINESTRING (732417.5 135977...\n649 LINESTRING (746990.9 135527...\n728 LINESTRING (746990.9 135527...\n958 LINESTRING (553191.7 122647...\n959 LINESTRING (554462.3 122043...\n964 LINESTRING (535888.5 122030...\n\nThe returned object (int) is a data.frame with 238 observations (far fewer than the original 1149 in the roads_tr data). Note that it also contains all the variables from both roads_tr and buffers_tr, so this operates a bit like an inner_join, which means it only includes observations that are in both datasets. We can see the implications of this by making a quick map. The full road network is shown in gray, the buffer circles are in black and the roads that fall into the circles are highlighted in red. Based on this map, it looks like there are a few buffer circles that don’t contain any roads. We want to be sure we account for this.\n\n\n# plot intersection with buffers and road networks \nggplot() +\n  geom_sf(data = buffers_tr) +\n  geom_sf(data = roads_tr, color = \"grey\") +\n  geom_sf(data = int, color = \"red\")\n\n\n\n\nWe can merge in the full list of EAIDs to make sure we don’t miss this one (or any others) using sf:st_join(), which works like dplyr::left_join(). It’s important that when we do the join, the first argument is int, so that it will retain the LINESTRING geometry from this dataset, which we need to calculate the road length. Then, we’ll calculate the length of the road networks contained in each buffer. We can do this with sf::st_length(). Because many of the buffer circles contain multiple roads, we first need calculate the length of each road then we need to aggregate to get the length of all roads in a given enumeration area. We’ll convert from meters to km for greater readability. It’s important to note that any EAs with buffers that don’t contain any roads will not be in the int dataset, so we’ll do a dplyr::full_join() with gps_tr to make sure we get them all.\nBecause int and gps_tr are both sf objects, it’s not possible to do a standard join – you can only use sf::join() when you have two sf objects. That’s why we convert both to data.frames for the dplyr::full_join() and then back into an sf object. Finally, we’ll convert int back into an sf object, retaining the POINT geometry from gps_tr, and replace all NA road lengths as 0.\n\n\n# join, calculate length, & summarize\nint <- int %>%\n  mutate(road_length = st_length(geometry)) %>%\n  group_by(EAID) %>%\n  summarise(road_length = sum(road_length, na.rm = T)) %>%\n  mutate(road_length = set_units(road_length, \"km\")) %>%\n  as.data.frame() %>%\n  full_join(as.data.frame(gps_tr), by = \"EAID\") %>%\n  st_sf(sf_column_name = 'geometry.y') %>%\n  dplyr::select(-geometry.x) %>%\n  mutate(road_length = ifelse(is.na(road_length), 0, road_length))\n\nint\n\n\nSimple feature collection with 83 features and 7 fields\ngeometry type:  POINT\ndimension:      XY\nbbox:           xmin: 260943.3 ymin: 1114669 xmax: 1025472 ymax: 1653511\nprojected CRS:  WGS 84 / UTM zone 30N\nFirst 10 features:\n   EAID road_length PMACC PMAYEAR               REGION DATUM\n1  7003    22.41682    BF    2017       5. centre-nord WGS84\n2  7006    13.35579    BF    2017       5. centre-nord WGS84\n3  7009    20.45756    BF    2017               8. est WGS84\n4  7016    20.31970    BF    2017 1. boucle-du-mouhoun WGS84\n5  7026    18.99583    BF    2017      6. centre-ouest WGS84\n6  7042    14.52567    BF    2017               8. est WGS84\n7  7048    24.29931    BF    2017        4. centre-est WGS84\n8  7056    15.21114    BF    2017     9. hauts-bassins WGS84\n9  7082    44.15771    BF    2017          2. cascades WGS84\n10 7092    26.30945    BF    2017        7. centre-sud WGS84\n   pop_density               geometry.y\n1     29.55058 POINT (422741.8 1383385)\n2     29.98035 POINT (371721.6 1276582)\n3     37.77861 POINT (348264.9 1114669)\n4    154.07399 POINT (346019.4 1218739)\n5     57.09591 POINT (352638.7 1209502)\n6     63.08637 POINT (752580.5 1219236)\n7     74.19692 POINT (479619.4 1234717)\n8     35.07390 POINT (359915.1 1392593)\n9   2731.76393 POINT (669089.2 1375002)\n10   152.23504 POINT (559070.9 1346179)\n\nThe added benefit of the full_join() with gps_tr is that it brings in the pop_density variable we created earlier. So now everything is in one dataset!\nThis can now be merged into other PMA data, such as the individual level dataset bf_merged we worked with in the other posts in this module, and the variables can be used for analysis!\nAs always, let us know if you have any questions and if you’re doing anything exciting with the PMA spatial data!\nSpecial thanks to Tracy Kugler, Nicholas Nagle, and Jonathan Schroeder for excellent help with this post.\n\nLinard, C., Gilbert, M., Snow, R. W., Noor, A. M., & Tatem, A. J. (2012). Population distribution, settlement patterns and accessibility across Africa in 2010. PloS one, 7(2), e31743.↩︎\nNEON: https://www.neonscience.org/resources/learning-hub/tutorials/raster-res-extent-pixels-r↩︎\nWikipedia↩︎\nhttps://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/intro-to-coordinate-reference-systems/↩︎\n",
    "preview": "posts/2021-02-04-merging-external-spatial-data/images/road_map.png",
    "last_modified": "2021-02-17T15:54:48-06:00",
    "input_file": {},
    "preview_width": 936,
    "preview_height": 574
  },
  {
    "path": "posts/2021-02-02-blog-post-workflow/",
    "title": "Blog post workflow",
    "description": "How to create or review a PMA Data Hub blog post",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-02-02",
    "categories": [
      "internal"
    ],
    "contents": "\n\nContents\nAll users: first time setupR, RStudio, and required packages\nInitialize your UMN GitHub account\nUsing UMN GitHub from RStudio\nThe PMA Data Hub Repository\n\nAuthors: Creating a new postCreate a new branch\nCreate a new folder in \"_posts\"\nPush your post to GitHub\n\nEditorsPull the author’s branch to your computer\nLocate and edit the new post\nPush the edited post back to GitHub\n\nAll users: making revisions\nSite AdminSetup\nMerging\nOther tips\n\n\nAll users: first time setup\nR, RStudio, and required packages\nTo get a copy of R, visit the Comprehensive R Archive Network (CRAN) and choose the right download link for your operating system.\nThe PMA Data Hub is organized as an RStudio Project, so you’ll also need to use RStudio (not base R).\nNote: a copy of RStudio running R version 4.0.2 (or higher) lives on the MPC gp1 server here. Members of the MPC GitHub organization can access an article specifically about using RStudio on gp1 (e.g. how to build a package library) here.\nWhen you’ve got RStudio set up, install these packages:\n\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"knitr\")\ninstall.packages(\"distill\")\ninstall.packages(\"usethis\")\n\n\n\nTroubleshooting note: the package rmarkdown comes along with RStudio, but you may receive an older version than we need to build the site. So, when you try install.packages(\"rmarkdown\"), you may get a message asking to restart R (avoiding a conflict with the prior version). We’ve found that it’s best to reply ‘No’ to the restart prompt, wait for rmarkdown to install, and then re-launch RStudio. If you do choose to restart, you may experience a recursive loop of restart prompts!\nInitialize your UMN GitHub account\nContributors to the PMA Data Hub will work on an internal copy of the public site - it’s visible only to certain people affiliated with the MPC. A smaller team of Data Hub “admin” (currently Matt & Nina) will take care of migrating content from the private site to our public site: we’re always here to help with formatting, editing, and version control!\nSo what is UMN GitHub? GitHub, itself, is a company that hosts projects on proprietary server: when you make a repository “public”, anyone in the world can visit your project on a GitHub server. GitHub also makes its underlying software available to institutions that want to provide a similar service restricted to institutional members. In practice, UMN operates its own GitHub server where organizations like the MPC can host projects that are more private in scope.\nUMN GitHub is an instance of Enterprise GitHub, whereas the public version of our blog lives in a space that folks sometimes call “public GitHub”. It’s common for people to have one account for “public GitHub” and one account for their job associated with an “enterprise GitHub”. To initialize an account for UMN GitHub, visit github.umn.edu and log in with your University Internet ID and password.\nUsing UMN GitHub from RStudio\nFirst things first: you must install install Git on your computer if it isn’t there already. Mac OS comes with git installed,1 while other users should download the right Git for their operation system. If you’re using RStudio on the gp1 server, Git is already installed.\nNext, open the Global Options menu in RStudio and locate the Git/SVN tab. Ensure that the box shown below is checked, and then enter the location of the executable file2 for your Git installation:\n\n\n\nLastly, you should provide Git with a username, email, and Personal Access Token (PAT) for your UMN GitHub account. If you’ve installed usethis as shown above, you’ll be able to set these up with R commands (changes will be applied to globally wherever you use Git on your operating system). First, set the username and email address for your UMN GitHub account. For example, mine are:\nWhy a PAT? GitHub plans to deprecate password authentication in the near future. You could use one for now (like the example below), but you’ll need one soon!\n\n\ngert::git_config_global_set(\"user.name\", \"Matt Gunther\")\ngert::git_config_global_set(\"user.email\", \"mgunther@umn.edu\")\n\n\n\nThen, create a PAT for your account with:\n\n\nusethis::create_github_token(host = \"https://github.umn.edu\")\n\n\n\nYour browser will open to a webpage. Check all the boxes you see, then click the green Generate Token button. On the next page, notice the very long string shown in the green box: this is your PAT. Don’t close this page yet!. Return to R and call:\n\n\ngitcreds::gitcreds_set(\"https://github.umn.edu\")\n\n\n\nYou’ll be asked to enter a new password or token: copy and paste your PAT from your browser and press Enter. From now on, RStudio and Git will be able to access your UMN GitHub account automatically! (If you have a personal GitHub account at github.com, you could repeat this process substituting https://github.com for https://github.umn.edu, and Git will automatically choose the right credentials based on the repository associated with your project).\nThe PMA Data Hub Repository\nOpen RStudio and navigate to File > New Project, then select Version Control:\n\n\n\nChoose Git to clone our project from a GitHub repository:\n\n\n\nOn the next menu page, enter the address for the enterprise repository exactly as shown (do not clone the public repository):\nhttps://github.umn.edu/mpc/pma-data-hub/\nAlso enter the project directory name “pma-data-hub” as shown:\npma-data-hub\nIn the third field, choose a location where you would like to save this file on your computer (mine was “~/R” - insert your own path, instead). Finally, click Create Project.\n\nWhen choosing a place to save this project, do not save to a network drive. This seems to cause RStudio to crash!\n\n\n\nIf you have not configured Git to automatically use your UMN GitHub credentials with the steps shown above, you may be prompted to provide them in a pop-up window:\n\n\n\n\nUntil you configure Git with these credentials, you’ll have to do this every time you interact with GitHub. Additionally, password authentication for GitHub will be deprecated in the near future so you’ll need to do it soon!\nAfter a short bit, RStudio will relaunch and open the new project. If you adjust the windows to show the tabs Git (left) and Files (right), you should see something like this:\n\n\n\nYou have now downloaded a copy of the Enterprise repository to your computer!\nMoreover, because you’ve connected these files to a GitHub repository, the RStudio Project will now keep track of changes you make to the files in this folder, and it will prompt you to upload your changes back to GitHub: as you add, edit, or delete files, a list of changes will appear in the Git tab.\n\nTo open an RStudio project, click on the file pma-data-hub.Rproj. If you ever forget, RStudio won’t know to look for a Git history associated with all of the underlying files.\nNotice the word master shown in the Git tab - this shows that any changes we make to files will be recorded in our local copy of the “master” version of the repository. If we made changes here and then pushed them to GitHub, they would be reflected on the “master” version we’ve saved there, too.\nIn general, Matt and Nina will be responsible for merging finished blog posts to the master branch and deploying its contents to the “live” blog that’s seen by users (for details, see site admin instructions below). All other contributors should create their own branch when writing a new blog post; Matt or Nina will merge them to “master” after they’ve been reviewed and approved by an editor. Read on!\nAuthors: Creating a new post\nCreate a new branch\nNotice that the Git tab in RStudio has a purple icon:\n\n\n\nClick this icon to create a new branch. You can name it anything you like, but we recommend using your URL slug if possible (e.g. “blog-post-workflow” is the end of the URL for this webpage). Leave the box next to “Sync branch with remote” checked, as this will create your branch both locally and on our GitHub page:\n\n\n\nRStudio now displays the new branch in place of “master” to show that we’re working on the new branch, instead!\nCreate a new folder in \"_posts\"\nNow that you’ve created a new branch in the Git window, take a look at the File window.\n\n\n\nThe program we use to build the blog is called Distill, and it takes care of all the back-end work as long as we put every new blog post inside of a unique folder within the \"_posts\" directory. Opening \"_posts\", you can see that every post is contained within a time-stamped subfolder:\n\n\n\nTo create one of these folders for your new post, enter the following command into R:\n\n\ndistill::create_post(\"Blog post workflow\")\n\n\n\nThis does two things: it creates the folder automatically (circled in red), and it opens a new RMarkdown file where you can begin writing your post (circled in green).\n\n\n\nIn red: notice the folder appears in both your File tab and your Git tab. (Don’t worry about the date on this folder - it’s for internal use and does not need to match the publication date.)\nIn green: this is the RMarkdown file where you’ll write your post.\nCheck out our Quick-start Guide for Blogging with RMarkdown!\nAs you’re writing your post, you can preview it as a fully formatted webpage by hitting the Knit button at the top of your RMarkdown file:\n\n\n\n\nYou can switch between previewing the page in the RStudio Viewer tab, or in your computer’s default web browser. Click the settings icon next to “Knit” for preview options.\nPush your post to GitHub\nWhen you’re finished writing, follow these steps to share your post with the team on our Enterprise GitHub page (reminder: it won’t go “live” until Matt or Nina merges your post to master and publishes it to the public GitHub page).\nPress the “Knit” button one more time to render a final HTML version of the page. (At this point, you may see a number of automatically created files related to your RMarkdown file in the Git tab.)\nNow, enter the following commands directly into the R console, but please adjust the brief commit “message” as necessary to describe your change!\n\n\ngert::git_add(\".\")\ngert::git_commit(\"Draft published: blog-post-workflow\")\ngert::git_push()\n\n\n\nNow, if you visit our Enterprise GitHub page, your post will appear in a new branch! (It will not yet appear on the copy of the blog we have posted there, which is rendered only from master.)\n\n\n\nEditors\nPull the author’s branch to your computer\nAny time that you want to review an author’s post, you’ll always need to get the latest copy of their branch from our Enterprise GitHub page. If this is the first time you’ve read a draft for the post, the author’s branch won’t yet be listed in RStudio.\n\n\n\n\nThis editor’s RStudio only knows about the “master” branch so far.\nThe Pull button in RStudio’s Git tab will gather information about all of the new branches on GitHub, and it will download a copy of each one onto your computer:\n\n\n\nClicking it will bring up a dialogue screen. RStudio reports that it discovered a new branch blog-post-workflow living at the remote repository origin.\n\n\n\nReturning to RStudio’s main window, notice that you can now toggle between the working on the remote master branch, or the new remote branch called blog-post-workflow. When you’re ready to edit the author’s post, use this menu to select their branch.\n\n\n\nRStudio automatically creates a local version of this branch on your computer, and it reports that your changes will be tracked and pushed to the remote branch of the same name.\n\n\n\nLocate and edit the new post\nNow, RStudio shows that you’re working on the author’s branch in the Git tab, and you’ll see their post listed in the _posts folder on the Files tab. Navigate to the .Rmd file for their post, then click it to begin making edits.\n\n\n\nCheck out our Quick-start Guide for Blogging with RMarkdown!\nAs you’re writing your post, you can preview it as a fully formatted webpage by hitting the Knit button at the top of your RMarkdown file:\n\n\n\n\nYou can switch between previewing the page in the RStudio Viewer tab, or in your computer’s default web browser. Click the settings icon next to “Knit” for preview options.\nPush the edited post back to GitHub\nWhen you’re finished editing, follow these steps to send the revised file back to GitHub. (Reminder: it won’t appear on the website until Matt or Nina merges the post to master and publishes it to the public GitHub page).\nPress the “Knit” button one more time to render a final HTML version of the page. (At this point, you may see a number of automatically created files related to your RMarkdown file in the Git tab.)\nNow, enter the following commands directly into the R console, but please adjust the brief commit “message” as necessary to describe your change!\n\n\ngert::git_add(\".\")\ngert::git_commit(\"Draft edited: blog-post-workflow\")\ngert::git_push()\n\n\n\nNow, if you visit our Enterprise GitHub page, you’ll see that your edited files appear on the author’s branch. (They will not yet appear on the copy of the blog we have posted there, which is rendered only from master.)\n\n\n\nAll users: making revisions\nWhen you’ve finished pushing something to GitHub, please email your collaborators to let them know about next steps!\nTo get your collaborator’s latest updates from GitHub, you should launch the pma-data-hub RStudio project file called pma-data-hub.Rproj. If you forget to open the project file, RStudio will not be able to access the contents of your Git folder (it won’t know that there’s a GitHub repository for the project at all).\nWhen you open the project file, RStudio will again display a Git tab. Click on the Pull button to get your collaborator’s latest changes.\n\n\n\nSwitch from the master branch to the branch associated with your post:\n\n\n\nAfter you’re finished incorporating their feedback into the RMarkdown file (.Rmd), click Knit and then run these commands in Terminal again to send your work back to GitHub:\n\n\ngert::git_add(\".\")\ngert::git_commit(\"Draft complete: blog-post-workflow\")\ngert::git_push()\n\n\n\nPlease let Matt & Nina know when your revisions are complete and ready to appear on the live blog!\nSite Admin\nThese instructions will introduce an additional remote to your local repository. Only use them if you’ll be involved with managing updates to the live blog (e.g. Matt & Nina). All of these functions can be used the in RStudio Terminal.\nSetup\nBefore adding a second remote, it’s best to rename the UMN GitHub remote something like private. You can check the current name with:\ngit remote\nIf the UMN GitHub is still called origin (by default), rename it with:\ngit remote rename origin private\nLikewise, you should give the private branch master a different name, as the second remote will have a master branch, too. To see all of the remote branches currently in use:\ngit remote show private\nCheckout master and change its name to something like private-master:\ngit checkout master\ngit branch -m \"private-master\"\nNow, add the public remote and fetch its branches (there should only be one, called master):\ngit remote add public https://github.com/ipums/pma-data-hub\ngit fetch public\nCreate a local branch called public-master corresponding with master on the public remote:\ngit branch public-master public/master\nAt this point, you’ll notice that RStudio shows two remotes (with the branches you’ve fetched) and all of the local branches you’ve created so far.\n\n\n\n\nIn this example, we set the local “private-master” branch to track “master” on the private remote. The “public-master” branch tracks “master” on the public remote.\nMerging\nOur main goal is to avoid creating divergent commit histories between the internal repository and the public repository. In practice, that means a typical workflow will involve these steps:\nSquash and Merge the author’s branch to private/master\nAdd the new post to index.Rmd\nBuild, Commit, and Push to private/master\nMerge private/master to public/master\nPush to public/master\nOur authoring / editing workflow generates a commit each time someone adds a change to the branch git log (you can run git log on any branch to see its full commit history). We will squash and merge this commit history into a single commit on private-master.\nFor example, with a new post on the branch blog-post-workflow:\ngit checkout private-master\ngit merge --squash blog-post-workflow\nWe have two posts that should not be included in the blog index: this one, and the Quick-start Guide for Blogging with RMarkdown. In order to make that work, we have to whitelist the posts we do want in the file index.Rmd.3\nNow, in RStudio, hit the Build Website button. Look over the site to make sure everything looks good. (We won’t build on public-master to avoid merge conflicts, so get those edits in now.)\n\n\n\nWhen you’re ready, add all files and commit your changes with a message like:\ngit add .\ngit commit -m \"new post: blog-post-workflow\"\nAlthough your local branch has a different name, you can push your commit to master on the private remote with this command:\ngit push private HEAD:master\nWait a few minutes, and you’ll see that the GitHub Pages site hosted at UMN GitHub should update to reflect your changes.\nTo merge your private-master to public-master:\ngit checkout public-master\ngit merge private-master\nFinally, push your changes to the live site:\ngit push public HEAD:master\nOther tips\nNeed to rollback to a previous commit? Look for it in the git log, and do a hard reset:\ngit log\ngit reset --hard 58ba4f0396b985fb5ab82c88f7bbc5c9cc619e71\nFor a checklist of updated files and their commit status:\ngit status\n\nYou can check its location by running “which git” in Terminal, and “git –version” to check the installed version. If git is somehow not installed, use the “Install git using Homebrew” instructions here↩︎\nMac users: type “which git” in terminal and enter the result; Windows users: look for git.exe (most likely in Program Files)↩︎\nAnnoying, yes? Hopefully, an exclusion logic will become available in the next distll release.↩︎\n",
    "preview": "posts/2021-02-02-blog-post-workflow/images/git-menu.png",
    "last_modified": "2021-02-16T13:59:57-06:00",
    "input_file": {},
    "preview_width": 1180,
    "preview_height": 1160
  },
  {
    "path": "posts/2021-01-29-mapping-sdp-variables/",
    "title": "Mapping Service Delivery Point Data",
    "description": "Map spatial variation in the service delivery environment across enumeration areas.",
    "author": [
      {
        "name": "Nina Brooks",
        "url": "http://www.ninarbrooks.com/"
      }
    ],
    "date": "2021-01-30",
    "categories": [
      "Individuals in Context",
      "Service Delivery Points",
      "Data Manipulation",
      "Mapping",
      "sf"
    ],
    "contents": "\n\nContents\nData\nSetup\nMerge and MapBasic Maps\nMerge GPS and SDP Data\nMap SDP data\n\nPutting it All Together\n\n\n\n Download this page as R code\n\n\nIn our last post, we showed how PMA Service Delivery Point (SDP) data can be aggregated to the enumeration area they serve (captured by EASERVED) and linked to individual-level data from a PMA Household and Female survey. In this post, we’ll continue thinking about the spatial distribution of SDP summary data. We’ll first show how to merge our example data to a GPS dataset obtained from pmadata.org, and we’ll then use the new dataset to visualize a few of our variables on a map of Burkina Faso.\nData\nWe’ll be working with the final bf_merged dataset created in the previous post, which is included in the data folder when you click with Download button at the top of this page. We’ll focus on a handful of EASEARVED-level summary variables created in that post:\nNUM_METHODS_PROV - number of methods provided by at least one SDP\nNUM_METHODS_OUT - number of methods out of stock with at least one SDP\nNUM_METHODS_INSTOCK - number of methods in-stock with at least one SDP\nNUM_METHODS_OUT3MO - number of methods out of stock any time in the last 3 months with at least one SDP\nMEAN_OUTDAY - the mean length of a stockout for any family planning method (measured in days)\nN_SDP - number of SDPs\nWe’ll also use the URBAN variable, which identifies whether an EA is in an urban or rural area.\nWe’ll also be working with the PMA GPS datasets for Burkina Faso. The GPS data include one GPS coordinate per enumeration area. The Burkina Faso Round 5 and 6 surveys sampled the same enumeration areas, which means we can link the GPS data to both rounds. To use the GPS data you must request access directly from our partners at pmadata.org. If you want to follow along with this post without accessing the GPS data, we have provided a “toy” GPS dataset in the data folder for this post. The toy data contains randomly sampled locations within Burkina Faso which have no actual relationship to the EAs in the PMA data. This means none of the interpretations of spatial patterns will hold, but all the code will run.\n\nIf you request access to the GPS data, make sure to drop the file into the data folder for your local version of this post so you can run everything smoothly.\nThe last dataset we’ll use in this post are the administrative boundaries for Burkina Faso. Shapefiles with administrative boundaries are widely available for download, but we’ll use the ones made available from IPUMS PMA.\nSetup\nMake sure you have all of the following packages installed. Once installed, load the packages we’ll be using today:\n\n\nlibrary(ipumsr)\nlibrary(sf) # primary spatial package\nlibrary(viridis) # for color palettes\nlibrary(tabulator) # for pipe-friendly tabs & cross-tabs\nlibrary(tidyverse)\n\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nLet’s start by loading the bf_merged data. Remember that we merged the EA-level data into the individual-level data, but the GPS datasets provide coordinates for the enumeration area. So the first thing we’ll do is aggregate bf_merged to the EA-level, and assign the aggregated data to a new object called bf_ea.\n\n\nbf_merged <- readRDS(\"data/bf_merged.rds\") \n\nbf_ea <- bf_merged %>%\n  dplyr::select(-PERSONID) %>%\n  group_by(EAID, SAMPLE) %>%\n  summarise_all(mean, na.rm = T) %>%\n  filter(!is.na(N_SDP)) \n\n\n\n\nIn this example, we’ll exclude any EAs where no facilities in our SDP sample provide services with filter(!is.na(N_SDP))\nNow, let’s read in the GPS data from the data folder and see what the it contains.\n\n\ngps <- read_csv(\"data/bf_gps_fake.csv\")\ngps\n\n\n# A tibble: 83 x 7\n   PMACC PMAYEAR REGION               EA_ID DATUM GPSLAT GPSLONG\n   <chr>   <dbl> <chr>                <dbl> <chr>  <dbl>   <dbl>\n 1 BF       2017 5. centre-nord        7610 WGS84   14.2  0.126 \n 2 BF       2017 1. boucle-du-mouhoun  7820 WGS84   13.5 -3.08  \n 3 BF       2017 3. centre             7271 WGS84   12.2  1.43  \n 4 BF       2017 3. centre             7799 WGS84   12.2 -0.799 \n 5 BF       2017 8. est                7243 WGS84   11.0 -2.58  \n 6 BF       2017 6. centre-ouest       7026 WGS84   10.9 -4.35  \n 7 BF       2017 3. centre             7859 WGS84   12.4  0.0703\n 8 BF       2017 3. centre             7725 WGS84   12.7  1.42  \n 9 BF       2017 6. centre-ouest       7390 WGS84   10.8 -3.55  \n10 BF       2017 11. plateau-central   7104 WGS84   13.3  0.0957\n# … with 73 more rows\n\n\nIf you requested access to the actual GPS datasets, make sure to replace the bf_gps_fake.csv with the filename of the real data!\n\n\n\nThe gps data has 7 variables:\nPMACC: the country code\nPMAYEAR: the 4-digit year of data collection\nREGION: sub-national administrative division name\nEA_ID: the enumeration area ID (and how we’ll merge this data into other PMA datasets)\nGPSLAT: the displaced EA’s centroid latitude coordinate in decimal degrees\nGPSLONG: the displaced EA’s centroid longitude coordinate in decimal degrees\nDATUM: the coordinate reference system and geographic datum. This variable is always “WGS84” for the World Geodetic System 1984.\n\nNote that while the PMAYEAR variable is 2017 for all EAs, because the same EAs were sampled in the 2017 (Round 5) and 2018 (Round 6) surveys, we can link these coordinates to both samples.\nNote that the GPSLAT and GPSLONG are displaced coordinates of the EA centroid. This is because PMA randomly displaces the geographic coordinates to preserve the privacy of survey respondents. Coordinates are displaced randomly by both angle and distance. Urban EAs are displaced from their true location up to 2 km. Rural EAs are displaced from their true location up to 5 km. Additionally, a random sample of 1% of rural EAs are displaced up to 10km. The PMA GPS data come with documentation that explains the displacement in more detail. The primary spatial package we’ll use is simple features or sf. We’ll use sf::st_as_sf() to convert the GPS data to a spatial data object (known as a simple feature collection).\n\n\ngps <- gps %>%\n    rename(EAID = EA_ID) %>% # rename to be consistent with other PMA data\n    st_as_sf(\n      coords = c(\"GPSLONG\", \"GPSLAT\"), \n      crs = 4326) # 4326 is the coordinate reference system (CRS) identifier for WGS84\ngps\n\n\nSimple feature collection with 83 features and 5 fields\ngeometry type:  POINT\ndimension:      XY\nbbox:           xmin: -5.185229 ymin: 10.08082 xmax: 1.829187 ymax: 14.93619\ngeographic CRS: WGS 84\n# A tibble: 83 x 6\n   PMACC PMAYEAR REGION               EAID DATUM              geometry\n * <chr>   <dbl> <chr>               <dbl> <chr>           <POINT [°]>\n 1 BF       2017 5. centre-nord       7610 WGS84  (0.1263576 14.15999)\n 2 BF       2017 1. boucle-du-mouho…  7820 WGS84   (-3.075099 13.4676)\n 3 BF       2017 3. centre            7271 WGS84   (1.430382 12.17557)\n 4 BF       2017 3. centre            7799 WGS84 (-0.7991777 12.22721)\n 5 BF       2017 8. est               7243 WGS84  (-2.580105 11.03307)\n 6 BF       2017 6. centre-ouest      7026 WGS84  (-4.348525 10.93844)\n 7 BF       2017 3. centre            7859 WGS84 (0.07032293 12.44352)\n 8 BF       2017 3. centre            7725 WGS84   (1.417154 12.68821)\n 9 BF       2017 6. centre-ouest      7390 WGS84  (-3.549929 10.77006)\n10 BF       2017 11. plateau-central  7104 WGS84 (0.09573113 13.27184)\n# … with 73 more rows\n\n\n\n\nNow that gps is a simple features object, we’ve lost the GPSLAT and GPSLONG variables and gained a variable called geometry, which contains the spatial information for this data.\n\n\n\nThe last thing we need is the Burkina Faso shapefile, which are available from IPUMS PMA. You’ll need to download the shapefile (geobf.zip) from the IPUMS site and save it in the appropriate directory to use it. We’ve also provided it in the data folder for this post in a sub-folder called geobf. We can use sf::st_read() to read the shapefile into R as an sf object. Note that here the geometry variable is a POLYGON, whereas in the gps data it is a POINT.\n\nNote that what we call a shapefile is actually a collection of many files. More on this in a future post! But for now, just know that you’ll need all the files that come in the zipped download and can refer to the collectively with “geobf.shp”.\n\n\nbf_shp <- st_read(\"data/geobf/geobf.shp\") \n\n\n\nMerge and Map\nNow that we have all our data, we’ll show you how to map variables… but before we do that, let’s do some basic, exploratory mapping.\nBasic Maps\nggplot2 has support for sf objects, which makes it really easy to map things using the ggplot2 system. ggplot2::geom_sf() will automatically identify what kind of spatial data you’re plotting and handle it appropriately. For example, let’s plot the gps data (which are points) and the administrative region (which are polygons).\nggplot2 is included when you load library(tidyverse)\n\n\n# Plot EA centroids\nggplot() +\n  geom_sf(data = gps)\n\n\n\n# Plot regions of Burkina Faso\nggplot() +\n  geom_sf(data = bf_shp)\n\n\n\n\nThe building-block approach of ggplot2 (“Grammar of Graphics”) also makes it really easy to layer different spatial features on the same map.\n\n\n# Plot regions of Burkina Faso & EA centroids on the same map\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = gps)\n\n\n\n\nMerge GPS and SDP Data\nTo map the EA-level variables constructed in the last post, we need to merge the bf_ea data and the gps data by EAID. First, let’s rename the EASEARVED variable to match the GPS data and then use a dplyr::right_join to merge in the SDP data. We need to use a right_join() because the sf object must be listed first in our join command to retain the sf class, but we want to ensure that all rows from bf_ea are preserved.\n\nRemember, the SDP data contains information from both 2017 and 2018, while the GPS data has a single observation per EA.\n\n\nbf_ea <- right_join(gps, bf_ea, by = \"EAID\")\n\n\n\nMap SDP data\nRemember, the bf_ea data contains information from 2017 & 2018 for the same EA, which can clog up the map depending on how we use this information. To start out, let’s use only the 2017 data and add information about the number of service delivery providers that serve a given EA (N_SDP). By passing N_SDP to the size aesthetic, we can more easily visualize how EAs vary in their access to service delivery providers.\n\n\nbf_ea2017 <- bf_ea %>%\n  filter(SAMPLE == 85405) # this sample corresponds to the 2017 wave\n\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = bf_ea2017, \n          aes(size = N_SDP),\n          alpha = 0.4) \n\n\n\n\nFrom the map, it looks like there may be a few locations where there EAs are both close together and served by many SDPs, which are likely in urban areas. For example, the capital of Burkina Faso, Ouagadougou, is in the center of the map where there are a number of EAs on top of each other. But, it’s a little hard to see the variation in size when there are so many values for N_SDP and so many EAs on top of each other. Let’s do two things to make this more readable. First, we’ll create smaller categories of the N_SDP variable, and second, we’ll map the URBAN variable to the color aesthetic.\n\n\nbf_ea2017 <- bf_ea2017 %>%\n  mutate(\n    N_SDP_CAT = case_when(\n      N_SDP <= 2 ~ 1,\n      N_SDP >2 & N_SDP <= 4 ~ 2,\n      N_SDP >4 ~ 3),\n    N_SDP_CAT = factor(N_SDP_CAT,\n                       levels = c(1, 2, 3),\n                       labels = c(\"Low\", \"Mid\", \"High\"),\n                       ordered = T), # needs to be an ORDERED factor to map to the size aesthetic\n    MEAN_OUTDAY = ifelse(is.na(MEAN_OUTDAY), 0, MEAN_OUTDAY),\n    URBAN = factor(URBAN, \n                   levels = c(0,1),\n                   labels = c(\"Rural\", \"Urban\"))\n  )\n\n\n# let's take a look at the distribution of this new categorical variable\nbf_ea2017 %>% \n  tab(URBAN, N_SDP_CAT)\n\n\nSimple feature collection with 5 features and 5 fields\ngeometry type:  MULTIPOINT\ndimension:      XY\nbbox:           xmin: -5.18865 ymin: 9.883331 xmax: 1.634087 ymax: 14.39679\ngeographic CRS: WGS 84\n# A tibble: 5 x 6\n  URBAN N_SDP_CAT     N                        geometry  prop cum_prop\n  <fct> <ord>     <int>                <MULTIPOINT [°]> <dbl>    <dbl>\n1 Rural Mid          29 ((-5.18865 11.54962), (-4.3401…  0.35     0.35\n2 Urban Mid          23 ((-5.178223 10.66001), (-4.781…  0.28     0.63\n3 Urban Low          15 ((-4.307564 11.18051), (-4.299…  0.18     0.81\n4 Rural Low          13 ((-4.969563 10.45619), (-4.804…  0.16     0.96\n5 Urban High          3 ((-4.262726 11.14746), (-1.528…  0.04     1   \n\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = bf_ea2017, \n          aes(size = N_SDP_CAT,\n              color = URBAN),\n          alpha = 0.4) +\n  scale_color_viridis_d() \n\n\n\n\nFrom this map we can see that urban areas are generally served by more SDPs – in fact, no rural EAs fall into the “High” category – although the difference is perhaps not as stark as one might have expected. But, what is the service environment like? Do urban areas have more stockouts than rural areas? Do SDPs in urban areas offer a greater selection of family planning methods? Did the service environment change between 2017 and 2018? Mapping can shed a lot of light on these questions.\nLet’s look at the NUM_METHODS_PROV variable created in the last post. This variable captures the number of family planning methods provided by at least one SDP that serves a given EA.\n\n\nbf_ea2017 %>%\n  tab(NUM_METHODS_PROV) %>%\n  arrange(NUM_METHODS_PROV)\n\n\nSimple feature collection with 5 features and 4 fields\ngeometry type:  MULTIPOINT\ndimension:      XY\nbbox:           xmin: -5.18865 ymin: 9.883331 xmax: 1.634087 ymax: 14.39679\ngeographic CRS: WGS 84\n# A tibble: 5 x 5\n  NUM_METHODS_PROV     N                       geometry  prop cum_prop\n             <dbl> <int>               <MULTIPOINT [°]> <dbl>    <dbl>\n1                8     2 ((-4.299839 11.18039), (-2.96…  0.02     1   \n2                9    13 ((-4.340111 11.8743), (-4.281…  0.16     0.92\n3               10    34 ((-5.18865 11.54962), (-4.969…  0.41     0.41\n4               11    29 ((-5.178223 10.66001), (-3.84…  0.35     0.76\n5               12     5 ((-2.757122 11.53829), (-2.26…  0.06     0.98\n\nSince there is not a large range of number of FP methods provided, let’s dichotomize this so we can map it to the shape aesthetic.\n\n\nbf_ea2017 <- bf_ea2017 %>%\n  mutate(\n    NUM_METHODS_CAT = case_when(\n      NUM_METHODS_PROV <= 9 ~ 1,\n      NUM_METHODS_PROV >9  ~ 2),\n    NUM_METHODS_CAT = factor(NUM_METHODS_CAT,\n                       levels = c(1, 2),\n                       labels = c(\"Low (<=9)\", \"High (>9)\"),\n                       ordered = T)\n  )\n\n\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = bf_ea2017, \n          aes(size = NUM_METHODS_CAT,\n              shape = URBAN,\n              color = NUM_METHODS_OUT3MO),\n          alpha = 0.4) +\n  scale_color_viridis_c() \n\n\n\n\nPutting it All Together\nNow we have a map that shows spatial variation in availability of different methods of family planning and prevalence of stock-outs, as well as demonstrates how these characteristic differ across urban vs. rural EAs. It’s super quick to make a basic map like this, but let’s clean up a few things to make it look nicer.\n\n\nggplot() +\n  geom_sf(data = bf_shp, fill = \"#f2f2f5\") +\n  geom_sf(data = bf_ea2017, \n          aes(size = NUM_METHODS_CAT,\n              shape = URBAN,\n              color = NUM_METHODS_OUT3MO),\n          alpha = 0.4) +\n  scale_color_viridis_c(direction = -1) + # reversing the direction makes the high #s stand out more\n  labs(title = \"Spatial Variation in Family Planning Service Environment\",\n       subtitle = \"Burkina Faso 2017\",\n       caption = \"Source: IPUMS PMA\",\n       shape = \"\",\n       size = \"Methods\\nProvided\",\n       color = \"Out of Stock\\n(Past 3 Months)\",\n       x = NULL,\n       y = NULL) +\n  theme_minimal() +\n  theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank())\n\n\n\n\n\n\n\nThis map suggests there is spatial correlation to the stockouts – with 2 regions responsible for the majority of EAs with stockouts. It also looks like these EAs tend to have more methods provided by the SDPs that serve them. Finally, let’s use both years of data and see if there is any temporal variation. To do this, we’ll use the original bf_ea dataset (instead of sdp2017) and re-create the same NUM_METHODS_CAT factor variable that dichotomizes the NUM_METHODS_PROV variable. Then, we’ll use facet_wrap() to make a multi-panel plot, with one panel per year.\n\n\nbf_ea <- bf_ea %>%\n  mutate(\n    NUM_METHODS_CAT = case_when(\n      NUM_METHODS_PROV <= 9 ~ 1,\n      NUM_METHODS_PROV >9  ~ 2),\n    NUM_METHODS_CAT = factor(NUM_METHODS_CAT,\n                       levels = c(1, 2),\n                       labels = c(\"Low (<=9)\", \"High (>9)\"),\n                       ordered = T),\n    YEAR = case_when(\n      SAMPLE == 85405 ~ 2017,\n      SAMPLE == 85408 ~ 2018\n    ),\n    URBAN = factor(URBAN, \n                   levels = c(0,1),\n                   labels = c(\"Urban\", \"Rural\"))\n  )\n\nggplot() +\n  geom_sf(data = bf_shp, fill = \"#f2f2f5\") +\n  geom_sf(data = bf_ea, \n          aes(size = NUM_METHODS_CAT,\n              shape = URBAN,\n              color = NUM_METHODS_OUT3MO),\n          alpha = 0.4) +\n  facet_wrap(~ YEAR) +\n  # reversing the direction makes the high #s stand out more\n  scale_color_viridis_c(direction = -1) + \n  guides(color = guide_colorbar(barheight = .75,\n                                barwidth = 4.5,\n                                label.position = \"top\",\n                                label.hjust = 0)) + \n  labs(title = \"Spatial Variation in Family Planning Service Environment\",\n       subtitle = \"Burkina Faso 2017-2018\",\n       caption = \"Source: IPUMS PMA\",\n       shape = \"\",\n       size = \"Methods\\nProvided\",\n       color = \"# Methods\\nOut of Stock\\n(Past 3 Months)\",\n       x = NULL,\n       y = NULL) +\n  theme_minimal() +\n  theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    legend.title = element_text(size = 8),\n    legend.position = \"bottom\") \n\n\n\n\nWith the 2018 data included, it looks like the service environment may have improved between 2017 and 2018 with fewer stockouts. However, it also looks the EAs that faced more stockouts in 2017 are not always the same as those facing stockouts in 2018. But, there is still a spatial pattern to the stockouts in 2018. It also looks like some EAs had fewer family planning methods available from SDPs in 2018 than in 2017, specifically in the western part of the country.\nFuture posts may explore other supply-side factors that could influence the SDPs (and look at how these change over time) or examine demand-side factors by merging in the individual-level data.\nAs always, let us know what kinds of questions about fertility and family planning you’re answering – especially if you’re doing anything spatial!\n\n\n\n",
    "preview": "posts/2021-01-29-mapping-sdp-variables/images/bf_fp_map.png",
    "last_modified": "2021-02-02T15:48:43-06:00",
    "input_file": {},
    "preview_width": 2100,
    "preview_height": 1350
  },
  {
    "path": "posts/2021-01-28-summarize-by-easerved/",
    "title": "Merging Service Delivery Point Data to Household & Female Records",
    "description": "Create aggregate measures for women living the areas served by SDPs",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-29",
    "categories": [
      "Individuals in Context",
      "Data Manipulation",
      "Service Delivery Points",
      "tidyr"
    ],
    "contents": "\n\nContents\nReviewing SDP Sample Design\nSetup: Load an Example Dataset into R\nEAID and EASERVED\nPivot Longer: EASERVED in Rows\nSummarise by EASERVED and SAMPLE\nMerging to Household and Female Data\n\n\n\n Download this page as R code\n\n\nWelcome to the third post in a series all about using PMA Service Delivery Point (SDP) data to better understand Individuals in Context. In our last post, we discussed a few of the variable groups related to contraceptive availability, and we showed how to use functions like dplyr::across to recode and summarize these variable groups in preparation for merging with Household and Female data.\nBefore we dive in, let’s quickly revisit the geographic sampling units - or enumeration areas - we’ll be using to link SDPs with their counterparts in the Household and Female data.\nReviewing SDP Sample Design\nRemember: the SDP sample design selects facilities meant to reflect the health service environment experienced by individuals included in Household and Female samples. If you were designing survey with this goal in mind, how would you select facilities?\nWell, you might target a sample of facilities located within the same geographic sampling units PMA used to define Household and Female samples from the same country in the same year. Presumably, the health services available to a woman living in enumeration area X would be captured pretty well if we surveyed a list of facilities also located in enumeration area X.\nBut what happens if a lot of women living in enumeration area X travel to enumeration area Y to receive family planning services? In that case, you’d want to know as much as possible about the service catchment areas for facilities in that country. Then, you could select facilities based on whether they provide services to enumeration area X, rather than relying simply to those that are located there.\nIn fact, PMA partners with government health agencies to obtain information about the service catchment area for all of the public-sector health facilities in each participating country. As a result, public SDPs are sampled if one of the enumeration areas used in a corresponding Household and Female sample appears in their service catchment area.\nBecause service catchment data are only available for public facilities, PMA uses a different method to select private-sector facilities. A private facility will be selected for a SDP sample only if it is located inside the boundaries of an enumeration area included in a corresponding Household and Female sample.\nSetup: Load an Example Dataset into R\nLet’s take a look at an example SDP dataset to see how all of this information gets reported. We’ll use the same data we highlighted in our last post, which includes facilities sampled from Burkina Faso in 2017 and 2018. First, load the following packages into R:\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\n\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nYou’ll find the data for this example in the data folder if you use the Download button at the top of this page, or you can select a data extract for yourself. For this example, we’ll be working with all of the available contraceptive services variables ending with the suffixes PROV, OBS, OUT3MO, and OUTDAY; we’ll also use the variable group EASERVED, which - as we’ll see - stores information about the service catchment area for facilities where that information was available.\nWe’ll load the data, and also use the steps outlined in our last post to apply a couple of recoding functions from ipumsr.\n\n\nsdp <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00008.xml\",\n  data = \"data/pma_00008.dat.gz\") %>% \n  select(-EASERVED) %>% # error from extract system\n  mutate(\n    across(ends_with(\"OBS\"), ~lbl_relabel(\n      .x,\n      lbl(1, \"in stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    )),\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"Not interviewed (SDP questionnaire)\",\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    ))\n  )\n\n\n\nEAID and EASERVED\nFor the moment, let’s just take a look at the basic structure of our data, selecting only the variables FACILITYID, SAMPLE, AUTHORITY, CONSENTSQ, and EAID. For this preview, we’ll also arrange the data in ascending order of FACILITYID and SAMPLE:\n\n\nsdp %>% \n  select(FACILITYID, SAMPLE, AUTHORITY, CONSENTSQ, EAID) %>% \n  arrange(FACILITYID, SAMPLE)\n\n\n# A tibble: 234 x 5\n   FACILITYID                      SAMPLE    AUTHORITY CONSENTSQ  EAID\n    <int+lbl>                   <int+lbl>    <int+lbl> <int+lbl> <dbl>\n 1       7006 85405 [Burkina Faso 2017 R… 1 [Governme…   1 [Yes]  7390\n 2       7006 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7390\n 3       7027 85405 [Burkina Faso 2017 R… 1 [Governme…   1 [Yes]  7332\n 4       7027 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7332\n 5       7029 85405 [Burkina Faso 2017 R… 4 [Private]    1 [Yes]  7111\n 6       7029 85408 [Burkina Faso 2018 R… 4 [Private]    0 [No]   7111\n 7       7036 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7412\n 8       7046 85408 [Burkina Faso 2018 R… 4 [Private]    1 [Yes]  7798\n 9       7048 85405 [Burkina Faso 2017 R… 1 [Governme…   1 [Yes]  7009\n10       7051 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7798\n# … with 224 more rows\n\nEach row in our data represents one facility from one sample. Notice that some - but not all - facilities appear once in sample 85405 (from 2017), and again in sample 85408 (from 2018).\nThe variable AUTHORITY shows the managing authority for each facility. Following the discussion above, we’ll expect to find information about the service catchment area for each facility where the managing authority is 1 - Government.\nAlso notice CONSENTSQ, which indicates whether a respondent at each facility consented to be interviewed. When you first obtain a data extract, you should expect most variables to be marked Not interviewed (SDP questionnaire) for facilities where CONSENTSQ shows 0 - No. However, we’ve already taken the extra step of marking all non-response values NA: we should now expect to see NA substituted for Not interviewed (SDP questionnaire).\nLastly, take particular note of the variable EAID: in SDP data, EAID shows the identification code associated with the enumeration area where a facility is located.\nWe’ll find information about the service catchment area for each facility in a different set of variables, each starting with with prefix EASERVED:\n\n\nsdp %>% \n  select(starts_with(\"EASERVED\")) \n\n\n# A tibble: 234 x 18\n   EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n   <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl>\n 1      7380      7323      7491      7605      7142      7279\n 2      7879      7516      7111      7554      7934      7791\n 3      7483        NA        NA        NA        NA        NA\n 4      7185        NA        NA        NA        NA        NA\n 5      7725      7859      7472      7175        NA        NA\n 6      7082        NA        NA        NA        NA        NA\n 7      7650        NA        NA        NA        NA        NA\n 8      7955        NA        NA        NA        NA        NA\n 9      7323        NA        NA        NA        NA        NA\n10      7774        NA        NA        NA        NA        NA\n# … with 224 more rows, and 12 more variables: EASERVED7 <int+lbl>,\n#   EASERVED8 <int+lbl>, EASERVED9 <int+lbl>, EASERVED10 <int+lbl>,\n#   EASERVED11 <int+lbl>, …\n\nYou’ll notice that our extract contains 18 EASERVED variables. Why 18? If you created your own data extract, you’ll remember that you only selected one variable called EASERVED: once you’ve selected samples, the IPUMS extract system automatically determines the correct number of EASERVED variables for your dataset based on the facility with the largest service catchment list.\n\nSome samples include facilities serving as many as 42 enumeration areas, requiring 42 EASERVED variables!\nAs we’ve discussed, PMA only receives service catchment information about public-sector facilities. In their case, each EASERVED variable contains an ID code for one of the enumeration areas in its service catchment list, or else it’s NA. We’ll look at these public-sector facilities first:\n\n\nsdp %>% count(AUTHORITY)\n\n\n# A tibble: 3 x 2\n        AUTHORITY     n\n        <int+lbl> <int>\n1 1 [Government]    202\n2 3 [Faith-based]     3\n3 4 [Private]        29\n\nThe vast majority of SDPs in our sample are public-sector facilities. They comprise 202 of the 234 facilities in our sample.\n\n\nsdp %>% \n  filter(AUTHORITY == 1) %>% \n  select(starts_with(\"EASERVED\")) \n\n\n# A tibble: 202 x 18\n   EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n   <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl>\n 1      7380      7323      7491      7605      7142      7279\n 2      7879      7516      7111      7554      7934      7791\n 3      7483        NA        NA        NA        NA        NA\n 4      7185        NA        NA        NA        NA        NA\n 5      7725      7859      7472      7175        NA        NA\n 6      7082        NA        NA        NA        NA        NA\n 7      7650        NA        NA        NA        NA        NA\n 8      7955        NA        NA        NA        NA        NA\n 9      7323        NA        NA        NA        NA        NA\n10      7774        NA        NA        NA        NA        NA\n# … with 192 more rows, and 12 more variables: EASERVED7 <int+lbl>,\n#   EASERVED8 <int+lbl>, EASERVED9 <int+lbl>, EASERVED10 <int+lbl>,\n#   EASERVED11 <int+lbl>, …\n\nUsing two of the dplyr functions discussed in our last post - summarize and across - we’ll get a better sense of the catchment areas for our public-sector SDPs. Let’s see how many missing values exist for each of these EASERVED variables:\ndplyr is included when you load library(tidyverse)\n\n\nsdp %>% \n  filter(AUTHORITY == 1) %>% \n  summarise(across(starts_with(\"EASERVED\"), ~sum(is.na(.x))))\n\n\n# A tibble: 1 x 18\n  EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n      <int>     <int>     <int>     <int>     <int>     <int>\n1         0       156       173       181       190       192\n# … with 12 more variables: EASERVED7 <int>, EASERVED8 <int>,\n#   EASERVED9 <int>, EASERVED10 <int>, EASERVED11 <int>, …\n\nWe see that every public facility serves at least one enumeration area (there are no missing values for EASERVED1). However, there are 156 missing values for EASERVED2, which tells us that 156 public facilities only serve one enumeration area. Likewise: 173 facilities serve 2 enumeration areas or fewer, 181 serve 3 or fewer, and so forth.\nWhat about the 32 non-public facilities?\n\n\nsdp %>% \n  filter(AUTHORITY != 1) %>% \n  summarise(across(starts_with(\"EASERVED\"), ~sum(is.na(.x))))\n\n\n# A tibble: 1 x 18\n  EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n      <int>     <int>     <int>     <int>     <int>     <int>\n1         4        32        32        32        32        32\n# … with 12 more variables: EASERVED7 <int>, EASERVED8 <int>,\n#   EASERVED9 <int>, EASERVED10 <int>, EASERVED11 <int>, …\n\nPMA receives no information about the service catchment areas for these facilities, so - as you might expect - there are 32 missing values for EASERVED2 onward. Note, however, that there are only 4 missing values for EASERVED1: for non-public facilities, EASERVED1 usually contains that same enumeration area code shown in EAID (this is the enumeration area where the facility is, itself, located).\nThe exception to this rule comes from facilities where CONSENTSQ shows that no respondent provided consent to be interviewed. If we’d like, we can copy EAID to EASERVED1 for these facilities using dplyr::case_when:\n\n\nsdp <- sdp %>% \n  mutate(EASERVED1 = case_when(\n    is.na(EASERVED1) ~ EAID,\n    T ~ as.double(EASERVED1)\n  ))\n\n\n\n\nWe coerce EASERVED1 as a double, matching the class provided by EAID.\nNow, every SDP has at least one enumeration area included in the EASERVED group. This will be important in our next step, where we’ll see how to summarize the SDP data by groups of facilities serving the same enumeration area.\nPivot Longer: EASERVED in Rows\nNow that we’re familiar with EASERVED variables, let’s take a look at the kinds of summary statistics we might want to construct from variables related to contraceptive service availability. For example, consider EMRGPROV, which indicates whether a facility provides emergency contraceptives to clients.\nRemember that, right now, each row of our SDP dataset represents responses from one facility per sample. We’ll ultimately want to count the number of facilities providing emergency contraceptives to clients in each enumeration area, so we should use the tidyr function pivot_longer to reshape the data in a way that repeats each facility’s response to EMRGPROV once for every enumeration area that it serves.\ntidyr is included when you load library(tidyverse)\nTake, for example, the first 5 facilities in our dataset: for now, let’s just look at the first two EASERVED variables, along with each facility’s FACILITYID, EAID, and EMRGPROV response:\n\n\nsdp %>% \n  slice(1:5) %>% \n   select(FACILITYID, EASERVED1, EASERVED2, EMRGPROV)\n\n\n# A tibble: 5 x 4\n  FACILITYID EASERVED1 EASERVED2  EMRGPROV\n   <int+lbl>     <dbl> <int+lbl> <int+lbl>\n1       7250      7380      7323   0 [No] \n2       7399      7879      7516   0 [No] \n3       7506      7483        NA   0 [No] \n4       7982      7185        NA   0 [No] \n5       7065      7725      7859   1 [Yes]\n\nAmong these 5 facilities, only facility 7065 provides emergency contraceptives. This facility happens to provide services to 2 enumeration areas: 7725 and 7859. When we use pivot_longer, we’ll reshape the data to emphasize a different conclusion: our example shows two enumeration areas where individuals can access emergency contraceptives. We convey this information by placing each enumeration area from EASERVED1 or EASERVED2 in its own row:\n\n\nsdp %>% \n  slice(1:5) %>% \n  select(FACILITYID, EASERVED1, EASERVED2, EMRGPROV) %>% \n  pivot_longer(\n    cols = starts_with(\"EASERVED\"),\n    values_to = \"EASERVED\",\n    names_to = NULL\n  )\n\n\n# A tibble: 10 x 3\n   FACILITYID  EMRGPROV  EASERVED\n    <int+lbl> <int+lbl> <dbl+lbl>\n 1       7250   0 [No]       7380\n 2       7250   0 [No]       7323\n 3       7399   0 [No]       7879\n 4       7399   0 [No]       7516\n 5       7506   0 [No]       7483\n 6       7506   0 [No]         NA\n 7       7982   0 [No]       7185\n 8       7982   0 [No]         NA\n 9       7065   1 [Yes]      7725\n10       7065   1 [Yes]      7859\n\n\nHere, values_to gives the name of a new column where we store the values. If we wanted, we could use names_to to create another column storing the original variable names (EASERVED1 and EASERVED2) for each value.\nNow, we find that each of the values previously stored in EASERVED1 and EASERVED2 appear in a new column, EASERVED. Each facility occupies two rows: one for each of the enumeration areas that it serves.\nWhat about the rows where EASERVED contains NA? These rows are meaningless: we’re repeating each facility’s response to EMRGPROV twice to represent two enumeration areas, but facilities 7506 and 7982 only serve one enumeration area apiece. We should include the argument values_drop_na = T to drop these rows when we use pivot_longer():\n\n\nsdp %>% \n  slice(1:5) %>% \n  select(FACILITYID, EASERVED1, EASERVED2, EMRGPROV) %>% \n  pivot_longer(\n    cols = starts_with(\"EASERVED\"),\n    values_to = \"EASERVED\",\n    names_to = NULL,\n    values_drop_na = T\n  )\n\n\n# A tibble: 8 x 3\n  FACILITYID  EMRGPROV  EASERVED\n   <int+lbl> <int+lbl> <dbl+lbl>\n1       7250   0 [No]       7380\n2       7250   0 [No]       7323\n3       7399   0 [No]       7879\n4       7399   0 [No]       7516\n5       7506   0 [No]       7483\n6       7982   0 [No]       7185\n7       7065   1 [Yes]      7725\n8       7065   1 [Yes]      7859\n\nNow that we know how to pivot_longer, let’s apply the function to our full dataset:\n\n\nsdp <- sdp %>%\n  pivot_longer(\n    cols = starts_with(\"EASERVED\"),\n    values_to = \"EASERVED\",\n    values_drop_na = T,\n    names_to = NULL\n  ) %>%\n  distinct() # in case any facility listed the same EASERVED twice\n\n\n\nDropping each row where EASERVED is missing, we’re left with 372 rows where information about each SDP gets repeated once for every enumeration area that it serves. (Remember: our original dataset contained only 234 rows because SDPs occupied just one row apiece).\n\n\nsdp %>% select(FACILITYID, EASERVED, everything())\n\n\n# A tibble: 372 x 58\n   FACILITYID EASERVED      SAMPLE COUNTRY  YEAR ROUND  EAID CONSENTSQ\n    <int+lbl> <dbl+lb>   <int+lbl> <int+l> <int> <dbl> <dbl> <int+lbl>\n 1       7250     7380 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 2       7250     7323 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 3       7250     7491 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 4       7250     7605 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 5       7250     7142 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 6       7250     7279 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 7       7250     7370 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 8       7250     7725 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 9       7250     7811 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n10       7250     7859 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n# … with 362 more rows, and 50 more variables: STRATA <int+lbl>,\n#   FACILITYTYPE <int+lbl>, FACILITYTYPEGEN <int+lbl>,\n#   AUTHORITY <int+lbl>, CONPROV <int+lbl>, …\n\nSummarise by EASERVED and SAMPLE\nNow that we’ve reshaped our data, we’ll be able to create some simple summary statistics about each of the enumeration areas served by the facilities in our sample. First, let’s group_by(EASERVED, SAMPLE) and count() the number of facilities providing services to each enumeration area in each of our samples:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\", \n    N_SDP = n()\n  )\n\n\n# A tibble: 149 x 3\n# Groups:   EASERVED, SAMPLE [149]\n    EASERVED                            SAMPLE N_SDP\n   <dbl+lbl>                         <int+lbl> <int>\n 1      7003 85405 [Burkina Faso 2017 Round 5]     3\n 2      7003 85408 [Burkina Faso 2018 Round 6]     2\n 3      7006 85405 [Burkina Faso 2017 Round 5]     2\n 4      7006 85408 [Burkina Faso 2018 Round 6]     1\n 5      7009 85405 [Burkina Faso 2017 Round 5]     3\n 6      7009 85408 [Burkina Faso 2018 Round 6]     2\n 7      7016 85405 [Burkina Faso 2017 Round 5]     3\n 8      7016 85408 [Burkina Faso 2018 Round 6]     2\n 9      7026 85405 [Burkina Faso 2017 Round 5]     3\n10      7026 85408 [Burkina Faso 2018 Round 6]     3\n# … with 139 more rows\n\nContinuing with the variable EMRGPROV, we can now also count the number of sampled facilities providing emergency contraception to each EASERVED:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    N_EMRGPROV = sum(EMRGPROV)\n  )\n\n\n# A tibble: 149 x 3\n# Groups:   EASERVED, SAMPLE [149]\n    EASERVED                            SAMPLE N_EMRGPROV\n   <dbl+lbl>                         <int+lbl>      <int>\n 1      7003 85405 [Burkina Faso 2017 Round 5]          0\n 2      7003 85408 [Burkina Faso 2018 Round 6]          0\n 3      7006 85405 [Burkina Faso 2017 Round 5]          0\n 4      7006 85408 [Burkina Faso 2018 Round 6]          0\n 5      7009 85405 [Burkina Faso 2017 Round 5]          2\n 6      7009 85408 [Burkina Faso 2018 Round 6]          1\n 7      7016 85405 [Burkina Faso 2017 Round 5]          0\n 8      7016 85408 [Burkina Faso 2018 Round 6]          0\n 9      7026 85405 [Burkina Faso 2017 Round 5]          0\n10      7026 85408 [Burkina Faso 2018 Round 6]          0\n# … with 139 more rows\n\nWhat if we want to include a count of the facilities providing each of the different contraceptive methods in our data? Building on a technique showcased in our last post, we could use dplyr::across to iterate over all variables ending with the suffix PROV:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~sum(.x), .names = \"N_{.col}\")\n  ) \n\n\n# A tibble: 149 x 15\n# Groups:   EASERVED, SAMPLE [149]\n   EASERVED      SAMPLE N_CONPROV N_CYCBPROV N_DEPOPROV N_DIAPROV\n   <dbl+lb>   <int+lbl>     <int>      <int>      <int>     <int>\n 1     7003 85405 [Bur…         3          3          3         0\n 2     7003 85408 [Bur…         2          2          2         0\n 3     7006 85405 [Bur…         2          2          2         0\n 4     7006 85408 [Bur…         1          1          1         0\n 5     7009 85405 [Bur…         3          1          3         0\n 6     7009 85408 [Bur…         2          2          2         0\n 7     7016 85405 [Bur…         3          3          3         0\n 8     7016 85408 [Bur…         2          1          2         0\n 9     7026 85405 [Bur…         3          3          3         0\n10     7026 85408 [Bur…         3          3          3         0\n# … with 139 more rows, and 9 more variables: N_EMRGPROV <int>,\n#   N_FCPROV <int>, N_FSTPROV <int>, N_FJPROV <int>, N_IMPPROV <int>,\n#   …\n\nWe’ll reduce this information even further, creating a variable NUM_METHODS_PROV indicating the number of methods provided by at least one sampled facility:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~sum(.x), .names = \"N_{.col}\")\n  ) %>% \n  transmute(\n    EASERVED,\n    SAMPLE,\n    NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")) > 0, na.rm = T)\n  )\n\n\n# A tibble: 149 x 3\n# Groups:   EASERVED, SAMPLE [149]\n    EASERVED                            SAMPLE NUM_METHODS_PROV\n   <dbl+lbl>                         <int+lbl>            <int>\n 1      7003 85405 [Burkina Faso 2017 Round 5]               10\n 2      7003 85408 [Burkina Faso 2018 Round 6]                8\n 3      7006 85405 [Burkina Faso 2017 Round 5]               10\n 4      7006 85408 [Burkina Faso 2018 Round 6]                8\n 5      7009 85405 [Burkina Faso 2017 Round 5]                9\n 6      7009 85408 [Burkina Faso 2018 Round 6]               10\n 7      7016 85405 [Burkina Faso 2017 Round 5]                9\n 8      7016 85408 [Burkina Faso 2018 Round 6]                8\n 9      7026 85405 [Burkina Faso 2017 Round 5]               10\n10      7026 85408 [Burkina Faso 2018 Round 6]               10\n# … with 139 more rows\n\nIn our last post, we introduced 4 variable groups related to the availability of different contraceptive methods. We’ll now create a summary variable for each one, and then show how to attach our new variables to a Household and Female dataset:\nN_SDP - number of SDPs\nNUM_METHODS_PROV - number of methods provided by at least one SDP\nNUM_METHODS_INSTOCK - number of methods in-stock with at least one SDP\nNUM_METHODS_OUT3MO - number of methods out of stock in the last 3 months with at least one SDP\nMEAN_OUTDAY - the mean length of a stockout for all out of stock methods\n\n\nsdp <- sdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    N_SDP = n(),\n    across(ends_with(\"PROV\"), ~sum(.x, na.rm = T), .names = \"N_{.col}\"),\n    across(ends_with(\"OBS\"), ~sum(.x, na.rm = T), .names = \"N_{.col}\"),\n    across(ends_with(\"OUT3MO\"), ~sum(.x, na.rm = T), .names = \"N_{.col}\"),\n    across(ends_with(\"OUTDAY\"), ~mean(.x, na.rm = T), .names = \"N_{.col}\"),\n  ) %>% \n  transmute(\n    EASERVED,\n    SAMPLE,\n    N_SDP,\n    NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")) > 0, na.rm = T),\n    NUM_METHODS_INSTOCK = sum(c_across(ends_with(\"OBS\")) > 0, na.rm = T),\n    NUM_METHODS_OUT3MO = sum(c_across(ends_with(\"OUT3MO\")) > 0, na.rm = T),\n    MEAN_OUTDAY = mean(c_across(ends_with(\"OUTDAY\")), na.rm = T)\n  ) %>% \n  ungroup()\n\n\n\nMerging to Household and Female Data\nConsider the following female respondent dataset collected from Burkina Faso in 2017 and 2018. It contains a variable FPCURRUSE indicating whether the woman is currently using a method of family planning:\n\n\nhhf <- read_ipums_micro(\n  ddi = \"data/pma_00011.xml\",\n  data = \"data/pma_00011.dat.gz\"\n) %>% \n  select(PERSONID, EAID, URBAN, SAMPLE, FPCURRUSE) %>% \n  mutate(\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    ))\n  )\n\n\n\n\n\nhhf\n\n\n# A tibble: 6,944 x 5\n   PERSONID           EAID    URBAN                   SAMPLE FPCURRUSE\n   <chr>             <dbl> <int+lb>                <int+lbl> <int+lbl>\n 1 0762000000029022…  7620 1 [Urba… 85405 [Burkina Faso 201…  NA      \n 2 0735800000017142…  7358 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 3 0710400000020992…  7104 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 4 0704800000014092…  7048 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n 5 0715600000020782…  7156 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 6 0727900000021452…  7279 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n 7 0743100000024642…  7431 0 [Rura… 85405 [Burkina Faso 201…   1 [Yes]\n 8 0721200000025792…  7212 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 9 0704200000014542…  7042 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n10 0797200000013032…  7972 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n# … with 6,934 more rows\n\nYou’ll notice that each row represents one female respondent with a unique PERSONID (non-respondents and other household members have been removed beforehand). We’ve also got EAID, which represents the enumeration area where each respondent resides; the variable URBAN indicates whether the enumeration area is primarily “urban” or “rural”.\nThe variable SAMPLE contains the same values seen in our SDP data:\n85405 - Burkina Faso 2017 Round 5\n85408 - Burkina Faso 2018 Round 6\nWhen we merge, we’ll want to match each woman to both a SAMPLE and an EASERVED from the SDP data. We’ll rename EASERVED to match the variable EAID in the HHF data:\n\n\nbf_merged <- sdp %>% \n  rename(EAID = EASERVED) %>% \n  right_join(hhf, by = c(\"EAID\", \"SAMPLE\"))\n\n\n\nNow, each woman’s record contains all of the variables we created above summarizing the SDPs that serve her enumeration area. For example, for all sampled women living in EAID == 7003 in 2017, the value in NUM_METHODS_OUT3MO shows the number of family planning methods that were out of stock with any SDP serving the woman’s enumeration area within three months prior to the survey:\n\n\nbf_merged %>% \n  filter(EAID == 7003, SAMPLE == 85405) %>% \n  select(PERSONID, EAID, SAMPLE, NUM_METHODS_OUT3MO)\n\n\n# A tibble: 55 x 4\n   PERSONID             EAID                  SAMPLE NUM_METHODS_OUT3…\n   <chr>            <dbl+lb>               <int+lbl>             <int>\n 1 070030000001973…     7003 85405 [Burkina Faso 20…                 0\n 2 070030000001973…     7003 85405 [Burkina Faso 20…                 0\n 3 070030000002640…     7003 85405 [Burkina Faso 20…                 0\n 4 070030000001075…     7003 85405 [Burkina Faso 20…                 0\n 5 070030000001609…     7003 85405 [Burkina Faso 20…                 0\n 6 070030000000835…     7003 85405 [Burkina Faso 20…                 0\n 7 070030000001273…     7003 85405 [Burkina Faso 20…                 0\n 8 070030000000527…     7003 85405 [Burkina Faso 20…                 0\n 9 070030000002561…     7003 85405 [Burkina Faso 20…                 0\n10 070030000002391…     7003 85405 [Burkina Faso 20…                 0\n# … with 45 more rows\n\nYou’ll notice that 55 women were surveyed in EAID 7003 in 2017, and each one has the same value (0) for NUM_METHODS_OUT3MO.\nWe’ll dig deeper into the types of research questions that our new combined dataset can answer in our upcoming Data Analysis post. For now, take a look at the apparent relationship between FPCURRUSE and NUM_METHODS_OUT3MO for all of the women with non-missing responses for both variables:\n\n\nbf_merged %>% \n  filter(!is.na(FPCURRUSE) & !is.na(NUM_METHODS_OUT3MO)) %>% \n  group_by(NUM_METHODS_OUT3MO > 0) %>% \n  count(FPCURRUSE) %>% \n  mutate(pct = n/sum(n))\n\n\n# A tibble: 4 x 4\n# Groups:   NUM_METHODS_OUT3MO > 0 [2]\n  `NUM_METHODS_OUT3MO > 0` FPCURRUSE     n   pct\n  <lgl>                    <int+lbl> <int> <dbl>\n1 FALSE                      0 [No]   2721 0.648\n2 FALSE                      1 [Yes]  1475 0.352\n3 TRUE                       0 [No]   1124 0.700\n4 TRUE                       1 [Yes]   482 0.300\n\nNotably, among those respondents living in an enumeration area that experienced zero stockouts within the 3 months prior to the SDP survey, 35% indicated that they were actively using a family planning method. Compare that to the set of respondents living in an area where at least one method was out of stock during the same time period: only 30% were using a family planning method.\nWhile a 5% difference may or may not prove to be statistically significant under further analysis, it’s not entirely surprising that the reliable availability of contraceptive methods from service providers might influence the contraceptive prevalence rate among women in a given area.\nAs always, let us know what kinds of questions about fertility and family planning you’re answering with data merged from service providers!\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-02T15:48:43-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-28-across-sdp/",
    "title": "Recode and Summarize Variables from Multiple Response Questions",
    "description": "Use dplyr::across to summarize variables with a similar naming pattern.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-28",
    "categories": [
      "Individuals in Context",
      "Data Manipulation",
      "Service Delivery Points",
      "dplyr",
      "ipumsr"
    ],
    "contents": "\n\nContents\nSDP Multiple Response Questions\nSetup: Load an Example Dataset into R\nRecoding Variables with ipumsr\nIntroducing dplyr::across\nSummarize Variable Groups by Facility\nSummarize Variable Groups by EAID\n\n\n\n Download this page as R code\n\n\nIn our last post, we introduced PMA Service Delivery Point (SDP) data as an important resource for understanding the health services environment experienced by individuals sampled in PMA Household and Female data. For our second post in this Individuals in Context series, we’ll now take deeper dive into one of the important topics in SDP data: the range and availability of contraceptive methods provided at each facility.\nA common feature of the variables in this topic - and in many other topics - is that you’ll find several binary indicators constructed from the same multiple resposne item on the SDP questionnaire. We’ll see that IPUMS PMA uses a common naming convention to help users group these variables together for use in functions like dplyr::across.\nSDP Multiple Response Questions\nEvery SDP respondent receives a question associated with the variable FPOFFERED, which indicates whether to facility usually offers family planning services or products:\nDo you usually offer family planning services / products?\n\n  [] Yes\n  [] No\n  [] No response\nIf yes, they’ll then receive a multiple response-type question asking about the contraceptive methods provided to clients. The range of options provided on the questionnaire may vary across samples, but most look something like this:\nWhich of the following methods are provided to clients at this facility? \n\n  [] Female sterilization\n  [] Male sterilization\n  [] Implant\n  [] IUD\n  [] Injectables - Depo Provera\n  [] Injectables - Sayana Press\n  [] Pill\n  [] Emergency Contraception\n  [] Male Condom\n  [] Female Condom\n  [] Diaphragm\n  [] Foam/Jelly\n  [] Std. Days / Cycle beads\n  [] None of the above\n  [] No response\n\nIf the response to FPOFFERED was not “Yes”, this question will be skipped and marked “NIU (not in universe)”.\nThis is a multiple response question: each method in the list could be answered individually (Yes or No), or the respondent could reply None of the above or provide No response. The IPUMS PMA extract system generates one variable for each of the methods in the list:\nFSTPROV\nMSTPROV\nIMPPROV\nIUDPROV\nDEPOPROV\nSAYPROV\nPILLPROV\nEMRGPROV\nCONPROV\nFCPROV\nDIAPROV\nFJPROV\nCYCBPROV\nThe questionnaire continues for each one of the methods provided at a given facility. Next, it checks for the current availability of each of the provided methods:\nYou mentioned that you typically provide the [METHOD] at this facility,\ncan you show it to me? If no, probe: Is the [METHOD] out of stock today?\n\n  [] In-stock and observed\n  [] In-stock but not observed\n  [] Out of stock\n  [] No Response\nThe variables associated with each response end with the same suffix OBS:\nIMPOBS\nIUDOBS\nDEPOOBS\nSAYOBS\nPILLOBS\nEMRGOBS\nCONOBS\nFCOBS\nDIAOBS\nFJOBS\nCYCBOBS\nSterilization methods were not included in this question.\nNote: if a given method was not provided at a facility, it would be skipped and marked “NIU (not in universe)”.\nYou can always visit a variable’s Universe tab for details.\nIf a facility did have a particular method in-stock, it received a question asking whether supplies were unavailable any time in the previous three months:\nHas the [METHOD] been out of stock at any time in the last 3 months?\n\n  [] Yes \n  [] No \n  [] Don't know\n  [] No response\nThis question becomes a series of variables ending with the suffix OUT3MO:\nIMPOUT3MO\nIUDOUT3MO\nDEPOOUT3MO\nSAYOUT3MO\nPILLOUT3MO\nEMRGOUT3MO\nCONOUT3MO\nFCOUT3MO\nDIAOUT3MO\nFJOUT3MO\nCYCBOUT3MO\nAgain, sterilization methods were not included in this question.\nNote: if a given method was not in-stock at a facility where it’s normally provided, it would be skipped and marked “NIU (not in universe)”.\nOn the other hand, if a facility that normally provides a given method did not have supplies in-stock during the interview, it received a different question about the duration of the current stockout:\nHow many days has the [METHOD] been out of stock?\n\n  Number of days____\nThe resulting variables - each ending with the suffix OUTDAY - take an integer value representing the stockout duration in days (except where the value is a non-response code, see below):\nIMPOUTDAY\nIUDOUTDAY\nDEPOOUTDAY\nSAYOUTDAY\nPILLOUTDAY\nEMRGOUTDAY\nCONOUTDAY\nFCOUTDAY\nDIAOUTDAY\nFJOUTDAY\nCYCBOUTDAY\nAgain, sterilization methods were not included in this question.\nNote: if a given method was in-stock at a facility where it’s normally provided, it would be skipped and marked “NIU (not in universe)”.\nSetup: Load an Example Dataset into R\nAs you can see, we’re left with quite a few variables from just these 4 questions! That’s very useful if you’re interested in the availability of one method, in particular, but what if you want to get a picture of the full range of methods provided at a particular facility?\nFortunately, the repeated use of variable suffixes (PROV, OBS, OUT3MO, and OUTDAY) make these variables highly suitable for column-wise processing with dplyr::across.\nLet’s start with an example data extract containing all of the variables listed above, collected from just two samples:\nBurkina Faso - 2018 R6\nBurkina Faso - 2017 R5\nOnce you’ve downloaded an extract, open RStudio and load the packages tidyverse and ipumsr:\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\n\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nNext, use the file paths for your data extract to load it into R (the data for this example can also be found in the data folder when you use the Download button at the top of this page).\n\n\nsdp <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00008.xml\",\n  data = \"data/pma_00008.dat.gz\"\n)\n\n\n\n\nChange to file paths to match your own extract. You can also use the example extract included in the data folder when you download this post.\nUsing dplyr::ends_with, we’ll select only FACILITYID, SAMPLE, EAID, and the variables using one of the four suffixes PROV, OBS, OUT3MO, or OUTDAY.\n\n\nsdp <- sdp %>%  \n  select(\n    FACILITYID,\n    SAMPLE, \n    EAID, \n    ends_with(\"PROV\"),\n    ends_with(\"OBS\"),\n    ends_with(\"OUT3MO\"),\n    ends_with(\"OUTDAY\")\n  )\n\n\n\nThat leaves us with 234 rows - each a facility from one of our two samples - and 49 variables:\n\n\nsdp\n\n\n# A tibble: 234 x 49\n   FACILITYID      SAMPLE  EAID CONPROV CYCBPROV DEPOPROV DIAPROV\n    <int+lbl>   <int+lbl> <dbl> <int+l> <int+lb> <int+lb> <int+l>\n 1       7250 85405 [Bur…  7142 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 2       7399 85405 [Bur…  7879 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 3       7506 85405 [Bur…  7483 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 4       7982 85405 [Bur…  7185 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 5       7065 85405 [Bur…  7859 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 6       7729 85405 [Bur…  7082 1 [Yes]  0 [No]   1 [Yes]  0 [No]\n 7       7490 85405 [Bur…  7650 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 8       7311 85405 [Bur…  7955 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 9       7524 85405 [Bur…  7323 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n10       7932 85405 [Bur…  7774 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n# … with 224 more rows, and 42 more variables: EMRGPROV <int+lbl>,\n#   FCPROV <int+lbl>, FSTPROV <int+lbl>, FJPROV <int+lbl>,\n#   IMPPROV <int+lbl>, IUDPROV <int+lbl>, MSTPROV <int+lbl>,\n#   PILLPROV <int+lbl>, SAYPROV <int+lbl>, CONOBS <int+lbl>,\n#   CYCBOBS <int+lbl>, DEPOOBS <int+lbl>, DIAOBS <int+lbl>,\n#   EMRGOBS <int+lbl>, FCOBS <int+lbl>, FJOBS <int+lbl>,\n#   IMPOBS <int+lbl>, IUDOBS <int+lbl>, PILLOBS <int+lbl>,\n#   SAYOBS <int+lbl>, CONOUT3MO <int+lbl>, CYCBOUT3MO <int+lbl>,\n#   DEPOOUT3MO <int+lbl>, DIAOUT3MO <int+lbl>, EMRGOUT3MO <int+lbl>,\n#   FCOUT3MO <int+lbl>, FJOUT3MO <int+lbl>, IMPOUT3MO <int+lbl>,\n#   IUDOUT3MO <int+lbl>, PILLOUT3MO <int+lbl>, SAYOUT3MO <int+lbl>,\n#   CONOUTDAY <int+lbl>, CYCBOUTDAY <int+lbl>, DEPOOUTDAY <int+lbl>,\n#   DIAOUTDAY <int+lbl>, EMRGOUTDAY <int+lbl>, FCOUTDAY <int+lbl>,\n#   FJOUTDAY <int+lbl>, IMPOUTDAY <int+lbl>, IUDOUTDAY <int+lbl>,\n#   PILLOUTDAY <int+lbl>, SAYOUTDAY <int+lbl>\n\nRecoding Variables with ipumsr\nA key feature to remember about IPUMS PMA extracts is that variables often have value labels, which are text labels assigned to the different values taken by a variable. When we load the extract into R with an ipumsr function, these variables are imported as labelled objects rather than the more common factor class of objects.\nMore information on the difference between factors and IPUMS labelled variables.\nAs a result, IPUMS data users need to take some unusual steps when recoding a variable or handling NA values. Happily, the ipumsr package provide a few functions (starting with the prefix lbl_) that make this process very easy.\nLet’s take a look at the variable CONOBS:\n\n\nsdp %>% count(CONOBS)\n\n\n# A tibble: 5 x 2\n                                    CONOBS     n\n*                                <int+lbl> <int>\n1  1 [In-stock and observed]                 204\n2  2 [In-stock but not observed]               3\n3  3 [Out of stock]                            5\n4 94 [Not interviewed (SDP questionnaire)]     4\n5 99 [NIU (not in universe)]                  18\n\nNotice that we have two values representing SDPs with male condoms “in-stock”: SDPs where the interviewer personally observed the condoms get 1, while those where condoms where reported in-stock - but not actually observed by the interviewer - get 2.\nDepending on your research question, the interviewer’s personal observation of each method may or may not be important. You might decide that you’d prefer to recode this variable into a simple binary measure that could be easily plugged into a regression model as a dummy variable later on. To do that, you could use the ipumsr function lbl_relabel:\n\n\nsdp %>% \n  mutate(CONOBS = lbl_relabel(\n      CONOBS,\n      lbl(1, \"In-stock\") ~ .val %in% 1:2,\n      lbl(0, \"Out of stock\") ~ .val == 3\n    )) %>% \n  count(CONOBS)\n\n\n# A tibble: 4 x 2\n                                    CONOBS     n\n*                                <dbl+lbl> <int>\n1  0 [Out of stock]                            5\n2  1 [In-stock]                              207\n3 94 [Not interviewed (SDP questionnaire)]     4\n4 99 [NIU (not in universe)]                  18\n\nThat collapses the values 1 and 2 together, and it moves the value 3 (“Out of stock”) to 0. However, we’ve still got a the values 94 and 99, which are each a different type of non-response. The easiest strategy here would be to recode any value larger than 90 as NA, and we could do that with another ipumsr function, lbl_na_if:\n\n\nsdp %>% \n  mutate(\n    CONOBS = lbl_relabel(\n      CONOBS,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    ),\n    CONOBS = lbl_na_if(\n      CONOBS,\n      ~.val > 90\n    )\n  ) %>% \n  count(CONOBS)\n\n\n# A tibble: 3 x 2\n             CONOBS     n\n*         <dbl+lbl> <int>\n1  0 [out of stock]     5\n2  1 [in-stock]       207\n3 NA                   22\n\nThis works great for our example variable, CONOBS. Unfortunately, though, we can’t always rely on the rule ~.val > 90 to handle missing responses. For variables like CONOUTDAY, a value above 90 could be a valid response: what if a facility experienced a stockout lasting 94 days? For this reason, the non-response values for CONOUTDAY are padded with additional digits:\n\n\nsdp %>% count(CONOUTDAY)\n\n\n# A tibble: 7 x 2\n                                   CONOUTDAY     n\n*                                  <int+lbl> <int>\n1    1                                           1\n2    3                                           1\n3   10                                           1\n4   15                                           1\n5   60                                           1\n6 9994 [Not interviewed (SDP questionnaire)]     4\n7 9999 [NIU (not in universe)]                 225\n\nWe could write a different lbl_na_if function for our OUTDAY variables, but ipumsr provides a much nicer workaround: we can specify non-response labels rather than values, as long as we make sure to use all of the different non-response labels appearing throughout our dataset:\n\n\nsdp %>% \n  mutate(\n    CONOBS = lbl_relabel(\n      CONOBS,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    ),\n    CONOBS = lbl_na_if(\n      CONOBS,\n      ~.lbl %in% c(\n        \"Not interviewed (SDP questionnaire)\",\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    )\n  ) %>% \n  count(CONOBS)\n\n\n# A tibble: 3 x 2\n             CONOBS     n\n*         <dbl+lbl> <int>\n1  0 [out of stock]     5\n2  1 [in-stock]       207\n3 NA                   22\n\nNow, we’ll be able to recode all of our variables with the same pair of functions! To do that, we’ll first need to take a look at the column-wise workflow made available by dplyr::across.\nIntroducing dplyr::across\nWhile there are several ways to apply a function across a set of variables in R, the simplest method comes from a new addition to the dplyr package that’s loaded when you run library(tidyverse). The function dplyr::across takes two arguments: a function, and a selection of columns where you want that function to be applied.\ndplyr is included when you load library(tidyverse)\nRemember that we want collapse the values 1 - In-stock and observed and 2 - In-stock but not observed for all of the variables ending with OBS, not just CONOBS. Using across and a selection of variables ending with OBS, we’ll apply the same lbl_relabel function we used on CONOBS above:\n\n\nsdp %>% \n  mutate(\n    across(ends_with(\"OBS\"), ~lbl_relabel(\n      .x,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    )) \n  ) %>% \n  count(CONOBS)\n\n\n# A tibble: 4 x 2\n                                    CONOBS     n\n*                                <dbl+lbl> <int>\n1  0 [out of stock]                            5\n2  1 [in-stock]                              207\n3 94 [Not interviewed (SDP questionnaire)]     4\n4 99 [NIU (not in universe)]                  18\n\nHere, we stick lbl_relabel inside a lambda function with syntax from purrr: the ~ designates a tidy lambda function, which in turn uses .x as a kind of pronoun referencing each of the variables returned by ends_with(\"OBS\"). We’re showing that CONOBS still gets recoded as before, but so do all of the other variables in its group!\nWe’ll use across again with lbl_na_if, but this time we want to produce NA values for all of the variables in our dataset. In place of ends_with(\"OBS\"), we’ll use the selection function everything(). This will take care of all the recoding we want to do, so we’ll also reassign our data with sdp <- sdp:\n\n\nsdp <- sdp %>% \n  mutate(\n    across(ends_with(\"OBS\"), ~lbl_relabel(\n      .x,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    )),\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"Not interviewed (SDP questionnaire)\",\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    ))\n  )\n\n\n\nLet’s pick a few variables to check out work:\n\n\nsdp %>% count(CONOBS)\n\n\n# A tibble: 3 x 2\n             CONOBS     n\n*         <dbl+lbl> <int>\n1  0 [out of stock]     5\n2  1 [in-stock]       207\n3 NA                   22\n\nsdp %>% count(IMPOBS)\n\n\n# A tibble: 3 x 2\n             IMPOBS     n\n*         <dbl+lbl> <int>\n1  0 [out of stock]     3\n2  1 [in-stock]       204\n3 NA                   27\n\nsdp %>% count(CONOUTDAY)\n\n\n# A tibble: 6 x 2\n  CONOUTDAY     n\n* <int+lbl> <int>\n1         1     1\n2         3     1\n3        10     1\n4        15     1\n5        60     1\n6        NA   229\n\nsdp %>% count(IMPOUTDAY)\n\n\n# A tibble: 4 x 2\n  IMPOUTDAY     n\n* <int+lbl> <int>\n1         1     1\n2        14     1\n3        90     1\n4        NA   231\n\nSummarize Variable Groups by Facility\nEverything looks great! Now that we’ve finished reformatting the data, remember that our ultimate goal is to get some sense of the scope of methods available at a particular facility.\nWe’d like to use something like across again here, but this time we’ll only want to apply our function to a selection of columns within the same row (because each row of our dataset represents one facility). To do this, we’ll divide the dataset rowwise, and then use the related function c_across to apply a calculation across columns within each row.\nFor instance, suppose we want to create NUM_METHODS_PROV to show the total number of methods provided at each facility. Let’s look at the PROV variables for the first few facilities:\n\n\nsdp %>% select(ends_with(\"PROV\"))\n\n\n# A tibble: 234 x 13\n   CONPROV CYCBPROV DEPOPROV DIAPROV EMRGPROV  FCPROV FSTPROV FJPROV\n   <int+l> <int+lb> <int+lb> <int+l> <int+lb> <int+l> <int+l> <int+>\n 1 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 2 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 3 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n 4 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n 5 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  1 [Yes] 1 [Yes] 1 [Yes] 0 [No]\n 6 1 [Yes]  0 [No]   1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 7 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n 8 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 9 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n10 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n# … with 224 more rows, and 5 more variables: IMPPROV <int+lbl>,\n#   IUDPROV <int+lbl>, MSTPROV <int+lbl>, PILLPROV <int+lbl>,\n#   SAYPROV <int+lbl>\n\nTo calculate NUM_METHODS_PROV, we can just find the sum of values across all of the PROV variables (thanks to our recoding work, the only possible values here are 1 for “yes”, or 0 for “no”). Notice that c_across takes only one argument: a selection function like ends_with(\"PROV\"). That’s because c_across works like the familiar concatenate function c() used to provide a vector of values to a function like sum(c(1,2,3)).\nFirst, use rowwise() to signal that we’ll only calculate the sum across variables in the same row. Then, use c_across() to find the sum() of PROV variables in each row:\n\n\nsdp %>% \n  rowwise() %>% \n  transmute(NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")), na.rm = T))\n\n\n# A tibble: 234 x 1\n# Rowwise: \n   NUM_METHODS_PROV\n              <int>\n 1               10\n 2               10\n 3                8\n 4                8\n 5               10\n 6                9\n 7                8\n 8               10\n 9                8\n10                8\n# … with 224 more rows\n\nWe can now create a summary variable for each of the four variable groups. Let’s create:\nNUM_METHODS_PROV - number of methods provided\nNUM_METHODS_INSTOCK - number of methods in-stock\nNUM_METHODS_OUT3MO - number of methods out of stock in the last 3 months\nMEAN_OUTDAY - the mean length of a stockout for all out of stock methods\n\n\nsdp %>% \n  rowwise() %>% \n  transmute(\n    NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")), na.rm = T),\n    NUM_METHODS_INSTOCK = sum(c_across(ends_with(\"OBS\")), na.rm = T),\n    NUM_METHODS_OUT3MO = sum(c_across(ends_with(\"OUT3MO\")), na.rm = T),\n    MEAN_OUTDAY = mean(c_across(ends_with(\"OUTDAY\")), na.rm = T)\n  )\n\n\n# A tibble: 234 x 4\n# Rowwise: \n   NUM_METHODS_PROV NUM_METHODS_INSTOCK NUM_METHODS_OUT3MO MEAN_OUTDAY\n              <int>               <dbl>              <int>       <dbl>\n 1               10                   8                  0         NaN\n 2               10                   7                  0           8\n 3                8                   8                  0         NaN\n 4                8                   8                  0         NaN\n 5               10                   9                  0         NaN\n 6                9                   7                  0         NaN\n 7                8                   7                  0         365\n 8               10                   8                  1         NaN\n 9                8                   8                  1         NaN\n10                8                   7                  0          30\n# … with 224 more rows\n\nMEAN_OUTDAY is NaN (not a number) if no methods were out of stock.\nSummarize Variable Groups by EAID\nIn our last post, we mentioned that the best use case for SDP data is to aggregate information collected from facilities working in the same geographic sampling units - or enumeration areas - used to select individuals for PMA Household and Female samples. In our next post, we’ll take a close look at the variable group EASERVED, which lists all of the enumeration area codes where a facility is known to provide health services. We’ll then introduce a strategy using tidyr::pivot_longer to summarize the full scope of services available to women living in a particular enumeration area.\nFor now, let’s simply consider all of the sampled facilities located in a particular enumeration area. That is, rather than calculating the number of methods provided by one facility NUM_METHODS_PROV, let’s create one variable for each method indicating whether the method was provided by at least one facility in a given enumeration area EAID in a given SAMPLE.\nFor instance, look at the number of facilities providing IUDs in enumeration area 7111 for the Burkina Faso sample collected in 2017:\n\n\nsdp %>% \n  filter(EAID == 7111, SAMPLE == 85405) %>% \n  select(EAID, SAMPLE, FACILITYID, PILLPROV)\n\n\n# A tibble: 4 x 4\n   EAID                            SAMPLE FACILITYID  PILLPROV\n  <dbl>                         <int+lbl>  <int+lbl> <int+lbl>\n1  7111 85405 [Burkina Faso 2017 Round 5]       7210   1 [Yes]\n2  7111 85405 [Burkina Faso 2017 Round 5]       7029   0 [No] \n3  7111 85405 [Burkina Faso 2017 Round 5]       7441   1 [Yes]\n4  7111 85405 [Burkina Faso 2017 Round 5]       7403   1 [Yes]\n\nWe want to use a summarize function to create a variable like ANY_PILLPROV, which should simply indicate whether any of these four facilities provide contraceptive pills. Three of them do provide pills, so we want ANY_PILLPROV to be TRUE.\n\n\nsdp %>% \n  filter(EAID == 7111, SAMPLE == 85405) %>% \n  summarize(ANY_PILLPROV = any(PILLPROV == 1))\n\n\n# A tibble: 1 x 1\n  ANY_PILLPROV\n  <lgl>       \n1 TRUE        \n\nNow that we’re familiar with across, we should be able to do the same thing to all PROV variables for this particular group of facilities. Let’s also introduce a naming convention where we glue the prefix ANY_ to the column name referenced by the pronoun .x:\n\n\nsdp %>% \n  filter(EAID == 7111, SAMPLE == 85405) %>% \n  summarize(across(ends_with(\"PROV\"), ~any(.x == 1), .names = \"ANY_{.col}\"))\n\n\n# A tibble: 1 x 13\n  ANY_CONPROV ANY_CYCBPROV ANY_DEPOPROV ANY_DIAPROV ANY_EMRGPROV\n  <lgl>       <lgl>        <lgl>        <lgl>       <lgl>       \n1 TRUE        TRUE         TRUE         FALSE       FALSE       \n# … with 8 more variables: ANY_FCPROV <lgl>, ANY_FSTPROV <lgl>,\n#   ANY_FJPROV <lgl>, ANY_IMPPROV <lgl>, ANY_IUDPROV <lgl>,\n#   ANY_MSTPROV <lgl>, ANY_PILLPROV <lgl>, ANY_SAYPROV <lgl>\n\n\nIt looks like none of the sampled facilities in enumeration area 7111 provided emergency contraception in 2017. This could be very important context for understanding the health services available to women sampled from that area!\nLet’s repeat the same procedure for every enumeration area in each of our samples. Rather than using a filter to select one EAID in one SAMPLE, we’ll use group_by to work with each EAID in each SAMPLE.\n\n\nsdp %>% \n  group_by(EAID, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~any(.x == 1), .names = \"ANY_{.col}\")\n  )\n\n\n# A tibble: 142 x 15\n# Groups:   EAID, SAMPLE [142]\n    EAID      SAMPLE ANY_CONPROV ANY_CYCBPROV ANY_DEPOPROV ANY_DIAPROV\n   <dbl>   <int+lbl> <lgl>       <lgl>        <lgl>        <lgl>      \n 1  7003 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n 2  7003 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 3  7006 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n 4  7006 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 5  7009 85405 [Bur… TRUE        FALSE        TRUE         FALSE      \n 6  7009 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 7  7016 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n 8  7016 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 9  7026 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n10  7042 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n# … with 132 more rows, and 9 more variables: ANY_EMRGPROV <lgl>,\n#   ANY_FCPROV <lgl>, ANY_FSTPROV <lgl>, ANY_FJPROV <lgl>,\n#   ANY_IMPPROV <lgl>, ANY_IUDPROV <lgl>, ANY_MSTPROV <lgl>,\n#   ANY_PILLPROV <lgl>, ANY_SAYPROV <lgl>\n\nThis is still quite a bit of information! Suppose we want to summarize it even further: let’s calculate NUM_METHODS_PROV again with our summary output. This time, NUM_METHODS_PROV will count the number of methods provided by at least one facility in each group.\n\n\nsdp %>% \n  group_by(EAID, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~any(.x == 1), .names = \"ANY_{.col}\")\n  ) %>% \n  transmute(NUM_METHODS_PROV= sum(c_across(ends_with(\"PROV\")), na.rm = T))\n\n\n# A tibble: 142 x 3\n# Groups:   EAID, SAMPLE [142]\n    EAID                            SAMPLE NUM_METHODS_PROV\n   <dbl>                         <int+lbl>            <int>\n 1  7003 85405 [Burkina Faso 2017 Round 5]                8\n 2  7003 85408 [Burkina Faso 2018 Round 6]                8\n 3  7006 85405 [Burkina Faso 2017 Round 5]                8\n 4  7006 85408 [Burkina Faso 2018 Round 6]                8\n 5  7009 85405 [Burkina Faso 2017 Round 5]                8\n 6  7009 85408 [Burkina Faso 2018 Round 6]               10\n 7  7016 85405 [Burkina Faso 2017 Round 5]                8\n 8  7016 85408 [Burkina Faso 2018 Round 6]                8\n 9  7026 85405 [Burkina Faso 2017 Round 5]                8\n10  7042 85405 [Burkina Faso 2017 Round 5]               10\n# … with 132 more rows\n\nThese summaries are exactly the type of SDP data we’d like to attach to a Household and Female dataset! Watch for our next post, where we’ll show how to create summaries by both EAID and EASERVED, and then match them to records from female respondents sampled from Burkina Faso in 2017 and 2018.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-17T15:48:25-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-26-sdp-data/",
    "title": "Service Delivery Point Data Explained",
    "description": "SDP samples are not nationally representative. Learn how to use them to describe the health service environment experienced by individuals.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-26",
    "categories": [
      "Individuals in Context",
      "Data Discovery",
      "Service Delivery Points"
    ],
    "contents": "\n\nContents\nWhat is an SDP?\nSurvey Topics\nSample Design\n\n\n\n Download this page as R code\n\n\nWhen you visit pma.ipums.org and begin browsing data, you’ll notice that PMA data are available for several different units of analysis.\nYou can see which unit of analysis you’re currently browsing - or switch to a different unit of analysis - in this box:\n\n\n\nClick CHANGE, and you’ll see the different units of analysis that are available:\n\n\n\nThis Data Discovery post kicks off a series of posts all about the data available for the Family Planning - Service Delivery Point unit of analysis. As you’ll see, these data are meant to provide important context for the individuals included in the Family Planning - Person series: while SDP data are not nationally representative, they can help provide a rich portrait of the health service environment experienced by women and households.\nYou’ll find more blog posts about SDP data by following the Individuals in Context series. Look for upcoming posts about:\nWorking with variable groups created from multiple response questions\nMerging SDP summary data with Household and Female data\nMapping SDP Data with GPS Data from our partners at pmadata.org\nMerging SDP Data with spatial datasets from external sources\nAn example of the sort of spatial analysis you can perform with SDP data\nWhat is an SDP?\nA Service Delivery Point (SDP) is any type of facility that provides health services to a community: you’ll find a breakdown of the available facility types for each sample listed in FACILITYTYPE. Because countries may include regionally-specific facility types, we’ve integrated major groupings together in the variable FACILITYTYPEGEN. For example, you may find SDP data available from any of these general facility types:\nHospitals\nHealth Centers\nHealth Clinics\nOther Health Facilities\nPrivate Practices\nDispensaries\nPharmacies / Chemists / Drug Shops\nBoutiques / Shops\nOther\nPMA samples SDPs managed by governments, NGOs, faith-based organizations, private sector organizations, and a range of other institutions. You’ll find the managing authority for each SDP listed in AUTHORITY.\nSurvey Topics\nWhile all SDP surveys cover similar topics, individual questions may be posed somewhat differently - or not at all - for any given sample. That’s where IPUMS PMA comes in: we harmonize differences across samples and document the availability of every variable for each sample.\n\nYou’ll find the full text PDF of the original questionnaire administered to all SDPs in a particular sample here.\nIPUMS PMA also organizes SDP variables by topic. These topics currently include:\nFacility Characteristics\nGeneral Facility Characteristics\nGeography\nAreas Served\nStaffing\nMedical Equipment\nFunding\nManagement\nPerformance Feedback\nQuality of Care\nService Statistics\nMedical Records\nTransportation\n\nFamily Planning Services\nServices Provided\nContraceptive Stock\nReason for Stockout\nClients Served\nStock Supplier\nFees\nFacility Condition\n\nOther Health Services\nAbortion\nPost-abortion Care\nSTDs\nAntenatal Care\nLabor and Delivery\nPostpartum Care\nDelivery Medicines\nCommunity Health Workers\nVaccinations\nHealth Programs\nMedicines in Stock\nOther\n\nThese are listed in the TOPICS menu and are subject to growth & reorganization.\n\n\n\nAdditionally, there are a number of technical variables related to survey administration. For example, every SDP included in the sample frame receives a unique FACILITYID (this ID is preserved across survey rounds if a facility is surveyed more than once). However, some facilities never responded to the questionnaire if, for example, no individual respondent was present, competent, and available to be interviewed (see AVAILABLESQ); if no such person was available - or if such a person declined the interview - the variable CONSENTSQ will indicate that survey consent was never obtained. The variable RESULTSQ indicates whether the questionnaire was fully completed or, if not, it provides the reason.\nFor SDPs where CONSENTSQ is “No”, most variables will take the value “Not interviewed (SDP questionnaire)”.\nNote that the value “NIU (not in universe)” pertains to SDPs that were intentionally skipped because a question was deemed out-of-scope.\nYou may choose whether to include SDPs where RESULTSQ indicates that the questionnaire was not fully completed. Click CREATE DATA EXTRACT from you Data Cart:\n\n\n\nThen click CHANGE next to Sample Members:\n\n\n\nFinally, choose whether to include only “Facility Respondents” (those who fully completed the questionnaire), or “All Cases” instead:\n\n\n\nSample Design\nSo what conclusions can you draw from SDP data? First, it’s important to note that the SDP sample design is not nationally representative, and there are no sampling weights for SDP data.1 In other words, it is not possible to get a sense of the national health services profile in a particular country using SDP data.\nInstead, facilities were selected for the SDP survey using the same geographic enumeration areas used to select households for each Household and Female survey. To see how this works, let’s look at an example dataset collected from Burkina Faso in 2017, beginning with the set of female respondents to the Household questionnaire (other household members and female non-respondents have been excluded):\nRead more about the Household and Female sampling strategy.\n\n\nlibrary(tidyverse)\n\nbf17_hhf <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00011.xml\",\n  data = \"data/pma_00011.dat.gz\") %>% \n  filter(YEAR == 2017)\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nChange to file paths to match your own extract. You can also use the example extract included in the data folder when you download this post.\nThe Dataset Notes for this sample describe a two-stage cluster design with urban-rural strata, producing a sample of women from 83 enumeration areas. If we count the number of unique values from EAID in our data, we see that there are 83 unique identification numbers - one for each enumeration area:\n\n\nn_distinct(bf17_hhf$EAID)\n\n\n[1] 83\n\nWe can also see how these enumeration areas are distributed throughout the 13 administrative regions of Burkina Faso. Note that we have more enumeration areas in the Central region (including the capital, Ouagadougou), and we have fewer enumeration areas in regions where the population is lower (Centre-Sud, Plateau-Central, Sud-Ouest, etc.):\n\n\nbf17_hhf %>% \n  group_by(GEOBF) %>% \n  summarize(.groups = \"keep\", n_EAID = n_distinct(EAID)) %>% \n  arrange(n_EAID)\n\n\n# A tibble: 13 x 2\n# Groups:   GEOBF [13]\n                    GEOBF n_EAID\n                <int+lbl>  <int>\n 1  7 [Centre-Sud]             2\n 2 11 [Plateau-Central]        3\n 3 13 [Sud-Ouest]              3\n 4  2 [Cascades]               4\n 5 12 [Sahel]                  4\n 6  5 [Centre-Nord]            5\n 7  4 [Centre-Est]             6\n 8 10 [Nord]                   6\n 9  1 [Boucle du Mouhoun]      7\n10  6 [Centre-Ouest]           7\n11  8 [Est]                    7\n12  9 [Hauts-Bassins]         11\n13  3 [Centre]                18\n\nAlthough the same number of households are randomly selected from within each enumeration area (typically 35), this concentration of enumeration areas within population-dense regions helps to ensure that the Household and Female data are nationally representative.\nLet’s now look at the sample of SDPs collected from Burkina Faso in that same year:\n\n\nbf17_sdp <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00008.xml\",\n  data = \"data/pma_00008.dat.gz\") %>% \n  filter(YEAR == 2017)\n\n\n\n\nChange to file paths to match your own extract. You can also use the example extract included in the data folder when you download this post.\nDataset Notes for the SDP sample explain that the same 83 enumeration areas used in the Household and Female Sample were used to select facilities for the SDP sample. Moreover, we can confirm that all of enumeration areas in the SDP data also appear in the HHF data:\n\n\nall(bf17_sdp$EAID %in% bf17_hhf$EAID)\n\n\n[1] TRUE\n\nBut is the reverse true? Is every enumeration area from the Household and Female Sample represented in the SDP data?\n\n\nall(bf17_hhf$EAID %in% bf17_sdp$EAID)\n\n\n[1] FALSE\n\nPerhaps surprisingly, the answer is no. To learn why, we have to dig a bit deeper into the SDP Dataset Notes. There, we see that a facility located within the physical boundaries of one of the 83 enumeration areas from the Household and Female Survey would have been included in the SDP sample. However, there may be enumeration areas - particularly in remote areas - where no facilities are located.\nFortunately, PMA also includes data about the service catchment area for some facilities.2 You can include this information by selecting the variable series EASERVED. If a given facility serves more than one enumeration area, EASERVED1 will contain the enumeration area ID code for the first enumeration area on its catchment list, EASERVED2 will contain the ID code for the second one, and so forth. If that same facility serves 5 enumeration areas, the variables EASERVED6, EASERVED7, and so forth would be “NIU (not in universe)”.\n\nThe IPUMS PMA extract system automatically determines the right maximum number of EASERVED variables by finding the facility with the largest service catchment list in your extract.\nWhat does this mean? As we’ll show in an upcoming post in this series, it’s possible to create a portrait of the health service environment provided to individuals sampled in the Household and Female surveys. This portrait extends beyond the list of facilities located in an individual’s geographic enumeration area, but users should take care to understand that the scope of facilities providing services to that enumeration area is somewhat limited by sample design.\n\nThe files do contain a weight variable for the sampling units EAWEIGHT, which is a probability weight representing the likelihood of an enumeration area being selected for sampling. The collectors of the original data do not recommend using EAWEIGHT to weight SDP variables.↩︎\nThis information is only available for SDPs where the managing authority listed in AUTHORITY is “government”.↩︎\n",
    "preview": "posts/2021-01-26-sdp-data/images/choose-unit.png",
    "last_modified": "2021-02-02T15:48:43-06:00",
    "input_file": {},
    "preview_width": 909,
    "preview_height": 632
  },
  {
    "path": "posts/2021-02-02-blogging-with-rmarkdown/",
    "title": "Blogging with RMarkdown",
    "description": "Quick tips for authoring and editing blog posts with RMarkdown",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-25",
    "categories": [],
    "contents": "\n\nContents\nCreating a blog post\nYAML MetadataTitle\nDescription\nCategories\nAuthor\nDate\nOutput (Table of Contents)\nPreview\n\nCode chunksChunk options\nData Visualizations from chunks\n\nFormatted TextHeadings\nBold and Italics\nInline code (variable names, packages, functions, etc)\nHyperlinks\nAsides and Footnotes\n\n\nRMarkdown documents (.Rmd) work just like regular R scripts (.R) in that you can use them as a space to develop code before sending it to the R Console. The main difference is that an RMarkdown file breaks code into discrete “chunks” of code that can be separated by blocks of text. When you run an RMarkdown document, the Console ignores anything that’s not included in a code chunk (so there’s no need to use the comment indicator #).\nRMarkdown is a powerful tool for sharing and teaching R code, but it has become even more useful with the advent of packages like knitr, which can transform RMarkdown files into Word documents, PDFs, sideshows, HTML pages, and more. The PMA Data Hub is built with knitr and another package called distill, which transforms RMarkdown files into fully formatted blog posts.\nIf you’ve never used RMarkdown, knitr, or distill before, you’ll need to install them with your R Console now:\n\n\ninstall.packages(\"RMarkdown\")\ninstall.packages(\"knitr\")\ninstall.packages(\"distill\")\n\n\n\nCreating a blog post\nIn this post, we’ll assume that you’ve already reviewed the Blog Post Workflow, so you’re familiar with the process for creating a new blog post on a new branch of the Git repository.\nAssuming you’ve created a new branch for your post, you’ll create and open an RMarkdown file when you run:\n\n\ndistill::create_post(\"Getting started with RMarkdown\")\n\n\n\nAt this point, RStudio should look something like this:\n\n\n\n\nCircled in red: notice that a new folder was created in the \"_posts\" directory shown in the Files tab, and your new files also are now being tracked in the Git tab.\nAn RMarkdown file opens with a template showing a YAML metadata header, a code chunk called “setup”, and some boilerplate text.\nYAML Metadata\nYour RMarkdown template contains a header consisting of a series of key: \"value\" pairs written in YAML. This is where we store metadata for each article appearing both at 1) the top of every blog post, and 2) on the blog homepage.\nHere’s an example of a complete YAML header for a post on the Data Hub:\n\n\n\nTitle\nThis will be the main Title shown in CSS style H1 at the top of your post. We automatically reformat to all-caps, so this is not case-sensitive.\nPlease do not use sentence punctuation (unless your title is a question).\nEnsure that your Title matches the “H1 Title” on the Data Hub Tracking Sheet.\nDescription\nThis is the subtitle shown in CSS style H2 (just below the H1 Title). This subtitle is case-senstive.\nPlease do not use sentence punctuation (unless your subtitle is a question).\nTry your best to avoid repeating the subtitle pattern “How to X”. Subtitles should emphasize the importance of a post in the particular context of analyzing PMA data if possible.\nCategories\nThe are the “tags” that will help readers filter posts and navigate through different thematic modules. What tags should you include?\nModule name (Column A of the tracking sheet)\nPost type (Column B of the tracking sheet)\nImportant package functions (package::function) or techniques\nAnalysis tools\nAlways check to see if your tags have been used in a previous post and, if so, make sure to match their existing style, spelling, etc.\nAuthor\nAlways include your name and your affiliation with the project. Optionally, feel free to link to a personal website or social media account!\nExamples:\n\nauthor:\n  - first_name: \"Yihui\"\n    last_name: \"Xie\"\n    url: https://github.com/yihui\n    affiliation: RStudio\n    affiliation_url: https://www.rstudio.com\n    orcid_id: 0000-0003-0645-5666\n  - name: \"JJ Allaire\"\n    url: https://github.com/jjallaire\n    affiliation: RStudio\n    affiliation_url: https://www.rstudio.com\n  - name: \"Rich Iannone\"\n    url: https://github.com/rich-iannone\n    affiliation: RStudio\n    affiliation_url: https://www.rstudio.com\n\nDate\nAdmin will update this to reflect date of publication.\nOutput (Table of Contents)\nThe PMA Data Hub uses a floating table of contents that follows up to 3 heading depths. Please format exactly as shown:\n\n\noutput:\n  distill::distill_article:\n    self_contained: false\n    toc: true\n    toc_depth: 3\n    toc_float: true\n\n\n\nPreview\nThis is the image that will appear alongside your post in the blog homepage. If you don’t specify an image, distill will automatically select the first image in your post; if there are no images, the space will be left blank (but please consider using one!)\nThe file path to your image should be relative to the RMarkdown file, itself. For example:\n\n\n\nThis shows the folder “2020-12-09-mapping-contraceptive-stockouts” in the \"_posts\" folder. The .Rmd file lives in the top level of the folder, and the desired image Rlogo.png lives in the images subfolder. The correct way to reference the image is:\n\n\npreview: images/static-map.png\n\n\n\nCode chunks\nEach chunk of code in your RMarkdown file must be offset as shown here (note the tic-marks and “r” in curly brackets):\n\n\n\n\nThe “r” tells R to interpret this chunk as R code (RMarkdown also supports other languages like Python, Julia, C++, and SQL).\nYou can quickly insert a code chunk via the Code menu in RStudio’s menu bar, or by using the keyboard shortcut shown there (e.g. command + option + I for mac users).\nChunk options\nYou can set specific rendering instructions for RMarkdown inside the curly brackets. Some common options include:\n\nr, eval = F # don't run the code in this chunk\nr, echo = F # hide the code, but not the results \nr, eval = T, echo = F # combine options with commas like this\nr, error = F, message = F, warning = F # hide errors, messages, & warnings\n\nA full list of chunk options are explained here.\nYou can also set default options for all of your code chunks at the top of your RMarkdown document (see the setup chunk that opens in a new template):\n\n\nknitr::opts_chunk$set(\n  echo = TRUE, \n  eval = FALSE,\n  error = FALSE,\n  message = FALSE, \n  warning = FALSE\n)\n\n\n\nData Visualizations from chunks\nThe Distill website explains how to format figures, tables, and diagrams via code chunk arguments.\nFormatted Text\nHere, we’ll show some examples for adding formatted text to the body of your RMarkdown file (i.e. everything that not included in the YAML header or a code chunk).\nHeadings\nUse the # symbol once for an H1 heading, twice ## for an H2 heading, or three times ### for an H3 heading.\nLook at the table of contents for this page: H1 headings are left aligned, and H2 headings are indented once; any H3 headings would be indented twice if we had them.\nBold and Italics\nItalics are offset by one * like this:\n\n*Italics* are offset by one `*` like this:\n\nBold text is offset by two ** like this:\n\n**Bold** text is offset by two `**` like this:\n\nInline code (variable names, packages, functions, etc)\nWe use a particular font for code chunks, and this font also gets applied in the text body to any mention of a variable name, package, or function - basically anything that might appear in the console.\nNote: the first time you use a variable name, package, or function in a post, it’s usually best in include a hyperlink to the underlying documentation. When you insert a hyperlink, do not offset text for inline code.\nInline code is offset by one ` like this:\n\n`Inline code` is offset by one ` like this:\n\nHyperlinks\nA hyperlink should be used the first time you mention a variable, package, function, or anything else that has underlying documentation at an external source.\nA hyperlink can be inserted like this:\nA [hyperlink](http://bitly.com/98K8eH) can be inserted like this:\nIf you want to link to another page on the PMA Data Hub, use relative links (do not include the full path). For example:\nA relative path to the ABOUT page:\nA relative path to the [ABOUT](about.html) page:\nOr, a relative path to one of the blog posts:\nOr, a relative path to one of the [blog posts](posts/2020-12-09-mapping-contraceptive-stockouts/index.html)\nHere is a relative path to one of the headings on that blog post:\nHere is a relative path to one of the [headings](posts/2020-12-09-mapping-contraceptive-stockouts/index.html#shiny-application) on that blog post\nAsides and Footnotes\nAsides are designed for very brief comments rendered to the side of the text body. They must be offset with <aside> tags like this:\n<aside>\nFYI: formatted text in an \"aside\" must use <b>HTML tags<\/b>\n<\/aside>\n\nFYI: formatted text in an “aside” must use HTML tags\nAsides are associated with a particular paragraph or code chunk, so they will create white-space in the text body if they become longer than their partner! For longer comments, consider using a footnote.1\nInsert a footnote here^[This is my footnote]\n\nFootnotes appear as hover-text, and they also populate at the bottom of the page.↩︎\n",
    "preview": "posts/2021-02-02-blogging-with-rmarkdown/images/new_rmarkdown.png",
    "last_modified": "2021-02-16T13:59:57-06:00",
    "input_file": {},
    "preview_width": 2560,
    "preview_height": 1600
  },
  {
    "path": "posts/2020-12-10-get-ipums-pma-data/",
    "title": "Import IPUMS PMA Data into R",
    "description": "How to download an IPUMS PMA data extract and start using it in R",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2020-11-10",
    "categories": [
      "R Tips",
      "R Packages",
      "Importing Data"
    ],
    "contents": "\n\nContents\nHow to access the dataUser Guide\nYouTube Tutorials\n\nImporting the dataFixed-width Data Format (dat)\nThe ipumsr package\n\n\nIPUMS PMA is the harmonized version of the multinational survey Performance Monitoring for Action (formerly known as Performance Monitoring and Accountability 2020 - PMA2020). IPUMS PMA lets researchers easily browse the contents of the survey series and craft customized microdata files they download for analysis.\nHow to access the data\nUser Guide\nVisit the IPUMS PMA data dissemination website to browse the available data, and then follow the posted user guide to get started with an extract of interest.\nNote: all users must register for a free account. See user guide for details.\nYouTube Tutorials\nVisit the IPUMS PMA YouTube page for a video playlist showing how to do things like:\nregister for a free IPUMS account\nselect from the available units of analysis\nbuild a data extract\nselect cases of interest\nuse the available survey weights\nImporting the data\nFixed-width Data Format (dat)\nOnce you have registered and finished selecting PMA samples and variables for your extract, click the “View Cart” button to begin checkout.\nReview the contents of your extract and clik the “Create Data Extract” button as shown:\n\n\n\nOn this final page be sure to change the data format to “.dat (fixed-width text)” if it is not selected by default:\n\n\n\nYou will receive an email when your extract request has been processed. Click the included link to find a download page like this one. You must download both the data file and the DDI codebook:\n\n\n\nThe ipumsr package\nThe R package ipumsr provides the tools you will need to import the data file and DDI codebook into R. You can install the package from CRAN with:\n\nClick here for more information on R packages.\ninstall.packages(\"ipumsr\")\nNote the location where your data file and DDI codebook were saved (in my case, they were saved in my local “Downloads” folder). Substitute your own paths into the function shown below:\n\n\ndat <- ipumsr::read_ipums_micro(\n  ddi = \"~/Downloads/pma_00001.xml\",\n  data_file = \"~/Downloads/pma_00001.dat.gz\"\n)\n\n\n\nYou’re done! The dataset is now accessible as the R object, dat.\n\n\n\n",
    "preview": "posts/2020-12-10-get-ipums-pma-data/images/create-data-extract.png",
    "last_modified": "2021-02-02T12:16:04-06:00",
    "input_file": {},
    "preview_width": 910,
    "preview_height": 790
  },
  {
    "path": "posts/2020-12-10-get-r-and-packages/",
    "title": "Getting Started with R",
    "description": "How to download R for free and install some of the R packages used on this blog",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2020-11-02",
    "categories": [
      "R Tips",
      "R Packages"
    ],
    "contents": "\n\nContents\nWhy analyze PMA data with R?Getting started with ROur favorite resources\n\nDo I really need statistical software?\nAre there alternatives?\n\nRStudio\nR packagesEssentialsipumsr\ntidyverse\nshiny\n\nWatch for updates here\n\n\nWhy analyze PMA data with R?\nLike all IPUMS data projects, IPUMS PMA data is available free of charge to users who agree to our terms of use. That’s because we believe that cost and institutional affiliation should not be barriers to answering pressing concerns around women’s health.\nIn fact, users can analyze IPUMS PMA data with any software they like! We’ve chosen to highlight R, in particular, because it is also free and popular with data analysts throughout the world. It’s available for Windows, MacOS, and a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux).\nNon-R users: IPUMS data extracts are available as CSV or fixed-width DAT with syntax files formatted for SPSS, Stata, and SAS.\nGetting started with R\nTo get a copy of R for yourself, visit the Comprehensive R Archive Network (CRAN) and choose the right download link for your operating system.\nIf you’re new to R (or want to refresh your skills), we recommend the excellent, free introductory text R for Data Science. It also introduces tidyverse conventions, which we use throughout this blog.\nOur favorite resources\nR for Data Science, for beginners\nAdvanced R, for a deeper dive\nRSpatial, for analysis with spatial data\nggplot2, for data visualization\nMastering Shiny, for interactive applications\nR Markdown: The Definitive Guide, for producing annotated code, word documents, presentations, web pages, and more\nR-bloggers, for regular news and tutorials\n\n\n\n© 2016 The R Foundation (CC-BY-SA 4.0)\nDo I really need statistical software?\nIf you’re new to data analysis, you might wonder exactly what you’re going to find in a toolkit like R.\nPlenty of people come to R after working with more common types of data analysis software, like Microsoft Excel or other spreadsheet programs. If you wanted to, you could absolutely download a CSV file from the IPUMS PMA extract system and open it in Excel. You would find individual respondents in rows and their responses for variables in columns, and you could make use of built-in spreadsheet functions to do things like:\nCalculate and visualize the distribution of a variable\nBuild pivot tables and graphs examining basic relationships between variables\nCreate new variables of your own that combine data from several variables\nHowever, you might also notice that a spreadsheet comes with certain limitations:\nThere is no variable “metadata”, including labels for the variables and each response option. For example, you might see that the responses to a certain variable include the numbers 0, 1, and 99 - what do these values actually mean?\nYou might find yourself repeating the same “point” and “click” procedure over and over. Or, maybe you’ve had to build a library of custom macro functions on your own to help automate those procedures.\nWhile you can perform arithmetic with built-in functions, there is little support for more advanced statistical procedures\nGraphics are limited within a set of pre-built templates\nMerging data from external sources (like spatial data) can be very tricky\nStatistical software is designed specifically to address these and other issues related to data cleaning and analysis. Learning a program like R takes a lot of practice, but doing so will almost certainly make your work much more efficient!\nAre there alternatives?\nYes! Many data analysts use proprietary statistical software like Stata, SAS, or SPSS. These tools are also powerful, and you may even find them easier to use than R.\n\nComing soon, we hope to include Stata code for many of the blog posts currently written in R.\nBeyond price, R has a few additional advantages that make it a particularly useful tool for working with PMA data:\nCommunity support: R users are particularly active on forums like Stack Overflow and R-bloggers. Groups like R-ladies even organize in-person meetups in cities around the world to help promote inclusion within the R community.\nCustomizability: Because R is open-source, you can change just about anything you like! With a little practice, you’ll be able to create functions and graphics that perfectly match your own needs.\nBeyond statistics: You can use R to build a website (like this one), manage and share a code repository on GitHub, scrape and compile a social media database, or automatically generate word documents, slide presentations, and more! There are practically endless ways to use functional programming in R that have nothing to do with statistics at all.\nIf you’re a beginner, learning R can be a daunting task. Keep at it! And never hesitate to ask questions.\nRStudio\nWe strongly recommend running R within RStudio, an integrated development environment (IDE) designed to make your experience with R much easier. Some of the reasons we use it, ourselves:\nIncludes a multi-pane window that puts your R console, source code, output, and help files all in one place\nSyntax highlighting and code completion\nSupport for R Projects, a crucial approach to organizing your work and sharing it with others\nIncludes RMarkdown, an R package that allows you write text-based documents with embedded snippets of code that can be passed directly to your R console\nComing soon: tools like the command palette, an improved package manager, and integrated citation management\nLike R, it is available at no cost for users on Windows, Mac, and Linux\n\nThis blog is, itself, an R Project with an individual R Markdown file for each page on the site. Look for a download button at the top of every post: you can download the original R Markdown file, open it in RStudio, and run all of the included code.\nR packages\nAn R package is a collection of functions created by other R users that you can download and install for yourself. Packages can be distributed in many ways, but all of the packages we highlight on this blog can be downloaded from CRAN (the same resource used to download “base” R). A package like ipumsr can be downloaded from CRAN by typing the following function into the R console:\n\nThis function saves package files in your default “library” location. If you’re using a Linux machine and don’t have root access, you’ll need to set up R to save packages to a location where you’re able to write files.\n\n\ninstall.packages(\"ipumsr\")\n\n\n\nPackages also come with help files detailing the purpose and possible inputs (or “arguments”) of each included function. Other included metadata explains what version of R you’ll need to use the package, and also whether the package borrows functions from any other packages that should also be installed (usually these are called “dependencies”).\nIn order to access the functions and help files for a package, you need to load it after installation with:\n\n\nlibrary(ipumsr)\n\n\n\nOn this blog, we will often show functions together with their package like this:\n\n\nipumsr::read_ipums_micro(\n  ddi = \"~/Downloads/pma_00001.xml\",\n  data_file = \"~/Downloads/pma_00001.dat.gz\"\n)\n\n\n\nThe function read_ipums_micro comes from the package ipumsr. It is not necessary for you to include the package each time you call a function (as long as you’ve already loaded the package with library()); we’re using this notation simply as a reminder (in case you want to consult the original package documentation).\n\nYou can use the package::function() notation if you ever want to access a function from a package without loading everything else in the package.\nEssentials\nHere are the packages you’ll need to install to reproduce the code on this blog:\nipumsr\nThe ipumsr package contains functions that make it easy to load IPUMS PMS data into R (mainly read_ipums_micro).\nIt also contains functions that will return variable metadata (like the variable descriptions you see while browsing for data on pma.ipums.org.\ntidyverse\nThe tidyverse package actually installs a family of related packages, including:\nggplot2, for data visualization\ndplyr, for data manipulation\ntidyr, for data tidying\nreadr, for data import\npurrr, for functional programming\ntibble, for tibbles (a modern re-imagining of data frames)\nstringr, for strings\nforcats, for factors\nThis blog uses tidyverse functions and syntax wherever possible because so-called “tidy” conventions are designed with the expressed purpose of making code and console output more human readable. Sometimes, human readability imposes a performance cost: in our experience, IPUMS PMA datasets are small enough that this is not an issue.\n\nFor larger datasets, we recommend exploring the package data.table.\nshiny\nInteractive graphics shown throughout this blog are built with the shiny package.\nWatch for updates here\nWe may add more package suggestions for future posts!\n\n\n\n",
    "preview": "posts/2020-12-10-get-r-and-packages/images/Rlogo.png",
    "last_modified": "2021-02-02T12:16:04-06:00",
    "input_file": {},
    "preview_width": 724,
    "preview_height": 561
  }
]
