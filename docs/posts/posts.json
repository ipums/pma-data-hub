[
  {
    "path": "posts/2021-06-15-covid-discovery/",
    "title": "New PMA COVID-19 Survey Data",
    "description": "A new panel study promises insights into the impact of COVID-19 on family planning and reproductive health.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-06-15",
    "categories": [
      "COVID-19",
      "Panel Data",
      "Data Discovery",
      "New Data"
    ],
    "contents": "\n\nContents\nSample Design\nTopics\nHealthcare Access\nCOVID-related Experience\nCOVID Information Sources\nCOVID Knowledge\nCOVID Prevention\nPerceptions Around COVID\n\nNext Steps\n\nThe COVID-19 pandemic has strained healthcare systems across the globe, and researchers are already beginning to examine the short-term impacts of service disruption on family planning and reproductive health.1 This spring, IPUMS PMA released COVID-19 survey data collected from reproductive age women between May and August 2020 in these countries:\nBurkina Faso\nDemocratic Republic of Congo (DRC)\nKenya\nNigeria\n\n\n\nThese women are participants in an ongoing panel study focused on core PMA topics in reproductive health. The baseline survey data for this study have already been released, and we will demonstrate how to link records between the baseline survey and the COVID-19 follow-up in an upcoming post in this series. Subsequent waves of the panel study will help to show how women’s backgrounds and levels of knowledge, perceptions, and experiences with COVID-19 shape long-term family planning outcomes.\nClick here for more information on the COVID-19 Survey design, and to learn how it fits with the ongoing panel study.\nIn this post, we’ll cover the contents of the PMA COVID-19 survey. If you’re a registered IPUMS PMA user, you can obtain COVID-19 survey data by navigating to the new COVID-19 “Unit of Analysis.”\n\n\n\nClick here for help creating, downloading, and importing an IPUMS PMA data extract into R.\nSample Design\nThe PMA COVID-19 survey is a follow-up telephone survey administered to women who participated in an in-person baseline survey for a broader panel study. This baseline survey was collected between November 2019 and Februrary 2020 - prior to the appearance of COVID-19 in most countries.\nWhen the outbreak of COVID-19 grew into a global pandemic in the spring of 2020, PMA representatives partnered with the Ministries of Health in DRC, Kenya, Burkina Faso, and Nigeria to design a shorter - approximately 30 minute - survey responding directly to the effect of COVID-19 on women and their households.\n\nSeveral countries participating in the new PMA panel study had not completed baseline sample collection by March 2020 (Uganda, India, Niger, Cote d’Ivoire).\nWomen were selected for the baseline survey if they were age 15-49 and resided in a household screened at random from a sample cluster represented by EAID. All women surveyed at baseline where eligible to participate in the COVID-19 follow-up, provided that they 1) agreed to the interview, and 2) owned or had access to a telephone.\nA COVID-19 module was incorporated into their baseline surveys in late 2020, but these data have not yet been released.\nYou’ll find survey weights adjusted for the probability that a given woman had access to a telephone recorded in the new variable CVQWEIGHT. This weight is normalized for the target population of each sample (note that two of the samples are not nationally representative):\nBurkina Faso: nationally representative\nKenya: nationally representative\nDRC: Kinshasa only\nNigeria: Lagos and Kano only\nYou’ll find more detail about the construction of PMA COVID-19 survey weights here. For information about response rates for each sample, check out sample-specific Dataset Notes.\nTopics\nThe COVID-19 survey included a number of questions that you’ll also find in the baseline survey and in future rounds of the panel study. These include topics like fertility preferences, current or recent use of family planning, and core demographic information. You might use these variables, for example, to see if women who were using a particular contraceptive method at the time of the baseline survey had stopped using that method during the first few months of the COVID-19 outbreak.\nVariables from the remainder of the COVID-19 questionnaire are organized on the IPUMS PMA website under 6 topic headings:\nHealthcare Access\nCOVID-related Experience\nCOVID Information Sources\nCOVID Knowledge\nCOVID Prevention\nPerceptions Around COVID\n\n\n\nHealthcare Access\nAll sampled women report whether they have needed to visit a health facility since COVID-19 restrictions began - including family planning visits - in CVFACVISIT. Additionally, all sampled women report whether they experienced any of the following difficulties accessing healthcare services during the same time period (select all that apply, or none):\nfacility closed / no appointment available\nnot affordable\npartner does not approve\nno available transportation\ngovernment restrictions on movement\nfear of being infected with COVID-19 at healthcare facilities\nFinally, women who did visit a healthcare facility since COVID-19 restrictions began report whether they successfully accessed needed services in HCACCESS.\nCOVID-related Experience\nIn addition to their own experiences accessing healthcare during the outbreak, women who confirmed that they had heard or read about COVID-19 were also asked to report the impact of the virus on their communities and in their households.\nSpecifically, these women were asked to estimate whether most, some, few, or no people in their community had been infected, and whether any close relatives or friends had been infected. They were also asked to rate their level on concern about the spread of COVID-19 in their community.\nYou’ll find several measures related to household-level impacts, including indicators for whether anyone in the woman’s household experienced food insecurity, and whether the the household had experienced income loss. Related questions measure changes in married / partnered women’s autonomy during the outbreak, including whether they became more or less reliant on their partner for basic needs (if at all), and whether they or their partner now makes decisions about household purchases.\nCOVID Information Sources\nWomen who confirmed that they had heard or read about COVID-19 were also asked about several different sources of information about COVID-19. For each source of information, women were asked both:\nwhether they had learned about COVID-19 from the source, and\nwhether they trust the source for accurate information about COVID-19\n13 sources of information were listed (select all that apply, or none):\nNewspaper\nRadio\nTelevision\nPoster / billboard\nTown crier\nPhone message\nFamily\nFriends / neighbors\nCommunity/religious leaders\nSocial media (Twitter, Facebook, WhatsApp)\nHealth personnel\nMessages from government or authorities\nSchool\n\nNotably, all four samples used the same list of information sources.\nYou’ll also find variables in this topic heading related to awareness, trust, and use of an emergency number or call center for reporting suspected cases of COVID-19.\nCOVID Knowledge\nWomen who confirmed that they had heard or read about COVID-19 were asked to identify common symptoms of COVID-19 from this list (select all that apply, or none):\nFever\nCough\nShortness of breath/difficulty breathing\nChest pain\nSore throat\nRunny or stuffy nose\nMuscle or body aches\nHeadaches\nFatigue (tiredness)\nDiarrhea\nLoss of taste\nLoss of smell\nRash\nDizziness\nSneezing\nOther\nThese women were also asked whether any of the following actions could reduce the risk of being infected (available responses are “yes,” “no,” or “do not know” for each action):\nWashing hands with soap and water frequently\nWashing hands with hand sanitizer frequently\nAvoiding any close contact (2 meters) with people when you go out\nStaying in your home\nGetting vaccinated\nTraditional practices\nWearing something that covers your mouth and nose when you go out (a mask)\nAvoiding shaking hands with others\nCoughing/sneezing into your elbow or tissue\nPrayer\nCOVID Prevention\nWomen who confirmed that they had heard or read about COVID-19 were asked if they had personally taken any action to prevent becoming infected. If so, they were asked which of the following actions they had personally taken (select all that apply):\nWashing hands with soap and water frequently\nWashing hands with hand sanitizer frequently\nAvoiding any close contact (2 meters) with people when you go out\nStaying in your home\nGetting vaccinated\nTraditional practices\nWearing something that covers your mouth and nose when you go out (a mask)\nAvoiding shaking hands with others\nCoughing/sneezing into your elbow or tissue\nPrayer\nOther\nWomen who confirmed that they had heard or read about COVID-19 were also asked if they were able to avoid contact with people outside of their own household. If not, they were asked if any of the following reasons explained why they might not be able to avoid contact (select all that apply, or none):\nMy work or way of earning money requires me to leave the house\nI need to visit the market\nI need to visit the water source / well\nMy studies require me to leave the household\nI need to attend funerals in the community\nI need to attend religious services\nI need to visit my family/relatives\nTo seek out health care\nPerceptions Around COVID\nWomen who confirmed that they had heard or read about COVID-19 were asked several questions about their overall level of concern about COVID-19, including how concerned they were about getting infected, whether they were worried about the impact of COVID-19 on their household’s finances in the future, and whether they would conceal information about a family member’s COVID-19 infection.\nThese women were also asked whether each of the following statements are true about COVID-19 (available responses are “yes,” “no,” or “don’t know” for each statement):\nSome people cannot be infected with Coronavirus (COVID-19)\nMost people experience mild or no symptoms\nMost people develop serious illness requiring hospitalization\nPeople can be infected and not have symptoms\nOnly people with symptoms are contagious\nYou can become infected by shaking hands with someone who is infected\nYou can become infected by close contact with infected people even if you are not touching\nPeople of all ages can become infected\nCoronavirus (COVID-19) is mostly a risk to rich people\nNext Steps\nFor the next two months, we’ll be taking a deep dive into the PMA COVID-19 survey data. Along the way, we’ll showcase several examples of R code you can use to create publication-ready tables and data visualizations, and we’ll explore some of the research questions you might answer by linking COVID-19 data to the baseline survey. Check back here for a new post every two weeks!\n\n\n\nFerreira-Filho, Edson Santos, Nilson Roberto de Melo, Isabel Cristina Esposito Sorpreso, Luis Bahamondes, Ricardo Dos Santos Simões, José Maria Soares-Júnior, and Edmund Chada Baracat. 2020. “Contraception and Reproductive Planning During the COVID-19 Pandemic.” Expert Review of Clinical Pharmacology 13 (6): 615–22. http://dx.doi.org/10.1080/17512433.2020.1782738.\n\n\nSenderowicz, Leigh, and Jenny Higgins. 2020. “Reproductive Autonomy Is Nonnegotiable, Even in the Time of COVID-19.” Perspectives on Sexual and Reproductive Health 52 (2): 81–85. http://dx.doi.org/10.1363/psrh.12152.\n\n\nTemmerman, Marleen. 2021. “Family Planning in COVID-19 Times: Access for All.” The Lancet. Global Health 9 (6): e728–29. http://dx.doi.org/10.1016/S2214-109X(21)00231-X.\n\n\nFor discussion, see (Senderowicz and Higgins 2020), (Temmerman 2021), and (Ferreira-Filho et al. 2020).↩︎\n",
    "preview": "posts/2021-06-15-covid-discovery/images/new_data_white.png",
    "last_modified": "2021-06-14T10:31:54-05:00",
    "input_file": {},
    "preview_width": 2555,
    "preview_height": 1437
  },
  {
    "path": "posts/2021-05-24-migration-data-analysis/",
    "title": "Visualizing migration patterns over time",
    "description": "How to visualize patterns in migration data using alluvial plots, line plots, and density plots.",
    "author": [
      {
        "name": "Nina Brooks",
        "url": "http://www.ninarbrooks.com/"
      }
    ],
    "date": "2021-06-02",
    "categories": [
      "Migration",
      "Data Analysis",
      "Data Visualization",
      "Descriptive Analysis",
      "ggalluvial",
      "ggridges"
    ],
    "contents": "\n\nContents\nSet Up\nVisualizing Migration: Alluvial Plots\nWhy people migrate\nWhen people migrate\n\nMigration is an incredibly important, global demographic process. Yet, studying migration is often challenging due to data limitations. As we showed in a recent post, several new PMA samples include data about each respondent’s complete migration history, organized in chronological order. They include information on when respondents move (their age), the district or region they moved from, whether the place they moved from was a city, a town, peri-urban, or rural, and reasons why they moved.\n\nMake sure to check out our Data Discovery post on these migration variables for a lot more detail on what information is collected and the unique data structure for the migration variables!\nThis data opens up the opportunity to examine many interesting questions about migration such as:\nIs migration primarily from rural to urban places?\nWhy do people migrate?\nWhen do people migrate?\nIn this post, we’ll walk through three descriptive analyses that address aspects of these questions using the 2019 Kenya sample migration data from the last post in this series. Descriptive, exploratory work like this is an essential first step to any good analysis – and visualizing the data can help illuminate patterns across different dimensions. First, we’ll demonstrate how to visualize flows of migration across the urban-rural spectrum. Then, we’ll dig into the reasons why people moved to previous residences. Finally, we’ll explore how these reasons vary according to age at the time of migration.\nSet Up\nWe’ll be working with the same Kenya 2019 data extract we created for the previous post in this series (female respondents only). It contains all of the variables shown on the migration topic page.\nIn addition to loading the two standard packages we always use (ipumsr and tidyverse), we also load the ggalluvial package that we’ll use to make an alluvial plot and the ggridges package that we’ll use to make an overlapping ridgeline (aka density) plot. If this is the first time you’re using ggalluvial and ggridges, make sure to install them first using install.packages(c(\"ggalluvial\", \"ggridges\")).\nRecall that the migration data are stored in wide format, with many variables to capture information about each move for every single individual. We’ll quickly run the code from the last post that uses pivot_longer() to convert this into the much more useful long format. We’ll also replace the special codes as NA and create an ID that represents a short identification number for each person.\n\n\nlibrary(ipumsr)\nlibrary(ggalluvial)\nlibrary(ggridges)\nlibrary(tidyverse)\n\ndat <- read_ipums_micro(\n  ddi = \"data/pma_00016.xml\",\n  data = \"data/pma_00016.dat.gz\"\n)\n\ndat <- dat %>% \n  mutate(across(everything(), ~{\n    lbl_na_if(.x, ~.lbl %in% c(\n      \"No response or missing\",\n      \"NIU (not in universe)\"\n    ))\n  })) %>% \n  rowid_to_column(\"ID\") %>% \n  select(ID, starts_with(\"PLACE\"), -PLACELIVENUM) %>% \n  pivot_longer(\n    cols = starts_with(\"PLACE\"), \n    names_pattern = \"PLACE([0-9]*)(.*)\",\n    names_to = c(\"PLACE\", \".value\"),\n    values_drop_na = TRUE\n  ) %>%\n  mutate(\n    UR = as_factor(UR),\n    PLACE = as.numeric(PLACE)\n  )\n\n\n\n\nRemember: change these file paths to match the download location for your own data extract!\nNow that the data are in a long format, each row of the data represents one place in a respondent’s migration history. For example, notice that person ID == 20 occupies two rows, one for each of the two places she listed:\n\n\ndat\n\n\n# A tibble: 5,251 x 24\n      ID PLACE    COUNTRY  DISTRICTKE MOVEAGE UR    YCHILDEDU YCOHABIT\n   <int> <dbl>  <int+lbl>   <int+lbl> <int+l> <fct> <int+lbl> <int+lb>\n 1     2     1 404 [Keny…  6 [Nairob…      16 Rural    0 [No]   0 [No]\n 2    11     1 404 [Keny… 10 [Kakame…      17 Rural    0 [No]   0 [No]\n 3    12     1 404 [Keny… 10 [Kakame…      16 City…    0 [No]   0 [No]\n 4    14     1 404 [Keny…  7 [Nandi]       29 Rural    0 [No]   0 [No]\n 5    14     2 404 [Keny… 32 [Migori]      19 City…    0 [No]   0 [No]\n 6    16     1 404 [Keny…  6 [Nairob…      21 Rural    0 [No]   0 [No]\n 7    19     1 404 [Keny… 33 [Mombas…      31 Peri…    0 [No]   0 [No]\n 8    20     1 404 [Keny…  8 [Nyamir…      19 City…    0 [No]   0 [No]\n 9    20     2 404 [Keny…  8 [Nyamir…      21 Peri…    0 [No]   0 [No]\n10    21     1 404 [Keny…  6 [Nairob…      34 Peri…    0 [No]   0 [No]\n# … with 5,241 more rows, and 16 more variables: YCONFLICT <int+lbl>,\n#   YDIVORCE <int+lbl>, YFARM <int+lbl>, YHLTHACCESS <int+lbl>,\n#   YHLTHPROB <int+lbl>, YJOBSEARCH <int+lbl>, YOTHER <int+lbl>,\n#   YOTHERSOCIAL <int+lbl>, YPOSTMAR <int+lbl>,\n#   YSCHOOLATTEND <int+lbl>, YSCHOOLDONE <int+lbl>,\n#   YSICKREL <int+lbl>, YSPOUSEJOB <int+lbl>, YWKCHANGE <int+lbl>,\n#   YWKNONSEAS <int+lbl>, YWKSEASON <int+lbl>\n\nVisualizing Migration: Alluvial Plots\nAlluvial plots are a useful way to represent flows of data according by categorical variables. A “classic” alluvial plot maps flows of passengers on the Titanic according to various characteristics and whether or not they survived.\n\n\n\nFigure 1: Figure from https://cran.r-project.org/web/packages/ggalluvial/vignettes/ggalluvial.html\n\n\n\nAlluvial plots are particularly useful for visualizing flows over time – this means we can map the characteristics of respondents across different moves and places they’ve lived! This can be really informative for migration data.\nOne topic migration researchers are often interested in studying is how people move across the urban-rural spectrum. The PMA migration module includes a variable that classifies each residence respondents previously lived in as urban, peri-urban, or rural. Here we can see that individual 2 previously lived in a rural location. Individual 14 lived in two previous residences: the most recent (PLACE == 1) was a rural location, and prior to that (PLACE == 2) she lived in a city or town.\n\nRemember, PMA considers moves in the migration data only if respondents lived in at least one other location for six months or more after the age of 15 or after her first marriage if married before the age of 15.\n\n\ndat %>%\n  select(ID, PLACE, UR)\n\n\n# A tibble: 5,251 x 3\n      ID PLACE UR        \n   <int> <dbl> <fct>     \n 1     2     1 Rural     \n 2    11     1 Rural     \n 3    12     1 City/town \n 4    14     1 Rural     \n 5    14     2 City/town \n 6    16     1 Rural     \n 7    19     1 Peri-urban\n 8    20     1 City/town \n 9    20     2 Peri-urban\n10    21     1 Peri-urban\n# … with 5,241 more rows\n\nOne way to visualize this data is to make a bar plot that shows the number of respondents in each residence category across all seven previous residences that PMA collects information about.\n\n\ndat %>%\n  ggplot(aes(x = factor(PLACE), fill = UR)) +\n    geom_bar(stat = \"count\") +\n    scale_fill_viridis_d()\n\n\n\n\nThis bar plot shows the distribution of living in an urban/peri-urban/rural location over different residences across the sample, but it doesn’t tell us anything about the flows. For example, we might want to know if people are moving from rural to urban places and vice versa. This is where an alluvial plot can add a lot of value! Another thing that is very apparent from this plot is that very few people have lived in more than three previous residences, so going forward we’ll restrict the sample to people who have lived in at least three previous locations.\nThe ggalluvial package makes it easy to generate alluvial plots using the ggplot2 grammar of graphics. There are a few key elements to an alluvial plot:\nAxes: axes are the dimensions represented by the vertical bars. In this example, the axes are the places respondents have lived.\nStrata: strata are the groups or categories each axis is divided into. In this example, each axis has the same strata (city/town, peri-urban, rural). But in the titanic example above, each axis represents a different categorical variable (class, sex, age) with different values.\nAlluvia: alluvia are the flows between categories across axes. The width or thickness of the alluvia depends on the size of that group.\n\n\n\n© RStudio (CC0 1.0)\nggplot2 is included with library(tidyverse).\nWe’ll specify these elements using the standard ggplot2 syntax. First, we’ll count the total number of places each respondent has lived so that we can restrict the alluvial plot to respondents with at least three previous residences and then look at the flows between different categories of residence for the three most recent places. When making the alluvial plot, we’ll add the axes using + geom_stratum(), the alluvia using + geom_flow(), and fill in the colors according to the UR variable.\n\n\ndat <- dat %>%\n  group_by(ID) %>%\n  mutate(TOTAL_PLACES = max(PLACE)) %>%\n  ungroup() \n\n\n# alluvial plot\ndat %>%\n  filter(TOTAL_PLACES >= 3 & PLACE < 4) %>%\n  ggplot(aes(x = PLACE, \n             stratum = UR, \n             alluvium = ID, \n             fill = UR)) +\n  geom_stratum() + \n  geom_flow() + \n  scale_fill_viridis_d() +\n  theme_minimal()\n\n\n\n\nWe can see quite a bit more detail about the types of places people are moving to and from with this alluvial plot compared to the bar plot. For example, we can see that most people who lived in a city/town moved to another urban location from place 3 to 2 to 1. Rural residents followed a similar pattern. In contrast, we see the most movement across categories from people whose 3rd most recent residence was peri-urban. Despite the fact that most urban migrants move to other urban locations and most rural migrants move to other rural locations, this plot also makes it clear there is a fair amount of rural to urban/peri-urban migration, and even urban to rural migration! Finally, we can also see there is a small number of people for whom we’re missing data on their oldest (PLACE == 3) location.\nIt was pretty simple to create this alluvial plot off-the-shelf, but we can do a bit of work to improve the clarity and presentation of this plot.\n\n\ndat %>%\n  filter(TOTAL_PLACES >= 3 & PLACE < 4) %>%\n  mutate(PLACE = PLACE %>% as_factor %>% fct_recode(\n    \"Most recent place\" = \"1\",\n    \"2nd most recent place\" = \"2\",\n    \"3rd most recent place\" = \"3\"\n  )) %>%\n  ggplot(aes(x = PLACE, \n             stratum = UR, \n             alluvium = ID, \n             fill = UR)) +\n  scale_x_discrete(expand = c(.1, .1)) +\n  geom_flow() + \n  geom_stratum(alpha = .5) + # increases the transparency of the axes' colors \n  scale_fill_viridis_d() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        axis.title.y = element_text(angle = 0, hjust = 0)) + \n  labs(title = \"Migration from Place 3 to Place 1: Kenya 2019 Sample\",\n       subtitle = \"Flows by residence category\",\n       x = NULL,\n       fill = NULL,\n       y = \"Number of\\nPeople\") + # the \\n adds a line break\n  geom_segment(aes( # adds an arrow to indicate that time is moving from right to left\n    x = 0.75, xend = 3.16,\n    y = 0, yend = 0),\n    arrow = arrow(length=unit(0.30,\"cm\"), \n                  ends=\"first\", \n                  type = \"closed\"))\n\n\n\n\nWhy people migrate\nWe might also be interested in seeing how the reasons people migrate change from move to move. The PMA surveys asked why respondents moved to each previous place they lived, allowing people to select multiple responses. In fact, there are 19 possible answers for respondents to choose from. To make this a bit more manageable, let’s summarize the reasons people moved to their previous three locations and identify the most common reasons:\n\nUnfortunately, this information is not available for the respondent’s migration to their current place of residence.\n\n\ndat %>% \n  filter(TOTAL_PLACES >= 3 & PLACE < 4) %>%\n  group_by(PLACE) %>%\n  summarise(\n    across(starts_with(\"Y\"),\n           ~100*mean(.x))) %>%\n  pivot_longer( # pivot_longer to sort by reason\n    cols = starts_with(\"Y\"),\n    names_to = \"REASON\") %>%\n  pivot_wider(\n    names_from = \"PLACE\" # pivot wider to easily see each of the 3 places as columns\n  ) %>%\n  arrange(-`1`, -`2`, -`3`)\n\n\n# A tibble: 18 x 4\n   REASON           `1`    `2`    `3`\n   <chr>          <dbl>  <dbl>  <dbl>\n 1 YSCHOOLATTEND 21.0   21.6   21.6  \n 2 YPOSTMAR      19.3   13.5   14.9  \n 3 YJOBSEARCH    17.2   19.5   18.4  \n 4 YOTHERSOCIAL  12.9   18.7   17.0  \n 5 YOTHER        10.1   10.1   11.2  \n 6 YCONFLICT      8.91   8.05   8.91 \n 7 YWKSEASON      7.76   8.91   9.20 \n 8 YWKNONSEAS     7.18   4.60   4.31 \n 9 YSCHOOLDONE    5.17   2.87   4.89 \n10 YSPOUSEJOB     4.02   2.87   4.02 \n11 YFARM          3.45   3.45   5.75 \n12 YWKCHANGE      3.16   2.30   3.74 \n13 YHLTHACCESS    2.59   2.30   3.16 \n14 YCHILDEDU      1.72   1.72   2.59 \n15 YCOHABIT       1.15   2.59   2.30 \n16 YSICKREL       1.15   2.30   0.575\n17 YHLTHPROB      0.862  1.72   2.01 \n18 YDIVORCE       0.862  0.862  1.15 \n\nWe can see that across the three locations, the most common reasons to move are: to attend school (YSCHOOLATTEND), to join a spouse after marriage (YPOSTMAR), to look for a job (YJOBSEARCH), other social reasons (YOTHERSOCIAL), other (YOTHER), because of family or village conflict (YCONFLICT), and for seasonal work (YWKSEASON). Because it will be difficult to see much with so many categories, we’ll aggregate all the less common reasons into YOTHER.\nSince each reason is stored as a different binary variable, we’ll first pivot_longer() again to create a variable called REASON that store the reason for migrating and a binary variable, VALUE, that equals 1 if the individual selected this as a reason for migrating. Then, we’ll use the very handy forcats::fct_lump_n() from the tidyverse to lump together less common responses into a single “other” category.\nAdditionally, since individuals can select multiple reasons, this variable is not well-suited to an alluvial plot, so we’ll look at how the proportion of respondents who selected each reason changes from each migration.\n\n\nmig_reasons <- dat %>% \n  filter(TOTAL_PLACES >= 3 & PLACE < 4) %>%\n  pivot_longer( # pivot_longer to sort by reason\n    cols = starts_with(\"Y\"),\n    names_to = \"REASON\",\n    values_to = \"VALUE\"\n  ) %>% \n  mutate(REASON = REASON %>%\n           as_factor %>%\n           fct_lump_n(n = 8, w = VALUE, other = \"YOTHER\") %>% \n           fct_relevel(sort)\n  ) %>%\n  group_by(ID, PLACE, REASON, VALUE) %>%\n  slice(1) %>% # to get rid of duplicate OTHER rows\n  group_by(PLACE, REASON) %>% \n  summarise(PROP = mean(VALUE)) %>% \n  ungroup()\n\n\n\n\n\n\n© RStudio (CC0 1.0)\nforcats is included with library(tidyverse).\n\n\nmig_reasons %>%\n  ggplot(aes(x = PLACE, \n             y = PROP,\n             color = REASON,\n             group = REASON)) +\n  geom_line() +\n  geom_point() +\n  scale_color_viridis_d() +\n  theme_minimal()\n\n\n\n\n\nNote that because respondents can select multiple reasons for moving to each previous residence, the proportions of all reasons to migrate to each previous residence will not add up to 1.\nAlthough many of the reasons have a similar proportion of responses across migrations, we can see that the proportion of people moving to join a spouse after marriage and moving for non-seasonal work increased from the least recent place to most recent place, while the proportion moving for seasonal work and other social reasons decreased. The most common reason for migrating – to attend school – remains pretty constant across moves for these respondents.\nTo make things a little easier to read, we’ll rename the reasons for migrating and tidy up the plot labels.\n\n\nmig_reasons %>%\n  mutate(\n    PLACE = PLACE %>% \n      as_factor %>% \n      fct_recode(\n        \"Most recent place\" = \"1\",\n        \"2nd most recent place\" = \"2\",\n        \"3rd most recent place\" = \"3\"\n      ),\n    REASON = REASON %>% \n      fct_recode(\n        \"Family or village conflict\" = \"YCONFLICT\",\n        \"To look for a job\" = \"YJOBSEARCH\",\n        \"Other\" = \"YOTHER\",\n        \"Other social reasons\" = \"YOTHERSOCIAL\",\n        \"To join spouse after marriage\" = \"YPOSTMAR\",\n        \"To attend school\" = \"YSCHOOLATTEND\",\n        \"For work (non-seasonal)\" = \"YWKNONSEAS\",\n        \"For seasonal work\" = \"YWKSEASON\"\n      )\n  )  %>%\n  ggplot(aes(x = PLACE, \n             y = PROP,\n             color = REASON,\n             group = REASON)) +\n  geom_line() +\n  geom_point() +\n  scale_color_viridis_d() +\n  scale_x_discrete(expand = c(.1, .1)) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  theme_minimal() +\n  labs(color = NULL,\n       x = NULL,\n       y = NULL,\n       title = \"Reasons for Migrating from Place 3 to Place 1\",\n       subtitle = \"% of respondents with three moves\") +\n  theme(legend.position = \"bottom\",\n        axis.title.y = element_text(angle = 0, hjust = 0))\n\n\n\n\nWhen people migrate\nThe reasons people migrate likely differ by age and is of interest to researchers. To look at how the reasons for migration might differ by age, we’ll again look at the three most recent moves, where we have the most data. To visualize how the reasons for migration differ by age, we’ll use ggridges to make an overlapping density plot that shows the distribution of ages for each reason. We’ll use the same code from before to aggregate the less common reasons into an OTHER category and pivot_longer() to store all the reasons in a single variable.\n\nYou should see a warning that says “Removed 6 rows containing non-finite values (stat_density_ridges).” This is because MOVEAGE is missing for 6 moves.\n\n\ndat %>% \n  filter(TOTAL_PLACES >= 3 & PLACE < 4) %>%\n  pivot_longer( # pivot_longer to sort by reason\n    cols = starts_with(\"Y\"),\n    names_to = \"REASON\",\n    values_to = \"VALUE\"\n  ) %>% \n  mutate(REASON = REASON %>%\n           as_factor %>%\n           fct_lump_n(n = 8, w = VALUE, other = \"YOTHER\") %>% \n           fct_relevel(sort) %>% \n           fct_recode(\n             \"Family or village conflict\" = \"YCONFLICT\",\n             \"To look for a job\" = \"YJOBSEARCH\",\n             \"Other\" = \"YOTHER\",\n             \"Other social reasons\" = \"YOTHERSOCIAL\",\n             \"To join spouse after marriage\" = \"YPOSTMAR\",\n             \"To attend school\" = \"YSCHOOLATTEND\",\n             \"For work (non-seasonal)\" = \"YWKNONSEAS\",\n             \"For seasonal work\" = \"YWKSEASON\"\n           )\n  ) %>% \n  group_by(ID, PLACE, REASON, VALUE) %>%\n  slice(1)  %>% # to get rid of duplicate OTHER rows \n  ungroup %>% \n  filter(VALUE == 1) %>% \n  ggplot(aes(x = MOVEAGE,  y = REASON, fill = REASON)) +\n    geom_density_ridges() +\n    scale_fill_viridis_d() +\n    theme_minimal() +\n    theme(legend.position = \"none\") +\n    labs(y = NULL,\n         x = \"Age at move\",\n         title = \"Reasons for Migrating from Place 3 to Place 1\",\n         subtitle = \"Distribution by age at move\")\n\n\nWarning: Removed 6 rows containing non-finite values\n(stat_density_ridges).\n\n\nThis figure shows that people tend to migrate to attend school at younger ages than most of the other reasons, which is unsurprising. Comparing the distributions by age for migrating for seasonal and non-seasonal work, it appears more young people migrate for seasonal work. In contrast, the age distributions for moving to join a spouse after marriage, to look for a job, and for non-seasonal work are all pretty similar – suggesting individuals move for these reasons at similar ages. Interestingly, there are masses at the young end of the distribution for family or village conflict, other social reasons, and other. Something further investigation could dig into more!\nWe hope this helps generate ideas for how to visualize the PMA migration data! Let us know what migration questions you’re interested in researching!\n\n\n\n",
    "preview": "posts/2021-05-24-migration-data-analysis/images/migration_alluvial.png",
    "last_modified": "2021-06-14T10:14:11-05:00",
    "input_file": {},
    "preview_width": 936,
    "preview_height": 574
  },
  {
    "path": "posts/2021-05-15-paa-2021/",
    "title": "Making the Contraceptive Calendar Data Work For You",
    "description": "R and Stata code with video from an event held at the Population Association of America 2021 Annual Meeting.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-05-15",
    "categories": [
      "PMA Publications",
      "PAA 2021",
      "Contraceptive Calendar",
      "Stata"
    ],
    "contents": "\n\nContents\nBreakout Session: R Users\nBreakout Session: Stata Users\nDownload Links\n\nOn May 4th, PMA and IPUMS PMA co-hosted a Population Association of America 2021 virtual data workshop showcasing the new PMA contraceptive calendar data available for these samples:\nBurkina Faso 2020\nCongo (DR), Kinshasa 2019\nCongo (DR), Kongo Central 2019\nKenya 2019\nNigeria, Kano 2019\nNigeria, Lagos 2019\nThese data represent contraceptive use, pregnancy, pregnancy termination, and birth information recalled by female respondents for each of several months preceding the PMA interview. Women sampled in Burkina Faso and Democratic Republic of the Congo were each asked to recall monthly information for up to 24 months, while women sampled from Kenya and Nigeria were asked to recall monthly information for up to 36 months. Their responses are recorded in a single comma delimited string, where information about each month is represented by one of the following codes:\nB = Birth\nP = Pregnant\nT = Pregnancy ended\n0 = No family planning method used\n1 = Female Sterilization\n2 = Male Sterilization\n3 = Implant\n4 = IUD\n5 = Injectables\n7 = Pill\n8 = Emergency Contraception\n9 = Male Condom\n10 = Female Condom\n11 = Diaphragm\n12 = Foam / Jelly\n13 = Standard Days / Cycle beads\n14 = LAM\n30 = Rhythm method\n31 = Withdrawal\n39 = Other traditional methods\nIn this video, PMA and IPUMS PMA explain the background behind contraceptive calendar data and show some of the ways you might consider using it in longitudinal analysis. We also give a conceptual overview of the steps both R and Stata users should take to reshape the data into a long format. After the overview, R and Stata users split into separate breakout sessions to work with a hands-on coding example using data from the Kenya 2019 sample; this example shows how to build a Kaplan-Meier survival curve for cohorts of women who were using the same family planning method in the first month of the contraceptive calendar.\n\nCheck out our recent post on migration recall data for information and practice with longitudinal data structures. The same 2019 and 2020 samples that contain contraceptive calendar data also include migration recall data.\n \nDownload Powerpoint slides here. A transcript of the chat from this session (including typed responses from the Q&A) is also available here (participant names are redacted).\nBreakout Session: R Users\nR users can load a fixed-width IPUMS PMA data extract with help from the ipumsr package (if you’re new to this blog, check out detailed instructions here). We also use packages from tidyverse to reformat the data, as well as survival and ggfortify for specific survival analysis functions.\n\n\nlibrary(ipumsr)\nlibrary(tidyverse)\nlibrary(survival)\nlibrary(ggfortify)\noptions(tibble.print_min = 20, tibble.min_extra_cols = 5)\n\ndat <- read_ipums_micro(\n  ddi = \"data/pma_00001.xml\",\n  data = \"data/pma_00001.dat.gz\"\n) \n\n\n\n\nThis code looks for my data file and DDI codebook in a folder called “data” inside of my R working directory. Make sure to change these paths as needed!\nWhen you open any IPUMS PMA data extract from the Household and Female Survey, you’ll find the data organized with one respondent per row. Here, there are 9,549 rows each representing one female respondent (all other household members have been excluded):\n\n\ndat \n\n\n# A tibble: 9,549 x 17\n         SAMPLE COUNTRY  YEAR HHID  PERSONID ELIGIBLE   EAID CONSENTFQ\n      <int+lbl> <int+l> <int> <chr> <chr>    <int+lb>  <dbl> <int+lbl>\n 1 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 2 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 3 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 4 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 5 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 6 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 7 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 8 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n 9 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n10 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n11 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n12 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n13 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n14 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n15 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n16 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n17 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n18 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n19 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n20 40410 [Keny… 7 [Ken…  2019 4042… 4042019… 1 [Yes,… 4.04e8   1 [Yes]\n# … with 9,529 more rows, and 9 more variables: FQINSTID <chr>,\n#   CONSENTHQ <int+lbl>, FQWEIGHT <dbl>, STRATA <int+lbl>,\n#   SUBNATIONAL <int+lbl>, AGE <int+lbl>, BIRTHEVENT <int+lbl>,\n#   WORKYR <int+lbl>, CALENDARKE <chr+lbl>\n\nFor the purpose of this exercise only we create a short identifying number for each respondent called ID. Then, we select only the variables ID and CALENDARKE (dropping all of the other variables pre-selected for every IPUMS PMA extract).\n\nIn practice, you should use the variable PERSONID to track unique respondents; we use ID instead here only because it fits better on the screen.\n\n\ndat <- dat %>% \n  rowid_to_column(\"ID\") %>% \n  select(ID, CALENDARKE)\n\ndat\n\n\n# A tibble: 9,549 x 2\n      ID CALENDARKE                                                   \n   <int> <chr+lbl>                                                    \n 1     1 0,0,B,P,P,P,P,P,P,P,P,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,3,3,3,…\n 2     2 ,7,7,7,7,7,7,7,7,0,B,P,P,P,P,P,P,P,P,0,0,0,0,0,9,9,9,9,9,9,9…\n 3     3 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,…\n 4     4 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,…\n 5     5 ,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,B,P,P,P,P,P,P,P,P,0,0,0,0,0…\n 6     6 5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,…\n 7     7 5,5,5,5,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,B,P,P,P,P,P,…\n 8     8 P,P,P,P,P,P,P,P,0,0,0,0,0,0,0,14,14,14,14,14,14,14,B,P,P,P,P…\n 9     9 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,…\n10    10 ,P,P,P,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9…\n11    11 ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0…\n12    12 ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0…\n13    13 ,P,P,P,P,P,P,P,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9…\n14    14 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,…\n15    15 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14…\n16    16 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,…\n17    17 ,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5…\n18    18 ,5,5,5,5,5,5,5,5,5,5,5,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7…\n19    19 0,0,0,0,0,0,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,…\n20    20 ,P,P,P,P,P,P,P,5,5,5,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0…\n# … with 9,529 more rows\n\nAs you can see, CALENDARKE includes the response codes shown above, each separated by a comma. Each string contains 36 codes: these represent the 36 months from January 2017 through December 2019 (the last month in which Kenya 2019 samples were collected). The left-most code represents the most recent available month.\nSome strings begin with a comma (i.e. the most recent month is blank). These are individuals who were interviewed in November 2019, rather than December. When we split the string into 36 columns, we must shift these individuals to the right, leaving a blank value in the left-most column (December 2019). For example, notice the blank value that appears in the new column cal_ke36 for the person ID == 2.\n\n\ndat <- dat %>% \n  separate(\n    col = CALENDARKE,\n    into = paste0(\"cal_ke\", 36:1),\n    fill = \"left\"\n  ) \n\ndat\n\n\n# A tibble: 9,549 x 37\n      ID cal_ke36 cal_ke35 cal_ke34 cal_ke33 cal_ke32 cal_ke31\n   <int> <chr>    <chr>    <chr>    <chr>    <chr>    <chr>   \n 1     1 \"0\"      0        B        P        P        P       \n 2     2 \"\"       7        7        7        7        7       \n 3     3 \"0\"      0        0        0        0        0       \n 4     4 \"0\"      0        0        0        0        0       \n 5     5 \"\"       5        5        5        5        5       \n 6     6 \"5\"      5        5        5        5        5       \n 7     7 \"5\"      5        5        5        5        5       \n 8     8 \"P\"      P        P        P        P        P       \n 9     9 \"0\"      0        0        0        0        0       \n10    10 \"\"       P        P        P        9        9       \n11    11 \"\"       0        0        0        0        0       \n12    12 \"\"       0        0        0        0        0       \n13    13 \"\"       P        P        P        P        P       \n14    14 \"0\"      0        0        0        0        0       \n15    15 \"0\"      0        0        0        0        0       \n16    16 \"3\"      3        3        3        3        3       \n17    17 \"\"       5        5        5        5        5       \n18    18 \"\"       5        5        5        5        5       \n19    19 \"0\"      0        0        0        0        0       \n20    20 \"\"       P        P        P        P        P       \n# … with 9,529 more rows, and 30 more variables: cal_ke30 <chr>,\n#   cal_ke29 <chr>, cal_ke28 <chr>, cal_ke27 <chr>, cal_ke26 <chr>,\n#   cal_ke25 <chr>, cal_ke24 <chr>, cal_ke23 <chr>, cal_ke22 <chr>,\n#   cal_ke21 <chr>, cal_ke20 <chr>, cal_ke19 <chr>, cal_ke18 <chr>,\n#   cal_ke17 <chr>, cal_ke16 <chr>, cal_ke15 <chr>, cal_ke14 <chr>,\n#   cal_ke13 <chr>, cal_ke12 <chr>, cal_ke11 <chr>, cal_ke10 <chr>,\n#   cal_ke9 <chr>, cal_ke8 <chr>, cal_ke7 <chr>, cal_ke6 <chr>,\n#   cal_ke5 <chr>, cal_ke4 <chr>, cal_ke3 <chr>, cal_ke2 <chr>,\n#   cal_ke1 <chr>\n\nLet’s now pivot the data from wide to long format so that we’ll be able to mark time in a new column called MONTH. The argument names_pattern pulls the number from each variable starting with cal_ke, which we then put in the new column MONTH.\n\n\noptions(tibble.print_min = 40)\n\ndat <- dat %>% \n  pivot_longer(\n    starts_with(\"cal_ke\"),\n    names_pattern = \"cal_ke(.*)\",\n    names_to = \"MONTH\",\n    values_to = \"FP\"\n  ) \n\ndat\n\n\n# A tibble: 343,764 x 3\n      ID MONTH FP   \n   <int> <chr> <chr>\n 1     1 36    \"0\"  \n 2     1 35    \"0\"  \n 3     1 34    \"B\"  \n 4     1 33    \"P\"  \n 5     1 32    \"P\"  \n 6     1 31    \"P\"  \n 7     1 30    \"P\"  \n 8     1 29    \"P\"  \n 9     1 28    \"P\"  \n10     1 27    \"P\"  \n11     1 26    \"P\"  \n12     1 25    \"0\"  \n13     1 24    \"0\"  \n14     1 23    \"0\"  \n15     1 22    \"0\"  \n16     1 21    \"0\"  \n17     1 20    \"0\"  \n18     1 19    \"0\"  \n19     1 18    \"0\"  \n20     1 17    \"0\"  \n21     1 16    \"0\"  \n22     1 15    \"0\"  \n23     1 14    \"0\"  \n24     1 13    \"3\"  \n25     1 12    \"3\"  \n26     1 11    \"3\"  \n27     1 10    \"3\"  \n28     1 9     \"3\"  \n29     1 8     \"3\"  \n30     1 7     \"3\"  \n31     1 6     \"3\"  \n32     1 5     \"3\"  \n33     1 4     \"3\"  \n34     1 3     \"3\"  \n35     1 2     \"3\"  \n36     1 1     \"3\"  \n37     2 36    \"\"   \n38     2 35    \"7\"  \n39     2 34    \"7\"  \n40     2 33    \"7\"  \n# … with 343,724 more rows\n\n\nNotice: each person ID now occupies 36 rows, so we increase the length of all printed dataframes to 40.\nWe’ve now created a variable FP containing the original CALENDARKE variable codes. This variable will be much easier to work with if we 1) convert it into a factor, and 2) replace missing values with NA (e.g. month 36 for individuals interviewed in November 2018). We’ll also coerce MONTH from a “character” to an “integer” class.\n\n\ndat <- dat %>%\n  mutate(\n    MONTH = as.integer(MONTH),\n    FP = FP %>%\n      na_if(\"\") %>%\n      fct_recode(\n        \"Birth\" = \"B\",\n        \"Pregnant\" = \"P\",\n        \"Pregnancy ended\" = \"T\",\n        \"No family planning method used\" = \"0\",\n        \"Female Sterilization\" = \"1\",\n        \"Male Sterilization\" = \"2\",\n        \"Implant\" = \"3\",\n        \"IUD\" = \"4\",\n        \"Injectables\" = \"5\",\n        \"Pill\" = \"7\",\n        \"Emergency Contraception\" = \"8\",\n        \"Male Condom\" = \"9\",\n        \"Female Condom\" = \"10\",\n        \"Diaphragm\" = \"11\",\n        \"Foam / Jelly\" = \"12\",\n        \"Standard Days / Cycle beads\" = \"13\",\n        \"LAM\" = \"14\",\n        \"Rhythm method\" = \"30\",\n        \"Withdrawal\" = \"31\",\n        \"Other traditional methods\" = \"39\"\n      )\n  )\n\ndat\n\n\n# A tibble: 343,764 x 3\n      ID MONTH FP                            \n   <int> <int> <fct>                         \n 1     1    36 No family planning method used\n 2     1    35 No family planning method used\n 3     1    34 Birth                         \n 4     1    33 Pregnant                      \n 5     1    32 Pregnant                      \n 6     1    31 Pregnant                      \n 7     1    30 Pregnant                      \n 8     1    29 Pregnant                      \n 9     1    28 Pregnant                      \n10     1    27 Pregnant                      \n11     1    26 Pregnant                      \n12     1    25 No family planning method used\n13     1    24 No family planning method used\n14     1    23 No family planning method used\n15     1    22 No family planning method used\n16     1    21 No family planning method used\n17     1    20 No family planning method used\n18     1    19 No family planning method used\n19     1    18 No family planning method used\n20     1    17 No family planning method used\n21     1    16 No family planning method used\n22     1    15 No family planning method used\n23     1    14 No family planning method used\n24     1    13 Implant                       \n25     1    12 Implant                       \n26     1    11 Implant                       \n27     1    10 Implant                       \n28     1     9 Implant                       \n29     1     8 Implant                       \n30     1     7 Implant                       \n31     1     6 Implant                       \n32     1     5 Implant                       \n33     1     4 Implant                       \n34     1     3 Implant                       \n35     1     2 Implant                       \n36     1     1 Implant                       \n37     2    36 <NA>                          \n38     2    35 Pill                          \n39     2    34 Pill                          \n40     2    33 Pill                          \n# … with 343,724 more rows\n\nWe’re now ready to begin our analysis. To keep our example simple, our survival curves will show the duration of continuously used family planning methods for cohorts of women who were using the same method in January 2017. These survival curves will estimate the probability that an individual “survives” - or continues using - a given method at each of 36 months, assuming that she used it in month 1. We’ll exclude the duration of use after any break (for example, if a woman stopped using family planning to become pregnant, but then started again afterward).\nLet’s begin with women who were using the contraceptive pill in January 2017. Remove all other women, saving those who remain as a sub-sample called pills:\n\n\npills <- dat %>% \n  group_by(ID) %>% \n  mutate(use_m1 = case_when(FP == \"Pill\" & MONTH == 1 ~ TRUE) %>% any()) %>% \n  filter(use_m1)\n\npills \n\n\n# A tibble: 10,332 x 4\n# Groups:   ID [287]\n      ID MONTH FP          use_m1\n   <int> <int> <fct>       <lgl> \n 1    18    36 <NA>        TRUE  \n 2    18    35 Injectables TRUE  \n 3    18    34 Injectables TRUE  \n 4    18    33 Injectables TRUE  \n 5    18    32 Injectables TRUE  \n 6    18    31 Injectables TRUE  \n 7    18    30 Injectables TRUE  \n 8    18    29 Injectables TRUE  \n 9    18    28 Injectables TRUE  \n10    18    27 Injectables TRUE  \n11    18    26 Injectables TRUE  \n12    18    25 Injectables TRUE  \n13    18    24 Pill        TRUE  \n14    18    23 Pill        TRUE  \n15    18    22 Pill        TRUE  \n16    18    21 Pill        TRUE  \n17    18    20 Pill        TRUE  \n18    18    19 Pill        TRUE  \n19    18    18 Pill        TRUE  \n20    18    17 Pill        TRUE  \n21    18    16 Pill        TRUE  \n22    18    15 Pill        TRUE  \n23    18    14 Pill        TRUE  \n24    18    13 Pill        TRUE  \n25    18    12 Pill        TRUE  \n26    18    11 Pill        TRUE  \n27    18    10 Pill        TRUE  \n28    18     9 Pill        TRUE  \n29    18     8 Pill        TRUE  \n30    18     7 Pill        TRUE  \n31    18     6 Pill        TRUE  \n32    18     5 Pill        TRUE  \n33    18     4 Pill        TRUE  \n34    18     3 Pill        TRUE  \n35    18     2 Pill        TRUE  \n36    18     1 Pill        TRUE  \n37    39    36 Pill        TRUE  \n38    39    35 Pill        TRUE  \n39    39    34 Pill        TRUE  \n40    39    33 Pill        TRUE  \n# … with 10,292 more rows\n\nThe next several steps will help us remove every record for each woman except for the last recorded month in which she used the pill. For those whose last month is month 36, we will say she “survived” the full observation period.\nTo avoid re-entry cases (returning to use of the pill), we’ll find the earliest month that a woman in this cohort was not using the pill. The month prior to this will be her last_month of using the pill. For the first person in this cohort ID == 18, for example, the last_month of use should be MONTH == 24.\n\n\npills <- pills %>% \n  mutate(\n    non_use_month = case_when(FP != \"Pill\" | is.na(FP) ~ MONTH),\n    last_month = ifelse(\n      all(is.na(non_use_month)),\n      36,\n      min(non_use_month, na.rm = T) - 1\n    )\n  ) \n\npills\n\n\n# A tibble: 10,332 x 6\n# Groups:   ID [287]\n      ID MONTH FP          use_m1 non_use_month last_month\n   <int> <int> <fct>       <lgl>          <int>      <dbl>\n 1    18    36 <NA>        TRUE              36         24\n 2    18    35 Injectables TRUE              35         24\n 3    18    34 Injectables TRUE              34         24\n 4    18    33 Injectables TRUE              33         24\n 5    18    32 Injectables TRUE              32         24\n 6    18    31 Injectables TRUE              31         24\n 7    18    30 Injectables TRUE              30         24\n 8    18    29 Injectables TRUE              29         24\n 9    18    28 Injectables TRUE              28         24\n10    18    27 Injectables TRUE              27         24\n11    18    26 Injectables TRUE              26         24\n12    18    25 Injectables TRUE              25         24\n13    18    24 Pill        TRUE              NA         24\n14    18    23 Pill        TRUE              NA         24\n15    18    22 Pill        TRUE              NA         24\n16    18    21 Pill        TRUE              NA         24\n17    18    20 Pill        TRUE              NA         24\n18    18    19 Pill        TRUE              NA         24\n19    18    18 Pill        TRUE              NA         24\n20    18    17 Pill        TRUE              NA         24\n21    18    16 Pill        TRUE              NA         24\n22    18    15 Pill        TRUE              NA         24\n23    18    14 Pill        TRUE              NA         24\n24    18    13 Pill        TRUE              NA         24\n25    18    12 Pill        TRUE              NA         24\n26    18    11 Pill        TRUE              NA         24\n27    18    10 Pill        TRUE              NA         24\n28    18     9 Pill        TRUE              NA         24\n29    18     8 Pill        TRUE              NA         24\n30    18     7 Pill        TRUE              NA         24\n31    18     6 Pill        TRUE              NA         24\n32    18     5 Pill        TRUE              NA         24\n33    18     4 Pill        TRUE              NA         24\n34    18     3 Pill        TRUE              NA         24\n35    18     2 Pill        TRUE              NA         24\n36    18     1 Pill        TRUE              NA         24\n37    39    36 Pill        TRUE              NA         36\n38    39    35 Pill        TRUE              NA         36\n39    39    34 Pill        TRUE              NA         36\n40    39    33 Pill        TRUE              NA         36\n# … with 10,292 more rows\n\nWe must now identify whether the last_month represents cessation or right-censoring. Remember that a large number of women in our sample have missing values in the 36th month: they are right-censored at month 35 if they had been continuously using the pill until that time, so we cannot say that they ceased using at month 35!\nTo make this easier, we’ll create a logical variable right_censored that simply indicates whether each person is missing a value for MONTH == 36.\n\n\npills <- pills %>% \n  mutate(right_censored = ifelse(MONTH == 36 & is.na(FP), T, F) %>% any())\n\npills \n\n\n# A tibble: 10,332 x 7\n# Groups:   ID [287]\n      ID MONTH FP       use_m1 non_use_month last_month right_censored\n   <int> <int> <fct>    <lgl>          <int>      <dbl> <lgl>         \n 1    18    36 <NA>     TRUE              36         24 TRUE          \n 2    18    35 Injecta… TRUE              35         24 TRUE          \n 3    18    34 Injecta… TRUE              34         24 TRUE          \n 4    18    33 Injecta… TRUE              33         24 TRUE          \n 5    18    32 Injecta… TRUE              32         24 TRUE          \n 6    18    31 Injecta… TRUE              31         24 TRUE          \n 7    18    30 Injecta… TRUE              30         24 TRUE          \n 8    18    29 Injecta… TRUE              29         24 TRUE          \n 9    18    28 Injecta… TRUE              28         24 TRUE          \n10    18    27 Injecta… TRUE              27         24 TRUE          \n11    18    26 Injecta… TRUE              26         24 TRUE          \n12    18    25 Injecta… TRUE              25         24 TRUE          \n13    18    24 Pill     TRUE              NA         24 TRUE          \n14    18    23 Pill     TRUE              NA         24 TRUE          \n15    18    22 Pill     TRUE              NA         24 TRUE          \n16    18    21 Pill     TRUE              NA         24 TRUE          \n17    18    20 Pill     TRUE              NA         24 TRUE          \n18    18    19 Pill     TRUE              NA         24 TRUE          \n19    18    18 Pill     TRUE              NA         24 TRUE          \n20    18    17 Pill     TRUE              NA         24 TRUE          \n21    18    16 Pill     TRUE              NA         24 TRUE          \n22    18    15 Pill     TRUE              NA         24 TRUE          \n23    18    14 Pill     TRUE              NA         24 TRUE          \n24    18    13 Pill     TRUE              NA         24 TRUE          \n25    18    12 Pill     TRUE              NA         24 TRUE          \n26    18    11 Pill     TRUE              NA         24 TRUE          \n27    18    10 Pill     TRUE              NA         24 TRUE          \n28    18     9 Pill     TRUE              NA         24 TRUE          \n29    18     8 Pill     TRUE              NA         24 TRUE          \n30    18     7 Pill     TRUE              NA         24 TRUE          \n31    18     6 Pill     TRUE              NA         24 TRUE          \n32    18     5 Pill     TRUE              NA         24 TRUE          \n33    18     4 Pill     TRUE              NA         24 TRUE          \n34    18     3 Pill     TRUE              NA         24 TRUE          \n35    18     2 Pill     TRUE              NA         24 TRUE          \n36    18     1 Pill     TRUE              NA         24 TRUE          \n37    39    36 Pill     TRUE              NA         36 FALSE         \n38    39    35 Pill     TRUE              NA         36 FALSE         \n39    39    34 Pill     TRUE              NA         36 FALSE         \n40    39    33 Pill     TRUE              NA         36 FALSE         \n# … with 10,292 more rows\n\nWe’ll create another logical variable ceased to indicate whether each woman actually stopped using the pill at her last_month. If not (either because last_month is 36, or she is right-censored and last_month is 35), it will take the value FALSE.\n\n\npills <- pills %>% \n  mutate(\n    ceased = case_when(\n      right_censored & last_month == 35 ~ F,\n      last_month == 36 ~ F,\n      last_month < 36 ~ T\n    )\n  ) %>% \n  select(ID, MONTH, FP, last_month, ceased)\n\npills\n\n\n# A tibble: 10,332 x 5\n# Groups:   ID [287]\n      ID MONTH FP          last_month ceased\n   <int> <int> <fct>            <dbl> <lgl> \n 1    18    36 <NA>                24 TRUE  \n 2    18    35 Injectables         24 TRUE  \n 3    18    34 Injectables         24 TRUE  \n 4    18    33 Injectables         24 TRUE  \n 5    18    32 Injectables         24 TRUE  \n 6    18    31 Injectables         24 TRUE  \n 7    18    30 Injectables         24 TRUE  \n 8    18    29 Injectables         24 TRUE  \n 9    18    28 Injectables         24 TRUE  \n10    18    27 Injectables         24 TRUE  \n11    18    26 Injectables         24 TRUE  \n12    18    25 Injectables         24 TRUE  \n13    18    24 Pill                24 TRUE  \n14    18    23 Pill                24 TRUE  \n15    18    22 Pill                24 TRUE  \n16    18    21 Pill                24 TRUE  \n17    18    20 Pill                24 TRUE  \n18    18    19 Pill                24 TRUE  \n19    18    18 Pill                24 TRUE  \n20    18    17 Pill                24 TRUE  \n21    18    16 Pill                24 TRUE  \n22    18    15 Pill                24 TRUE  \n23    18    14 Pill                24 TRUE  \n24    18    13 Pill                24 TRUE  \n25    18    12 Pill                24 TRUE  \n26    18    11 Pill                24 TRUE  \n27    18    10 Pill                24 TRUE  \n28    18     9 Pill                24 TRUE  \n29    18     8 Pill                24 TRUE  \n30    18     7 Pill                24 TRUE  \n31    18     6 Pill                24 TRUE  \n32    18     5 Pill                24 TRUE  \n33    18     4 Pill                24 TRUE  \n34    18     3 Pill                24 TRUE  \n35    18     2 Pill                24 TRUE  \n36    18     1 Pill                24 TRUE  \n37    39    36 Pill                36 FALSE \n38    39    35 Pill                36 FALSE \n39    39    34 Pill                36 FALSE \n40    39    33 Pill                36 FALSE \n# … with 10,292 more rows\n\n\nFor display purposes, we drop all variables except for ID, MONTH, FP, last_month, and ceased.\nRemove all rows except for the row containing each woman’s last_month. The result will be a data frame where each woman in the pills cohort occupies only one row, which 1) shows her last month of recorded use and 2) indicates whether we know that she actually stopped using the pill in her last month.\n\n\npills <- pills %>% filter(last_month == MONTH) \n\npills\n\n\n# A tibble: 287 x 5\n# Groups:   ID [287]\n      ID MONTH FP    last_month ceased\n   <int> <int> <fct>      <dbl> <lgl> \n 1    18    24 Pill          24 TRUE  \n 2    39    36 Pill          36 FALSE \n 3    44     5 Pill           5 TRUE  \n 4    59    36 Pill          36 FALSE \n 5    60     6 Pill           6 TRUE  \n 6    99    36 Pill          36 FALSE \n 7   188    36 Pill          36 FALSE \n 8   202    36 Pill          36 FALSE \n 9   218    35 Pill          35 FALSE \n10   233    12 Pill          12 TRUE  \n11   260     2 Pill           2 TRUE  \n12   290    35 Pill          35 FALSE \n13   298    20 Pill          20 TRUE  \n14   335    12 Pill          12 TRUE  \n15   340    11 Pill          11 TRUE  \n16   419    36 Pill          36 FALSE \n17   423    36 Pill          36 FALSE \n18   428     4 Pill           4 TRUE  \n19   460    16 Pill          16 TRUE  \n20   488    17 Pill          17 TRUE  \n21   551    32 Pill          32 TRUE  \n22   669    36 Pill          36 FALSE \n23   697    23 Pill          23 TRUE  \n24   707    26 Pill          26 TRUE  \n25   757    33 Pill          33 TRUE  \n26   767    23 Pill          23 TRUE  \n27   785    18 Pill          18 TRUE  \n28   836     1 Pill           1 TRUE  \n29   837    12 Pill          12 TRUE  \n30   850    35 Pill          35 FALSE \n31   930    36 Pill          36 FALSE \n32   943    13 Pill          13 TRUE  \n33   955     5 Pill           5 TRUE  \n34   977    36 Pill          36 FALSE \n35  1001    36 Pill          36 FALSE \n36  1037    36 Pill          36 FALSE \n37  1120     7 Pill           7 TRUE  \n38  1167    19 Pill          19 TRUE  \n39  1233    36 Pill          36 FALSE \n40  1271    32 Pill          32 TRUE  \n# … with 247 more rows\n\nLet’s now fit the Kaplan Meier estimator with survfit, which takes a survival object created by Surv. The function summary shows the survival probabilities at each month in a column called survival:\n\n\npills <- survfit(Surv(last_month, ceased) ~ 1, data = pills)\n\nsummary(pills)\n\n\nCall: survfit(formula = Surv(last_month, ceased) ~ 1, data = pills)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    287       4    0.986 0.00692        0.973        1.000\n    2    283       6    0.965 0.01082        0.944        0.987\n    3    277       7    0.941 0.01393        0.914        0.968\n    4    270      10    0.906 0.01723        0.873        0.940\n    5    260      14    0.857 0.02066        0.818        0.899\n    6    246       4    0.843 0.02146        0.802        0.886\n    7    242       7    0.819 0.02274        0.775        0.865\n    8    235       5    0.801 0.02355        0.757        0.849\n    9    230       3    0.791 0.02400        0.745        0.839\n   10    227       5    0.774 0.02471        0.727        0.823\n   11    222       8    0.746 0.02571        0.697        0.798\n   12    214      21    0.672 0.02770        0.620        0.729\n   13    193       9    0.641 0.02831        0.588        0.699\n   14    184       6    0.620 0.02865        0.567        0.679\n   15    178       2    0.613 0.02875        0.559        0.672\n   16    176       5    0.596 0.02897        0.542        0.655\n   17    171       8    0.568 0.02924        0.513        0.628\n   18    163       6    0.547 0.02938        0.492        0.608\n   19    157       7    0.523 0.02948        0.468        0.584\n   20    150       2    0.516 0.02950        0.461        0.577\n   21    148       2    0.509 0.02951        0.454        0.570\n   22    146       3    0.498 0.02951        0.444        0.560\n   23    143       6    0.477 0.02948        0.423        0.539\n   24    137       9    0.446 0.02934        0.392        0.507\n   25    128       3    0.436 0.02927        0.382        0.497\n   26    125       4    0.422 0.02915        0.368        0.483\n   27    121       1    0.418 0.02912        0.365        0.479\n   28    120       3    0.408 0.02901        0.355        0.469\n   29    117       2    0.401 0.02893        0.348        0.462\n   31    115       2    0.394 0.02884        0.341        0.455\n   32    113       3    0.383 0.02870        0.331        0.444\n   33    110       3    0.373 0.02854        0.321        0.433\n   34    107       3    0.362 0.02837        0.311        0.422\n\nWe can plot this with autoplot:\n\n\nautoplot(\n  pills,\n  main = \"Kaplan-Meier survival estimate: Pills\",\n  xlab = \"Months\",\n  ylab = \"Probability of Continuous Use\",\n  ylim = c(0, 1),\n  censor = F\n)\n\n\n\n\nFor additional examples using other family planning methods, download the R Markdown script from this breakout session. Video from the session is included below:\n \nBreakout Session: Stata Users\nIPUMS PMA extracts for Stata should be decompressed before use and loaded with an appropriate filepath:\n. clear\n. use \"[filepath]/pma_00001.dta\"\n. cd \"[filepath]\"\n. set more off\n\nRemember to set your own filepath and change the name of your file to match your own data extract!\nAs shown in the R example above, this dataset contains 9,549 rows where each row represents one respondent to the Female Questionnaire. You’ll find a unique identification number for each respondent in personid, and their comma-separated contraceptive calendar strings in calendarke. We’ll show these two variables for the first 3 respondents:\n. list personid calendarke in 1/3\n\n     +----------------------------------------------------------------------+\n  1. |                                           personid                   |\n     |                              404201900050442019002                   |\n     |----------------------------------------------------------------------|\n     | calendarke                                                           |\n     |   0,0,B,P,P,P,P,P,P,P,P,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,3,3,3,3,3,.. |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  2. |                                           personid                   |\n     |                              404201900009272019002                   |\n     |----------------------------------------------------------------------|\n     | calendarke                                                           |\n     |   ,7,7,7,7,7,7,7,7,0,B,P,P,P,P,P,P,P,P,0,0,0,0,0,9,9,9,9,9,9,9,9,9.. |\n     +----------------------------------------------------------------------+\n\n     +----------------------------------------------------------------------+\n  3. |                                           personid                   |\n     |                              404201900099612019003                   |\n     |----------------------------------------------------------------------|\n     | calendarke                                                           |\n     |   0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,.. |\n     +----------------------------------------------------------------------+\nAs you can see, calendarke includes the same contraceptive calendar codes shown above, each separated by a comma. Each string contains 36 codes: these represent the 36 months from January 2017 through December 2019 (the last month in which Kenya 2019 samples were collected). The left-most code represents the most recent available month.\nNotice the second listed person (personid 404201900009272019002); their calendarke string begins with a comma rather than a response code. This indicates a person who was interviewed in November 2019, rather than December. Because December 2019 would have been a future month for such a person, their first value is blank.\nSplit the calendarke string into 36 separate columns with the split function:\n. split calendarke, p(,) gen(cal_ke)\nvariables created as string: \ncal_ke1   cal_ke7   cal_ke13  cal_ke19  cal_ke25  cal_ke31\ncal_ke2   cal_ke8   cal_ke14  cal_ke20  cal_ke26  cal_ke32\ncal_ke3   cal_ke9   cal_ke15  cal_ke21  cal_ke27  cal_ke33\ncal_ke4   cal_ke10  cal_ke16  cal_ke22  cal_ke28  cal_ke34\ncal_ke5   cal_ke11  cal_ke17  cal_ke23  cal_ke29  cal_ke35\ncal_ke6   cal_ke12  cal_ke18  cal_ke24  cal_ke30  cal_ke36\nThen, reshape the data from wide to long format as shown in the R example above. The index number for each month will pivot downward into a new column called month, and the woman’s response code for each month will pivot downward into an adjacent column called cal_ke:\n. reshape long cal_ke, i(personid) j(month)\n(note: j = 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \n> 26 27 28 29 30 31 32 33 34 35 36)\n\nData                               wide   ->   long\n-----------------------------------------------------------------------------\nNumber of obs.                     9549   ->  343764\nNumber of variables                  52   ->      18\nj variable (36 values)                    ->   month\nxij variables:\n           cal_ke1 cal_ke2 ... cal_ke36   ->   cal_ke\n-----------------------------------------------------------------------------\nNotice that the dataset now has 343,764 rows, where each of our 9,549 respondents occupies 36 rows apiece (one for each month).\nBy default, Stata numbered each month in increasing order from left to right. As we’ve discussed, however, the contraceptive calendar data are chronologically organized from right to left (the most recent month is stored in the first month of the calendarke string). We suggest renumbering the cal_ke variables here so that cal_ke1 represents the first month, January 2017.\n. replace month = 37 - month\n(343,764 real changes made)\n. sort personid month\nYou might also find it useful to create century month codes cmc for each month, beginning with 1405 for January 2017.\n. gen cmc = month + 1404\nAs a final clean-up step, we create a numeric version of cal_ke called numcal_ke by changing the codes for pregnancy, pregnancy termination, and birth to 90, 91, and 92 respectively. We then provide labels to each of the values in numcal_ke.\n. gen numcal_ke = cal_ke\n(3,755 missing values generated)\n. replace numcal_ke = \"90\" if numcal_ke == \"P\"\n(24,364 real changes made)\n. replace numcal_ke = \"91\" if numcal_ke == \"T\"\n(341 real changes made)\n. replace numcal_ke = \"92\" if numcal_ke == \"B\"\n(2,934 real changes made)\n. destring numcal_ke, replace\nnumcal_ke: all characters numeric; replaced as byte\n(3755 missing values generated)\n\n. label define calendar 92 \"Birth\" 90 \"Pregnant\" 91 \"Pregnancy ended\" 0 \"No \n> family planning method used\" 1 \"Female Sterilization\" 2 \"Male Sterilization\" 3 \n> \"Implant\" 4 \"IUD\" 5 \"Injectables\" 7 \"Pill\" 8 \"Emergency Contraception\" 9 \"Male \n> Condom\" 10 \"Female Condom\" 11 \"Diaphragm\" 12 \"Foam / Jelly\" 13 \"Standard Days / \n> Cycle beads\" 14 \"LAM\" 30 \"Rhythm method\" 31 \"Withdrawal\" 39 \"Other traditional \n> methods\"\n\n. label values numcal_ke calendar\nFinally, we’re ready to begin our analysis. As with the R example above, we’ll create survival curves showing the duration of continuously used family planning methods for cohorts of women who were using the same method in January 2017. These curves will estimate the probability that an individual “survives” - or continues using - a given method at each of 36 months, assuming that she used it in month 1. We’ll exclude the duration of use after any break (for example, if a woman stopped using family planning to become pregnant, but then started again afterward).\nConsider the cohort of women who were all using the contraceptive pill in January 2017 (month 1). We’ll flag these cases and include all rows from those women in a sub-sample called pill_sample:\n. recode numcal_ke (7=1) (else=0), gen(pill)\n(162306 differences between numcal_ke and pill)\n. gen pill_temp = 0 \n. replace pill_temp = 1 if pill == 1 & month == 1\n(287 real changes made)\n. egen pill_sample = max(pill_temp), by(personid)\nThe function stset establishes the data in memory as “survival-time” data, where the variable month records time in months, the identification number for each person is provided by id(personid), and cessation of use (i.e. failure) is marked by the first instance where pill==0 for each person in the sub-sample.\n. stset month, id(personid) failure(pill==0)\n\n                id:  personid\n     failure event:  pill == 0\nobs. time interval:  (month[_n-1], month]\n exit on or before:  failure\n\n-----------------------------------------------------------------------------\n> -\n    343,764  total observations\n    328,001  observations begin on or after (first) failure\n-----------------------------------------------------------------------------\n> -\n     15,763  observations remaining, representing\n      9,549  subjects\n      9,463  failures in single-failure-per-subject data\n     15,763  total analysis time at risk and under observation\n                                                at risk from t =         0\n                                     earliest observed entry t =         0\n                                          last observed exit t =        36\nThe function sts list produces a similar table to the one produced by summary(pills) in the R example above. The column Survivor Function estimates the probability of “surviving” - or continuously using - the pill at each month shown in the column Time.\n.  sts list if pill_sample == 1\n\n         failure _d:  pill == 0\n   analysis time _t:  month\n                 id:  personid\n\n           Beg.          Net            Survivor      Std.\n  Time    Total   Fail   Lost           Function     Error     [95% Conf. Int.]\n-------------------------------------------------------------------------------\n     2      287      4      0             0.9861    0.0069     0.9633    0.9947\n     3      283      6      0             0.9652    0.0108     0.9362    0.9811\n     4      277      7      0             0.9408    0.0139     0.9064    0.9628\n     5      270     10      0             0.9059    0.0172     0.8658    0.9345\n     6      260     14      0             0.8571    0.0207     0.8111    0.8927\n     7      246      4      0             0.8432    0.0215     0.7957    0.8805\n     8      242      7      0             0.8188    0.0227     0.7692    0.8588\n     9      235      5      0             0.8014    0.0235     0.7504    0.8431\n    10      230      3      0             0.7909    0.0240     0.7392    0.8336\n    11      227      5      0             0.7735    0.0247     0.7206    0.8177\n    12      222      8      0             0.7456    0.0257     0.6911    0.7920\n    13      214     21      0             0.6725    0.0277     0.6149    0.7234\n    14      193      9      0             0.6411    0.0283     0.5827    0.6936\n    15      184      6      0             0.6202    0.0286     0.5614    0.6735\n    16      178      2      0             0.6132    0.0287     0.5543    0.6668\n    17      176      5      0             0.5958    0.0290     0.5366    0.6500\n    18      171      8      0             0.5679    0.0292     0.5085    0.6229\n    19      163      6      0             0.5470    0.0294     0.4876    0.6025\n    20      157      7      0             0.5226    0.0295     0.4633    0.5786\n    21      150      2      0             0.5157    0.0295     0.4564    0.5717\n    22      148      2      0             0.5087    0.0295     0.4495    0.5648\n    23      146      3      0             0.4983    0.0295     0.4391    0.5545\n    24      143      6      0             0.4774    0.0295     0.4185    0.5337\n    25      137      9      0             0.4460    0.0293     0.3878    0.5024\n    26      128      3      0             0.4355    0.0293     0.3776    0.4920\n    27      125      4      0             0.4216    0.0291     0.3641    0.4779\n    28      121      1      0             0.4181    0.0291     0.3607    0.4744\n    29      120      3      0             0.4077    0.0290     0.3506    0.4639\n    30      117      2      0             0.4007    0.0289     0.3438    0.4568\n    32      115      2      0             0.3937    0.0288     0.3371    0.4498\n    33      113      3      0             0.3833    0.0287     0.3271    0.4391\n    34      110      3      0             0.3728    0.0285     0.3170    0.4285\n    35      107      3      0             0.3624    0.0284     0.3070    0.4178\n    36      104     18     86             0.2997    0.0270     0.2477    0.3532\n-------------------------------------------------------------------------------\nA survival curve representing this table can be made with:\nsts graph if pill_sample == 1\n\n\n\n\nThis plot is similar to the one generated by autoplot in R, except that 1) it does not include a shaded 95% confidence interval, and 2) right-censored cases are not excluded, producing a sharp downward turn in month 36.\nFor additional examples using other family planning methods, download the Stata .do file from this breakout session here. Video from the session is included below:\n \nDownload Links\nPowerpoint Slides\nR Markdown Script\nStata .do File\nChat Transcript (participant names are redacted)\n\n\n\n",
    "preview": "posts/2021-05-15-paa-2021/images/featured.png",
    "last_modified": "2021-05-17T12:54:11-05:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 1800
  },
  {
    "path": "posts/2021-05-01-storymaps/",
    "title": "Visualizing PMA Data with StoryMaps",
    "description": "Five outstanding undergraduate research projects integrate dynamic data visualization with spatial analysis and narrative.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-05-03",
    "categories": [
      "PMA Publications",
      "StoryMaps",
      "Undergrads",
      "Teaching"
    ],
    "contents": "\n\nContents\nAttainment of Sex Preference in India\nIntimate Partner Violence and Body Weight\nHigher Probability that Women Report IPV\nEmpowered Women Raise Healthy Children\nWealth and Healthcare can Save Pregnant Women\n\nWhen great new research gets published with PMA data, we like to share it with you in a PMA Publications post. Sometimes we’ll take a deep dive into the R code you can use to reproduce great analysis, but today we’ll take a look at a different tool that undergraduate students are using to learn about IPUMS PMA and other IPUMS Global Health data at the University of Minnesota.\n\nHave a recent publication using PMA data that you’d like to feature in a PMA Publications post? Please let us know!\nThis semester, students in the Global Health Survey Analysis course used an amazing tool called StoryMaps to develop interactive narratives exploring different topics related to family planning. StoryMaps have been used in both the undergraduate and graduate curriculum throughout the College of Liberal Arts and beyond - we encourage you to check out the full gallery of student projects here!\nThanks to course professors Elizabeth Boyle and Kathryn Grace for sharing this great work!\nAttainment of Sex Preference in India\nAuthor: Lara Rae Erdmann\n\n\n\n\nIntimate Partner Violence and Body Weight\nAuthor: Jaclyn Willems\n\n\n\n\nHigher Probability that Women Report IPV\nAuthor: Peyton Retterath\n\n\n\n\nEmpowered Women Raise Healthy Children\nAuthor: Kassandra Fate\n\n\n\n\nWealth and Healthcare can Save Pregnant Women\nAuthor: Hana al’Absi\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-05-01-storymaps/images/featured.png",
    "last_modified": "2021-05-14T09:23:13-05:00",
    "input_file": {},
    "preview_width": 2190,
    "preview_height": 1254
  },
  {
    "path": "posts/2021-04-15-migration-discovery/",
    "title": "Formatting Migration Recall Data for Longitudinal Analysis",
    "description": "Use tidyr::pivot_longer to reshape wide data into a long format.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-04-15",
    "categories": [
      "Migration",
      "Data Discovery",
      "Data Manipulation",
      "pivot_longer",
      "regex"
    ],
    "contents": "\n\nContents\nData Availability\nLongitudinal Data Structures\nPivot Longer into One Column\nPivot Longer into Multiple Columns\n\n\nMost of the data you’ll find at IPUMS PMA comes from cross-sectional surveys, where each respondent is interviewed only once. However, there are some items on the Female Questionnaire that ask respondents to recall past events. When these recall data are linked to a measure of time, the data can be restructured to simulate longitudinal data – or repeated observations on individuals over time. Once the data are in this structure, we can use a range of analytic tools to determine how the frequency or duration of past experiences explains current outcomes.\n\nCurrently, only the 2016 Ethiopia Maternal and Newborn Health survey contains data from follow-up interviews. Panel data related to contraceptive use is coming soon!\nOne place where you’ll find this type of data is on the migration variables page. In our last post, we saw that PMA samples from Ethiopia began including information about each respondent’s single most most recent migration experience beginning with the 2016 sample. More recently, a number of samples from other countries have collected data about each respondent’s complete migration history, organized in chronological order. These samples include:\nBurkina Faso 2020\nCongo (DR), Kinshasa 2019\nCongo (DR), Kongo Central 2019\nKenya 2019\nNigeria, Kano 2019\nNigeria, Lagos 2019\nIn this post, we’ll take a look at the available information in the migration data collected from these newer samples. As we’ll see, female respondents who indicate that they have migrated at least once receive the same set of questions for each place they have lived, resulting in a dataset that is exceptionally wide and cumbersome to use in most time-dependent applications. We’ll show how to reshape an example data extract into a much friendlier long format using the function tidyr::pivot_longer.\nData Availability\nThe samples listed above contain data from nearly identical survey questions. In the interview, a respondent is first asked how long they have lived in their current place of residence; if they indicate that they have not “always” lived in the same place, they are then asked how many places they’ve lived for more than six months after age 15 or their first marriage (whichever happened first).\n\nInterviewers were instructed to define a place as “a community, village, or neighborhood”.\nRespondents who list at least one such place are then asked to recall information about each place, starting with the place before their current residence. Information about the most recent place is represented by variables beginning with the prefix PLACE1. Information about the second most recent place is represented by the prefix PLACE2, and so forth.\nIn each of these samples, the same questions are repeated for each place until all of the respondent’s previous places of residence are fully enumerated. The available information about each place includes:\nits country\nits district or region\nthe respondent’s age when she moved to the place\nwhether the place was a city, a town, peri-urban, or rural (not available for Nigeria samples)\nAdditionally, the respondent could list multiple reasons for migrating to each place (note that this information is not available for the respondent’s migration to their current place of residence). Options include:\nlooking for a job\nseasonal work\nwork (non-seasonal)\nwant to change jobs\nfamily or village conflict\nto attend school\nmove after completed school\njoin spouse after marriage\nco-reside with boy/girlfriend\ndivorce/widowhood\nhospitalization/health problem\nbetter access to health service\ncaring for sick relative\nfollowed spouse to job\nbetter land for farming\nbetter education for children\nother social reasons\nother\nBecause the respondent can choose multiple reasons, we’ll find one binary indicator for each of these 18 reasons. As you might imagine, this results in a very wide dataset! Some respondents move as many as 11 times, resulting in 198 columns from just this one repeated multiple-response question.\nThe wide shape of these data is more than an inconvenience: most longitudinal analysis applications require easy access to the time interval between events. In their current format, each numbered PLACE variable represents a single migration event, but it’s difficult to tell how much time passed between any two migrations for a given person. To make this kind of analysis possible, we need to pivot the migration variables into a long format accompanied by a new variable showing the time interval between each migration.\nLongitudinal Data Structures\nLet’s take a look at the way migration history variables are currently formatted for one of the samples we discussed above. For this example, we’ve created a data extract containing all of the available migration data for the Kenya 2019 sample (female respondents only). We’ll load it and the following packages into R:\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\n\ndat <- read_ipums_micro(\n  ddi = \"data/pma_00022.xml\",\n  data = \"data/pma_00022.dat.gz\"\n)\n\n\n\n\nRemember: change these file paths to match the download location for your own data extract!\n\n\n\nLike all IPUMS PMA data extracts, this dataset reflects a cross-sectional survey design where every response from each person is stored in a single row. If you’re familiar with longitudinal data structures, you know that repeated observations from the same respondents are stored in separate rows, where each row represents a different moment in time. Why might this be the case?\nAs we’ll see in our own data, the values for repeated observations and the amount of time that passes between observations are related only by a common pattern in the names for each variable when they’re stored together in a wide format. In our case, the only mark of time between migrations is the respondent’s age. Consider the following respondents, who have each migrated at least twice:\n\n\ndat %>% \n  filter(PLACELIVENUM %in% 2:7) %>% \n  select(ends_with(\"DISTRICTKE\"), ends_with(\"MOVEAGE\")) %>% \n  relocate(starts_with(\"PLACE1\"), starts_with(\"PLACE2\"))\n\n\n# A tibble: 1,206 x 14\n  PLACE1DISTRICTKE PLACE1MOVEAGE PLACE2DISTRICTKE PLACE2MOVEAGE\n         <int+lbl>     <int+lbl>        <int+lbl>     <int+lbl>\n1  7 [Nandi]                  29    32 [Migori]              19\n2  8 [Nyamira]                19     8 [Nyamira]             21\n3  6 [Nairobi]                34     3 [Kiambu]              28\n4  6 [Nairobi]                25    35 [Nakuru]               0\n5 43 [Trans-Nzoia]            15    28 [Machakos]             0\n# … with 1,201 more rows, and 10 more variables:\n#   PLACE3DISTRICTKE <int+lbl>, PLACE4DISTRICTKE <int+lbl>,\n#   PLACE5DISTRICTKE <int+lbl>, PLACE6DISTRICTKE <int+lbl>,\n#   PLACE7DISTRICTKE <int+lbl>, PLACE3MOVEAGE <int+lbl>,\n#   PLACE4MOVEAGE <int+lbl>, PLACE5MOVEAGE <int+lbl>,\n#   PLACE6MOVEAGE <int+lbl>, PLACE7MOVEAGE <int+lbl>\n\nPLACE1DISTRICTKE shows the administrative district of the last place a respondent lived before their current place of residence, and PLACE1MOVEAGE shows her age when she moved there. PLACE2DISTRICTKE shows the district of the second most recent place she lived, and PLACE2MOVEAGE shows her age when she moved there. The same pattern would be repeated for all of the places a person might have lived (in this sample, some respondents migrated as many as 7 times).\n\nYou might notice that the respondent in the second row recalls her prior places of residence in reverse chronological order. This particular type of recall error is also easier to fix when we pivot_longer.\nSuppose you wanted to know something very simple about the relationship between these variables, such as the average age of female migrants arriving at each district in the sample. In this wide format, you would first have to find the mean PLACE1MOVEAGE for every district in PLACE1DISTRICTKE, then the mean PLACE2MOVEAGE for every district in PLACE2DISTRICTKE, and so forth for all 7 places. Then, you’d need to find the frequency weighted mean for each district in all 7 places. That’s quite a bit of extra work for just one simple statistic!\nImagine you wanted to model the effect of a time-dependent variable on an outcome of interest. For example, you might suppose that the number of times a female respondent gives birth could be related to the length of time she’s lived in a district where there are relatively few family planning services available. As you can see, we’d have a hard time building an appropriate model because the relevant data are currently strewn across 14 different variables. Instead, we’d much prefer two work with the data in a long format with only two variables: one representing DISTRICTKE and one representing MOVEAGE.\nPivot Longer into One Column\nFor the moment, let’s continue working just with the district for each place in an individual’s migration history. To keep things simple, we’ll create a dataset called district containing only the variables ending with the string DISTRICTKE.\n\n\ndistrict <- dat %>% select(ends_with(\"DISTRICTKE\")) \n\ndistrict\n\n\n# A tibble: 9,549 x 7\n   PLACE1DISTRICTKE PLACE2DISTRICTKE PLACE3DISTRICTKE PLACE4DISTRICTKE\n          <int+lbl>        <int+lbl>        <int+lbl>        <int+lbl>\n1 99 [NIU (not in … 99 [NIU (not in… 99 [NIU (not in… 99 [NIU (not in…\n2  6 [Nairobi]      99 [NIU (not in… 99 [NIU (not in… 99 [NIU (not in…\n3 99 [NIU (not in … 99 [NIU (not in… 99 [NIU (not in… 99 [NIU (not in…\n4 99 [NIU (not in … 99 [NIU (not in… 99 [NIU (not in… 99 [NIU (not in…\n5 99 [NIU (not in … 99 [NIU (not in… 99 [NIU (not in… 99 [NIU (not in…\n# … with 9,544 more rows, and 3 more variables:\n#   PLACE5DISTRICTKE <int+lbl>, PLACE6DISTRICTKE <int+lbl>,\n#   PLACE7DISTRICTKE <int+lbl>\n\nAs you can see, a lot of the information contained in district isn’t really necessary. Every row holds information on 7 places of residence, but most respondents migrated only to 1 or 2 places if they ever migrated at all. The best approach here is to tell R that labels like NIU (not in universe) and No response or missing each represent a type of missing data that we can recode as NA. We can do that with help from ipumsr::lbl_na_if and dplyr::across:\n\n\ndistrict <- district %>% \n  mutate(across(everything(), ~{\n    lbl_na_if(.x, ~.lbl %in% c(\n      \"No response or missing\",\n      \"NIU (not in universe)\"\n    ))\n  }))\n\ndistrict\n\n\n# A tibble: 9,549 x 7\n  PLACE1DISTRICTKE PLACE2DISTRICTKE PLACE3DISTRICTKE PLACE4DISTRICTKE\n         <int+lbl>        <int+lbl>        <int+lbl>        <int+lbl>\n1     NA                         NA               NA               NA\n2      6 [Nairobi]               NA               NA               NA\n3     NA                         NA               NA               NA\n4     NA                         NA               NA               NA\n5     NA                         NA               NA               NA\n# … with 9,544 more rows, and 3 more variables:\n#   PLACE5DISTRICTKE <int+lbl>, PLACE6DISTRICTKE <int+lbl>,\n#   PLACE7DISTRICTKE <int+lbl>\n\n\nSee this post for additional details on recoding variables with ipumsr and dplyr::across.\nIn order to keep track of individuals, let’s also add a column ID that represents a short identification number for each person:\n\n\ndistrict <- district %>% rowid_to_column(\"ID\")\ndistrict\n\n\n# A tibble: 9,549 x 8\n     ID PLACE1DISTRICTKE PLACE2DISTRICTKE PLACE3DISTRICTKE\n  <int>        <int+lbl>        <int+lbl>        <int+lbl>\n1     1     NA                         NA               NA\n2     2      6 [Nairobi]               NA               NA\n3     3     NA                         NA               NA\n4     4     NA                         NA               NA\n5     5     NA                         NA               NA\n# … with 9,544 more rows, and 4 more variables:\n#   PLACE4DISTRICTKE <int+lbl>, PLACE5DISTRICTKE <int+lbl>,\n#   PLACE6DISTRICTKE <int+lbl>, PLACE7DISTRICTKE <int+lbl>\n\nNow, we’re ready to use the function tidyr::pivot_longer to reshape district. The simplest way to use this function is to specify a group of columns with the argument cols:\n\n\n\n\n\ndistrict %>% pivot_longer(cols = ends_with(\"DISTRICTKE\"))\n\n\n# A tibble: 66,843 x 3\n      ID name                    value\n   <int> <chr>               <int+lbl>\n 1     1 PLACE1DISTRICTKE NA          \n 2     1 PLACE2DISTRICTKE NA          \n 3     1 PLACE3DISTRICTKE NA          \n 4     1 PLACE4DISTRICTKE NA          \n 5     1 PLACE5DISTRICTKE NA          \n 6     1 PLACE6DISTRICTKE NA          \n 7     1 PLACE7DISTRICTKE NA          \n 8     2 PLACE1DISTRICTKE  6 [Nairobi]\n 9     2 PLACE2DISTRICTKE NA          \n10     2 PLACE3DISTRICTKE NA          \n# … with 66,833 more rows\n\n\n\n\n© 2018 RStudio (CC0 1.0)\ntidyr is included with library(tidyverse).\nBy default, the name of each column moves into a single column called name, and the value of each column moves into an adjascent column called value. We can manually change the names of these columns with the arguments names_to and values_to:\n\n\ndistrict %>% pivot_longer(\n  cols = ends_with(\"DISTRICTKE\"),\n  names_to = \"PLACE\",\n  values_to = \"DISTRICTKE\"\n)\n\n\n# A tibble: 66,843 x 3\n      ID PLACE              DISTRICTKE\n   <int> <chr>               <int+lbl>\n 1     1 PLACE1DISTRICTKE NA          \n 2     1 PLACE2DISTRICTKE NA          \n 3     1 PLACE3DISTRICTKE NA          \n 4     1 PLACE4DISTRICTKE NA          \n 5     1 PLACE5DISTRICTKE NA          \n 6     1 PLACE6DISTRICTKE NA          \n 7     1 PLACE7DISTRICTKE NA          \n 8     2 PLACE1DISTRICTKE  6 [Nairobi]\n 9     2 PLACE2DISTRICTKE NA          \n10     2 PLACE3DISTRICTKE NA          \n# … with 66,833 more rows\n\nEven more conveniently, we can generate these columns automatically if we identify a pattern in the original column names. This approach efficiently handles both the names of the new columns and the values stored in each column. Notice that we’ve manually specified the name of the column DISTRICTKE above; this is fine if we’re only pivoting one column, but we want to avoid manually writing a name for each new column when we start working with several variables at once. Also, notice the values that appear in the PLACE column; wouldn’t it be more convenient to extract the index number for each place, rather than the full names of the original columns?\nWe’ll use the additional argument names_pattern to solve both problems at once. Any string enclosed with parentheses () in names_pattern can be passed, in sequential order, to names_to. In this example, we specify a pattern where the string PLACE will be followed by a single-digit number ([0-9]) followed by the string (DISTRICTKE). The argument names_to places the single-digit number in a column called PLACE, and it places the string DISTRICTKE in a column that uses a pronoun .value to represent the contents of the string.\n\n\ndistrict %>% \n  pivot_longer(\n    cols = ends_with(\"DISTRICTKE\"), \n    names_pattern = \"PLACE([0-9])(DISTRICTKE)\",\n    names_to = c(\"PLACE\", \".value\")\n  ) \n\n\n# A tibble: 66,843 x 3\n      ID PLACE   DISTRICTKE\n   <int> <chr>    <int+lbl>\n 1     1 1     NA          \n 2     1 2     NA          \n 3     1 3     NA          \n 4     1 4     NA          \n 5     1 5     NA          \n 6     1 6     NA          \n 7     1 7     NA          \n 8     2 1      6 [Nairobi]\n 9     2 2     NA          \n10     2 3     NA          \n# … with 66,833 more rows\n\nWe can improve the scalability of this code just a little bit more by adding the wildcard . in names_pattern to represent “any character” and the operator * to represent “any number of times”. This allows us to write ([0-9]*) to find an integer of any length (in case some respondents move 10 places or more), and (.*) to find a string of any length afterward (this saves us the trouble of writing “DISTRICTKE”).\n\n\ndistrict %>% \n  pivot_longer(\n    cols = ends_with(\"DISTRICTKE\"), \n    names_pattern = \"PLACE([0-9]*)(.*)\",\n    names_to = c(\"PLACE\", \".value\")\n  ) \n\n\n# A tibble: 66,843 x 3\n      ID PLACE   DISTRICTKE\n   <int> <chr>    <int+lbl>\n 1     1 1     NA          \n 2     1 2     NA          \n 3     1 3     NA          \n 4     1 4     NA          \n 5     1 5     NA          \n 6     1 6     NA          \n 7     1 7     NA          \n 8     2 1      6 [Nairobi]\n 9     2 2     NA          \n10     2 3     NA          \n# … with 66,833 more rows\n\nNotice that there are now 66,843 rows in district: that’s 7 rows for 7 places per respondent. Adding the argument values_drop_NA = TRUE drops placeholder values for respondents who lived in fewer than 7 places:\n\n\ndistrict %>% \n  pivot_longer(\n    cols = ends_with(\"DISTRICTKE\"), \n    names_pattern = \"PLACE([0-9]*)(.*)\",\n    names_to = c(\"PLACE\", \".value\"),\n    values_drop_na = TRUE\n  ) \n\n\n# A tibble: 5,165 x 3\n      ID PLACE    DISTRICTKE\n   <int> <chr>     <int+lbl>\n 1     2 1      6 [Nairobi] \n 2    11 1     10 [Kakamega]\n 3    12 1     10 [Kakamega]\n 4    14 1      7 [Nandi]   \n 5    14 2     32 [Migori]  \n 6    16 1      6 [Nairobi] \n 7    19 1     33 [Mombasa] \n 8    20 1      8 [Nyamira] \n 9    20 2      8 [Nyamira] \n10    21 1      6 [Nairobi] \n# … with 5,155 more rows\n\nThis step causes any respondent who has never migrated from a place they lived for 6 months or more after age 15 / first marriage to be filtered out of the data. Here, we see the first 10 rows from all of the remaining female respondents. Individuals 14 and 20 lived in two such places: individual 14 first lived in the district Migori, then moved to Nandi, and finally moved to her current residence (not shown). Individual 20 first lived in Nyamira, then moved to another place also in Nyamira, and finally moved to her current residence (not shown). All of the other displayed respondents lived in exactly one such place. Next, we’ll add the age at which each of the women moved to each location.\n\nRemember: these migration history variables contain information about each place a person has lived prior to their current residence. You’ll find information on the woman’s current district of residence in either GEOKE (for Kenya samples) or SUBNATIONAL (for all samples, including Kenya).\nPivot Longer into Multiple Columns\nNow that we know how to use wildcard operators in pivot_longer, we’ll be able to start pivoting multiple columns at once. Let’s start by adding the respondent’s age when they moved to each place. Using the same processing steps we used to make district, we’ll create a new dataset called age.\n\n\nage <- dat %>% \n  select(ends_with(\"DISTRICTKE\"), ends_with(\"MOVEAGE\")) %>% \n  mutate(across(everything(), ~{\n    lbl_na_if(.x, ~.lbl %in% c(\n      \"No response or missing\",\n      \"NIU (not in universe)\"\n    ))\n  })) %>% \n  rowid_to_column(\"ID\")\n\n\n\n\n\nage %>% relocate(ID, starts_with(\"PLACE1\"), starts_with(\"PLACE2\"))\n\n\n# A tibble: 9,549 x 15\n      ID PLACE1DISTRICTKE PLACE1MOVEAGE PLACE2DISTRICTKE PLACE2MOVEAGE\n   <int>        <int+lbl>     <int+lbl>        <int+lbl>     <int+lbl>\n 1     1     NA                      NA               NA            NA\n 2     2      6 [Nairobi]            16               NA            NA\n 3     3     NA                      NA               NA            NA\n 4     4     NA                      NA               NA            NA\n 5     5     NA                      NA               NA            NA\n 6     6     NA                      NA               NA            NA\n 7     7     NA                      NA               NA            NA\n 8     8     NA                      NA               NA            NA\n 9     9     NA                      NA               NA            NA\n10    10     NA                      NA               NA            NA\n# … with 9,539 more rows, and 10 more variables:\n#   PLACE3DISTRICTKE <int+lbl>, PLACE4DISTRICTKE <int+lbl>,\n#   PLACE5DISTRICTKE <int+lbl>, PLACE6DISTRICTKE <int+lbl>,\n#   PLACE7DISTRICTKE <int+lbl>, PLACE3MOVEAGE <int+lbl>,\n#   PLACE4MOVEAGE <int+lbl>, PLACE5MOVEAGE <int+lbl>,\n#   PLACE6MOVEAGE <int+lbl>, PLACE7MOVEAGE <int+lbl>\n\n\nWe’re using relocate here just so that we can display the DISTRICTKE and MOVEAGE variables side-by-side. It doesn’t change anything else about the structure of the data!\nBecause we’re using the wildcard pattern (.*), the function will treat the string MOVEEAGE the same way it treats DISTRICTKE. We only need to add the new columns to cols:\n\n\nage <- age %>% \n  pivot_longer(\n    cols = c(ends_with(\"DISTRICTKE\"), ends_with(\"MOVEAGE\")), \n    names_pattern = \"PLACE([0-9]*)(.*)\",\n    names_to = c(\"PLACE\", \".value\"),\n    values_drop_na = TRUE\n  ) \n\nage\n\n\n# A tibble: 5,246 x 4\n      ID PLACE    DISTRICTKE   MOVEAGE\n   <int> <chr>     <int+lbl> <int+lbl>\n 1     2 1      6 [Nairobi]         16\n 2    11 1     10 [Kakamega]        17\n 3    12 1     10 [Kakamega]        16\n 4    14 1      7 [Nandi]           29\n 5    14 2     32 [Migori]          19\n 6    16 1      6 [Nairobi]         21\n 7    19 1     33 [Mombasa]         31\n 8    20 1      8 [Nyamira]         19\n 9    20 2      8 [Nyamira]         21\n10    21 1      6 [Nairobi]         34\n# … with 5,236 more rows\n\nThe advantages we’ve gained with a longer data format are starting to become clear! Suppose you wanted to know the average age of migrants arriving at each of Kenya’s administrative districts in this sample. You could find this information easily with just one summarise function:\n\n\nage %>% \n  group_by(DISTRICTKE) %>% \n  summarise(MEAN_AGE = mean(MOVEAGE, na.rm = T))\n\n\n# A tibble: 48 x 2\n      DISTRICTKE MEAN_AGE\n *     <int+lbl>    <dbl>\n 1  1 [Bungoma]      18.5\n 2  2 [Kericho]      18.0\n 3  3 [Kiambu]       20.7\n 4  4 [Kilifi]       17.1\n 5  5 [Kitui]        19.3\n 6  6 [Nairobi]      20.0\n 7  7 [Nandi]        18.4\n 8  8 [Nyamira]      17.9\n 9  9 [Siaya]        16.1\n10 10 [Kakamega]     17.1\n# … with 38 more rows\n\nLet’s now pivot all of the migration history columns in our original dataset dat. This time, we’ll specify that all of the desired cols start with the same prefix PLACE (but we’ll drop the column PLACELIVENUM, since it contains the string “PLACE” we’re using in names_pattern):\n\n\ndat <- dat %>% \n  mutate(across(everything(), ~{\n    lbl_na_if(.x, ~.lbl %in% c(\n      \"No response or missing\",\n      \"NIU (not in universe)\"\n    ))\n  })) %>% \n  rowid_to_column(\"ID\") %>% \n  select(ID, starts_with(\"PLACE\"), -PLACELIVENUM) %>% \n  pivot_longer(\n    cols = starts_with(\"PLACE\"), \n    names_pattern = \"PLACE([0-9]*)(.*)\",\n    names_to = c(\"PLACE\", \".value\"),\n    values_drop_na = TRUE\n  )\n\ndat\n\n\n# A tibble: 5,251 x 24\n      ID PLACE COUNTRY  DISTRICTKE MOVEAGE       UR YCHILDEDU YCOHABIT\n   <int> <chr>   <int>   <int+lbl> <int+l> <int+lb> <int+lbl> <int+lb>\n 1     2 1         404  6 [Nairob…      16 30 [Rur…    0 [No]   0 [No]\n 2    11 1         404 10 [Kakame…      17 30 [Rur…    0 [No]   0 [No]\n 3    12 1         404 10 [Kakame…      16 10 [Cit…    0 [No]   0 [No]\n 4    14 1         404  7 [Nandi]       29 30 [Rur…    0 [No]   0 [No]\n 5    14 2         404 32 [Migori]      19 10 [Cit…    0 [No]   0 [No]\n 6    16 1         404  6 [Nairob…      21 30 [Rur…    0 [No]   0 [No]\n 7    19 1         404 33 [Mombas…      31 20 [Per…    0 [No]   0 [No]\n 8    20 1         404  8 [Nyamir…      19 10 [Cit…    0 [No]   0 [No]\n 9    20 2         404  8 [Nyamir…      21 20 [Per…    0 [No]   0 [No]\n10    21 1         404  6 [Nairob…      34 20 [Per…    0 [No]   0 [No]\n# … with 5,241 more rows, and 16 more variables: YCONFLICT <int+lbl>,\n#   YDIVORCE <int+lbl>, YFARM <int+lbl>, YHLTHACCESS <int+lbl>,\n#   YHLTHPROB <int+lbl>, YJOBSEARCH <int+lbl>, YOTHER <int+lbl>,\n#   YOTHERSOCIAL <int+lbl>, YPOSTMAR <int+lbl>,\n#   YSCHOOLATTEND <int+lbl>, …\n\nWe’re left with a very manageable 24 migration history variables. Among these, all of the variables starting with Y indicate a possible reason “why” a respondent migrated to a particular PLACE. The simplest way to work with these Y variables is to use tidy selection functions, like starts_with(\"Y\"). For example, suppose you wanted to know the percentage of all migrations in the sample that happened for all of the available reasons:\n\n\ndat %>% \n  summarise(across(starts_with(\"Y\"), ~100*mean(.x))) %>% \n  glimpse()\n\n\nRows: 1\nColumns: 18\n$ YCHILDEDU     <dbl> 1.980575\n$ YCOHABIT      <dbl> 2.418587\n$ YCONFLICT     <dbl> 8.931632\n$ YDIVORCE      <dbl> 0.8188916\n$ YFARM         <dbl> 4.05637\n$ YHLTHACCESS   <dbl> 1.866311\n$ YHLTHPROB     <dbl> 0.9331556\n$ YJOBSEARCH    <dbl> 16.85393\n$ YOTHER        <dbl> 9.502952\n$ YOTHERSOCIAL  <dbl> 12.53095\n$ YPOSTMAR      <dbl> 24.01447\n$ YSCHOOLATTEND <dbl> 22.73853\n$ YSCHOOLDONE   <dbl> 4.361074\n$ YSICKREL      <dbl> 1.371167\n$ YSPOUSEJOB    <dbl> 2.704247\n$ YWKCHANGE     <dbl> 1.885355\n$ YWKNONSEAS    <dbl> 3.77071\n$ YWKSEASON     <dbl> 6.189297\n\nNow that we’ve reshaped our migration recall data from a wide format to a long format, obtaining this summary data is a snap. And, as we’ll see in an upcoming migration Data Analysis post, using these data in longitudinal analysis can be just as easy.\n\n\n\n\n",
    "preview": "posts/2021-04-15-migration-discovery/images/tidyr_wide.png",
    "last_modified": "2021-06-08T13:09:29-05:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 540
  },
  {
    "path": "posts/2021-04-01-et-internal-migration/",
    "title": "Unmet need for family planning after internal migration",
    "description": "Summary and source code from a recent article using data from Ethiopia.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-04-02",
    "categories": [
      "Migration",
      "PMA Publications",
      "svyglm",
      "bootstraps"
    ],
    "contents": "\n\nContents\nMotivation\nData\nDependent variable\nKey independent variable\nCovariates\nSub-sample\nReference groups\n\nRegression Model\nInterpretation\nPredicted Probabilities\n\n\n\n\n\nWhen great new research gets published with PMA data on a topic we’re covering here on the Data Analysis Hub, we’ll cover the highlights and explore some source code in a PMA Publications post.\n\nHave a recent publication using PMA data that you’d like to feature in a PMA Publications post? Please let us know!\nAs part of our new series on women’s migration experiences and their impact on family planning, let’s dig into a paper from University of Minnesota researchers Emily Groene and Devon Kristiansen (2021) published in the journal Population, Space and Place.\nMotivation\nAs we’ll see throughout this series, migration can be associated with major changes in an individual’s fertility intentions and family planning access, and it can either increase or decrease the likelihood of experiencing unmet need for family planning under different circumstances. Groene & Kristiansen focus their attention on the particular circumstances around rural-to-urban internal (within-country) migration, which is one of the prevailing modes of migration throughout the countries surveyed by PMA (McAuliffe and Ruhs 2017).\nConsider all of the potential changes a person might experience when moving from a rural to an urban area: Groene & Kristiansen outline literature that suggests quite a few ways that these changes might impact fertility behavior. Some are likely to increase demand for family planning, for example:\nIncreased availability and access to health services and long-acting contraceptives (Skiles et al. 2015)\nNew opportunities for employment, education, and greater wealth that can delay or limit plans for additional births (Schultz 1994)\nDiminished incentives for larger family sizes tied to rural culture and livelihoods (Abebe 2007)\nAcculturation, or adoption of destination cultural roles and values (Kohler 2000)\nOn the other hand, several offsetting factors may push to maintain or even decrease demand for family planning:\nSpousal separation tied to seasonal migration for employment (Sevoyan and Agadjanian 2013)\nFamily planning preferences established prior to migration (Kulu 2005)\nSelection into destinations where familiar cultural roles and values are prevalent (Courgeau 1989)\nEven when we focus our analysis on rural-urban internal migrants, it’s very hard to predict how these and other factors might react to determine the family planning needs for any given person. From a policy perspective, where planning is needed to identify and address unmet need for family planning services on a larger scale, Groene & Kristiansen offer important insights into the ways that migration experiences are tied to a particular place. Using female respondents from the Ethiopia 2017 and 2018 samples, they compare unmet need among rural-urban internal migrants to the unmet need experienced by non-migrants in both rural and urban settings. They find that migrants are less likely to experience unmet need compared to non-migrants, controlling for a number of demographic factors.\nUnmet need is the difference between an individual’s reproductive intentions and contraceptive behavior.\nIn this post, we’ll show how to recreate their analysis using an IPUMS PMA data extract in R.\nData\nThe Ethiopia 2017 and 2018 samples were among the first PMA samples to include questions related to women’s most recent migration experience, and about the region where they were born. Their responses are included in variables listed in the migration variable group:\nLIVEINREGION - How long living continuously in current region\nLIVEINREGIONYRS - Number of years continuously living in current region\nLASTREGION - Region/country of residence before current region\nLASTUR - Urban/rural status of residence before current region\nBIRTHREGIONET - Region of woman’s birth, Ethiopia\nBIRTHUR - Urban/rural status of region of woman’s birth\nMIGMAINRSN - The main reason why moved to current place of residence\nMIGPREKID - Gave birth before moved to current region\nMIGPREKIDNUM - Number of sons/daughters before moving to current region\n\nA 2013 sample of women from Kinshasa, DRC were also given questions related to their recent migration history, but these data have not been made available for public use. See Anglewicz et al. (2017).\nGroene & Kristiansen use the variable LIVEINREGION to determine whether a woman has always lived in the same place and, if not, they use BIRTHUR together with URBAN to identify those who ultimately moved from a rural place to an urban place. We’ve created a data extract containing these and all of the other variables discussed below (female respondents only); we’ll start by loading it and the following packages in R:\n\n\nlibrary(ipumsr)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(survey)\nlibrary(srvyr)\n\ndat <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00019.xml\",\n  data = \"data/pma_00019.dat.gz\"\n)\n\n\n\nIf you’re a registered user at pma.ipums.org, you can recreate the authors’ data extract by selecting the variables mentioned in this post.\nSee our guide for help importing IPUMS data extracts into R.\nWe’ll first label the various non-response values in this dataset with the value NA using ipumsr::lbl_na_if applied to all variables with dplyr::across:\n\n\ndat <- dat %>% \n  mutate(\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\",\n        \"NIU (not in universe) or missing\"\n      )\n    ))\n  )\n\n\n\nEthiopia samples are stratified by region (GEOET) and urban status (URBAN), resulting in 21 sampling strata from which primary sampling units (EAID) are selected. The authors exclude women from any STRATA where fewer than 200 women were sampled across both sample years:\n\nPMA uses a multistage, stratified cluster sample design. For more information, see PMA’s sample design memo.\n\n\ndat %>% count(STRATA) \n\n\n# A tibble: 21 x 2\n                                  STRATA     n\n *                             <int+lbl> <int>\n 1 23101 [Addis Ababa - urban, Ethiopia]  1833\n 2 23102 [Amhara - urban, Ethiopia]       1005\n 3 23103 [Amhara - rural, Ethiopia]       1651\n 4 23104 [Oromiya - urban, Ethiopia]      1216\n 5 23105 [Oromiya - rural, Ethiopia]      2275\n 6 23106 [Other, Ethiopia]                 839\n 7 23107 [SNNP - urban, Ethiopia]         1904\n 8 23108 [SNNP - rural, Ethiopia]         1241\n 9 23109 [Tigray - urban, Ethiopia]       1455\n10 23110 [Tigray - rural, Ethiopia]        810\n11 23111 [Dire Dawa - urban, Ethiopia]      22\n12 23112 [Dire Dawa - rural, Ethiopia]      29\n13 23113 [Afar - urban, Ethiopia]           32\n14 23114 [Afar - rural, Ethiopia]          218\n15 23115 [Somali - urban, Ethiopia]         73\n16 23116 [Somali - rural, Ethiopia]        123\n17 23117 [Gambella - urban, Ethiopia]       34\n18 23118 [Gambella - rural, Ethiopia]       23\n19 23119 [Harari - urban, Ethiopia]         29\n20 23120 [Harari - rural, Ethiopia]         26\n21 23121 [BG - rural, Ethiopia]            172\n\nNote that this will drop women from STRATA numbered 23111-23113 and 23115-23121:\n\n\ndat <- dat %>% \n  group_by(STRATA) %>% \n  mutate(STRATA_N = n()) %>% \n  ungroup() %>% \n  filter(STRATA_N > 200)\n\n\n\nDependent variable\nNow, consider the dependent variable UNMETYN, which is a constructed variable indicating whether each respondent currently has an unmet need for family planning. All respondents to the female questionnaire are included in the universe for UNMETYN, so women who are not able to become pregnant or are not sexually active are determined to have “no unmet need.”\nUNMETYN is a recoded binary indicator from UNMETNEED, which contains additional details on types of unmet need.\nWithin the combined sample of female respondents from both years, about 12% of women demonstrated unmet need for family planning:\n\n\ndat %>% summarize(mean_UNMETYN = mean(UNMETYN, na.rm = T)) \n\n\n# A tibble: 1 x 1\n  mean_UNMETYN\n         <dbl>\n1        0.120\n\nWe’ll use the survey package - and its tidy companion srvyr - to specify PMA sample design in our population estimates. The function srvyr::survey_mean uses information about the survey design (given by srvyr::as_survey_design) to estimate that an average woman aged 15-49 in Ethiopia has about a 15% chance of experiencing unmet need for family planning, with a 95% confidence interval ranging between 13.5% and 16.7%:\n\n\n\n© Greg Freedman Ellis et al. (GPL 2 | GPL 3)\n\n\ndat %>% \n  as_survey_design(\n    id = EAID,\n    nest = T,\n    weight = FQWEIGHT,\n    strata = STRATA\n  ) %>% \n  summarize(pop_UNMETYN = survey_mean(UNMETYN, vartype = \"ci\", na.rm = T))\n\n\n# A tibble: 1 x 3\n  pop_UNMETYN pop_UNMETYN_low pop_UNMETYN_upp\n        <dbl>           <dbl>           <dbl>\n1       0.151           0.135           0.167\n\nsrvyr brings parts of dplyr syntax to survey analysis, using the survey package.\nKey independent variable\nIn order to conduct a three-way comparison between rural-urban migrants, rural non-migrants, and urban non-migrants, the authors construct a variable we’ll call MIGRANT_DIR.\nThe first component of MIGRANT_DIR evaluates whether each woman ever migrated from her place of birth. Using LIVEINREGION, any woman reporting that she has “always” lived in her current region is not a migrant, and any woman who has lived in her current region for a number of “months or years” is a migrant (note: we cannot determine the migration history for all of the remaining cases, so they will be excluded from further analysis).\n\n\ndat %>% count(LIVEINREGION)\n\n\n# A tibble: 4 x 2\n             LIVEINREGION     n\n*               <int+lbl> <int>\n1 10 [Always]             11777\n2 20 [Currently visiting]   124\n3 30 [Months or years]     2539\n4 NA                          7\n\nMigrants who were born in a rural place !BIRTHUR and now live in an urban place URBAN meet the definition for rural-urban migrant. Non-migrants are classified by their current URBAN status only. All other women are implicitly given the value NA and then filtered out of the dataset.\n\nWe implicitly assign NA to any cases that aren’t specified by the logical statements inside case_when().\n\n\ndat <- dat %>% \n   mutate(\n     across(c(BIRTHUR, URBAN), ~.x %>% zap_labels),\n     MIGRANT = case_when(\n       LIVEINREGION == 30 ~ T, \n       LIVEINREGION == 10 ~ F\n     ),\n     MIGRANT_DIR = case_when(\n       MIGRANT & !BIRTHUR & URBAN ~ \"rural to urban\",\n       !MIGRANT & !URBAN ~ \"nonmigrant - rural\",\n       !MIGRANT & URBAN ~ \"nonmigrant - urban\"\n     ) \n   ) %>% \n  filter(!is.na(MIGRANT_DIR))\n\n\n\n\nRemember: BIRTHUR and URBAN are labeled integers. Our use of zap_labels allows R to ignore their assigned labels and, instead, treat them as logicals where 1 == TRUE and 0 == FALSE.\n\n\ndat %>% count(MIGRANT_DIR)\n\n\n# A tibble: 3 x 2\n  MIGRANT_DIR            n\n* <chr>              <int>\n1 nonmigrant - rural  6437\n2 nonmigrant - urban  5340\n3 rural to urban      1421\n\nYou may notice that we’ve created MIGRANT_DIR as a string, or a character object. We’ll coerce it as a factor later so we can easily use each of the three classifications in a logistic regression model.\nCovariates\nThe authors control for a number of covariates in addition to MIGRANT_DIR. The following covariates are recoded versions of existing PMA variables:\nBIRTHS: number of children ever born CHEB (2017), or the woman’s total number of birth events BIRTHEVENT (2018, e.g. birth of twins is a single event)\nPARTNERED: recoded MARSTAT, indicating if the woman is either currently married or living with a partner\nRELGEN: recoded RELIGION as “muslim,” “christian,” or “other”\n\n\ndat <- dat %>% \n  mutate(\n    across(c(BIRTHEVENT, CHEB), ~.x %>% zap_labels),\n    BIRTHS = case_when(\n      YEAR == 2018 ~ BIRTHEVENT, \n      T ~ CHEB\n    ),\n    PARTNERED = case_when(\n      MARSTAT %in% 21:22 ~ T, \n      !is.na(MARSTAT) ~ F\n    ),\n    RELGEN = case_when(\n      RELIGION == 100 ~ \"muslim\",\n      RELIGION >= 200 & RELIGION < 300 ~ \"christian\",\n      T ~ \"other\"\n    )\n  )\n\n\n\nThe remaining covariates are used without further modification:\nAGE: the woman’s age (years)\nWEALTHQ: wealth quintile\nEDUCATTGEN: education level (general)\nHCVISITY: whether the woman visited a health facility in the last 12 months\nSUBNATIONAL: subnational region\nYEAR: survey year (2017 or 2018)\nSub-sample\nAs discussed above, the authors exclude any female respondents from small STRATA (n < 200) and those who are neither rural-urban migrants nor non-migrants. Additionally, they remove rural-urban migrants who moved to Ethiopia from another country. Women can indicate this information in two places: they may either list a foreign country in LASTREGION or indicate “abroad” as their region of birth in BIRTHREGIONET.\n\n\ndat %>% count(LASTREGION)\n\n\n# A tibble: 18 x 2\n                   LASTREGION     n\n *                  <int+lbl> <int>\n 1 101 [Tigray]                  50\n 2 102 [Afar]                     7\n 3 103 [Amhara]                 399\n 4 104 [Oromia]                 360\n 5 105 [Ethiopia Somali]          1\n 6 106 [Benishangul Gumuz]        4\n 7 107 [SNNPR]                  390\n 8 108 [Gambella]                 4\n 9 109 [Harari]                   7\n10 110 [Addis Ababa]            116\n11 111 [Dire Dawa]                7\n12 202 [Saudi Arabia]            33\n13 204 [Beirut]                   5\n14 205 [United Arab Emirates]     9\n15 206 [Sudan]                    5\n16 210 [Lebanon]                  1\n17 300 [Other]                   23\n18  NA                        11777\n\ndat %>% count(BIRTHREGIONET)\n\n\n# A tibble: 12 x 2\n                                               BIRTHREGIONET     n\n *                                                 <int+lbl> <int>\n 1  1 [Tigray Region]                                         2110\n 2  2 [Afar Region]                                            430\n 3  3 [Amhara Region]                                         2911\n 4  4 [Oromia Region]                                         3472\n 5  5 [Somali Region]                                          166\n 6  6 [Benishangul-Gumuz Region]                               148\n 7  7 [Southern Nations, Nationalities, and Peoples' Region]  3057\n 8  8 [Gambella Region]                                         26\n 9  9 [Harari Region]                                           35\n10 10 [Addis Ababa (city)]                                     814\n11 11 [Dire Dawa (city)]                                        27\n12 12 [Abroad]                                                   2\n\ndat <- dat %>% \n  mutate(\n    EXTERNAL = case_when(\n      LASTREGION %in% 200:900 | BIRTHREGIONET == 12 ~ T,\n      T ~ F\n    )\n  ) %>% \n  filter(!EXTERNAL)\n\n\n\nThe authors also exclude women whose PARTNERED status (i.e. sexual activity) cannot be determined, and women who indicate that they are either “infertile” in FERTPREF or “menopausal / hysterectomy” in TIMEMENSTRUATE. Women who are not at risk of pregnancy for these reasons cannot have unmet need, so they are removed from the sample.\n\n\ndat %>% count(PARTNERED)\n\n\n# A tibble: 3 x 2\n  PARTNERED     n\n* <lgl>     <int>\n1 FALSE      5513\n2 TRUE       7606\n3 NA            2\n\ndat %>% count(FERTPREF)\n\n\n# A tibble: 4 x 2\n                 FERTPREF     n\n*               <int+lbl> <int>\n1  1 [Have another child]  8983\n2  2 [No more children]    2702\n3  3 [Infertile]            225\n4 NA                       1211\n\ndat %>% count(TIMEMENSTRUATE)\n\n\n# A tibble: 8 x 2\n                TIMEMENSTRUATE     n\n*                    <int+lbl> <int>\n1  1 [Days]                     3113\n2  2 [Weeks]                    4111\n3  3 [Months]                   2955\n4  4 [Years]                    1276\n5  5 [Menopausal/hysterectomy]    85\n6  6 [Before last birth]        1231\n7  7 [Never menstruated]         317\n8 NA                              33\n\ndat <- dat %>% \n  mutate(\n    INFERTILE = case_when(\n      FERTPREF == 3 | TIMEMENSTRUATE == 5 ~ T,\n      T ~ F\n    )\n  ) %>% \n  filter(!is.na(PARTNERED), !INFERTILE)\n\n\n\nLastly, they exclude women with missing values on any of the remaining covariates.\n\n\ndat <- dat %>%\n  filter(\n    !if_any(c(UNMETYN, EDUCATTGEN, HCVISITY, BIRTHS, MCP), is.na),\n  )\n\n\n\nFrom 15,010 female respondents included in the original extract, this sub-sampling procedure leaves us with 12,630 remaing cases.\n\n\ndat %>% summarize(n = n())\n\n\n# A tibble: 1 x 1\n      n\n  <int>\n1 12630\n\nReference groups\nAs a final processing step, we’ll coerce each of our categorical variables (including MIGRANT_DIR) as factors. All but one of these is a labelled integer object where we’ll use the response with the lowest value as a reference group; because we created MIGRANT_DIR as a character object, we’ll specify its reference group manually:\n\n\ndat <- dat %>% \n  mutate(\n    across(\n      c(\n        MIGRANT_DIR, \n        RELGEN, \n        WEALTHQ, \n        EDUCATTGEN, \n        HCVISITY, \n        SUBNATIONAL, \n        PARTNERED,\n        YEAR\n      ), \n      ~as_factor(.) %>% droplevels()\n    ),\n    MIGRANT_DIR = fct_relevel(MIGRANT_DIR, \"nonmigrant - urban\")\n  )\n\n\n\nRegression Model\nFinally, we’re ready to build a regression model for UNMETYN using MIGRANT_DIR and the covariates discussed above!\nRecall that the function srvyr::survey_mean estimated that 15.1% of all women aged 15-49 in Ethiopia experience unmet need for family planning. This estimate used all of the women in our original sample prior to the sub-sampling procedure we just discussed. Now that we’ve created a sub-sample from the original dataset, let’s see how the population estimate has changed:\n\n\ndat %>% \n  as_survey_design(\n    id = EAID,\n    nest = T,\n    weight = FQWEIGHT,\n    strata = STRATA\n  ) %>% \n  summarize(pop_UNMETYN = survey_mean(UNMETYN, vartype = \"ci\", na.rm = T))\n\n\n# A tibble: 1 x 3\n  pop_UNMETYN pop_UNMETYN_low pop_UNMETYN_upp\n        <dbl>           <dbl>           <dbl>\n1       0.157           0.140           0.174\n\nNow that we’ve removed some cases (notably, all women who are infertile), the estimated population mean is close, but somewhat higher at 15.7%.\nGroene & Kristiansen build a multilevel logistic regression model for UNMETYN that breaks down this full-population estimate for each of the sub-groups represented by our independent variables. We’ll report the exponentiated coefficient estimates for each variable, which means that we’ll need to interpret each estimate as a change in the odds that a woman will experience UNMETYN relative to a woman in a reference group.\nWe’ll build the authors’ model m1 using the function survey::svyglm, which - like survey_mean - uses information about the sample design provided by as_survey_design to generate cluster-robust standard error estimates:\n\n\nm1 <- dat %>% \n  as_survey_design(\n    id = EAID,\n    nest = T,\n    weight = FQWEIGHT,\n    strata = STRATA\n  ) %>% \n  svyglm(\n    UNMETYN ~  \n      AGE + \n      MIGRANT_DIR +\n      RELGEN + \n      WEALTHQ +\n      EDUCATTGEN + \n      BIRTHS + \n      HCVISITY + \n      SUBNATIONAL +\n      PARTNERED + \n      YEAR,\n    design = .,\n    family = \"quasibinomial\",\n  ) \n\n\n\nWe tell svyglm to fit a logistic regression model with family = \"quasibinomial\".\nWhy “quasi” binomial? A simple binomial distribution yields the same point estimates and standard errors, but generates a warning because our use of sample weights produces a non-integer count of women with unmet need.\nTo simplify the output a bit, we’ll show a tidy table with just the term, point estimate, the 95% confidence interval, and the p-value (each rounded to two decimal places):\n\n\nm1 %>% \n  tidy(exp = T, conf.int = T) %>% \n  select(term, estimate, conf.low, conf.high, p.value) %>% \n  mutate(across(where(is.numeric), ~round(.x, 2))) \n\n\n# A tibble: 27 x 5\n   term                            estimate conf.low conf.high p.value\n   <chr>                              <dbl>    <dbl>     <dbl>   <dbl>\n 1 (Intercept)                         0.13    0.06       0.27    0   \n 2 AGE                                 0.96    0.94       0.97    0   \n 3 MIGRANT_DIRnonmigrant - rural       1.15    0.85       1.55    0.38\n 4 MIGRANT_DIRrural to urban           0.75    0.59       0.95    0.02\n 5 RELGENother                         0.88    0.580      1.33    0.54\n 6 RELGENchristian                     0.59    0.48       0.72    0   \n 7 WEALTHQLower quintile               0.99    0.79       1.25    0.96\n 8 WEALTHQMiddle quintile              1.06    0.84       1.33    0.63\n 9 WEALTHQHigher quintile              0.82    0.65       1.04    0.11\n10 WEALTHQHighest quintile             0.84    0.56       1.27    0.42\n11 EDUCATTGENPrimary/Middle school     1.05    0.88       1.26    0.59\n12 EDUCATTGENSecondary/post-prima…     0.76    0.55       1.05    0.1 \n13 EDUCATTGENTertiary/post-second…     1       0.68       1.49    0.98\n14 BIRTHS                              1.24    1.18       1.31    0   \n15 HCVISITYYes                         0.85    0.71       1.01    0.06\n16 SUBNATIONALAfar, Ethiopia           0.23    0.12       0.44    0   \n17 SUBNATIONALAmhara, Ethiopia         0.55    0.44       0.69    0   \n18 SUBNATIONALOromiya, Ethiopia        0.91    0.73       1.13    0.41\n19 SUBNATIONALSomali, Ethiopia         0.74    0.33       1.69    0.48\n20 SUBNATIONALBenishangul-Gumuz, …     0.55    0.290      1.08    0.08\n21 SUBNATIONALSNNP, Ethiopia           0.9     0.68       1.2     0.48\n22 SUBNATIONALGambella, Ethiopia       0.86    0.15       4.85    0.86\n23 SUBNATIONALHarari, Ethiopia         2.06    1.69       2.52    0   \n24 SUBNATIONALAddis Ababa, Ethiop…     0.98    0.69       1.4     0.93\n25 SUBNATIONALDire Dawa, Ethiopia      0.77    0.47       1.27    0.3 \n26 PARTNEREDTRUE                       6.69    4.76       9.41    0   \n27 YEAR2018                            0.88    0.74       1.05    0.15\n\nInterpretation\nControlling for all of the covariates we’ve discussed, the authors find that rural-urban internal migrants are less likely than both urban and rural non-migrants to experience unmet need for family planning!\nHow do we identify this finding in the model output? Notice that the estimated odds of experiencing UNMETYN for rural to urban migrants is 0.75, and that the associated 95% confidence interval ranges from 0.59 to 0.95: this represents the migrants’ odds compared to urban non-migrants. If the 95% confidence interval included the value 1.0, we would say that there’s more than a 5% chance that the migrants’ odds could be equal to the odds experienced by urban non-migrants. Because it does not include 1.0, we instead say that there’s a statistically significant difference between the two groups (at the 5% confidence threshold).\nBecause the authors selected urban non-migrants as a reference group, our model output shows the relationship between rural non-migrants and rural-urban migrants a bit less clearly. Although the point estimate for rural to urban migrants (0.75) is lower than the point estimate for nonmigrant - rural (1.15), their respective confidence intervals overlap. In order to see that they actually are statistically different, we’d need to run the model again with a different reference group.\nPredicted Probabilities\n\n\n\nWhile the output from our logistic regression model helps show the relative difference between groups, we’re not yet able to predict the absolute risk of UNMETYN for each group. Recall that, before building our model, we calculated that the average unmet need for all women in Ethiopia (excluding external migrants, infertile women, etc) was about 15.7%. We’ll now estimate the average unmet need experienced by all women in Ethiopia sorted into the three groups represented by MIGRANT_DIR.\nThe predict function allows us to make a prediction about each woman’s likelihood of experiencing unmet need according to the model m1. When we tell predict to return type = \"response\", it gives us the predicted probability that each woman should have UNMETYN.\n\n\ntibble(predicted = predict(m1, type = \"response\"))\n\n\n# A tibble: 12,630 x 1\n   predicted  \n   <svystat>  \n 1 0.072659141\n 2 0.338704987\n 3 0.033291206\n 4 0.376765517\n 5 0.025819303\n 6 0.528723102\n 7 0.009790659\n 8 0.088323159\n 9 0.044348246\n10 0.027093896\n# … with 12,620 more rows\n\nIf we wanted to compare each individual’s predicted probability to the value they actually do have for UNMETYN, we could attach our prediction back to our dataset. Remember that the original UNMETYN variable is binary, whereas the predictions are continuous probabilities that range from 0 to 1. Here, we hope to see that women whose predicted probability exceeds 0.50 have UNMETYN == 1, while those whose predicted probability is less than 0.50 have UNMETYN == 0:\n\n\ntibble(predicted = predict(m1, type = \"response\")) %>% \n  bind_cols(dat) %>% \n  select(predicted, UNMETYN) \n\n\n# A tibble: 12,630 x 2\n   predicted             UNMETYN\n   <svystat>           <int+lbl>\n 1 0.072659141 0 [No unmet need]\n 2 0.338704987 0 [No unmet need]\n 3 0.033291206 0 [No unmet need]\n 4 0.376765517 0 [No unmet need]\n 5 0.025819303 0 [No unmet need]\n 6 0.528723102 1 [Unmet need]   \n 7 0.009790659 0 [No unmet need]\n 8 0.088323159 0 [No unmet need]\n 9 0.044348246 0 [No unmet need]\n10 0.027093896 0 [No unmet need]\n# … with 12,620 more rows\n\nWe can also use predict to calculate predicted probabilities for hypothetical samples. For instance, the authors provide the predicted probabilities for a hypothetical sample of women that is completely identical to the real sample, except that they all share the same value for MIGRANT_DIR (all other variables are kept at their originial values). The mean predicted probability derived from this kind of hypothetical sample is known as a predictive margin.\nWhile the point estimates for each group in MIGRANT_DIR are easy to calculate with predict, the confidence intervals for those estimates are a bit harder to obtain. Here, we’ll use rsample::bootstraps to generate 100 replicates of our sample. This will allow us to rebuild our model 100 times:\n\n\nset.seed(1) # This ensures reproducible bootstrap sampling\n\nboots_dat <- dat %>% \n  rsample::bootstraps(100, EAID) %>% \n  transmute(\n    id = parse_number(id),\n    splits = map(splits, as_tibble),\n    model = map(splits, ~{\n      .x %>% \n        as_survey_design(\n          id = EAID,\n          nest = T,\n          weight = FQWEIGHT,\n          strata = STRATA\n        ) %>% \n        svyglm(\n          UNMETYN ~  \n            AGE + \n            MIGRANT_DIR +\n            RELGEN + \n            WEALTHQ +\n            EDUCATTGEN + \n            BIRTHS + \n            HCVISITY + \n            SUBNATIONAL +\n            PARTNERED + \n            YEAR,\n          design = .,\n          family = \"quasibinomial\",\n        ) \n    })\n  )\n\nboots_dat\n\n\n# A tibble: 100 x 3\n      id splits                      model   \n   <dbl> <list>                      <list>  \n 1     1 <tibble[,38] [12,630 × 38]> <svyglm>\n 2     2 <tibble[,38] [12,630 × 38]> <svyglm>\n 3     3 <tibble[,38] [12,630 × 38]> <svyglm>\n 4     4 <tibble[,38] [12,630 × 38]> <svyglm>\n 5     5 <tibble[,38] [12,630 × 38]> <svyglm>\n 6     6 <tibble[,38] [12,630 × 38]> <svyglm>\n 7     7 <tibble[,38] [12,630 × 38]> <svyglm>\n 8     8 <tibble[,38] [12,630 × 38]> <svyglm>\n 9     9 <tibble[,38] [12,630 × 38]> <svyglm>\n10    10 <tibble[,38] [12,630 × 38]> <svyglm>\n# … with 90 more rows\n\n\n\n\n© 2018 RStudio (CC0 1.0)\nrsample is included with library(tidymodels).\nNotice that each row of boots_dat contains a completely resampled version of dat contained in each row of the column splits. The column model contains the output from a model that’s uniquely fitted to the resampled data in splits.\nNext, we’ll use predict separately for each row in boots_dat. Because we generate three new rows each time - one prediction for each group in MIGRANT_DIR - the resulting data frame has 300 rows.\n\n\nboots_dat <- boots_dat %>% \n  rowwise() %>%\n  mutate(nested_predictions = list(map_df(levels(dat$MIGRANT_DIR), ~{\n    predict(\n      model, \n      type = \"response\", \n      newdata = splits %>% mutate(MIGRANT_DIR = .x)) %>% \n      tibble() %>% \n      bind_cols(splits) %>% \n      as_survey_design(\n        id = EAID,\n        nest = T,\n        weight = FQWEIGHT,\n        strata = STRATA\n      ) %>% \n      summarise(predicted = survey_mean(., vartype = \"ci\")) %>% \n      mutate(MIGRANT_DIR = .x) %>% \n      select(MIGRANT_DIR, predicted) \n  }))) %>% \n  unnest(nested_predictions) \n\nboots_dat\n\n\n# A tibble: 300 x 5\n      id splits                    model   MIGRANT_DIR       predicted\n   <dbl> <list>                    <list>  <chr>                 <dbl>\n 1     1 <tibble[,38] [12,630 × 3… <svygl… nonmigrant - urb…    0.151 \n 2     1 <tibble[,38] [12,630 × 3… <svygl… nonmigrant - rur…    0.163 \n 3     1 <tibble[,38] [12,630 × 3… <svygl… rural to urban       0.112 \n 4     2 <tibble[,38] [12,630 × 3… <svygl… nonmigrant - urb…    0.149 \n 5     2 <tibble[,38] [12,630 × 3… <svygl… nonmigrant - rur…    0.155 \n 6     2 <tibble[,38] [12,630 × 3… <svygl… rural to urban       0.0840\n 7     3 <tibble[,38] [12,630 × 3… <svygl… nonmigrant - urb…    0.142 \n 8     3 <tibble[,38] [12,630 × 3… <svygl… nonmigrant - rur…    0.155 \n 9     3 <tibble[,38] [12,630 × 3… <svygl… rural to urban       0.133 \n10     4 <tibble[,38] [12,630 × 3… <svygl… nonmigrant - urb…    0.133 \n# … with 290 more rows\n\nFinally, we’ll calculate:\nthe predicted probability for each group from the mean of 100 bootstrap predictions\nthe standard error of each group’s predicted probability from the standard deviation of 100 bootstrap predictions\nthe 95% confidence interval from the product of each group’s standard error and qnrom(0.975)\n\n\ngroup_predictions <- boots_dat %>% \n  group_by(MIGRANT_DIR) %>% \n  summarise(\n    mean = mean(predicted),\n    se = sd(predicted),\n    lower = mean - se*qnorm(0.975),\n    upper = mean + se*qnorm(0.975)\n  )\n\ngroup_predictions\n\n\n# A tibble: 3 x 5\n  MIGRANT_DIR         mean      se  lower upper\n* <chr>              <dbl>   <dbl>  <dbl> <dbl>\n1 nonmigrant - rural 0.161 0.00568 0.149  0.172\n2 nonmigrant - urban 0.143 0.0125  0.119  0.168\n3 rural to urban     0.117 0.0171  0.0831 0.150\n\nAnd here are those intervals plotted with geom_errorbarh and geom_point:\n\n\nggplot(group_predictions) +\n  geom_errorbarh(\n    color = \"#A2269C\", \n    aes(height = .2, xmin = lower, xmax = upper, y = MIGRANT_DIR)\n  ) + \n  geom_point(\n    color = \"#A2269C\", \n    aes(x = mean, y = MIGRANT_DIR)\n  ) +\n  geom_text(\n    nudge_y = 0.2,\n    aes(label = round(mean, 3), x = mean, y = MIGRANT_DIR)\n  ) + \n  scale_x_continuous(breaks = seq(.08, .18, by = .02)) +\n  theme_minimal() + \n  labs(\n    subtitle = \"95% Confidence Interval\",\n    title = \"Predicted Probability of Unmet Need for Family Planning\",\n    y = \"\",\n    x = \"\"\n  ) + \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 10)\n  ) \n\n\n\n\nAs you can see, the confidence intervals for each group in MIGRANT_DIR overlap quite a bit. However, the probability that a rural-urban internal migrant will experience unmet need for family planning seems to be generally lower than the other groups: we show a point-estimate of just 11.7% for migrants compared to 14.3% and 16.1% respectively for urban and rural non-migrants.\nTo learn more about the conceptual reasons why rural-urban internal migrants in Ethiopia might experience less unmet need for family planning compared to non-migrants, be sure to checkout out Groene & Kristiansen’s full article published at Populations, Space and Place! And, for more information about migration data available in other PMA samples, stay tuned for upcoming posts in this series.\n\n\n\nAbebe, Tatek. 2007. “Changing Livelihoods, Changing Childhoods: Patterns of Children’s Work in Rural Southern Ethiopia.” Children’s Geographies 5 (1-2): 77–93. https://doi.org/10.1080/14733280601108205.\n\n\nAnglewicz, Philip, Jamaica Corker, and Patrick Kayembe. 2017. “The Fertility of Internal Migrants to Kinshasa.” Genus 73 (1): 4. http://dx.doi.org/10.1186/s41118-017-0020-8.\n\n\nCourgeau, D. 1989. “Family Formation and Urbanization.” Population. English Selection 44 (1): 123–46. https://www.ncbi.nlm.nih.gov/pubmed/12157901.\n\n\nGroene, Emily A, and Devon Kristiansen. 2021. “Unmet Need for Family Planning After Internal Migration: Analysis of Ethiopia 2017–2018 PMA Survey Data.” Population, Space and Place 27 (1). https://onlinelibrary.wiley.com/doi/10.1002/psp.2376.\n\n\nKohler, Hans-Peter. 2000. “Social Interactions and Fluctuations in Birth Rates.” Population Studies 54 (2): 223–37. https://doi.org/10.1080/713779084.\n\n\nKulu, Hill. 2005. “Migration and Fertility: Competing Hypotheses Re-Examined.” European Journal of Population/Revue Européenne de Démographie 21 (1): 51–87. https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/content/pdf/10.1007/s10680-005-3581-8.pdf&casa_token=ofnVP7_3oy4AAAAA:nOmsfJTITgKjtiNcXj6u9GsVD9yHCkWsqmAtwTs6aG3vrQaL9DlhaOwcxFQMYweYNSt1mAtGNNonsZ-9.\n\n\nMcAuliffe, Marie, and Martin Ruhs. 2017. “World Migration Report 2018.” International Office of Migration, Geneva. https://publications.iom.int/fr/system/files/pdf/wmr_2018_en_chapter7.pdf.\n\n\nSchultz, T Paul. 1994. “Human Capital, Family Planning, and Their Effects on Population Growth.” The American Economic Review 84 (2): 255–60. http://www.jstor.org/stable/2117839.\n\n\nSevoyan, Arusyak, and Victor Agadjanian. 2013. “Contraception and Abortion in a Low-Fertility Setting: The Role of Seasonal Migration.” International Perspectives on Sexual and Reproductive Health 39 (3): 124–32. http://dx.doi.org/10.1363/3912413.\n\n\nSkiles, Martha Priedeman, Marc Cunningham, Andrew Inglis, Becky Wilkes, Ben Hatch, Ariella Bock, and Janine Barden-O’Fallon. 2015. “The Effect of Access to Contraceptive Services on Injectable Use and Demand for Family Planning in Malawi.” International Perspectives on Sexual and Reproductive Health 41 (1): 20–30. http://dx.doi.org/10.1363/4102015.\n\n\n\n\n",
    "preview": "posts/2021-04-01-et-internal-migration/images/pred_prob_h4.png",
    "last_modified": "2021-05-14T09:23:13-05:00",
    "input_file": {},
    "preview_width": 2683,
    "preview_height": 1200
  },
  {
    "path": "posts/2021-02-09-march-2021-data-release/",
    "title": "New SDP Data Available Spring 2021",
    "description": "Get details on new variables related to labor & delivery services, antenatal care, vaccinations, facility shipment schedules, and more!",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [
      "New Data",
      "Data Discovery"
    ],
    "contents": "\n\nContents\nLabor and Delivery\nAntenatal Care\nVaccinations\nFacility Stockouts & Expected Shipments\nNational Health Programs\n\n\n\n\nWe’re excited to announce the release of several new Service Delivery Point samples this month over at pma.ipums.org! As always, you’ll find the new data harmonized with older samples wherever the new surveys repeat questions you’ve seen before. The new samples also contain a big batch of new variables derived from questions that were posed for the very first time in PMA surveys, so we’d like to introduce a few of the highlights here.\n\n\n\nThe new samples included in this release represent data collected from:\nBurkina Faso 2020\nCongo (DR) 2019\nEthiopia 2019\nKenya 2019\nNigeria 2019\nUganda 2019\nLabor and Delivery\nWe’ve added a variable group in the Other Health Services topic offering more than 60 new variables related to Labor and Delivery. Many of these are currently available only for the Ethiopia 2019 sample, which piloted the new questions.\nFor example, you’ll find new variables about delivery personnel, including those showing whether a facility has a skilled birth attendant or a provider able to perform C-section delivery available 24 hours per day. Other variables describe the infrastructure available for labor and delivery, including the number of delivery rooms and beds, labor rooms, maternity waiting rooms, and newborn resuscitation tables. You’ll also find a number of variables describing the environment inside of a facility’s delivery rooms, including whether they are private, heated, and whether they have several specific delivery guidelines and protocols posted in the room.\nSeveral labor and delivery statistics are also provided for the month preceding the interview. These include the total number of facility deliveries, cesarian deliveries, stillbirths (both fresh and macerated), and neonatal deaths (reported separately for those occurring within 24 hours and one week). Other variables report whether certain services were provided any time within 3 months preceding the interview, including:\ninstrument / assisted delivery\ncaesarean section\nparenteral antibiotics for infections\nparenteral anticonvulsants for high blood pressure (Diazepam, Magnesium Sulfate, or other) or hypertension (Hydralazine, Nifedipine, Methyldopa, or other)\nparenteal / oral uterotonics for hemorrhage (Ergometrine, Misoprostol tablets, Oxytocin, or other)\ncortisteroids for fetal lung maturation\nmanual placenta removal\npartographs to monitor labor\nnewborn resuscitation\nblood transfusion\npostpartum implant insertion\npostpartum IUD insertion\npostpartum tubal ligation\nLastly, a number of variables indicate whether a particular service is typically provided at a facility. These include,\nneonatal intensive care, and whether it was available on the day of the interview\nreferrals for outgoing newborns and pregnant, laboring, and postpartum women; policies on referrals made from other facilities\nbreastfeeding assistance, newborn skin contact, and family planning discussions with new mothers prior to discharge\nAntenatal Care\nAs with Labor and Delivery, a new Antenatal Care group contains a number of variables that are currently available only for the Ethiopia 2019 sample, such as:\nwhether antenatal care was available on the day of the interview\nwhether trained staff are available to use ultrasound, and whether they were available on the day of the interview\nthe total number of rooms for antenatal care, and whether they are private\nwhether a number of different procedures are typically provided as a routine part of antenatal care (for example: blood pressure, weight, HIV testing, and several blood / urine tests)\nBeyond Ethiopia 2019, several of the other new samples included questions related to topics that are normally discussed with patients during an antenatal visit:\nimmediate and exclusive breastfeeding\nreturn to fertility after pregnancy\nhealthy timing and spacing of pregnancies\nfamily planning methods available to use while breastfeeding\nuse of the lactational amenorrhea method (LAM) for family planning, and plans for a transition to other methods\ninterest in a postpartum IUD or other long-acting family planning methods\nIn earlier survey rounds, PMA questionnaires included questions on whether these topics were discussed with a new mother after birth. The new samples differentiate between whether these topics were covered before the mother left the facility after delivery (e.g DISPPSPACE) or at a postnatal care visit later on (e.g. DISPNCSPACE).\nVaccinations\nThe Ethiopia 2019 sample also includes some of the first PMA variables related to vaccination. You’ll find indicators for whether a facility typically provides immunization services, whether those services were provided on the day of the interview, and whether a woman visiting the facility for her child’s immunization would be offered family planning services or counseling during the visit.\nThe availability of the following vaccines are also provided:\nBCG\nIPV and oral polio\nMeasles\nPCV\nPentavalent\nRota\nTetanus toxoid\nVitamin A\n\nEach of these may be “observed” by the interviewer, or else “reported” without observation.\nFacility Stockouts & Expected Shipments\nPMA has included variables related to contraceptive stockouts for many samples dating back to 2014. Four of the new samples dig deeper into the reasons why facilities experience stockouts, and also report the expected delivery time for methods that were out of stock on the day of the interview.\nFor each of the following methods, the expected delivery time is reported by two variables: a numeric value and a unit (e.g. days, weeks, months) that defines the value.\ndiaphragms\nDepo Provera\nemergency contraception\nfemale condoms\nfoam / jelly\nimplants\nIUDs\nmale condoms\npills\nSayana Press\nStandard Days/Cycle Beads\n\n\n\nIt’s recommended that users construct their own derived variables for expected delivery times using whichever unit of time they prefer. For example, notice that the values for expected delivery of Depo Provera DEPOVAL are reported either in weeks or months in DEPOUNIT:\n\n\ndat %>% count(DEPOVAL, DEPOUNIT)\n\n\n# A tibble: 18 x 3\n                      DEPOVAL                    DEPOUNIT     n\n                    <int+lbl>                   <int+lbl> <int>\n 1  0                          1 [Weeks]                      3\n 2  1                          1 [Weeks]                     55\n 3  1                          2 [Months]                    29\n 4  2                          1 [Weeks]                     16\n 5  2                          2 [Months]                     9\n 6  3                          1 [Weeks]                      8\n 7  3                          2 [Months]                     9\n 8  4                          1 [Weeks]                      2\n 9  4                          2 [Months]                     1\n10  6                          2 [Months]                     1\n11  7                          1 [Weeks]                      1\n12 12                          2 [Months]                     1\n13 14                          1 [Weeks]                      1\n14 30                          2 [Months]                     5\n15 60                          2 [Months]                     1\n16 99 [NIU (not in universe)] 97 [Don't know]                92\n17 99 [NIU (not in universe)] 98 [No response or missing]     1\n18 99 [NIU (not in universe)] 99 [NIU (not in universe)]   1492\n\nSuppose you wanted to create a derived variable called DEPO_WKS that simply reports the expected delivery time of Depo Provera in weeks. For any value that’s currently reported in months (DEPOUNIT == 2), you might decide to multiply the value in DEPOVAL by 4. Don’t forget to handle non-response values (e.g. 97, 98, 99) separately!\n\n\ndat %>% \n  mutate(DEPO_WKS = case_when(\n      DEPOUNIT == 2 ~ DEPOVAL * 4, \n      DEPOUNIT > 90 ~ NA_real_, \n      T ~ as.double(DEPOVAL) \n    )) %>% \n  count(DEPO_WKS)\n\n\n# A tibble: 15 x 2\n   DEPO_WKS     n\n *    <dbl> <int>\n 1        0     3\n 2        1    55\n 3        2    16\n 4        3     8\n 5        4    31\n 6        7     1\n 7        8     9\n 8       12     9\n 9       14     1\n10       16     1\n11       24     1\n12       48     1\n13      120     5\n14      240     1\n15       NA  1585\n\nDEPOVAL is an integer, but R coerces it into a double when you apply multiplication (what if multiplication creates non-integer values?). This is why we tell R to use the double class NA_real_ if DEPOUNIT > 90, and then to use as.double(DEPOVAL) if neither DEPOUNIT == 2 nor DEPOUNIT > 90. All of the values created by case_when have to be in the same class!\nFor facilities that were currently out of stock of a method that they normally provide, these new samples include variables explaining why the method was out of stock. With Depo Provera, for example, we can now see that a majority of stockouts across samples are caused by shipments that were ordered, but did not arrive:\n\n\ndat %>% count(OUTWHYDEPO)\n\n\n# A tibble: 8 x 2\n                                         OUTWHYDEPO     n\n*                                         <int+lbl> <int>\n1  1 [Did not place order for shipment]                23\n2  2 [Ordered but did not receive shipment]           154\n3  3 [Did not order right quantities]                   8\n4  4 [Ordered but did not receive right quantities]    17\n5  5 [Unexpected increase in consumption]               5\n6  9 [Other]                                           27\n7 97 [Don't know]                                       1\n8 99 [NIU (not in universe)]                         1492\n\nNational Health Programs\nLastly, we’ve created a new variable group related to participation in national health programs. While we may see more data in upcoming samples, these variables currently describe facility participation in programs provided by the National Hospital Insurance Fund (NHIF) for Kenya. Specifically, you’ll find an indicator for whether a facility in the Kenya 2019 sample provides family planning methods / services covered by NHIF and, if so, whether it provides each of these:\nEdu Afya\nLinda Mama (number of enrolled adolescents and adult women)\nStandard Program\nSuper Program\nother program\n\n\n\n",
    "preview": "posts/2021-02-09-march-2021-data-release/images/new_data_white.png",
    "last_modified": "2021-03-26T16:30:30-05:00",
    "input_file": {},
    "preview_width": 2555,
    "preview_height": 1437
  },
  {
    "path": "posts/2021-02-19-analyzing-the-individual-in-context/",
    "title": "Putting It All Together: Analyzing the Individual in Context",
    "description": "Analyzing women's contraceptive use while considering service delivery point and spatial contextual factors.",
    "author": [
      {
        "name": "Nina Brooks",
        "url": "http://www.ninarbrooks.com/"
      }
    ],
    "date": "2021-03-02",
    "categories": [
      "Individuals in Context",
      "Service Delivery Points",
      "Data Analysis",
      "survey",
      "dotwhisker"
    ],
    "contents": "\n\nContents\nSetup: Load Packages and Data\nRecoding covariates\nRegression Models\nIndividual factors: model with glm\nIndividual factors: model with svyglm\nAvailabillity: modeling with SDP variables\nAccessibility: modeling with external spatial variables\n\n\n\n\n\nThroughout our series on Individuals in Context, we’ve been looking at PMA Service Delivery Point (SDP) data as a resource for understanding the health services environment experienced by women surveyed in PMA Household and Female samples. We created summary variables that capture the SDPs that provide services within the same enumeration areas PMA uses to construct samples of individuals. We’ve also shown how to complement SDP data with additional information about women’s lived environment collected from external geospatial data sources.\nIn this final post, we’ll bring everything together and demonstrate the kind of analysis you might want to do with the contextual data we’ve collected in this series. Specifically, we’ll analyze women’s current contraceptive use, FPCURRUSE, taking into account:\nFPCURRUSE indicates whether a woman is currently using any method of family planning, or doing something to delay or avoid pregnancy.\nindividual factors about each woman collected in the Household and Female survey\navailability factors related to the supply of family planning services provided by SDPs in each woman’s enumeration area\naccessibility factors in each woman’s enumeration area - including measures of population density and transportation infrastructure - that we collected from external data sources\nThe availability of both detailed individual data on family planning and supply-side (service delivery) factors is one of the unique advantages of the PMA data.\nSetup: Load Packages and Data\nWe’ll load the packages tidyverse and ipumsr, as usual. Additionally, we’ll load tidymodels, which helps apply tidyverse principles to the models we’ll be building, and a few other packages we’ll discuss below.\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\nlibrary(tidymodels)\nlibrary(survey)\nlibrary(dotwhisker)\n\n\n\nWe’ll be using both of the Burkina Faso datasets we created in earlier posts in this series:\nbf_merged contains a handful of variables from each sampled woman merged with summary variables about the SDPs that serve her enumeration area (created in this post).\nint contains population and road density variables for each enumeration area (created in this post).\n\nRemember, to use the GPS data you must request access directly from our partners at pmadata.org. The version of int we’re using in this post is based on the real GPS locations but the GPS data itself is not included.\nAs a reminder, let’s take a glimpse at the variables we’ve currently got in each:\n\n\n\n\n\nglimpse(bf_merged)\n\n\nRows: 6,944\nColumns: 10\n$ EAID                <dbl+lbl> 7003, 7003, 7003, 7003, 7003, 7003,…\n$ SAMPLE              <int+lbl> 85405, 85405, 85405, 85405, 85405, …\n$ N_SDP               <int> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ NUM_METHODS_PROV    <int> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ NUM_METHODS_INSTOCK <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, …\n$ NUM_METHODS_OUT3MO  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MEAN_OUTDAY         <dbl> NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN,…\n$ PERSONID            <chr> \"0700300000019732017504\", \"070030000001…\n$ URBAN               <int+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FPCURRUSE           <int+lbl> 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,…\n\nThe variables N_SDP, NUM_METHODS_PROV, NUM_METHODS_INSTOCK, NUM_METHODS_OUT3MO, MEAN_OUTDAY, and URBAN all describe the the enumeration area (EAID) where a woman identified by PERSONID resides. The only other variable from the Household and Female questionnaire, itself, is FPCURRUSE. We’ll add more variables describing each woman in a moment.\n\n\n\n\n\nglimpse(int)\n\n\nRows: 83\nColumns: 7\n$ EAID        <dbl> 7003, 7006, 7009, 7016, 7026, 7042, 7048, 7056,…\n$ ROAD_LENGTH <dbl> 30.29857, 28.87695, 24.08644, 41.92500, 67.6741…\n$ PMACC       <chr> \"BF\", \"BF\", \"BF\", \"BF\", \"BF\", \"BF\", \"BF\", \"BF\",…\n$ PMAYEAR     <dbl> 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017,…\n$ REGION      <chr> \"5. centre-nord\", \"5. centre-nord\", \"8. est\", \"…\n$ DATUM       <chr> \"WGS84\", \"WGS84\", \"WGS84\", \"WGS84\", \"WGS84\", \"W…\n$ POP_DENSITY <dbl> 74.04364, 51.75554, 15.66303, 91.25882, 416.232…\n\nWe’ll be using ROAD_LENGTH and POP_DENSITY, but first we’ll need to merge int to bf_merged by matching up the EAID for each woman:\n\n\nbf_merged <- left_join(bf_merged, int, by = \"EAID\")\n\n\n\nLet’s now introduce some new variables obtained from each woman’s responses to the Household and Female questionnaire. We’ll merge a new data extract with the following variables collected from the Burkina Faso 2017 and 2018 surveys (female respondents only):\nAGE - Age (in years)\nMARSTAT - Marital status\nEDUCATTGEN - Highest level of school attended, general (4 categories)\nWEALTHQ - Wealth score quintile\nBIRTHEVENT - Number of birth events\n\nFor a refresher on accessing and importing PMA data in R, check out our post Import IPUMS PMA Data Into R.\nFollowing the practice we used when we made bf_merged, we’ll simply handle all of the different non-response codes in this new extract by recoding them as NA. Then, we’ll merge the extract to bf_merged by matching up each person by PERSONID:\n\n\nbf_merged <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00018.xml\",\n  data_file = \"data/pma_00018.dat.gz\") %>% \n  select(PERSONID, AGE, MARSTAT, EDUCATTGEN, WEALTHQ, BIRTHEVENT, STRATA) %>% \n  mutate(across(everything(), ~lbl_na_if(\n    .x,\n    ~.lbl %in% c(\n      \"Not interviewed (female questionnaire)\",\n      \"Not interviewed (household questionnaire)\",\n      \"Don't know\",\n      \"No response or missing\",\n      \"NIU (not in universe)\"\n    )\n  ))) %>% \n  right_join(bf_merged, by = \"PERSONID\")\n\n\n\nRecoding covariates\nAll five of the new variables we’ve introduced are loaded into R as members of both the integer and the haven_labelled class of objects. But really, only AGE and BIRTHEVENT should be treated like continuous measures in our analysis. For MARSTAT, EDUCATTGEN, and WEALTHQ, the integer values associated with each response are arbitrary; we’re much more interested in the labels associated with these numeric values because each of these three variables reflects a categorical measurement.\n\n\nbf_merged %>% \n  select(MARSTAT, EDUCATTGEN, WEALTHQ) %>% \n  map(class)\n\n\n$MARSTAT\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"integer\"       \n\n$EDUCATTGEN\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"integer\"       \n\n$WEALTHQ\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"integer\"       \n\nAs you might know, the normal way to handle categorical variables in a regression model is to create a binary dummy variable associated with each response, and R normally performs this task automatically when it encounters a variable that’s a member of the factor class.\nIf we want, we can simply coerce these variables as factors. When we do this and then use the factor in a regression model, R will select the lowest numbered response as a “reference group” and create binary dummy variables for the other responses. This makes sense with WEALTHQ, where we’d interpret the coefficient for each wealth quintile as an effect relative to being in the lowest quintile.\n\n\nbf_merged %>% count(WEALTHQ)\n\n\n# A tibble: 6 x 2\n                WEALTHQ     n\n*             <int+lbl> <int>\n1  1 [Lowest quintile]   1200\n2  2 [Lower quintile]    1031\n3  3 [Middle quintile]    984\n4  4 [Higher quintile]   1253\n5  5 [Highest quintile]  2474\n6 NA                        2\n\nbf_merged <- bf_merged %>% \n  mutate(WEALTHQ = as_factor(WEALTHQ)) \n\nbf_merged %>% count(WEALTHQ)\n\n\n# A tibble: 6 x 2\n  WEALTHQ              n\n* <fct>            <int>\n1 Lowest quintile   1200\n2 Lower quintile    1031\n3 Middle quintile    984\n4 Higher quintile   1253\n5 Highest quintile  2474\n6 <NA>                 2\n\nAlternatively, we might decide to make our own binary dummy variables. This makes sense when we might want to collapse several responses into one larger category, as with MARSTAT: here, for the purpose of analyzing FPCURRUSE, we probably only care about whether the woman is partnered (the reasons why she might not be partnered are less meaningful).\n\n\nbf_merged %>% count(MARSTAT)\n\n\n# A tibble: 5 x 2\n                             MARSTAT     n\n*                          <int+lbl> <int>\n1 10 [Never married]                  1876\n2 21 [Currently married]              4307\n3 22 [Currently living with partner]   410\n4 31 [Divorced or separated]           163\n5 32 [Widow or widower]                188\n\nbf_merged <- bf_merged %>%\n  mutate(MARSTAT = lbl_relabel(\n      MARSTAT,\n      lbl(1, \"partnered\") ~ .val %in% 21:22,\n      lbl(0, \"unpartnered\") ~ .val %in% c(10, 31, 32)\n  )) \n\nbf_merged %>% count(MARSTAT)\n\n\n# A tibble: 2 x 2\n          MARSTAT     n\n*       <dbl+lbl> <int>\n1 0 [unpartnered]  2227\n2 1 [partnered]    4717\n\nAnother reason to consider recoding categorical variables: what if one response option dominates a huge proportion of the responses in your data? Is it worth sacrificing additional degrees of freedom to accommodate dummy variables that could otherwise be merged together? This is the case with EDUCATTGEN, where over half of the responses are “never attended.” We’ll create a single, simplified binary variable where the responses are “some schooling” or “no schooling.”\n\n\nbf_merged %>% count(EDUCATTGEN)\n\n\n# A tibble: 5 x 2\n                    EDUCATTGEN     n\n*                    <int+lbl> <int>\n1  1 [Never attended]           3605\n2  2 [Primary/Middle school]    1212\n3  3 [Secondary/post-primary]   1893\n4  4 [Tertiary/post-secondary]   231\n5 NA                               3\n\nbf_merged <- bf_merged %>% \n  mutate(EDUCATTGEN = lbl_relabel(\n      EDUCATTGEN,\n      lbl(1, \"some schooling\") ~ .val %in% 2:4,\n      lbl(0, \"no school\") ~ .val == 1\n  )) \n\nbf_merged %>% count(EDUCATTGEN)\n\n\n# A tibble: 3 x 2\n           EDUCATTGEN     n\n*           <dbl+lbl> <int>\n1  0 [no school]       3605\n2  1 [some schooling]  3336\n3 NA                      3\n\nThe last thing we’ll do here is coerce SAMPLE as a factor so that we can control for arbitrary differences caused by selection into the two samples (recall that our dataset contains two samples from Burkina Faso 2017 and 2018). Because the women in each SAMPLE were surveyed in two different years, this essentially operates like a survey-year fixed effect.\n\n\nbf_merged <- bf_merged %>% \n  mutate(SAMPLE = as.factor(SAMPLE)) \n\nbf_merged %>% count(SAMPLE)\n\n\n# A tibble: 2 x 2\n  SAMPLE     n\n* <fct>  <int>\n1 85405   3556\n2 85408   3388\n\nRegression Models\nWe’re now ready to examine the relative effects individual factors on FPCURRUSE compared to the availability and accessibility of family planning services in each woman’s enumeration area. Let’s begin with a simple model containing the factors we added to the dataset above.\nIndividual factors: model with glm\nMost R users probably use the generalized linear model function glm for this purpose. To keep our demonstration as simple as possible, we’ll fit a model using the Ordinary Least-Squares (OLS) method that glm adopts by default. We’ll use the tidymodels function broom::tidy to clean up the output for our model’s coefficient estimates.\n\nRecall that FPCURRUSE is a binary response (“yes” or “no”), so you might consider fitting a logit model by adding the argument family = 'binomial' to glm().\n\n\nm1 <- glm(\n  FPCURRUSE ~\n    AGE + \n    MARSTAT + \n    EDUCATTGEN + \n    BIRTHEVENT  + \n    WEALTHQ + \n    SAMPLE,\n  data = bf_merged\n)\n\ntidy(m1)\n\n\n# A tibble: 10 x 5\n   term                    estimate std.error statistic  p.value\n   <chr>                      <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)              0.0421   0.0260       1.62  1.06e- 1\n 2 AGE                     -0.00280  0.000948    -2.95  3.17e- 3\n 3 MARSTAT                  0.201    0.0145      13.9   5.21e-43\n 4 EDUCATTGEN               0.159    0.0143      11.2   9.42e-29\n 5 BIRTHEVENT               0.0289   0.00390      7.40  1.48e-13\n 6 WEALTHQLower quintile    0.0319   0.0204       1.56  1.18e- 1\n 7 WEALTHQMiddle quintile   0.0222   0.0208       1.07  2.85e- 1\n 8 WEALTHQHigher quintile   0.0884   0.0198       4.46  8.25e- 6\n 9 WEALTHQHighest quintile  0.166    0.0191       8.67  5.24e-18\n10 SAMPLE85408              0.00515  0.0114       0.453 6.51e- 1\n\nBecause the outcome (FPCURRUSE) is binary, this linear regression is a linear probability model and the coefficients on each term should be interpreted as a percentage point change in the probability of current family planning use.\nFor each of the binary dummy variables we created, the coefficient estimate shows how much the probability FPCURRUSE == \"yes\" increases if the value of the dummy variable is 1. For example, in MARSTAT the value 1 represents “partnered” women, while the value 0 represented “unpartnered” women. The coefficient on MARSTAT is 0.201, meaning our model predicts that being partnered is associated with an increase in the expected probability of family planning use by 0.201.\nIs this a meaningful difference? Consider that the mean of FPCURRUSE is 0.34: this is the probability you might use to guess a woman’s likelihood for using family planning if we didn’t have access to any other variables. Relative to that, an increase of 0.201 is pretty substantive.\nWhat about the other coefficients? We also see a large increase in the probability of family planning use for women who have “some schooling,” and a smaller increase for those who have more children.\nNotice what happened with WEALTHQ, the variable we coerced as a factor above. As expected, R created a binary dummy variable from each response option except the reference group, which is the “lowest quintile.” It’s important to remember that each of these dummy variables represents the effect a being in a particular quintile relative to the “lowest quintile.” These results show that family planning use increases with wealth, which is expected (although the effects don’t become large or statistically significant until we get to the “high” and “highest” income quintiles).\nIndividual factors: model with svyglm\nThere is one problem with the model we created above: as we’ve discussed, PMA samples households randomly within the same enumeration area, and it’s likely that households located together will share many common features. This violates one of the basic assumptions of OLS regression, where we expect modeling errors to be uncorrelated (Cameron and Miller 2015). To address this, we’ll need to use a model that allows us to specify the complexities of PMA survey design. A common approach uses the survey package developed by Thomas Lumley.1\n\nWe’ll use the package svyglm to specify PMA survey design whenever we create analytic models on the PMA Data Analysis Hub!\nLumley’s modeling function survey::svyglm is similar to glm, except that it takes a special design argument where glm takes a data argument. We use the function survey::svydesign to specify the data, the cluster ids from EAID, and the sampling strata STRATA (if we were using the sample weights from FQWEIGHT, we could do that here, too):\n\n\nm2 <- svyglm(\n  FPCURRUSE ~\n    AGE + \n    MARSTAT + \n    EDUCATTGEN + \n    BIRTHEVENT  + \n    WEALTHQ + \n    SAMPLE,\n  design = svydesign(\n    ids = ~EAID,\n    strata = ~STRATA,\n    data = bf_merged\n  )\n)\n\ntidy(m2)\n\n\n# A tibble: 10 x 5\n   term                    estimate std.error statistic  p.value\n   <chr>                      <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)              0.0421    0.0366      1.15  2.54e- 1\n 2 AGE                     -0.00280   0.00114    -2.45  1.65e- 2\n 3 MARSTAT                  0.201     0.0157     12.8   3.19e-20\n 4 EDUCATTGEN               0.159     0.0167      9.53  2.13e-14\n 5 BIRTHEVENT               0.0289    0.00405     7.13  6.41e-10\n 6 WEALTHQLower quintile    0.0319    0.0244      1.31  1.95e- 1\n 7 WEALTHQMiddle quintile   0.0222    0.0282      0.788 4.33e- 1\n 8 WEALTHQHigher quintile   0.0884    0.0302      2.92  4.62e- 3\n 9 WEALTHQHighest quintile  0.166     0.0283      5.86  1.30e- 7\n10 SAMPLE85408              0.00515   0.0145      0.356 7.23e- 1\n\nTo see how this impacts our model estimates, let’s visualize the confidence interval for each coefficient with dotwhisker::dwplot. We’ll use the same function a few times here, and we’ll want to repeat the same visual elements each time, so we’ll just wrap everything together in a custom function we’re calling pma_dwplot():\n\n\npma_dwplot <- function(...){\n  dwplot(\n    bind_rows(...),\n    dodge_size = 0.8,\n    vline = geom_vline(xintercept = 0, colour = \"grey60\", linetype = 2)) +\n    scale_color_viridis_d(option = \"plasma\", end = .7) +\n    theme_minimal()\n}\n\npma_dwplot(\n  tidy(m1) %>% mutate(model = \"glm\"),\n  tidy(m2) %>% mutate(model = \"svyglm\")\n)\n\n\n\n\nWe can see from this plot that the confidence intervals obtained from svyglm are wider than those we got from glm, but the point estimates for each coefficient are unchanged. We also added a dashed line at 0 to make it really easy to see when coefficients are statistically insignificant at the 5% level (if so, the “whiskers” of a 95% confidence interval will cross 0).\nAvailabillity: modeling with SDP variables\nWhile these individual factors are important, we should also expect the availability and accessibility of family planning services to partially determine their use (Bongaarts 2011). Back in an earlier post, we observed that the women in our sample appeared to be 5% more likely to use family planning if they lived in an enumeration area where no SDPs reported a recent contraceptive stockout, compared to women living in areas where at least one SDP did experience a recent stockout. Now we’ll see if that difference is statistically significant, controlling for other factors.\n\nFor our purposes, a “recent stockout” is a stockout of any contraceptive method normally provided by an SDP if the stockout occurred within 3 months prior to the survey.\nFirst, we’ll create a binary variable STOCKOUT indicating whether each woman lives in an enumeration area where at least one SDP reported a recent stockout:\n\n\nbf_merged <- bf_merged %>%\n  mutate(STOCKOUT = case_when(\n    NUM_METHODS_OUT3MO > 0 ~ 1,\n    NUM_METHODS_OUT3MO == 0 ~ 0\n  ))\n\nbf_merged %>% count(STOCKOUT)\n\n\n# A tibble: 3 x 2\n  STOCKOUT     n\n*    <dbl> <int>\n1        0  4561\n2        1  1725\n3       NA   658\n\nNext, we’ll add STOCKOUT to our previous model, along with NUM_METHODS_PROV (the number of methods available from at least one SDP serving the woman’s enumeration area) and N_SDP (the number of sampled SDPs serving the woman’s enumeration area).\n\n\nm3 <- svyglm(\n  FPCURRUSE ~\n    AGE + \n    MARSTAT + \n    EDUCATTGEN + \n    BIRTHEVENT  + \n    WEALTHQ + \n    SAMPLE + \n    STOCKOUT + \n    NUM_METHODS_PROV + \n    N_SDP,\n  design = svydesign(\n    ids = ~EAID,\n    strata = ~STRATA,\n    data = bf_merged\n  )\n)\n\ntidy(m3)\n\n\n# A tibble: 13 x 5\n   term                     estimate std.error statistic  p.value\n   <chr>                       <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)              0.113      0.105      1.08   2.85e- 1\n 2 AGE                     -0.00249    0.00114   -2.19   3.23e- 2\n 3 MARSTAT                  0.201      0.0161    12.5    1.94e-19\n 4 EDUCATTGEN               0.168      0.0169     9.93   6.35e-15\n 5 BIRTHEVENT               0.0272     0.00414    6.57   7.77e- 9\n 6 WEALTHQLower quintile    0.0380     0.0244     1.56   1.24e- 1\n 7 WEALTHQMiddle quintile   0.0211     0.0287     0.736  4.64e- 1\n 8 WEALTHQHigher quintile   0.0879     0.0300     2.93   4.58e- 3\n 9 WEALTHQHighest quintile  0.162      0.0270     6.02   7.55e- 8\n10 SAMPLE85408              0.000951   0.0175     0.0542 9.57e- 1\n11 STOCKOUT                -0.0444     0.0206    -2.16   3.44e- 2\n12 NUM_METHODS_PROV        -0.00199    0.0104    -0.192  8.48e- 1\n13 N_SDP                   -0.0165     0.0120    -1.38   1.72e- 1\n\nIndeed, women living in an enumeration area where we’re aware of recent stockouts are less likely to be currently using family planning! The effect isn’t quite as large as some of the individual level factors we’ve examined, but it is statistically significant (p < 0.05).\nDoes the introduction of SDP variables change our estimates for the individual factors we examined previously? A new dwplot seems to show little difference:\n\n\npma_dwplot(\n  tidy(m2) %>% mutate(model = \"Individual-only\"),\n  tidy(m3) %>% mutate(model = \"SDP + Individual\")\n)\n\n\n\n\nAccessibility: modeling with external spatial variables\nAvailability of family planning methods (or lack thereof) is not the same as accessibility. The variables we created in our last post using external geospatial data allow us to explore some factors related to accessibility, which is what we’ll add now. We’ll complement these external variables with URBAN, indicating whether the woman lives in an urban area.\n\n\nm4 <- svyglm(\n  FPCURRUSE ~\n    AGE + \n    MARSTAT + \n    EDUCATTGEN + \n    BIRTHEVENT  + \n    WEALTHQ + \n    SAMPLE + \n    STOCKOUT + \n    NUM_METHODS_PROV + \n    N_SDP + \n    POP_DENSITY + \n    ROAD_LENGTH + \n    URBAN,\n  design = svydesign(\n    ids = ~EAID,\n    strata = ~STRATA,\n    data = bf_merged\n  )\n)\n\ntidy(m4) \n\n\n# A tibble: 16 x 5\n   term                       estimate  std.error statistic  p.value\n   <chr>                         <dbl>      <dbl>     <dbl>    <dbl>\n 1 (Intercept)              0.0833     0.117         0.712  4.79e- 1\n 2 AGE                     -0.00303    0.00112      -2.71   8.61e- 3\n 3 MARSTAT                  0.210      0.0157       13.4    1.97e-20\n 4 EDUCATTGEN               0.162      0.0174        9.30   1.28e-13\n 5 BIRTHEVENT               0.0284     0.00417       6.82   3.41e- 9\n 6 WEALTHQLower quintile    0.0343     0.0249        1.38   1.72e- 1\n 7 WEALTHQMiddle quintile   0.0124     0.0291        0.426  6.72e- 1\n 8 WEALTHQHigher quintile   0.0405     0.0312        1.30   1.99e- 1\n 9 WEALTHQHighest quintile  0.0734     0.0384        1.91   6.07e- 2\n10 SAMPLE85408              0.00133    0.0179        0.0744 9.41e- 1\n11 STOCKOUT                -0.0483     0.0211       -2.29   2.53e- 2\n12 NUM_METHODS_PROV        -0.00225    0.0104       -0.216  8.30e- 1\n13 N_SDP                   -0.0158     0.0124       -1.27   2.09e- 1\n14 POP_DENSITY             -0.00000144 0.00000522   -0.276  7.83e- 1\n15 ROAD_LENGTH              0.00110    0.00144       0.760  4.50e- 1\n16 URBAN                    0.0773     0.0370        2.09   4.03e- 2\n\npma_dwplot(\n  tidy(m2) %>% mutate(model = \"Individual-only\"),\n  tidy(m3) %>% mutate(model = \"SDP + Individual\"),\n  tidy(m4) %>% mutate(model = \"All\")\n) \n\n\n\n\nThis figure with all three models reveals that marital status, education, number of births, and living in an enumeration area that faced recent stockouts are all significantly associated with current family planning use. However, it’s pretty difficult to compare the effects across all the variables. The coefficients on age, population density, and road length are particularly hard to examine and compare. dotwhisker includes a very handy function that re-scales continuous variables on the right-hand side of your regression model to make them more comparable to binary predictors. Specifically, dotwhisker::by_2sd() re-scales continuous input variables by 2 standard deviations following Gelman (2008).2\nWhile we’re adding some last touches to make the plot more readable, we’ll also provide a title, clearer variable names on the Y axis, a tighter scale on the X axis, and a caption at the bottom.\n\n\nlist(\n  tidy(m2) %>% mutate(model = \"Individual-only\"),\n  tidy(m3) %>% mutate(model = \"SDP + Individual\"),\n  tidy(m4) %>% mutate(model = \"All\")\n) %>% \n  map(~by_2sd(.x, bf_merged)) %>% \n  bind_rows() %>% \n  relabel_predictors(\n    c(\n      AGE = \"Age\",  \n      MARSTAT = \"Married\", \n      EDUCATTGEN = \"Some Schooling\", \n      BIRTHEVENT = \"No. of Children\",\n      `WEALTHQLower quintile` = \"Lower Wealth Quintile\", \n      `WEALTHQMiddle quintile` = \"Middle Wealth Quintile\",\n      `WEALTHQHigher quintile` = \"Higher Wealth Quintile\", \n      `WEALTHQHighest quintile` = \"Highest Wealth Quintile\", \n      STOCKOUT = \"Recent Stockout\", \n      NUM_METHODS_PROV = \"No. of FP Methods\",\n      N_SDP = \"No. of SDP Providers\",\n      POP_DENSITY = \"Population Density (w/i 10 km)\",\n      ROAD_LENGTH = \"Road length (w/i 10 km)\",\n      URBAN = \"Lives in Urban EA\",\n      SAMPLE85408 = \"2018 Sample\"\n    )\n  ) %>% \n  dwplot(\n    dodge_size = 0.8,\n    vline = geom_vline(xintercept = 0, colour = \"grey60\", linetype = 2)\n  ) + \n  scale_color_viridis_d(option = \"plasma\", end = .7) +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient\",\n    color = NULL,\n    title = \"Marital Status Is the Strongest Predictor of Family Planning Use\",\n    subtitle = \"Impact of Individual and Contextual Factors on Family Planning Use\",\n    caption = \"Source: IPUMS PMA (Burkina Faso 2017-2018), DIVA-GIS (road length), and WorldPop (population density)\"\n  ) +\n  scale_x_continuous(limits = c(-0.1, 0.3)) + # to make space for the legend\n  theme(\n    legend.position = c(0.8, 0.2),\n    title = element_text(size = 8),\n    legend.text = element_text(size = 8),\n    plot.caption = element_text(hjust = 0), #left align the caption\n    legend.background = element_rect(colour = \"grey80\")\n  )\n\n\n\n\nNow that we’ve re-scaled the continuous input variables by two standard deviations, we can much more easily see the relationship between age and family planning use. Across all models a one-year increase in a woman’s age is associated with a five percentage point lower expected probability of using family planning. This effect is statistically significant at the 5% level in all three models as well.\nThe relationships we observed with marital status, education, and number of children are quite stable across all the models – even as we added variables representing the service environment and broader context of contraceptive availability the coefficients did not meaningfully change.\nThis is in pretty striking contrast to what happens to the wealth quintile variables. When we included only woman and SDP characteristics, being in either the higher and highest wealth quintiles was associated with large and statistically significant increases in the probability of using family planning. But as we added geospatial variables and the URBAN variable in particular, the coefficients become smaller and the confidence intervals become wider. This indicates that there was likely omitted variable bias because wealth is correlated with living in an urban area but when we excluded URBAN the wealth quintile variables were capturing some of this relationship with family planning use.\nEven though this analysis was relatively simple, it was quite informative about different drivers of family planning use. You could easily extend this analysis to include other factors that influence family planning use, incorporate fixed or random effects, or take advantage of the multiple years of survey data!\nAs always, let us know if you have any questions on this post or if you’re working any fertility related analyses and you have a question that we can help address with the blog!\n\n\n\nBongaarts, John. 2011. “Can Family Planning Programs Reduce High Desired Family Size in Sub-Saharan Africa?” International Perspectives on Sexual and Reproductive Health 37 (4): 209–16. https://doi.org/10.1363/3720911.\n\n\nCameron, A. Colin, and Douglas L. Miller. 2015. “A Practitioner’s Guide to Cluster-Robust Inference.” Journal of Human Resources 50 (2): 317–72.\n\n\nGelman, Andrew. 2008. “Scaling Regression Inputs by Dividing by Two Standard Deviations.” Statistics in Medicine 27 (15): 2865–73. https://doi.org/10.1002/sim.3107.\n\n\nLumley, Thomas. 2011. Complex Surveys: A Guide to Analysis Using R. Wiley Series in Survey Methodology. John Wiley & Sons.\n\n\nWe highly recommend Lumley’s (2011) book, Complex Surveys: A Guide to Analysis Using R.↩︎\nWe recommend checking out the full paper, but the short explanation is that with binary predictors you are comparing a value of 0 to a value of 1 when interpreting coefficients. A 1-unit change in a binary predictor is equivalent to a 2 standard deviation change because the standard deviation of a binary variable with equal probabilities is 0.5.↩︎\n",
    "preview": "posts/2021-02-19-analyzing-the-individual-in-context/images/results.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 1950,
    "preview_height": 1199
  },
  {
    "path": "posts/2021-02-04-merging-external-spatial-data/",
    "title": "Merging external spatial data",
    "description": "How to integrate external spatial data with PMA data.",
    "author": [
      {
        "name": "Nina Brooks",
        "url": "http://www.ninarbrooks.com/"
      }
    ],
    "date": "2021-02-15",
    "categories": [
      "Individuals in Context",
      "Service Delivery Points",
      "Data Manipulation",
      "join",
      "sf",
      "raster",
      "Spatial"
    ],
    "contents": "\n\nContents\nData\nSetup: Load packages and data\nPopulation Density: working with raster data\nRoad Networks: Working with vector data\n\n\n\n\nOur last post showed how to read, merge, and map the PMA GPS data - and how mapping can shed light on interesting spatial variation. A big advantage of the PMA GPS data is that you can also merge in other sources of spatial data, which opens up enormous opportunities for analyzing how contextual and environmental factors affect topics of interest in the PMA data. In this post, we’ll show how to merge in two different types of spatial data and construct variables of interest.\nData\nWe’ll be using toy PMA GPS data for this post. To use real PMA GPS data you must request access directly from our partners at pmadata.org. The toy data we’ll use here contains randomly sampled locations within Burkina Faso which have no actual relationship to the EAs in the PMA data. This means none of the interpretations of spatial patterns will hold, but all the code will run.\nWe will also be introducing two different spatial datasets that represent different kinds of spatial data. The first is population density data from WorldPop.1 If you want to download the data from the WorldPop site, we’re using the “Unconstrained individual countries 2000-2020 (1 km resolution)” data from 2017 for Burkina Faso. This is raster data, which means the data are stored as a grid of values which are rendered on a map as pixels. You can think of this as a matrix that is spatially referenced – that is each pixel represents a specific area of land on the Earth. Lots of spatial data are stored as rasters including climate data (e.g., temperature and rainfall), elevation, and satellite images. Note that the raster data is saved as a .tiff (which is a common way of storing raster data). The resolution of the raster maps to the area that each pixel represents in the real world. The population density is 1 km resolution, which means that each pixel represents a 1 km by 1 km square on the ground. The figure below shows the impact of different spatial resolutions for the same raster data.2\n\n\n\nFigure 1: Source: NEON\n\n\n\n\nThere are tons of resources on earth data science in R. We recommend the resources by Earth Lab and NEON by NSF. This post is an excellent introduction to working with rasters in R!\nPopulation density is also conceptually important to the SDP data on contraceptive supply that we’ve been examining through this series of posts. Population density may provide a more nuanced characterization of urbanization than the URBAN variable. Additionally, density may be correlated with longer wait times at clinics, which may also impact contraceptive use at the individual level.\nThe second spatial dataset we’ll introduce is data on road networks in Burkina Faso from the Digital Chart of the World and made publicly available by DIVA-GIS, an excellent source for publicly available spatial datasets. Road networks serve as a proxy for accessibility to health clinics – an important component of the contraceptive service environment – that may be more nuanced than the binary urban/rural distinction. To download the road data, go to DIVA-GIS Data and select Burkina Faso from the Country dropdown and Roads from the Subject dropdown. The road data is called vector data and is stored in a shapefile (.shp). Vector data is used to represent real world features and are three basic types: points, lines, and polygons. The road data we’re using in this post is an example of vector line data.\n\nRemember the administrative boundaries we used in the previous post were polygons and the GPS points for the PMA enumeration areas were points. Both are vector data!\nSetup: Load packages and data\nWe’ll be using many of the packages from the last few posts, as well as a new package for specifically working with raster data – appropriately called raster – and one called units, which enables easy conversion between objects of different units. Make sure to install the raster and units packages first and then load everything we’ll be using today:\n\n\nlibrary(sf) # primary spatial package\nlibrary(raster) # for working with raster data\nlibrary(viridis) # for color palettes\nlibrary(units) # to easily convert between units\nlibrary(tidyverse)\n\n\n\nLet’s start by reading in the raster using raster::raster() and check out the meta-data.\n\n\npop_density <- raster(\"bfa_pd_2017_1km.tif\")\npop_density\n\n\nclass      : RasterLayer \ndimensions : 682, 951, 648582  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -5.517917, 2.407083, 9.407917, 15.09125  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : /Users/Matt/R/pma-data-hub/_posts/2021-02-04-merging-external-spatial-data/bfa_pd_2017_1km.tif \nnames      : bfa_pd_2017_1km \nvalues     : 0.281988, 8820.016  (min, max)\n\nBecause rasters are essentially just matrices, you can think of the dimensions in the same way. At a spatial resolution of 1 km, this raster covers all of Burkina Faso with 648,582 cells. The resolution describes the size of the cells (the length of one side of each square cell). You may be wondering why this is showing up as 0.00833 when the data has a spatial scale of 1 km by 1 km. This is because the units that the resolution is reported in depend on the coordinate reference system of the data. More on this in a moment.\nThe extent (or spatial extent) refers to the geographic area that the raster covers. The values are in the same coordinate reference system as the raster. The coordinate reference system or crs is the next piece of meta-data we have. “A coordinate reference system (CRS) is a coordinate-based local, regional or global system used to locate geographical entities.”3 The crs for this raster is +proj=longlat +datum=WGS84 +no_defs. The crs contains several pieces of information including the datum (WGS84) and the projection.4 The appropriate CRS to use for any given spatial task depends on what part of the world the data represent and what kind of spatial operations you’ll be performing. It’s really important to know what crs your data are in and make sure that all your spatial data are in the same  crs if you use more than one kind. Otherwise, they won’t line up on a map and any spatial analysis or processing you do will be incorrect.\nThe projection of this raster data is described as longlat, which actually is not a projection. A projection refers to how the Earth’s surface is flattened so it can be represented as a 2-dimensional raster grid. These data use a geographic coordinate system, simply the raw latitude and longitude coordinates, rather than a projected coordinate system, which would transform the coordinates into a 2-dimensional plane. Latitude and longitude locate positions on the Earth using angles, so the spacing of each line of latitude as you move north or south along the Earth is not uniform. The units of this reference system are in degrees (of latitude and longitude), so the 0.00833 resolution we saw above is reporting the spatial resolution in degrees, rather than meters or kilometers. This crs is not ideal for measuring distances because the distance covered by a single degree of latitude or longitude varies greatly across the Earth’s surface. This also means that the stated 1 km resolution is only nominal. At the equator, 0.00833 degrees is approximately equal to 1 km, but this distance, and the ground area represented by each pixel, will vary. Fortunately, Burkina Faso is relatively close to the equator, so the pixels will be quite close to 1 km by 1 km.\nThe last piece of meta-data to look at are the values – this is reporting the minimum and maximum values across all of the cells. Because these are population density data, it can be interpreted as the number of people in each pixel divided by the area of each pixel (which we know is 1 km2)\nNow that we’ve reviewed the raster attributes, let’s see what it looks like. We can use the basic plot function to do this.\n\n\nplot(pop_density)\n\n\n\n\nWe can see three locations stand out in terms of population density. First is Ouagadougou the capital of Burkina Faso and largest city, right in the center. Then we can see higher density around Bobo Dioulasso and Banfora in the southwest of the country, which are the second and third largest cities in the country.\nNext we’ll load the roads data using sf::st_read().\n\n\nroads <- st_read(\"BFA_roads/BFA_roads.shp\", quiet = TRUE)\nroads\n\n\nSimple feature collection with 1149 features and 5 fields\ngeometry type:  MULTILINESTRING\ndimension:      XY\nbbox:           xmin: -5.482261 ymin: 9.407643 xmax: 2.393089 ymax: 15.08071\ngeographic CRS: WGS 84\nFirst 10 features:\n       MED_DESCRI      RTT_DESCRI F_CODE_DES ISO   ISOCOUNTRY\n1  Without Median Secondary Route       Road BFA BURKINA FASO\n2  Without Median Secondary Route       Road BFA BURKINA FASO\n3  Without Median Secondary Route       Road BFA BURKINA FASO\n4  Without Median Secondary Route       Road BFA BURKINA FASO\n5  Without Median Secondary Route       Road BFA BURKINA FASO\n6  Without Median Secondary Route       Road BFA BURKINA FASO\n7  Without Median Secondary Route       Road BFA BURKINA FASO\n8  Without Median Secondary Route       Road BFA BURKINA FASO\n9  Without Median Secondary Route       Road BFA BURKINA FASO\n10 Without Median Secondary Route       Road BFA BURKINA FASO\n                         geometry\n1  MULTILINESTRING ((-0.720550...\n2  MULTILINESTRING ((-0.583273...\n3  MULTILINESTRING ((-0.397415...\n4  MULTILINESTRING ((-0.142728...\n5  MULTILINESTRING ((-0.403059...\n6  MULTILINESTRING ((-0.171111...\n7  MULTILINESTRING ((-0.116756...\n8  MULTILINESTRING ((0.0672155...\n9  MULTILINESTRING ((-1.245636...\n10 MULTILINESTRING ((-1.50246 ...\n\nThis sf object also contains meta-data (shown at the top). In terms of meta-data, the geometry type field tells us this data is a MULTILINESTRING object, which makes sense since these are roads. The bbox (short for bounding box), is the same information as the extent field for the raster data – it tells us the bounds of the geographic area that this spatial data covers. We see the geographic CRS which is the coordinate reference system of the data. For this roads dataset it is WGS84, which is the same as the population density raster data.\nThe roads data contains several variables: MED_DESCRI, RTT_DESCRI, F_CODE_DES, ISO, ISOCOUNTRY, and geometry. The first three variables provide some information about the types of roads in this data. ISO and ISOCOUNTRY simply provide country codes and names for the data. Finally, we see the geometry variable, which is the variable that contains the spatial information in an sf object.\nWe can also plot this roads data to see what it looks like.\n\n\nplot(roads)\n\n\n\n\nBy calling the basic plot function, we get a panel of plots of the road network, with one plot for each variable. We can see some variation in color MED_DESCRI and RTT_DESCRI, indicating that there multiple values for those variables. If we wanted just a single plot of the road network, we can get that by calling plot on the geometry variable:\n\n\nplot(roads$geometry)\n\n\n\n\nFinally, we’ll load the “toy” GPS data and convert it to an sf object. The option crs = 4326 means that we are creating this with the WGS84 coordinate reference system because 4326 is the EPSG code for WGS84.\n\nMost crs are assigned an “EPSG code”, which is a unique ID that can be used to identify a CRS.\n\n\ngps <- read_csv(\"bf_gps_fake.csv\") %>%\n  rename(EAID = EA_ID) %>% # rename to be consistent with other PMA data\n  st_as_sf(\n    coords = c(\"GPSLONG\", \"GPSLAT\"), \n    crs = 4326)\n\n\n\n\n\n\nPopulation Density: working with raster data\nWe want to construct a variable that captures the population density at each enumeration area in the data. We’ll use sf::st_buffer() to do this, which will construct a buffer circle around each GPS point. The PMA GPS data are randomly displaced to protect the privacy of respondents, so it’s imperative to consider this displacement when working with the GPS data to do spatial operations. Because the maximum displacement distance is 10 km, if we construct buffers with a radius of 10 km we can be 100% confident that the true locations of each GPS point fall within that buffer.\n\nUrban EAs are displaced from their true location up to 2 km. Rural EAs are displaced from their true location up to 5 km. Additionally, a random sample of 1% of rural EAs are displaced up to 10km.\n\n\nbuffers <- st_buffer(gps, dist = 10000)\n\n\nWarning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle\n= endCapStyle, : st_buffer does not correctly buffer longitude/\nlatitude data\ndist is assumed to be in decimal degrees (arc_degrees).\n\nggplot() +\n  geom_sf(data = buffers) +\n  geom_sf(data = gps)\n\n\n\n\nThis giant circle is certainly not what we would expect! What’s going on here? Earlier in this post we mentioned that the WGS84 crs is a geographic coordinate system that simply uses the latitude and longitude coordinates to identify locations and the units are in degrees, rather than meters or kilometers. This circle thus has a radius of 10,000 degrees and since the Earth only spans 360 degrees it is fully covered by this circle. As we mentioned, the WGS84 crs is not ideal for measuring distances. R alerted us of this problem with two warnings: st_buffer does not correctly buffer longitude/latitude data and dist is assumed to be in decimal degrees (arc_degrees). This is why it’s so important to pay attention to the crs of your data.\nTo properly construct a buffer circle around these GPS points, we need to transform the data to a different projection that uses meters or kilometers. And, because it’s essential that all of our data are in the same crs, we need to transform or reproject everything. For vector data, we can do this using sf::st_transform() and for raster data we’ll do this with raster::projectRaster(). For the transformation, we’re using a crs that is projected to meters and is appropriate to the local geography of Burkina Faso. You can read about it on the epsg.io site. After reprojecting, we’ll calculate the buffer again and plot it to make sure this looks right.\n\n\n# transform the GPS data\ngps_tr <- gps %>% st_transform(crs = 32630)\ngps_tr\n\n\nSimple feature collection with 83 features and 5 fields\ngeometry type:  POINT\ndimension:      XY\nbbox:           xmin: 260943.3 ymin: 1114669 xmax: 1025472 ymax: 1653511\nprojected CRS:  WGS 84 / UTM zone 30N\n# A tibble: 83 x 6\n   PMACC PMAYEAR REGION                EAID DATUM           geometry\n * <chr>   <dbl> <chr>                <dbl> <chr>        <POINT [m]>\n 1 BF       2017 5. centre-nord        7610 WGS84 (837531.4 1567675)\n 2 BF       2017 1. boucle-du-mouhoun  7820 WGS84 (491871.7 1488848)\n 3 BF       2017 3. centre             7271 WGS84   (982414 1349907)\n 4 BF       2017 3. centre             7799 WGS84   (739431 1352652)\n 5 BF       2017 8. est                7243 WGS84 (545866.2 1219668)\n 6 BF       2017 6. centre-ouest       7026 WGS84 (352638.7 1209502)\n 7 BF       2017 3. centre             7859 WGS84 (833822.1 1377527)\n 8 BF       2017 3. centre             7725 WGS84 (980025.8 1406727)\n 9 BF       2017 6. centre-ouest       7390 WGS84 (439876.7 1190609)\n10 BF       2017 11. plateau-central   7104 WGS84 (835483.2 1469280)\n# … with 73 more rows\n\n# reproject the raster data\npop_density_tr <- projectRaster(\n  pop_density, \n  crs = \"+proj=utm +zone=30 +datum=WGS84 +units=m +no_defs\"\n)\npop_density_tr\n\n\nclass      : RasterLayer \ndimensions : 699, 970, 678030  (nrow, ncol, ncell)\nresolution : 907, 922  (x, y)\nextent     : 218942.9, 1098733, 1035714, 1680192  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=30 +datum=WGS84 +units=m +no_defs \nsource     : memory\nnames      : bfa_pd_2017_1km \nvalues     : 0.9709167, 8775.492  (min, max)\n\n\nNote: the projectRaster function takes crs as a character string, rather than the EPSG code 32630. We’re using the PROJ.4 code shown in the “Export” menu on the epsg.io site.\n\n\n# calculate 10 km (10,000 meter) buffer circles\nbuffers_tr <- st_buffer(gps_tr, dist = 10000) # because the units are in meters\n\n# plot\nggplot() +\n  geom_sf(data = buffers_tr) +\n  geom_sf(data = gps_tr, color = \"red\")\n\n\n\n\nLooking at the meta-data for both the gps_tr and raster_tr objects, we can see they have the same new projected crs: UTM zone 30N. The raster_tr meta-data also includes information on the units (+units=m) confirming that distances are measured in meters. Turning to the plot, we can see the GPS coordinates marked in red and each has a circle around it.\nNow that we have correctly estimated 10 km buffer circles, we can calculate the average population density within each buffer using the raster::extract() command and specifying fun = mean. This produces an 83 x 1 vector of results, which means we have one population density value for each enumeration area. Printing the first 5 results shows there is some substantial variation in population density.\n\n\nbuffer_density <- raster::extract(\n  pop_density_tr, \n  buffers_tr, \n  fun = mean, \n  na.rm = TRUE,\n  cellnumbers = TRUE\n)\ndim(buffer_density)\n\n\n[1] 83  1\n\nhead(buffer_density)\n\n\n          [,1]\n[1,]  35.58080\n[2,]  21.36878\n[3,]  17.56161\n[4,] 113.36298\n[5,]  31.73017\n[6,]  57.09591\n\nNote, that we don’t actually need to create the buffers first to extract the mean values of the raster. We can do it all in one step, shown below. Just make sure to use the gps_tr object instead of the buffer_tr object! But, we’ll use those buffers again with the road data.\n\n\nbuffer_density_alt <- raster::extract(\n  pop_density_tr, gps_tr, \n  buffer = 10000,\n  fun = mean, \n  na.rm = TRUE\n)\nhead(buffer_density_alt)\n\n\n[1]  35.58080  21.36878  17.56161 113.21781  31.73017  57.09591\n\nFinally, so we can merge everything together by EAID, let’s add the population density calculation directly to the gps_tr data. Note that the raster::extract() command preserves the order of the inputs, so we know the first row of the density calculation corresponds to the first row of the gps_tr data.\n\n\ngps_tr$pop_density <- raster::extract(\n  pop_density_tr, gps_tr, \n  buffer = 10000,\n  fun = mean, na.rm = TRUE\n)\n\n\n\nRoad Networks: Working with vector data\nBefore we do anything with the road data, let’s make sure to reproject it to match the rest of our data.\n\n\nroads_tr <- roads %>%\n  st_transform(crs = 32630)\n\n\n\nBecause enumeration areas with better access to roads may make it easier for women to reach local service delivery providers. We are going to calculate the total length of roads within each buffer as a proxy for this accessibility. Because each of these buffers was constructed with the same 10 km radius, they have the same area, which means the sum of road length can also be thought of as a road density measure.\nFirst, we need to identify which portions of the road fall into each buffer. We’ll use sf::st_intersection(), which returns a new sf object that contains observations from the first argument that touch (geographically) the second argument.\n\nNote that there is also an sf::intersects() command. This is different than the one we’re using because it returns a logical matrix that indicates whether each geometry pair intersects. See more on these types of operations in the sf vignette.\n\n\nint <- st_intersection(roads_tr, buffers_tr)\nint\n\n\nSimple feature collection with 238 features and 10 fields\ngeometry type:  LINESTRING\ndimension:      XY\nbbox:           xmin: 251238.7 ymin: 1104708 xmax: 1033002 ymax: 1658158\nprojected CRS:  WGS 84 / UTM zone 30N\nFirst 10 features:\n        MED_DESCRI      RTT_DESCRI F_CODE_DES ISO   ISOCOUNTRY PMACC\n240 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n241 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n268 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n711 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n640    With Median   Primary Route       Road BFA BURKINA FASO    BF\n649    With Median   Primary Route       Road BFA BURKINA FASO    BF\n728 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n958 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n959 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n964 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n    PMAYEAR               REGION EAID DATUM\n240    2017 1. boucle-du-mouhoun 7820 WGS84\n241    2017 1. boucle-du-mouhoun 7820 WGS84\n268    2017 1. boucle-du-mouhoun 7820 WGS84\n711    2017            3. centre 7271 WGS84\n640    2017            3. centre 7799 WGS84\n649    2017            3. centre 7799 WGS84\n728    2017            3. centre 7799 WGS84\n958    2017               8. est 7243 WGS84\n959    2017               8. est 7243 WGS84\n964    2017               8. est 7243 WGS84\n                          geometry\n240 LINESTRING (501051.3 149280...\n241 LINESTRING (481961.6 149016...\n268 LINESTRING (489661.7 148171...\n711 LINESTRING (981157 1359825,...\n640 LINESTRING (732417.5 135977...\n649 LINESTRING (746990.9 135527...\n728 LINESTRING (746990.9 135527...\n958 LINESTRING (553191.7 122647...\n959 LINESTRING (554462.3 122043...\n964 LINESTRING (535888.5 122030...\n\nThe returned object (int) is a data.frame with 238 observations (far fewer than the original 1149 in the roads_tr data). Note that it also contains all the variables from both roads_tr and buffers_tr, so this operates a bit like an inner_join, which means it only includes observations that are in both datasets. We can see the implications of this by making a quick map. The full road network is shown in gray, the buffer circles are in black and the roads that fall into the circles are highlighted in red. Based on this map, it looks like there are a few buffer circles that don’t contain any roads. We want to be sure we account for this.\n\n\n# plot intersection with buffers and road networks \nggplot() +\n  geom_sf(data = buffers_tr) +\n  geom_sf(data = roads_tr, color = \"grey\") +\n  geom_sf(data = int, color = \"red\")\n\n\n\n\nWe can merge in the full list of EAIDs to make sure we don’t miss this one (or any others) using sf:st_join(), which works like dplyr::left_join(). It’s important that when we do the join, the first argument is int, so that it will retain the LINESTRING geometry from this dataset, which we need to calculate the road length. Then, we’ll calculate the length of the road networks contained in each buffer. We can do this with sf::st_length(). Because many of the buffer circles contain multiple roads, we first need calculate the length of each road then we need to aggregate to get the length of all roads in a given enumeration area. We’ll convert from meters to km for greater readability. It’s important to note that any EAs with buffers that don’t contain any roads will not be in the int dataset, so we’ll do a dplyr::full_join() with gps_tr to make sure we get them all.\nBecause int and gps_tr are both sf objects, it’s not possible to do a standard join – you can only use sf::join() when you have two sf objects. That’s why we convert both to data.frames for the dplyr::full_join() and then back into an sf object. Finally, we’ll convert int back into an sf object, retaining the POINT geometry from gps_tr, and replace all NA road lengths as 0.\n\n\n# join, calculate length, & summarize\nint <- int %>%\n  mutate(road_length = st_length(geometry)) %>%\n  group_by(EAID) %>%\n  summarise(road_length = sum(road_length, na.rm = T)) %>%\n  mutate(road_length = set_units(road_length, \"km\")) %>%\n  as.data.frame() %>%\n  full_join(as.data.frame(gps_tr), by = \"EAID\") %>%\n  st_sf(sf_column_name = 'geometry.y') %>%\n  dplyr::select(-geometry.x) %>%\n  mutate(road_length = ifelse(is.na(road_length), 0, road_length))\n\nint\n\n\nSimple feature collection with 83 features and 7 fields\ngeometry type:  POINT\ndimension:      XY\nbbox:           xmin: 260943.3 ymin: 1114669 xmax: 1025472 ymax: 1653511\nprojected CRS:  WGS 84 / UTM zone 30N\nFirst 10 features:\n   EAID road_length PMACC PMAYEAR               REGION DATUM\n1  7003    22.41682    BF    2017       5. centre-nord WGS84\n2  7006    13.35579    BF    2017       5. centre-nord WGS84\n3  7009    20.45756    BF    2017               8. est WGS84\n4  7016    20.31970    BF    2017 1. boucle-du-mouhoun WGS84\n5  7026    18.99583    BF    2017      6. centre-ouest WGS84\n6  7042    14.52567    BF    2017               8. est WGS84\n7  7048    24.29931    BF    2017        4. centre-est WGS84\n8  7056    15.21114    BF    2017     9. hauts-bassins WGS84\n9  7082    44.15771    BF    2017          2. cascades WGS84\n10 7092    26.30945    BF    2017        7. centre-sud WGS84\n   pop_density               geometry.y\n1     29.55058 POINT (422741.8 1383385)\n2     29.98035 POINT (371721.6 1276582)\n3     37.77861 POINT (348264.9 1114669)\n4    154.07399 POINT (346019.4 1218739)\n5     57.09591 POINT (352638.7 1209502)\n6     63.08637 POINT (752580.5 1219236)\n7     74.19692 POINT (479619.4 1234717)\n8     35.07390 POINT (359915.1 1392593)\n9   2731.76393 POINT (669089.2 1375002)\n10   152.23504 POINT (559070.9 1346179)\n\nThe added benefit of the full_join() with gps_tr is that it brings in the pop_density variable we created earlier. So now everything is in one dataset!\nThis can now be merged into other PMA data, such as the individual level dataset bf_merged we worked with in the other posts in this module, and the variables can be used for analysis!\nAs always, let us know if you have any questions and if you’re doing anything exciting with the PMA spatial data!\nSpecial thanks to Tracy Kugler, Nicholas Nagle, and Jonathan Schroeder for excellent help with this post.\n\nLinard, C., Gilbert, M., Snow, R. W., Noor, A. M., & Tatem, A. J. (2012). Population distribution, settlement patterns and accessibility across Africa in 2010. PloS one, 7(2), e31743.↩︎\nNEON: https://www.neonscience.org/resources/learning-hub/tutorials/raster-res-extent-pixels-r↩︎\nWikipedia↩︎\nhttps://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/intro-to-coordinate-reference-systems/↩︎\n",
    "preview": "posts/2021-02-04-merging-external-spatial-data/images/road_map.png",
    "last_modified": "2021-05-14T09:23:13-05:00",
    "input_file": {},
    "preview_width": 936,
    "preview_height": 574
  },
  {
    "path": "posts/2021-02-02-blog-post-workflow/",
    "title": "Blog post workflow",
    "description": "How to create or review a PMA Data Hub blog post",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-02-02",
    "categories": [
      "internal"
    ],
    "contents": "\n\nContents\nAll users: first time setup\nR, RStudio, and required packages\nInitialize your UMN GitHub account\nUsing UMN GitHub from RStudio\nThe PMA Data Hub Repository\n\nAuthors: Creating a new post\nCreate a new branch\nCreate a new folder in \"_posts\"\nPut your data in a “data” folder!\nPush your post to GitHub\n\nEditors\nPull the author’s branch to your computer\nLocate and edit the new post\nPush the edited post back to GitHub\n\nAll users: making revisions\nSite Admin\nSetup\nCreate a Git Hook to remove data from the public repo\nMerging\nOther git tips\n\n\nAll users: first time setup\nR, RStudio, and required packages\nTo get a copy of R, visit the Comprehensive R Archive Network (CRAN) and choose the right download link for your operating system.\nThe PMA Data Hub is organized as an RStudio Project, so you’ll also need to use RStudio (not base R).\nNote: a copy of RStudio running R version 4.0.2 (or higher) lives on the MPC gp1 server here. Members of the MPC GitHub organization can access an article specifically about using RStudio on gp1 (e.g. how to build a package library) here.\nWhen you’ve got RStudio set up, install these packages:\n\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"knitr\")\ninstall.packages(\"distill\")\ninstall.packages(\"usethis\")\n\n\n\nTroubleshooting note: the package rmarkdown comes along with RStudio, but you may receive an older version than we need to build the site. So, when you try install.packages(\"rmarkdown\"), you may get a message asking to restart R (avoiding a conflict with the prior version). We’ve found that it’s best to reply ‘No’ to the restart prompt, wait for rmarkdown to install, and then re-launch RStudio. If you do choose to restart, you may experience a recursive loop of restart prompts!\nInitialize your UMN GitHub account\nContributors to the PMA Data Hub will work on an internal copy of the public site - it’s visible only to certain people affiliated with the MPC. A smaller team of Data Hub “admin” (currently Matt & Nina) will take care of migrating content from the private site to our public site: we’re always here to help with formatting, editing, and version control!\nSo what is UMN GitHub? GitHub, itself, is a company that hosts projects on proprietary server: when you make a repository “public”, anyone in the world can visit your project on a GitHub server. GitHub also makes its underlying software available to institutions that want to provide a similar service restricted to institutional members. In practice, UMN operates its own GitHub server where organizations like the MPC can host projects that are more private in scope.\nUMN GitHub is an instance of Enterprise GitHub, whereas the public version of our blog lives in a space that folks sometimes call “public GitHub”. It’s common for people to have one account for “public GitHub” and one account for their job associated with an “enterprise GitHub”. To initialize an account for UMN GitHub, visit github.umn.edu and log in with your University Internet ID and password.\nUsing UMN GitHub from RStudio\nFirst things first: you must install install Git on your computer if it isn’t there already. Mac OS comes with git installed,1 while other users should download the right Git for their operation system. If you’re using RStudio on the gp1 server, Git is already installed.\nNext, open the Global Options menu in RStudio and locate the Git/SVN tab. Ensure that the box shown below is checked, and then enter the location of the executable file2 for your Git installation:\n\n\n\nLastly, you should provide Git with a username, email, and Personal Access Token (PAT) for your UMN GitHub account. If you’ve installed usethis as shown above, you’ll be able to set these up with R commands (changes will be applied to globally wherever you use Git on your operating system). First, set the username and email address for your UMN GitHub account. For example, mine are:\nWhy a PAT? GitHub plans to deprecate password authentication in the near future. You could use one for now (like the example below), but you’ll need one soon!\n\n\ngert::git_config_global_set(\"user.name\", \"Matt Gunther\")\ngert::git_config_global_set(\"user.email\", \"mgunther@umn.edu\")\n\n\n\nThen, create a PAT for your account with:\n\n\nusethis::create_github_token(host = \"https://github.umn.edu\")\n\n\n\nYour browser will open to a webpage. Check all the boxes you see, then click the green Generate Token button. On the next page, notice the very long string shown in the green box: this is your PAT. Don’t close this page yet!. Return to R and call:\n\n\ngitcreds::gitcreds_set(\"https://github.umn.edu\")\n\n\n\nYou’ll be asked to enter a new password or token: copy and paste your PAT from your browser and press Enter. From now on, RStudio and Git will be able to access your UMN GitHub account automatically! (If you have a personal GitHub account at github.com, you could repeat this process substituting https://github.com for https://github.umn.edu, and Git will automatically choose the right credentials based on the repository associated with your project).\nThe PMA Data Hub Repository\nOpen RStudio and navigate to File > New Project, then select Version Control:\n\n\n\nChoose Git to clone our project from a GitHub repository:\n\n\n\nOn the next menu page, enter the address for the enterprise repository exactly as shown (do not clone the public repository):\nhttps://github.umn.edu/mpc/pma-data-hub/\nAlso enter the project directory name “pma-data-hub” as shown:\npma-data-hub\nIn the third field, choose a location where you would like to save this file on your computer (mine was “~/R” - insert your own path, instead). Finally, click Create Project.\n\nWhen choosing a place to save this project, do not save to a network drive. This seems to cause RStudio to crash!\n\n\n\nIf you have not configured Git to automatically use your UMN GitHub credentials with the steps shown above, you may be prompted to provide them in a pop-up window:\n\n\n\n\nUntil you configure Git with these credentials, you’ll have to do this every time you interact with GitHub. Additionally, password authentication for GitHub will be deprecated in the near future so you’ll need to do it soon!\nAfter a short bit, RStudio will relaunch and open the new project. If you adjust the windows to show the tabs Git (left) and Files (right), you should see something like this:\n\n\n\nYou have now downloaded a copy of the Enterprise repository to your computer!\nMoreover, because you’ve connected these files to a GitHub repository, the RStudio Project will now keep track of changes you make to the files in this folder, and it will prompt you to upload your changes back to GitHub: as you add, edit, or delete files, a list of changes will appear in the Git tab.\n\nTo open an RStudio project, click on the file pma-data-hub.Rproj. If you ever forget, RStudio won’t know to look for a Git history associated with all of the underlying files.\nNotice the word master shown in the Git tab - this shows that any changes we make to files will be recorded in our local copy of the “master” version of the repository. If we made changes here and then pushed them to GitHub, they would be reflected on the “master” version we’ve saved there, too.\nIn general, Matt and Nina will be responsible for merging finished blog posts to the master branch and deploying its contents to the “live” blog that’s seen by users (for details, see site admin instructions below). All other contributors should create their own branch when writing a new blog post; Matt or Nina will merge them to “master” after they’ve been reviewed and approved by an editor. Read on!\nAuthors: Creating a new post\nCreate a new branch\nNotice that the Git tab in RStudio has a purple icon:\n\n\n\nClick this icon to create a new branch. You can name it anything you like, but we recommend using your URL slug if possible (e.g. “blog-post-workflow” is the end of the URL for this webpage). Leave the box next to “Sync branch with remote” checked, as this will create your branch both locally and on our GitHub page:\n\n\n\nRStudio now displays the new branch in place of “master” to show that we’re working on the new branch, instead!\nCreate a new folder in \"_posts\"\nNow that you’ve created a new branch in the Git window, take a look at the File window.\n\n\n\nThe program we use to build the blog is called Distill, and it takes care of all the back-end work as long as we put every new blog post inside of a unique folder within the \"_posts\" directory. Opening \"_posts\", you can see that every post is contained within a time-stamped sub-folder:\n\n\n\nTo create one of these folders for your new post, enter the following command into R:\n\n\ndistill::create_post(\"Blog post workflow\")\n\n\n\nThis does two things: it creates the folder automatically (circled in red), and it opens a new RMarkdown file where you can begin writing your post (circled in green).\n\n\n\nIn red: notice the folder appears in both your File tab and your Git tab. (Don’t worry about the date on this folder - it’s for internal use and does not need to match the publication date.)\nIn green: this is the RMarkdown file where you’ll write your post.\nCheck out our Quick-start Guide for Blogging with RMarkdown!\nAs you’re writing your post, you can preview it as a fully formatted webpage by hitting the Knit button at the top of your RMarkdown file:\n\n\n\n\nYou can switch between previewing the page in the RStudio Viewer tab, or in your computer’s default web browser. Click the settings icon next to “Knit” for preview options.\nPut your data in a “data” folder!\nIf you’re working with a PMA data extract in your post put it in a sub-folder called data. Your editors will need a copy of your extract in order to render your post! For example, I would store the data and the ddi for this post like this:\n./_posts/2021-02-02-blog-post-workflow/data/pma_00008.dat.gz\n./_posts/2021-02-02-blog-post-workflow/data/pma_00008.xml\nIt’s OK to push your data to UMN Github! As long as you put it in a folder called data, admin will automatically remove it before posting the public version of the blog.\nPush your post to GitHub\nWhen you’re finished writing, follow these steps to share your post with the team on our Enterprise GitHub page (reminder: it won’t go “live” until Matt or Nina merges your post to master and publishes it to the public GitHub page).\nPress the “Knit” button one more time to render a final HTML version of the page. (At this point, you may see a number of automatically created files related to your RMarkdown file in the Git tab.)\nNow, enter the following commands directly into the R console, but please adjust the brief commit “message” as necessary to describe your change!\n\n\ngert::git_add(\".\")\ngert::git_commit(\"Draft published: blog-post-workflow\")\ngert::git_push()\n\n\n\nNow, if you visit our Enterprise GitHub page, your post will appear in a new branch! (It will not yet appear on the copy of the blog we have posted there, which is rendered only from master.)\n\n\n\nEditors\nPull the author’s branch to your computer\nAny time that you want to review an author’s post, you’ll always need to get the latest copy of their branch from our Enterprise GitHub page. If this is the first time you’ve read a draft for the post, the author’s branch won’t yet be listed in RStudio.\n\n\n\n\nThis editor’s RStudio only knows about the “master” branch so far.\nThe Pull button in RStudio’s Git tab will gather information about all of the new branches on GitHub, and it will download a copy of each one onto your computer:\n\n\n\nClicking it will bring up a dialogue screen. RStudio reports that it discovered a new branch blog-post-workflow living at the remote repository origin.\n\n\n\nReturning to RStudio’s main window, notice that you can now toggle between the working on the remote master branch, or the new remote branch called blog-post-workflow. When you’re ready to edit the author’s post, use this menu to select their branch.\n\n\n\nRStudio automatically creates a local version of this branch on your computer, and it reports that your changes will be tracked and pushed to the remote branch of the same name.\n\n\n\nLocate and edit the new post\nNow, RStudio shows that you’re working on the author’s branch in the Git tab, and you’ll see their post listed in the _posts folder on the Files tab. Navigate to the .Rmd file for their post, then click it to begin making edits.\n\n\n\nCheck out our Quick-start Guide for Blogging with RMarkdown!\nAs you’re writing your post, you can preview it as a fully formatted webpage by hitting the Knit button at the top of your RMarkdown file:\n\n\n\n\nYou can switch between previewing the page in the RStudio Viewer tab, or in your computer’s default web browser. Click the settings icon next to “Knit” for preview options.\nPush the edited post back to GitHub\nWhen you’re finished editing, follow these steps to send the revised file back to GitHub. (Reminder: it won’t appear on the website until Matt or Nina merges the post to master and publishes it to the public GitHub page).\nPress the “Knit” button one more time to render a final HTML version of the page. (At this point, you may see a number of automatically created files related to your RMarkdown file in the Git tab.)\nNow, enter the following commands directly into the R console, but please adjust the brief commit “message” as necessary to describe your change!\n\n\ngert::git_add(\".\")\ngert::git_commit(\"Draft edited: blog-post-workflow\")\ngert::git_push()\n\n\n\nNow, if you visit our Enterprise GitHub page, you’ll see that your edited files appear on the author’s branch. (They will not yet appear on the copy of the blog we have posted there, which is rendered only from master.)\n\n\n\nAll users: making revisions\nWhen you’ve finished pushing something to GitHub, please email your collaborators to let them know about next steps!\nTo get your collaborator’s latest updates from GitHub, you should launch the pma-data-hub RStudio project file called pma-data-hub.Rproj. If you forget to open the project file, RStudio will not be able to access the contents of your Git folder (it won’t know that there’s a GitHub repository for the project at all).\nWhen you open the project file, RStudio will again display a Git tab. Click on the Pull button to get your collaborator’s latest changes.\n\n\n\nSwitch from the master branch to the branch associated with your post:\n\n\n\nAfter you’re finished incorporating their feedback into the RMarkdown file (.Rmd), click Knit and then run these commands in Terminal again to send your work back to GitHub:\n\n\ngert::git_add(\".\")\ngert::git_commit(\"Draft complete: blog-post-workflow\")\ngert::git_push()\n\n\n\nPlease let Matt & Nina know when your revisions are complete and ready to appear on the live blog!\nSite Admin\nThese instructions will introduce an additional remote to your local repository. Only use them if you’ll be involved with managing updates to the live blog (e.g. Matt & Nina). All of these functions can be used the in RStudio Terminal.\nSetup\nBefore adding a second remote, it’s best to rename the UMN GitHub remote something like private. You can check the current name with:\ngit remote\nIf the UMN GitHub is still called origin (by default), rename it with:\ngit remote rename origin private\nLikewise, you should give the private branch master a different name, as the second remote will have a master branch, too. To see all of the remote branches currently in use:\ngit remote show private\nCheckout master and change its name to something like private-master:\ngit checkout master\ngit branch -m \"private-master\"\nNow, add the public remote and fetch its branches (there should only be one, called master):\ngit remote add public https://github.com/ipums/pma-data-hub\ngit fetch public\nCreate a local branch called public-master corresponding with master on the public remote:\ngit branch public-master public/master\nAt this point, you’ll notice that RStudio shows two remotes (with the branches you’ve fetched) and all of the local branches you’ve created so far.\n\n\n\n\nIn this example, we set the local “private-master” branch to track “master” on the private remote. The “public-master” branch tracks “master” on the public remote.\nCreate a Git Hook to remove data from the public repo\nWe should never push PMA data extracts to the public repository. Yet, we need to have these data available on the private repository in order to build each others’ posts when we want to review or merge new content.\nYou might imagine that the only way to do this would be to manually delete the data folder for each post just before we push something from our local version of public-master to the remote public/master. Luckily, git hooks offers a way to automatically delete any incoming data folder headed to a merge with public-master.\nSimply create a file at .git/hooks/post-merge containing the following code:\n#! /bin/sh\ngreen='\\033[0;32m'\nnc='\\033[0m'\n\n# Start from the repository root.\ncd ./$(git rev-parse --show-cdup)\n\n# Delete data files and empty directories.\nif [ `git rev-parse --abbrev-ref HEAD` == \"public-master\" ]; then\n  echo \"${green}Deleting data files...${nc}\"\n  find . -path '*/data/*' -delete\n  find . -type d -empty -delete\nfi\n\nThis will only delete data folders if a branch called public-master is currently checked out when we run git merge (see below). If you gave a different name to your local public branch, you’ll have to edit this code!\nYou also must make this file executable. For Mac OS users, you can do this by entering the following command into Terminal (assuming you’re already in the pma data hub directory):\nchmod +x .git/hooks/post-merge\nNow, when you follow the merging steps below, any file saved in a folder called data (including all sub-folders) will be deleted automatically.\nMerging\nOur main goal is to avoid creating divergent commit histories between the internal repository and the public repository. In practice, that means a typical workflow will involve these steps:\nSquash and Merge the author’s branch to private/master\nAdd the new post to index.Rmd\nBuild, Commit, and Push to private/master\nMerge private/master to public/master\nPush to public/master\nOur authoring / editing workflow generates a commit each time someone adds a change to the branch git log (you can run git log on any branch to see its full commit history). We will squash and merge this commit history into a single commit on private-master.\nFor example, with a new post on the branch blog-post-workflow:\ngit checkout private-master\ngit merge --squash blog-post-workflow\nWe have two posts that should not be included in the blog index: this one, and the Quick-start Guide for Blogging with RMarkdown. In order to make that work, we have to whitelist the posts we do want in the file index.Rmd.3\nNow, in RStudio, hit the Build Website button. Look over the site to make sure everything looks good. (We won’t build on public-master to avoid merge conflicts, so get those edits in now.)\n\n\n\nWhen you’re ready, add all files and commit your changes with a message like:\ngit add .\ngit commit -m \"new post: blog-post-workflow\"\nAlthough your local branch has a different name, you can push your commit to master on the private remote with this command:\ngit push private HEAD:master\nWait a few minutes, and you’ll see that the GitHub Pages site hosted at UMN GitHub should update to reflect your changes.\nTo merge your private-master to public-master:\ngit checkout public-master\ngit merge --squash private-master -X theirs\ngit add .\ngit commit -m \"new post: blog-post-workflow\"\n(It’s good practice to squash commits to private-master, too, since we sometimes make quick-fixes there. The -X theirs part tells Git to defer to private-master over public-master in case there are any conflicts.)\nIf you’ve created a Git Hook to automatically delete any data folders (see above), you’re all set! If not, manually delete them now. Finally, push your changes to the live site:\ngit push public HEAD:master\nOther git tips\nNeed to rollback to a previous commit? Look for it in the git log, and do a hard reset:\ngit log\ngit reset --hard 58ba4f0396b985fb5ab82c88f7bbc5c9cc619e71\nFor a checklist of updated files and their commit status:\ngit status\nTo see a list of files with unresolved conflicts\ngit diff --name-only --diff-filter=U\nIf you ever need to force a push to GitHub (e.g. if you REALLY must overwrite a commit), you can do that with the force option:\ngit push -f \n\nYou can check its location by running “which git” in Terminal, and “git –version” to check the installed version. If git is somehow not installed, use the “Install git using Homebrew” instructions here↩︎\nMac users: type “which git” in terminal and enter the result; Windows users: look for git.exe (most likely in Program Files)↩︎\nAnnoying, yes? Hopefully, an exclusion logic will become available in the next distll release.↩︎\n",
    "preview": "posts/2021-02-02-blog-post-workflow/images/git-menu.png",
    "last_modified": "2021-06-14T10:14:11-05:00",
    "input_file": {},
    "preview_width": 1180,
    "preview_height": 1160
  },
  {
    "path": "posts/2021-01-29-mapping-sdp-variables/",
    "title": "Mapping Service Delivery Point Data",
    "description": "Map spatial variation in the service delivery environment across enumeration areas.",
    "author": [
      {
        "name": "Nina Brooks",
        "url": "http://www.ninarbrooks.com/"
      }
    ],
    "date": "2021-01-30",
    "categories": [
      "Individuals in Context",
      "Service Delivery Points",
      "Data Manipulation",
      "Mapping",
      "sf"
    ],
    "contents": "\n\nContents\nData\nSetup\nMerge and Map\nBasic Maps\nMerge GPS and SDP Data\nMap SDP data\n\nPutting it All Together\n\n\n\n\nIn our last post, we showed how PMA Service Delivery Point (SDP) data can be aggregated to the enumeration area they serve (captured by EASERVED) and linked to individual-level data from a PMA Household and Female survey. In this post, we’ll continue thinking about the spatial distribution of SDP summary data. We’ll first show how to merge our example data to a GPS dataset obtained from pmadata.org, and we’ll then use the new dataset to visualize a few of our variables on a map of Burkina Faso.\nData\nBuilding on the steps we’ve covered in the last two posts in this series, we’ll be working with an example dataset we’re calling bf_merged that contains records from female respondents to the 2017 and 2018 Burkina Faso Household and Female surveys merged with five variables we’ve created from the 2017 and 2018 SDP surveys. These five variables describe services provided within the enumeration area where each woman resides:\nNUM_METHODS_PROV - number of methods provided by at least one SDP\nNUM_METHODS_INSTOCK - number of methods in-stock with at least one SDP\nNUM_METHODS_OUT3MO - number of methods out of stock any time in the last 3 months with at least one SDP\nMEAN_OUTDAY - the mean length of a stockout for any family planning method (measured in days)\nN_SDP - number of SDPs\nThe remaining four variables in bf_merged were taken directly from a data extract containing only female respondents:\nPERSONID - unique identifer for each woman\nEAID - unique identifier for each woman’s enumeration area\nURBAN - whether each woman lives in an urban enumeration area\nFPCURRUSE - whether each woman is currently using any family planning method\nWe’ll also be working with toy PMA GPS datasets for Burkina Faso. PMA GPS data include one GPS coordinate per enumeration area. The Burkina Faso Round 5 and 6 surveys sampled the same enumeration areas, which means we can link the GPS data to both rounds. To use real PMA GPS data you must request access directly from our partners at pmadata.org. For the purpose of use in this post, we’ve created a “toy” GPS dataset: the toy data contains randomly sampled locations within Burkina Faso that have no actual relationship to the EAs in the PMA data.\nThe last dataset we’ll use in this post are the administrative boundaries for Burkina Faso. Shapefiles with administrative boundaries are widely available for download, but we’ll use the ones made available from IPUMS PMA.\nSetup\nMake sure you have all of the following packages installed. Once installed, load the packages we’ll be using today:\n\n\nlibrary(ipumsr)\nlibrary(sf) # primary spatial package\nlibrary(viridis) # for color palettes\nlibrary(tabulator) # for pipe-friendly tabs & cross-tabs\nlibrary(tidyverse)\n\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nIf you followed along with our last post, glimpse(bf_merged) should list the first few records for all of the variables we have so far:\n\n\n\n\n\nglimpse(bf_merged)\n\n\nRows: 6,944\nColumns: 10\n$ EAID                <dbl+lbl> 7003, 7003, 7003, 7003, 7003, 7003,…\n$ SAMPLE              <int+lbl> 85405, 85405, 85405, 85405, 85405, …\n$ N_SDP               <int> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ NUM_METHODS_PROV    <int> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ NUM_METHODS_INSTOCK <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, …\n$ NUM_METHODS_OUT3MO  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MEAN_OUTDAY         <dbl> NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN,…\n$ PERSONID            <chr> \"0700300000019732017504\", \"070030000001…\n$ URBAN               <int+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FPCURRUSE           <int+lbl> 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,…\n\nRemember that we merged the EA-level data into the individual-level data, but the GPS datasets provide coordinates for the enumeration area. So the first thing we’ll do is aggregate bf_merged to the EA-level, and assign the aggregated data to a new object called bf_ea.\n\n\nbf_ea <- bf_merged %>%\n  dplyr::select(-PERSONID) %>%\n  group_by(EAID, SAMPLE) %>%\n  summarise_all(mean, na.rm = T) %>%\n  filter(!is.na(N_SDP)) \n\n\n\n\nIn this example, we’ll exclude any EAs where no facilities in our SDP sample provide services with filter(!is.na(N_SDP))\nNow, let’s read in the GPS data from the data folder and see what the it contains.\n\n\ngps <- read_csv(\"bf_gps_fake.csv\")\ngps\n\n\n# A tibble: 83 x 7\n   PMACC PMAYEAR REGION               EA_ID DATUM GPSLAT GPSLONG\n   <chr>   <dbl> <chr>                <dbl> <chr>  <dbl>   <dbl>\n 1 BF       2017 5. centre-nord        7610 WGS84   14.2  0.126 \n 2 BF       2017 1. boucle-du-mouhoun  7820 WGS84   13.5 -3.08  \n 3 BF       2017 3. centre             7271 WGS84   12.2  1.43  \n 4 BF       2017 3. centre             7799 WGS84   12.2 -0.799 \n 5 BF       2017 8. est                7243 WGS84   11.0 -2.58  \n 6 BF       2017 6. centre-ouest       7026 WGS84   10.9 -4.35  \n 7 BF       2017 3. centre             7859 WGS84   12.4  0.0703\n 8 BF       2017 3. centre             7725 WGS84   12.7  1.42  \n 9 BF       2017 6. centre-ouest       7390 WGS84   10.8 -3.55  \n10 BF       2017 11. plateau-central   7104 WGS84   13.3  0.0957\n# … with 73 more rows\n\n\nIf you requested access to the actual GPS datasets, make sure to replace the bf_gps_fake.csv with the filename of the real data!\n\n\n\nThe gps data has 7 variables:\nPMACC: the country code\nPMAYEAR: the 4-digit year of data collection\nREGION: sub-national administrative division name\nEA_ID: the enumeration area ID (and how we’ll merge this data into other PMA datasets)\nGPSLAT: the displaced EA’s centroid latitude coordinate in decimal degrees\nGPSLONG: the displaced EA’s centroid longitude coordinate in decimal degrees\nDATUM: the coordinate reference system and geographic datum. This variable is always “WGS84” for the World Geodetic System 1984.\n\nNote that while the PMAYEAR variable is 2017 for all EAs, because the same EAs were sampled in the 2017 (Round 5) and 2018 (Round 6) surveys, we can link these coordinates to both samples.\nNote that the GPSLAT and GPSLONG are displaced coordinates of the EA centroid. This is because PMA randomly displaces the geographic coordinates to preserve the privacy of survey respondents. Coordinates are displaced randomly by both angle and distance. Urban EAs are displaced from their true location up to 2 km. Rural EAs are displaced from their true location up to 5 km. Additionally, a random sample of 1% of rural EAs are displaced up to 10km. The PMA GPS data come with documentation that explains the displacement in more detail. The primary spatial package we’ll use is simple features or sf. We’ll use sf::st_as_sf() to convert the GPS data to a spatial data object (known as a simple feature collection).\n\n\ngps <- gps %>%\n    rename(EAID = EA_ID) %>% # rename to be consistent with other PMA data\n    st_as_sf(\n      coords = c(\"GPSLONG\", \"GPSLAT\"), \n      crs = 4326) # 4326 is the coordinate reference system (CRS) identifier for WGS84\ngps\n\n\nSimple feature collection with 83 features and 5 fields\ngeometry type:  POINT\ndimension:      XY\nbbox:           xmin: -5.185229 ymin: 10.08082 xmax: 1.829187 ymax: 14.93619\ngeographic CRS: WGS 84\n# A tibble: 83 x 6\n   PMACC PMAYEAR REGION               EAID DATUM              geometry\n * <chr>   <dbl> <chr>               <dbl> <chr>           <POINT [°]>\n 1 BF       2017 5. centre-nord       7610 WGS84  (0.1263576 14.15999)\n 2 BF       2017 1. boucle-du-mouho…  7820 WGS84   (-3.075099 13.4676)\n 3 BF       2017 3. centre            7271 WGS84   (1.430382 12.17557)\n 4 BF       2017 3. centre            7799 WGS84 (-0.7991777 12.22721)\n 5 BF       2017 8. est               7243 WGS84  (-2.580105 11.03307)\n 6 BF       2017 6. centre-ouest      7026 WGS84  (-4.348525 10.93844)\n 7 BF       2017 3. centre            7859 WGS84 (0.07032293 12.44352)\n 8 BF       2017 3. centre            7725 WGS84   (1.417154 12.68821)\n 9 BF       2017 6. centre-ouest      7390 WGS84  (-3.549929 10.77006)\n10 BF       2017 11. plateau-central  7104 WGS84 (0.09573113 13.27184)\n# … with 73 more rows\n\n\n\n\nNow that gps is a simple features object, we’ve lost the GPSLAT and GPSLONG variables and gained a variable called geometry, which contains the spatial information for this data.\n\n\n\nThe last thing we need is the Burkina Faso shapefile, which are available from IPUMS PMA. You’ll need to download the shapefile (geobf.zip) from the IPUMS site and save it in your working directory to use it. We can use sf::st_read() to read the shapefile into R as an sf object. Note that here the geometry variable is a POLYGON, whereas in the gps data it is a POINT.\n\nNote that what we call a shapefile is actually a collection of many files. More on this in a future post! But for now, just know that you’ll need all the files that come in the zipped download and can refer to the collectively with “geobf.shp”.\n\n\nbf_shp <- st_read(\"geobf/geobf.shp\") \n\n\n\nMerge and Map\nNow that we have all our data, we’ll show you how to map variables… but before we do that, let’s do some basic, exploratory mapping.\nBasic Maps\nggplot2 has support for sf objects, which makes it really easy to map things using the ggplot2 system. ggplot2::geom_sf() will automatically identify what kind of spatial data you’re plotting and handle it appropriately. For example, let’s plot the gps data (which are points) and the administrative region (which are polygons).\nggplot2 is included when you load library(tidyverse)\n\n\n# Plot EA centroids\nggplot() +\n  geom_sf(data = gps)\n\n\n\n# Plot regions of Burkina Faso\nggplot() +\n  geom_sf(data = bf_shp)\n\n\n\n\nThe building-block approach of ggplot2 (“Grammar of Graphics”) also makes it really easy to layer different spatial features on the same map.\n\n\n# Plot regions of Burkina Faso & EA centroids on the same map\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = gps)\n\n\n\n\nMerge GPS and SDP Data\nTo map the EA-level variables constructed in the last post, we need to merge the bf_ea data and the gps data by EAID. First, let’s rename the EASEARVED variable to match the GPS data and then use a dplyr::right_join to merge in the SDP data. We need to use a right_join() because the sf object must be listed first in our join command to retain the sf class, but we want to ensure that all rows from bf_ea are preserved.\n\nRemember, the SDP data contains information from both 2017 and 2018, while the GPS data has a single observation per EA.\n\n\nbf_ea <- right_join(gps, bf_ea, by = \"EAID\")\n\n\n\nMap SDP data\nRemember, the bf_ea data contains information from 2017 & 2018 for the same EA, which can clog up the map depending on how we use this information. To start out, let’s use only the 2017 data and add information about the number of service delivery providers that serve a given EA (N_SDP). By passing N_SDP to the size aesthetic, we can more easily visualize how EAs vary in their access to service delivery providers.\n\n\nbf_ea2017 <- bf_ea %>%\n  filter(SAMPLE == 85405) # this sample corresponds to the 2017 wave\n\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = bf_ea2017, \n          aes(size = N_SDP),\n          alpha = 0.4) \n\n\n\n\nFrom the map, it looks like there may be a few locations where there EAs are both close together and served by many SDPs, which are likely in urban areas. For example, the capital of Burkina Faso, Ouagadougou, is in the center of the map where there are a number of EAs on top of each other. But, it’s a little hard to see the variation in size when there are so many values for N_SDP and so many EAs on top of each other. Let’s do two things to make this more readable. First, we’ll create smaller categories of the N_SDP variable, and second, we’ll map the URBAN variable to the color aesthetic.\n\n\nbf_ea2017 <- bf_ea2017 %>%\n  mutate(\n    N_SDP_CAT = case_when(\n      N_SDP <= 2 ~ 1,\n      N_SDP >2 & N_SDP <= 4 ~ 2,\n      N_SDP >4 ~ 3),\n    N_SDP_CAT = factor(N_SDP_CAT,\n                       levels = c(1, 2, 3),\n                       labels = c(\"Low\", \"Mid\", \"High\"),\n                       ordered = T), # needs to be an ORDERED factor to map to the size aesthetic\n    MEAN_OUTDAY = ifelse(is.na(MEAN_OUTDAY), 0, MEAN_OUTDAY),\n    URBAN = factor(URBAN, \n                   levels = c(0,1),\n                   labels = c(\"Rural\", \"Urban\"))\n  )\n\n\n# let's take a look at the distribution of this new categorical variable\nbf_ea2017 %>% \n  tab(URBAN, N_SDP_CAT)\n\n\nSimple feature collection with 5 features and 5 fields\ngeometry type:  MULTIPOINT\ndimension:      XY\nbbox:           xmin: -5.18865 ymin: 9.883331 xmax: 1.634087 ymax: 14.39679\ngeographic CRS: WGS 84\n# A tibble: 5 x 6\n  URBAN N_SDP_CAT     N                        geometry  prop cum_prop\n  <fct> <ord>     <int>                <MULTIPOINT [°]> <dbl>    <dbl>\n1 Rural Mid          29 ((-5.18865 11.54962), (-4.3401…  0.35     0.35\n2 Urban Mid          23 ((-5.178223 10.66001), (-4.781…  0.28     0.63\n3 Urban Low          15 ((-4.307564 11.18051), (-4.299…  0.18     0.81\n4 Rural Low          13 ((-4.969563 10.45619), (-4.804…  0.16     0.96\n5 Urban High          3 ((-4.262726 11.14746), (-1.528…  0.04     1   \n\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = bf_ea2017, \n          aes(size = N_SDP_CAT,\n              color = URBAN),\n          alpha = 0.4) +\n  scale_color_viridis_d() \n\n\n\n\nFrom this map we can see that urban areas are generally served by more SDPs – in fact, no rural EAs fall into the “High” category – although the difference is perhaps not as stark as one might have expected. But, what is the service environment like? Do urban areas have more stockouts than rural areas? Do SDPs in urban areas offer a greater selection of family planning methods? Did the service environment change between 2017 and 2018? Mapping can shed a lot of light on these questions.\nLet’s look at the NUM_METHODS_PROV variable created in the last post. This variable captures the number of family planning methods provided by at least one SDP that serves a given EA.\n\n\nbf_ea2017 %>%\n  tab(NUM_METHODS_PROV) %>%\n  arrange(NUM_METHODS_PROV)\n\n\nSimple feature collection with 5 features and 4 fields\ngeometry type:  MULTIPOINT\ndimension:      XY\nbbox:           xmin: -5.18865 ymin: 9.883331 xmax: 1.634087 ymax: 14.39679\ngeographic CRS: WGS 84\n# A tibble: 5 x 5\n  NUM_METHODS_PROV     N                       geometry  prop cum_prop\n             <dbl> <int>               <MULTIPOINT [°]> <dbl>    <dbl>\n1                8     2 ((-4.299839 11.18039), (-2.96…  0.02     1   \n2                9    13 ((-4.340111 11.8743), (-4.281…  0.16     0.92\n3               10    34 ((-5.18865 11.54962), (-4.969…  0.41     0.41\n4               11    29 ((-5.178223 10.66001), (-3.84…  0.35     0.76\n5               12     5 ((-2.757122 11.53829), (-2.26…  0.06     0.98\n\nSince there is not a large range of number of FP methods provided, let’s dichotomize this so we can map it to the shape aesthetic.\n\n\nbf_ea2017 <- bf_ea2017 %>%\n  mutate(\n    NUM_METHODS_CAT = case_when(\n      NUM_METHODS_PROV <= 9 ~ 1,\n      NUM_METHODS_PROV >9  ~ 2),\n    NUM_METHODS_CAT = factor(NUM_METHODS_CAT,\n                       levels = c(1, 2),\n                       labels = c(\"Low (<=9)\", \"High (>9)\"),\n                       ordered = T)\n  )\n\n\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = bf_ea2017, \n          aes(size = NUM_METHODS_CAT,\n              shape = URBAN,\n              color = NUM_METHODS_OUT3MO),\n          alpha = 0.4) +\n  scale_color_viridis_c() \n\n\n\n\nPutting it All Together\nNow we have a map that shows spatial variation in availability of different methods of family planning and prevalence of stock-outs, as well as demonstrates how these characteristic differ across urban vs. rural EAs. It’s super quick to make a basic map like this, but let’s clean up a few things to make it look nicer.\n\n\nggplot() +\n  geom_sf(data = bf_shp, fill = \"#f2f2f5\") +\n  geom_sf(data = bf_ea2017, \n          aes(size = NUM_METHODS_CAT,\n              shape = URBAN,\n              color = NUM_METHODS_OUT3MO),\n          alpha = 0.4) +\n  scale_color_viridis_c(direction = -1) + # reversing the direction makes the high #s stand out more\n  labs(title = \"Spatial Variation in Family Planning Service Environment\",\n       subtitle = \"Burkina Faso 2017\",\n       caption = \"Source: IPUMS PMA\",\n       shape = \"\",\n       size = \"Methods\\nProvided\",\n       color = \"Out of Stock\\n(Past 3 Months)\",\n       x = NULL,\n       y = NULL) +\n  theme_minimal() +\n  theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank())\n\n\n\n\n\n\n\nThis map suggests there is spatial correlation to the stockouts – with 2 regions responsible for the majority of EAs with stockouts. It also looks like these EAs tend to have more methods provided by the SDPs that serve them. Finally, let’s use both years of data and see if there is any temporal variation. To do this, we’ll use the original bf_ea dataset (instead of sdp2017) and re-create the same NUM_METHODS_CAT factor variable that dichotomizes the NUM_METHODS_PROV variable. Then, we’ll use facet_wrap() to make a multi-panel plot, with one panel per year.\n\n\nbf_ea <- bf_ea %>%\n  mutate(\n    NUM_METHODS_CAT = case_when(\n      NUM_METHODS_PROV <= 9 ~ 1,\n      NUM_METHODS_PROV >9  ~ 2),\n    NUM_METHODS_CAT = factor(NUM_METHODS_CAT,\n                       levels = c(1, 2),\n                       labels = c(\"Low (<=9)\", \"High (>9)\"),\n                       ordered = T),\n    YEAR = case_when(\n      SAMPLE == 85405 ~ 2017,\n      SAMPLE == 85408 ~ 2018\n    ),\n    URBAN = factor(URBAN, \n                   levels = c(0,1),\n                   labels = c(\"Urban\", \"Rural\"))\n  )\n\nggplot() +\n  geom_sf(data = bf_shp, fill = \"#f2f2f5\") +\n  geom_sf(data = bf_ea, \n          aes(size = NUM_METHODS_CAT,\n              shape = URBAN,\n              color = NUM_METHODS_OUT3MO),\n          alpha = 0.4) +\n  facet_wrap(~ YEAR) +\n  # reversing the direction makes the high #s stand out more\n  scale_color_viridis_c(direction = -1) + \n  guides(color = guide_colorbar(barheight = .75,\n                                barwidth = 4.5,\n                                label.position = \"top\",\n                                label.hjust = 0)) + \n  labs(title = \"Spatial Variation in Family Planning Service Environment\",\n       subtitle = \"Burkina Faso 2017-2018\",\n       caption = \"Source: IPUMS PMA\",\n       shape = \"\",\n       size = \"Methods\\nProvided\",\n       color = \"# Methods\\nOut of Stock\\n(Past 3 Months)\",\n       x = NULL,\n       y = NULL) +\n  theme_minimal() +\n  theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    legend.title = element_text(size = 8),\n    legend.position = \"bottom\") \n\n\n\n\nWith the 2018 data included, it looks like the service environment may have improved between 2017 and 2018 with fewer stockouts. However, it also looks the EAs that faced more stockouts in 2017 are not always the same as those facing stockouts in 2018. But, there is still a spatial pattern to the stockouts in 2018. It also looks like some EAs had fewer family planning methods available from SDPs in 2018 than in 2017, specifically in the western part of the country.\nFuture posts may explore other supply-side factors that could influence the SDPs (and look at how these change over time) or examine demand-side factors by merging in the individual-level data.\nAs always, let us know what kinds of questions about fertility and family planning you’re answering – especially if you’re doing anything spatial!\n\n\n\n",
    "preview": "posts/2021-01-29-mapping-sdp-variables/images/bf_fp_map.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 2100,
    "preview_height": 1350
  },
  {
    "path": "posts/2021-01-28-summarize-by-easerved/",
    "title": "Merging Service Delivery Point Data to Household & Female Records",
    "description": "Create aggregate measures for women living the areas served by SDPs",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-29",
    "categories": [
      "Individuals in Context",
      "Data Manipulation",
      "Service Delivery Points",
      "pivot_longer",
      "join"
    ],
    "contents": "\n\nContents\nReviewing SDP Sample Design\nSetup: Create and Load a Data Extract\nEAID and EASERVED\nPivot Longer: EASERVED in Rows\nSummarise by EASERVED and SAMPLE\nMerging to Household and Female Data\n\n\n\n\nWelcome to the third post in a series all about using PMA Service Delivery Point (SDP) data to better understand Individuals in Context. In our last post, we discussed a few of the variable groups related to contraceptive availability, and we showed how to use functions like dplyr::across to recode and summarize these variable groups in preparation for merging with Household and Female data.\nBefore we dive in, let’s quickly revisit the geographic sampling units - or enumeration areas - we’ll be using to link SDPs with their counterparts in the Household and Female data.\nReviewing SDP Sample Design\nRemember: the SDP sample design selects facilities meant to reflect the health service environment experienced by individuals included in Household and Female samples. If you were designing survey with this goal in mind, how would you select facilities?\nWell, you might target a sample of facilities located within the same geographic sampling units PMA used to define Household and Female samples from the same country in the same year. Presumably, the health services available to a woman living in enumeration area X would be captured pretty well if we surveyed a list of facilities also located in enumeration area X.\nBut what happens if a lot of women living in enumeration area X travel to enumeration area Y to receive family planning services? In that case, you’d want to know as much as possible about the service catchment areas for facilities in that country. Then, you could select facilities based on whether they provide services to enumeration area X, rather than relying simply to those that are located there.\nIn fact, PMA partners with government health agencies to obtain information about the service catchment area for all of the public-sector health facilities in each participating country. As a result, public SDPs are sampled if one of the enumeration areas used in a corresponding Household and Female sample appears in their service catchment area.\nBecause service catchment data are only available for public facilities, PMA uses a different method to select private-sector facilities. A private facility will be selected for a SDP sample only if it is located inside the boundaries of an enumeration area included in a corresponding Household and Female sample.\nSetup: Create and Load a Data Extract\nLet’s take a look at an example SDP dataset to see how all of this information gets reported. We’ll use the same data we highlighted in our last post, which includes facilities sampled from Burkina Faso in 2017 and 2018. First, load the following packages into R:\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\n\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nAgain in this post, we’ll be working with all of the available contraceptive services and stock variables ending with the suffixes PROV, OBS, OUT3MO, and OUTDAY. We’ll also add the variable group EASERVED, which - as we’ll see - stores information about the service catchment area for facilities where that information was available. Finally, we’ll add a few more variables that we’ll explore a bit later: AUTHORITY, FACILITYTYPE, and FACILITYTYPEGEN.\nWe’ll first load the data using ipumsr::read_ipums_micro:\n\n\nsdp <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00008.xml\",\n  data = \"data/pma_00008.dat.gz\") \n\n\n\n\nRemember: change these file paths to match your own data extract!\nThen, following the steps outlined in our last post, we’ll apply a couple of recoding functions from ipumsr.\n\n\nsdp <- sdp %>% \n  select(-EASERVED) %>% # error from extract system\n  mutate(\n    across(ends_with(\"OBS\"), ~lbl_relabel(\n      .x,\n      lbl(1, \"in stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    )),\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"Not interviewed (SDP questionnaire)\",\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    ))\n  )\n\n\n\nEAID and EASERVED\nFor the moment, let’s just take a look at the basic structure of our data, selecting only the variables FACILITYID, SAMPLE, AUTHORITY, CONSENTSQ, and EAID. For this preview, we’ll also arrange the data in ascending order of FACILITYID and SAMPLE:\nFACILITYID, SAMPLE, CONSENTSQ, and EAID are automatically included in every SDP data extract.\n\n\nsdp %>% \n  select(FACILITYID, SAMPLE, AUTHORITY, CONSENTSQ, EAID) %>% \n  arrange(FACILITYID, SAMPLE)\n\n\n# A tibble: 234 x 5\n   FACILITYID                      SAMPLE    AUTHORITY CONSENTSQ  EAID\n    <int+lbl>                   <int+lbl>    <int+lbl> <int+lbl> <dbl>\n 1       7006 85405 [Burkina Faso 2017 R… 1 [Governme…   1 [Yes]  7390\n 2       7006 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7390\n 3       7027 85405 [Burkina Faso 2017 R… 1 [Governme…   1 [Yes]  7332\n 4       7027 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7332\n 5       7029 85405 [Burkina Faso 2017 R… 4 [Private]    1 [Yes]  7111\n 6       7029 85408 [Burkina Faso 2018 R… 4 [Private]    0 [No]   7111\n 7       7036 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7412\n 8       7046 85408 [Burkina Faso 2018 R… 4 [Private]    1 [Yes]  7798\n 9       7048 85405 [Burkina Faso 2017 R… 1 [Governme…   1 [Yes]  7009\n10       7051 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7798\n# … with 224 more rows\n\nEach row in our data represents one facility from one sample. Notice that some - but not all - facilities appear once in sample 85405 (from 2017), and again in sample 85408 (from 2018).\nThe variable AUTHORITY shows the managing authority for each facility. Following the discussion above, we’ll expect to find information about the service catchment area for each facility where the managing authority is 1 - Government.\nAlso notice CONSENTSQ, which indicates whether a respondent at each facility consented to be interviewed. When you first obtain a data extract, you should expect most variables to be marked Not interviewed (SDP questionnaire) for facilities where CONSENTSQ shows 0 - No. However, we’ve already taken the extra step of marking all non-response values NA: we should now expect to see NA substituted for Not interviewed (SDP questionnaire).\nLastly, take particular note of the variable EAID: in SDP data, EAID shows the identification code associated with the enumeration area where a facility is located.\nWe’ll find information about the service catchment area for each facility in a different set of variables, each starting with with prefix EASERVED:\n\n\nsdp %>% \n  select(starts_with(\"EASERVED\")) \n\n\n# A tibble: 234 x 18\n   EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n   <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl>\n 1      7380      7323      7491      7605      7142      7279\n 2      7879      7516      7111      7554      7934      7791\n 3      7483        NA        NA        NA        NA        NA\n 4      7185        NA        NA        NA        NA        NA\n 5      7725      7859      7472      7175        NA        NA\n 6      7082        NA        NA        NA        NA        NA\n 7      7650        NA        NA        NA        NA        NA\n 8      7955        NA        NA        NA        NA        NA\n 9      7323        NA        NA        NA        NA        NA\n10      7774        NA        NA        NA        NA        NA\n# … with 224 more rows, and 12 more variables: EASERVED7 <int+lbl>,\n#   EASERVED8 <int+lbl>, EASERVED9 <int+lbl>, EASERVED10 <int+lbl>,\n#   EASERVED11 <int+lbl>, …\n\nYou’ll notice that our extract contains 18 EASERVED variables. Why 18? If you created your own data extract, you’ll remember that you only selected one variable called EASERVED: once you’ve selected samples, the IPUMS extract system automatically determines the correct number of EASERVED variables for your dataset based on the facility with the largest service catchment list.\n\nSome samples include facilities serving as many as 42 enumeration areas, requiring 42 EASERVED variables!\nAs we’ve discussed, PMA only receives service catchment information about public-sector facilities. In their case, each EASERVED variable contains an ID code for one of the enumeration areas in its service catchment list, or else it’s NA. We’ll look at these public-sector facilities first:\n\n\nsdp %>% count(AUTHORITY)\n\n\n# A tibble: 3 x 2\n        AUTHORITY     n\n*       <int+lbl> <int>\n1 1 [Government]    202\n2 3 [Faith-based]     3\n3 4 [Private]        29\n\nThe vast majority of SDPs in our sample are public-sector facilities. They comprise 202 of the 234 facilities in our sample.\n\n\nsdp %>% \n  filter(AUTHORITY == 1) %>% \n  select(starts_with(\"EASERVED\")) \n\n\n# A tibble: 202 x 18\n   EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n   <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl>\n 1      7380      7323      7491      7605      7142      7279\n 2      7879      7516      7111      7554      7934      7791\n 3      7483        NA        NA        NA        NA        NA\n 4      7185        NA        NA        NA        NA        NA\n 5      7725      7859      7472      7175        NA        NA\n 6      7082        NA        NA        NA        NA        NA\n 7      7650        NA        NA        NA        NA        NA\n 8      7955        NA        NA        NA        NA        NA\n 9      7323        NA        NA        NA        NA        NA\n10      7774        NA        NA        NA        NA        NA\n# … with 192 more rows, and 12 more variables: EASERVED7 <int+lbl>,\n#   EASERVED8 <int+lbl>, EASERVED9 <int+lbl>, EASERVED10 <int+lbl>,\n#   EASERVED11 <int+lbl>, …\n\nUsing two of the dplyr functions discussed in our last post - summarize and across - we’ll get a better sense of the catchment areas for our public-sector SDPs. Let’s see how many missing values exist for each of these EASERVED variables:\ndplyr is included when you load library(tidyverse)\n\n\nsdp %>% \n  filter(AUTHORITY == 1) %>% \n  summarise(across(starts_with(\"EASERVED\"), ~sum(is.na(.x))))\n\n\n# A tibble: 1 x 18\n  EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n      <int>     <int>     <int>     <int>     <int>     <int>\n1         0       156       173       181       190       192\n# … with 12 more variables: EASERVED7 <int>, EASERVED8 <int>,\n#   EASERVED9 <int>, EASERVED10 <int>, EASERVED11 <int>, …\n\nWe see that every public facility serves at least one enumeration area (there are no missing values for EASERVED1). However, there are 156 missing values for EASERVED2, which tells us that 156 public facilities only serve one enumeration area. Likewise: 173 facilities serve 2 enumeration areas or fewer, 181 serve 3 or fewer, and so forth.\nWhat about the 32 non-public facilities?\n\n\nsdp %>% \n  filter(AUTHORITY != 1) %>% \n  summarise(across(starts_with(\"EASERVED\"), ~sum(is.na(.x))))\n\n\n# A tibble: 1 x 18\n  EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n      <int>     <int>     <int>     <int>     <int>     <int>\n1         4        32        32        32        32        32\n# … with 12 more variables: EASERVED7 <int>, EASERVED8 <int>,\n#   EASERVED9 <int>, EASERVED10 <int>, EASERVED11 <int>, …\n\nPMA receives no information about the service catchment areas for these facilities, so - as you might expect - there are 32 missing values for EASERVED2 onward. Note, however, that there are only 4 missing values for EASERVED1: for non-public facilities, EASERVED1 usually contains that same enumeration area code shown in EAID (this is the enumeration area where the facility is, itself, located).\nThe exception to this rule comes from facilities where CONSENTSQ shows that no respondent provided consent to be interviewed. If we’d like, we can copy EAID to EASERVED1 for these facilities using dplyr::case_when:\n\n\nsdp <- sdp %>% \n  mutate(EASERVED1 = case_when(\n    is.na(EASERVED1) ~ EAID,\n    T ~ as.double(EASERVED1)\n  ))\n\n\n\n\nWe coerce EASERVED1 as a double, matching the class provided by EAID.\nNow, every SDP has at least one enumeration area included in the EASERVED group. This will be important in our next step, where we’ll see how to summarize the SDP data by groups of facilities serving the same enumeration area.\nPivot Longer: EASERVED in Rows\nNow that we’re familiar with EASERVED variables, let’s take a look at the kinds of summary statistics we might want to construct from variables related to contraceptive service availability. For example, consider EMRGPROV, which indicates whether a facility provides emergency contraceptives to clients.\nRemember that, right now, each row of our SDP dataset represents responses from one facility per sample. We’ll ultimately want to count the number of facilities providing emergency contraceptives to clients in each enumeration area, so we should use the tidyr function pivot_longer to reshape the data in a way that repeats each facility’s response to EMRGPROV once for every enumeration area that it serves.\ntidyr is included when you load library(tidyverse)\nTake, for example, the first 5 facilities in our dataset: for now, let’s just look at the first two EASERVED variables, along with each facility’s FACILITYID, EAID, and EMRGPROV response:\n\n\nsdp %>% \n  slice(1:5) %>% \n   select(FACILITYID, EASERVED1, EASERVED2, EMRGPROV)\n\n\n# A tibble: 5 x 4\n  FACILITYID EASERVED1 EASERVED2  EMRGPROV\n   <int+lbl>     <dbl> <int+lbl> <int+lbl>\n1       7250      7380      7323   0 [No] \n2       7399      7879      7516   0 [No] \n3       7506      7483        NA   0 [No] \n4       7982      7185        NA   0 [No] \n5       7065      7725      7859   1 [Yes]\n\nAmong these 5 facilities, only facility 7065 provides emergency contraceptives. This facility happens to provide services to 2 enumeration areas: 7725 and 7859. When we use pivot_longer, we’ll reshape the data to emphasize a different conclusion: our example shows two enumeration areas where individuals can access emergency contraceptives. We convey this information by placing each enumeration area from EASERVED1 or EASERVED2 in its own row:\n\n\nsdp %>% \n  slice(1:5) %>% \n  select(FACILITYID, EASERVED1, EASERVED2, EMRGPROV) %>% \n  pivot_longer(\n    cols = starts_with(\"EASERVED\"),\n    values_to = \"EASERVED\",\n    names_to = NULL\n  )\n\n\n# A tibble: 10 x 3\n   FACILITYID  EMRGPROV  EASERVED\n    <int+lbl> <int+lbl> <dbl+lbl>\n 1       7250   0 [No]       7380\n 2       7250   0 [No]       7323\n 3       7399   0 [No]       7879\n 4       7399   0 [No]       7516\n 5       7506   0 [No]       7483\n 6       7506   0 [No]         NA\n 7       7982   0 [No]       7185\n 8       7982   0 [No]         NA\n 9       7065   1 [Yes]      7725\n10       7065   1 [Yes]      7859\n\n\nHere, values_to gives the name of a new column where we store the values. If we wanted, we could use names_to to create another column storing the original variable names (EASERVED1 and EASERVED2) for each value.\nNow, we find that each of the values previously stored in EASERVED1 and EASERVED2 appear in a new column, EASERVED. Each facility occupies two rows: one for each of the enumeration areas that it serves.\nWhat about the rows where EASERVED contains NA? These rows are meaningless: we’re repeating each facility’s response to EMRGPROV twice to represent two enumeration areas, but facilities 7506 and 7982 only serve one enumeration area apiece. We should include the argument values_drop_na = T to drop these rows when we use pivot_longer():\n\n\nsdp %>% \n  slice(1:5) %>% \n  select(FACILITYID, EASERVED1, EASERVED2, EMRGPROV) %>% \n  pivot_longer(\n    cols = starts_with(\"EASERVED\"),\n    values_to = \"EASERVED\",\n    names_to = NULL,\n    values_drop_na = T\n  )\n\n\n# A tibble: 8 x 3\n  FACILITYID  EMRGPROV  EASERVED\n   <int+lbl> <int+lbl> <dbl+lbl>\n1       7250   0 [No]       7380\n2       7250   0 [No]       7323\n3       7399   0 [No]       7879\n4       7399   0 [No]       7516\n5       7506   0 [No]       7483\n6       7982   0 [No]       7185\n7       7065   1 [Yes]      7725\n8       7065   1 [Yes]      7859\n\nNow that we know how to pivot_longer, let’s apply the function to our full dataset:\n\n\nsdp <- sdp %>%\n  pivot_longer(\n    cols = starts_with(\"EASERVED\"),\n    values_to = \"EASERVED\",\n    values_drop_na = T,\n    names_to = NULL\n  ) %>%\n  distinct() # in case any facility listed the same EASERVED twice\n\n\n\nDropping each row where EASERVED is missing, we’re left with 372 rows where information about each SDP gets repeated once for every enumeration area that it serves. (Remember: our original dataset contained only 234 rows because SDPs occupied just one row apiece).\n\n\nsdp %>% select(FACILITYID, EASERVED, everything())\n\n\n# A tibble: 372 x 58\n   FACILITYID EASERVED      SAMPLE COUNTRY  YEAR ROUND  EAID CONSENTSQ\n    <int+lbl> <dbl+lb>   <int+lbl> <int+l> <int> <dbl> <dbl> <int+lbl>\n 1       7250     7380 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 2       7250     7323 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 3       7250     7491 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 4       7250     7605 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 5       7250     7142 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 6       7250     7279 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 7       7250     7370 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 8       7250     7725 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 9       7250     7811 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n10       7250     7859 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n# … with 362 more rows, and 50 more variables: STRATA <int+lbl>,\n#   FACILITYTYPE <int+lbl>, FACILITYTYPEGEN <int+lbl>,\n#   AUTHORITY <int+lbl>, CONPROV <int+lbl>, …\n\nSummarise by EASERVED and SAMPLE\nNow that we’ve reshaped our data, we’ll be able to create some simple summary statistics about each of the enumeration areas served by the facilities in our sample. First, let’s group_by(EASERVED, SAMPLE) and count() the number of facilities providing services to each enumeration area in each of our samples:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\", \n    N_SDP = n()\n  )\n\n\n# A tibble: 149 x 3\n# Groups:   EASERVED, SAMPLE [149]\n    EASERVED                            SAMPLE N_SDP\n   <dbl+lbl>                         <int+lbl> <int>\n 1      7003 85405 [Burkina Faso 2017 Round 5]     3\n 2      7003 85408 [Burkina Faso 2018 Round 6]     2\n 3      7006 85405 [Burkina Faso 2017 Round 5]     2\n 4      7006 85408 [Burkina Faso 2018 Round 6]     1\n 5      7009 85405 [Burkina Faso 2017 Round 5]     3\n 6      7009 85408 [Burkina Faso 2018 Round 6]     2\n 7      7016 85405 [Burkina Faso 2017 Round 5]     3\n 8      7016 85408 [Burkina Faso 2018 Round 6]     2\n 9      7026 85405 [Burkina Faso 2017 Round 5]     3\n10      7026 85408 [Burkina Faso 2018 Round 6]     3\n# … with 139 more rows\n\nContinuing with the variable EMRGPROV, we can now also count the number of sampled facilities providing emergency contraception to each EASERVED:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    N_EMRGPROV = sum(EMRGPROV)\n  )\n\n\n# A tibble: 149 x 3\n# Groups:   EASERVED, SAMPLE [149]\n    EASERVED                            SAMPLE N_EMRGPROV\n   <dbl+lbl>                         <int+lbl>      <int>\n 1      7003 85405 [Burkina Faso 2017 Round 5]          0\n 2      7003 85408 [Burkina Faso 2018 Round 6]          0\n 3      7006 85405 [Burkina Faso 2017 Round 5]          0\n 4      7006 85408 [Burkina Faso 2018 Round 6]          0\n 5      7009 85405 [Burkina Faso 2017 Round 5]          2\n 6      7009 85408 [Burkina Faso 2018 Round 6]          1\n 7      7016 85405 [Burkina Faso 2017 Round 5]          0\n 8      7016 85408 [Burkina Faso 2018 Round 6]          0\n 9      7026 85405 [Burkina Faso 2017 Round 5]          0\n10      7026 85408 [Burkina Faso 2018 Round 6]          0\n# … with 139 more rows\n\nWhat if we want to include a count of the facilities providing each of the different contraceptive methods in our data? Building on a technique showcased in our last post, we could use dplyr::across to iterate over all variables ending with the suffix PROV:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~sum(.x), .names = \"N_{.col}\")\n  ) \n\n\n# A tibble: 149 x 15\n# Groups:   EASERVED, SAMPLE [149]\n   EASERVED      SAMPLE N_CONPROV N_CYCBPROV N_DEPOPROV N_DIAPROV\n   <dbl+lb>   <int+lbl>     <int>      <int>      <int>     <int>\n 1     7003 85405 [Bur…         3          3          3         0\n 2     7003 85408 [Bur…         2          2          2         0\n 3     7006 85405 [Bur…         2          2          2         0\n 4     7006 85408 [Bur…         1          1          1         0\n 5     7009 85405 [Bur…         3          1          3         0\n 6     7009 85408 [Bur…         2          2          2         0\n 7     7016 85405 [Bur…         3          3          3         0\n 8     7016 85408 [Bur…         2          1          2         0\n 9     7026 85405 [Bur…         3          3          3         0\n10     7026 85408 [Bur…         3          3          3         0\n# … with 139 more rows, and 9 more variables: N_EMRGPROV <int>,\n#   N_FCPROV <int>, N_FSTPROV <int>, N_FJPROV <int>, N_IMPPROV <int>,\n#   …\n\nWe’ll reduce this information even further, creating a variable NUM_METHODS_PROV indicating the number of methods provided by at least one sampled facility:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~sum(.x), .names = \"N_{.col}\")\n  ) %>% \n  transmute(\n    EASERVED,\n    SAMPLE,\n    NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")) > 0, na.rm = T)\n  )\n\n\n# A tibble: 149 x 3\n# Groups:   EASERVED, SAMPLE [149]\n    EASERVED                            SAMPLE NUM_METHODS_PROV\n   <dbl+lbl>                         <int+lbl>            <int>\n 1      7003 85405 [Burkina Faso 2017 Round 5]               10\n 2      7003 85408 [Burkina Faso 2018 Round 6]                8\n 3      7006 85405 [Burkina Faso 2017 Round 5]               10\n 4      7006 85408 [Burkina Faso 2018 Round 6]                8\n 5      7009 85405 [Burkina Faso 2017 Round 5]                9\n 6      7009 85408 [Burkina Faso 2018 Round 6]               10\n 7      7016 85405 [Burkina Faso 2017 Round 5]                9\n 8      7016 85408 [Burkina Faso 2018 Round 6]                8\n 9      7026 85405 [Burkina Faso 2017 Round 5]               10\n10      7026 85408 [Burkina Faso 2018 Round 6]               10\n# … with 139 more rows\n\nIn our last post, we introduced 4 variable groups related to the availability of different contraceptive methods. We’ll now create a summary variable for each one, and then show how to attach our new variables to a Household and Female dataset:\nN_SDP - number of SDPs\nNUM_METHODS_PROV - number of methods provided by at least one SDP\nNUM_METHODS_INSTOCK - number of methods in-stock with at least one SDP\nNUM_METHODS_OUT3MO - number of methods out of stock in the last 3 months with at least one SDP\nMEAN_OUTDAY - the mean length of a stockout for all out of stock methods\n\n\nsdp <- sdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    N_SDP = n(),\n    across(ends_with(\"PROV\"), ~sum(.x, na.rm = T), .names = \"N_{.col}\"),\n    across(ends_with(\"OBS\"), ~sum(.x, na.rm = T), .names = \"N_{.col}\"),\n    across(ends_with(\"OUT3MO\"), ~sum(.x, na.rm = T), .names = \"N_{.col}\"),\n    across(ends_with(\"OUTDAY\"), ~mean(.x, na.rm = T), .names = \"N_{.col}\"),\n  ) %>% \n  transmute(\n    EASERVED,\n    SAMPLE,\n    N_SDP,\n    NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")) > 0, na.rm = T),\n    NUM_METHODS_INSTOCK = sum(c_across(ends_with(\"OBS\")) > 0, na.rm = T),\n    NUM_METHODS_OUT3MO = sum(c_across(ends_with(\"OUT3MO\")) > 0, na.rm = T),\n    MEAN_OUTDAY = mean(c_across(ends_with(\"OUTDAY\")), na.rm = T)\n  ) %>% \n  ungroup()\n\n\n\nMerging to Household and Female Data\nConsider the following female respondent dataset collected from Burkina Faso in 2017 and 2018. It contains a variable FPCURRUSE indicating whether the woman is currently using a method of family planning:\n\n\nhhf <- read_ipums_micro(\n  ddi = \"data/pma_00011.xml\",\n  data = \"data/pma_00011.dat.gz\"\n) %>% \n  select(PERSONID, EAID, URBAN, SAMPLE, FPCURRUSE) %>% \n  mutate(\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    ))\n  )\n\n\n\n\n\nhhf\n\n\n# A tibble: 6,944 x 5\n   PERSONID           EAID    URBAN                   SAMPLE FPCURRUSE\n   <chr>             <dbl> <int+lb>                <int+lbl> <int+lbl>\n 1 0762000000029022…  7620 1 [Urba… 85405 [Burkina Faso 201…  NA      \n 2 0735800000017142…  7358 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 3 0710400000020992…  7104 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 4 0704800000014092…  7048 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n 5 0715600000020782…  7156 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 6 0727900000021452…  7279 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n 7 0743100000024642…  7431 0 [Rura… 85405 [Burkina Faso 201…   1 [Yes]\n 8 0721200000025792…  7212 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 9 0704200000014542…  7042 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n10 0797200000013032…  7972 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n# … with 6,934 more rows\n\nYou’ll notice that each row represents one female respondent with a unique PERSONID (non-respondents and other household members have been removed beforehand). We’ve also got EAID, which represents the enumeration area where each respondent resides; the variable URBAN indicates whether the enumeration area is primarily “urban” or “rural”.\nThe variable SAMPLE contains the same values seen in our SDP data:\n85405 - Burkina Faso 2017 Round 5\n85408 - Burkina Faso 2018 Round 6\nWhen we merge, we’ll want to match each woman to both a SAMPLE and an EASERVED from the SDP data. We’ll rename EASERVED to match the variable EAID in the HHF data:\n\n\nbf_merged <- sdp %>% \n  rename(EAID = EASERVED) %>% \n  right_join(hhf, by = c(\"EAID\", \"SAMPLE\"))\n\n\n\nNow, each woman’s record contains all of the variables we created above summarizing the SDPs that serve her enumeration area. For example, for all sampled women living in EAID == 7003 in 2017, the value in NUM_METHODS_OUT3MO shows the number of family planning methods that were out of stock with any SDP serving the woman’s enumeration area within three months prior to the survey:\n\n\nbf_merged %>% \n  filter(EAID == 7003, SAMPLE == 85405) %>% \n  select(PERSONID, EAID, SAMPLE, NUM_METHODS_OUT3MO)\n\n\n# A tibble: 55 x 4\n   PERSONID             EAID                  SAMPLE NUM_METHODS_OUT3…\n   <chr>            <dbl+lb>               <int+lbl>             <int>\n 1 070030000001973…     7003 85405 [Burkina Faso 20…                 0\n 2 070030000001973…     7003 85405 [Burkina Faso 20…                 0\n 3 070030000002640…     7003 85405 [Burkina Faso 20…                 0\n 4 070030000001075…     7003 85405 [Burkina Faso 20…                 0\n 5 070030000001609…     7003 85405 [Burkina Faso 20…                 0\n 6 070030000000835…     7003 85405 [Burkina Faso 20…                 0\n 7 070030000001273…     7003 85405 [Burkina Faso 20…                 0\n 8 070030000000527…     7003 85405 [Burkina Faso 20…                 0\n 9 070030000002561…     7003 85405 [Burkina Faso 20…                 0\n10 070030000002391…     7003 85405 [Burkina Faso 20…                 0\n# … with 45 more rows\n\nYou’ll notice that 55 women were surveyed in EAID 7003 in 2017, and each one has the same value (0) for NUM_METHODS_OUT3MO.\nWe’ll dig deeper into the types of research questions that our new combined dataset can answer in our upcoming Data Analysis post. For now, take a look at the apparent relationship between FPCURRUSE and NUM_METHODS_OUT3MO for all of the women with non-missing responses for both variables:\n\n\nbf_merged %>% \n  filter(!is.na(FPCURRUSE) & !is.na(NUM_METHODS_OUT3MO)) %>% \n  group_by(NUM_METHODS_OUT3MO > 0) %>% \n  count(FPCURRUSE) %>% \n  mutate(pct = n/sum(n))\n\n\n# A tibble: 4 x 4\n# Groups:   NUM_METHODS_OUT3MO > 0 [2]\n  `NUM_METHODS_OUT3MO > 0` FPCURRUSE     n   pct\n  <lgl>                    <int+lbl> <int> <dbl>\n1 FALSE                      0 [No]   2721 0.648\n2 FALSE                      1 [Yes]  1475 0.352\n3 TRUE                       0 [No]   1124 0.700\n4 TRUE                       1 [Yes]   482 0.300\n\nNotably, among those respondents living in an enumeration area that experienced zero stockouts within the 3 months prior to the SDP survey, 35% indicated that they were actively using a family planning method. Compare that to the set of respondents living in an area where at least one method was out of stock during the same time period: only 30% were using a family planning method.\nWhile a 5% difference may or may not prove to be statistically significant under further analysis, it’s not entirely surprising that the reliable availability of contraceptive methods from service providers might influence the contraceptive prevalence rate among women in a given area.\nAs always, let us know what kinds of questions about fertility and family planning you’re answering with data merged from service providers!\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-14T09:23:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-28-across-sdp/",
    "title": "Recode and Summarize Variables from Multiple Response Questions",
    "description": "Use dplyr::across to summarize variables with a similar naming pattern.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-28",
    "categories": [
      "Individuals in Context",
      "Data Manipulation",
      "Service Delivery Points",
      "across",
      "ipumsr"
    ],
    "contents": "\n\nContents\nSDP Multiple Response Questions\nSetup: Load an Example Dataset into R\nRecoding Variables with ipumsr\nIntroducing dplyr::across\nSummarize Variable Groups by Facility\nSummarize Variable Groups by EAID\n\n\n\n\nIn our last post, we introduced PMA Service Delivery Point (SDP) data as an important resource for understanding the health services environment experienced by individuals sampled in PMA Household and Female data. For our second post in this Individuals in Context series, we’ll now take deeper dive into one of the important topics in SDP data: the range and availability of contraceptive methods provided at each facility.\nA common feature of the variables in this topic - and in many other topics - is that you’ll find several binary indicators constructed from the same multiple resposne item on the SDP questionnaire. We’ll see that IPUMS PMA uses a common naming convention to help users group these variables together for use in functions like dplyr::across.\nSDP Multiple Response Questions\nEvery SDP respondent receives a question associated with the variable FPOFFERED, which indicates whether to facility usually offers family planning services or products:\nDo you usually offer family planning services / products?\n\n  [] Yes\n  [] No\n  [] No response\nIf yes, they’ll then receive a multiple response-type question asking about the contraceptive methods provided to clients. The range of options provided on the questionnaire may vary across samples, but most look something like this:\nWhich of the following methods are provided to clients at this facility? \n\n  [] Female sterilization\n  [] Male sterilization\n  [] Implant\n  [] IUD\n  [] Injectables - Depo Provera\n  [] Injectables - Sayana Press\n  [] Pill\n  [] Emergency Contraception\n  [] Male Condom\n  [] Female Condom\n  [] Diaphragm\n  [] Foam/Jelly\n  [] Std. Days / Cycle beads\n  [] None of the above\n  [] No response\n\nIf the response to FPOFFERED was not “Yes”, this question will be skipped and marked “NIU (not in universe)”.\nThis is a multiple response question: each method in the list could be answered individually (Yes or No), or the respondent could reply None of the above or provide No response. The IPUMS PMA extract system generates one variable for each of the methods in the list:\nFSTPROV\nMSTPROV\nIMPPROV\nIUDPROV\nDEPOPROV\nSAYPROV\nPILLPROV\nEMRGPROV\nCONPROV\nFCPROV\nDIAPROV\nFJPROV\nCYCBPROV\nThe questionnaire continues for each one of the methods provided at a given facility. Next, it checks for the current availability of each of the provided methods:\nYou mentioned that you typically provide the [METHOD] at this facility,\ncan you show it to me? If no, probe: Is the [METHOD] out of stock today?\n\n  [] In-stock and observed\n  [] In-stock but not observed\n  [] Out of stock\n  [] No Response\nThe variables associated with each response end with the same suffix OBS:\nIMPOBS\nIUDOBS\nDEPOOBS\nSAYOBS\nPILLOBS\nEMRGOBS\nCONOBS\nFCOBS\nDIAOBS\nFJOBS\nCYCBOBS\nSterilization methods were not included in this question.\nNote: if a given method was not provided at a facility, it would be skipped and marked “NIU (not in universe)”.\nYou can always visit a variable’s Universe tab for details.\nIf a facility did have a particular method in-stock, it received a question asking whether supplies were unavailable any time in the previous three months:\nHas the [METHOD] been out of stock at any time in the last 3 months?\n\n  [] Yes \n  [] No \n  [] Don't know\n  [] No response\nThis question becomes a series of variables ending with the suffix OUT3MO:\nIMPOUT3MO\nIUDOUT3MO\nDEPOOUT3MO\nSAYOUT3MO\nPILLOUT3MO\nEMRGOUT3MO\nCONOUT3MO\nFCOUT3MO\nDIAOUT3MO\nFJOUT3MO\nCYCBOUT3MO\nAgain, sterilization methods were not included in this question.\nNote: if a given method was not in-stock at a facility where it’s normally provided, it would be skipped and marked “NIU (not in universe)”.\nOn the other hand, if a facility that normally provides a given method did not have supplies in-stock during the interview, it received a different question about the duration of the current stockout:\nHow many days has the [METHOD] been out of stock?\n\n  Number of days____\nThe resulting variables - each ending with the suffix OUTDAY - take an integer value representing the stockout duration in days (except where the value is a non-response code, see below):\nIMPOUTDAY\nIUDOUTDAY\nDEPOOUTDAY\nSAYOUTDAY\nPILLOUTDAY\nEMRGOUTDAY\nCONOUTDAY\nFCOUTDAY\nDIAOUTDAY\nFJOUTDAY\nCYCBOUTDAY\nAgain, sterilization methods were not included in this question.\nNote: if a given method was in-stock at a facility where it’s normally provided, it would be skipped and marked “NIU (not in universe)”.\nSetup: Load an Example Dataset into R\nAs you can see, we’re left with quite a few variables from just these 4 questions! That’s very useful if you’re interested in the availability of one method, in particular, but what if you want to get a picture of the full range of methods provided at a particular facility?\nFortunately, the repeated use of variable suffixes (PROV, OBS, OUT3MO, and OUTDAY) make these variables highly suitable for column-wise processing with dplyr::across.\nLet’s start with an example data extract containing all of the variables listed above, collected from just two samples:\nBurkina Faso - 2018 R6\nBurkina Faso - 2017 R5\nOnce you’ve downloaded an extract, open RStudio and load the packages tidyverse and ipumsr:\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\n\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nNext, use the file paths for your data extract to load it into R:\n\n\nsdp <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00008.xml\",\n  data = \"data/pma_00008.dat.gz\"\n)\n\n\n\n\nRemember: change these file paths to match your own extract!\nUsing dplyr::ends_with, we’ll select only FACILITYID, SAMPLE, EAID, and the variables using one of the four suffixes PROV, OBS, OUT3MO, or OUTDAY.\n\n\nsdp <- sdp %>%  \n  select(\n    FACILITYID,\n    SAMPLE, \n    EAID, \n    ends_with(\"PROV\"),\n    ends_with(\"OBS\"),\n    ends_with(\"OUT3MO\"),\n    ends_with(\"OUTDAY\")\n  )\n\n\n\nThat leaves us with 234 rows - each a facility from one of our two samples - and 49 variables:\n\n\nsdp\n\n\n# A tibble: 234 x 49\n   FACILITYID          SAMPLE  EAID  CONPROV CYCBPROV DEPOPROV DIAPROV\n    <int+lbl>       <int+lbl> <dbl> <int+lb> <int+lb> <int+lb> <int+l>\n 1       7250 85405 [Burkina…  7142  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 2       7399 85405 [Burkina…  7879  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 3       7506 85405 [Burkina…  7483  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 4       7982 85405 [Burkina…  7185  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 5       7065 85405 [Burkina…  7859  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 6       7729 85405 [Burkina…  7082  1 [Yes]  0 [No]   1 [Yes]  0 [No]\n 7       7490 85405 [Burkina…  7650  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 8       7311 85405 [Burkina…  7955  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 9       7524 85405 [Burkina…  7323  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n10       7932 85405 [Burkina…  7774  1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n# … with 224 more rows, and 42 more variables: EMRGPROV <int+lbl>,\n#   FCPROV <int+lbl>, FSTPROV <int+lbl>, FJPROV <int+lbl>,\n#   IMPPROV <int+lbl>, IUDPROV <int+lbl>, MSTPROV <int+lbl>,\n#   PILLPROV <int+lbl>, SAYPROV <int+lbl>, CONOBS <int+lbl>,\n#   CYCBOBS <int+lbl>, DEPOOBS <int+lbl>, DIAOBS <int+lbl>,\n#   EMRGOBS <int+lbl>, FCOBS <int+lbl>, FJOBS <int+lbl>,\n#   IMPOBS <int+lbl>, IUDOBS <int+lbl>, PILLOBS <int+lbl>,\n#   SAYOBS <int+lbl>, CONOUT3MO <int+lbl>, CYCBOUT3MO <int+lbl>,\n#   DEPOOUT3MO <int+lbl>, DIAOUT3MO <int+lbl>, EMRGOUT3MO <int+lbl>,\n#   FCOUT3MO <int+lbl>, FJOUT3MO <int+lbl>, IMPOUT3MO <int+lbl>,\n#   IUDOUT3MO <int+lbl>, PILLOUT3MO <int+lbl>, SAYOUT3MO <int+lbl>,\n#   CONOUTDAY <int+lbl>, CYCBOUTDAY <int+lbl>, DEPOOUTDAY <int+lbl>,\n#   DIAOUTDAY <int+lbl>, EMRGOUTDAY <int+lbl>, FCOUTDAY <int+lbl>,\n#   FJOUTDAY <int+lbl>, IMPOUTDAY <int+lbl>, IUDOUTDAY <int+lbl>,\n#   PILLOUTDAY <int+lbl>, SAYOUTDAY <int+lbl>\n\nRecoding Variables with ipumsr\nA key feature to remember about IPUMS PMA extracts is that variables often have value labels, which are text labels assigned to the different values taken by a variable. When we load the extract into R with an ipumsr function, these variables are imported as labelled objects rather than the more common factor class of objects.\nMore information on the difference between factors and IPUMS labelled variables.\nAs a result, IPUMS data users need to take some unusual steps when recoding a variable or handling NA values. Happily, the ipumsr package provide a few functions (starting with the prefix lbl_) that make this process very easy.\nLet’s take a look at the variable CONOBS:\n\n\nsdp %>% count(CONOBS)\n\n\n# A tibble: 5 x 2\n                                    CONOBS     n\n*                                <int+lbl> <int>\n1  1 [In-stock and observed]                 204\n2  2 [In-stock but not observed]               3\n3  3 [Out of stock]                            5\n4 94 [Not interviewed (SDP questionnaire)]     4\n5 99 [NIU (not in universe)]                  18\n\nNotice that we have two values representing SDPs with male condoms “in-stock”: SDPs where the interviewer personally observed the condoms get 1, while those where condoms where reported in-stock - but not actually observed by the interviewer - get 2.\nDepending on your research question, the interviewer’s personal observation of each method may or may not be important. You might decide that you’d prefer to recode this variable into a simple binary measure that could be easily plugged into a regression model as a dummy variable later on. To do that, you could use the ipumsr function lbl_relabel:\n\n\nsdp %>% \n  mutate(CONOBS = lbl_relabel(\n      CONOBS,\n      lbl(1, \"In-stock\") ~ .val %in% 1:2,\n      lbl(0, \"Out of stock\") ~ .val == 3\n    )) %>% \n  count(CONOBS)\n\n\n# A tibble: 4 x 2\n                                    CONOBS     n\n*                                <dbl+lbl> <int>\n1  0 [Out of stock]                            5\n2  1 [In-stock]                              207\n3 94 [Not interviewed (SDP questionnaire)]     4\n4 99 [NIU (not in universe)]                  18\n\n\n\n\n© 2017 (MPL 2.0)\nThat collapses the values 1 and 2 together, and it moves the value 3 (“Out of stock”) to 0. However, we’ve still got a the values 94 and 99, which are each a different type of non-response. The easiest strategy here would be to recode any value larger than 90 as NA, and we could do that with another ipumsr function, lbl_na_if:\n\n\nsdp %>% \n  mutate(\n    CONOBS = lbl_relabel(\n      CONOBS,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    ),\n    CONOBS = lbl_na_if(\n      CONOBS,\n      ~.val > 90\n    )\n  ) %>% \n  count(CONOBS)\n\n\n# A tibble: 3 x 2\n             CONOBS     n\n*         <dbl+lbl> <int>\n1  0 [out of stock]     5\n2  1 [in-stock]       207\n3 NA                   22\n\nThis works great for our example variable, CONOBS. Unfortunately, though, we can’t always rely on the rule ~.val > 90 to handle missing responses. For variables like CONOUTDAY, a value above 90 could be a valid response: what if a facility experienced a stockout lasting 94 days? For this reason, the non-response values for CONOUTDAY are padded with additional digits:\n\n\nsdp %>% count(CONOUTDAY)\n\n\n# A tibble: 7 x 2\n                                   CONOUTDAY     n\n*                                  <int+lbl> <int>\n1    1                                           1\n2    3                                           1\n3   10                                           1\n4   15                                           1\n5   60                                           1\n6 9994 [Not interviewed (SDP questionnaire)]     4\n7 9999 [NIU (not in universe)]                 225\n\nWe could write a different lbl_na_if function for our OUTDAY variables, but ipumsr provides a much nicer workaround: we can specify non-response labels rather than values, as long as we make sure to use all of the different non-response labels appearing throughout our dataset:\n\n\nsdp %>% \n  mutate(\n    CONOBS = lbl_relabel(\n      CONOBS,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    ),\n    CONOBS = lbl_na_if(\n      CONOBS,\n      ~.lbl %in% c(\n        \"Not interviewed (SDP questionnaire)\",\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    )\n  ) %>% \n  count(CONOBS)\n\n\n# A tibble: 3 x 2\n             CONOBS     n\n*         <dbl+lbl> <int>\n1  0 [out of stock]     5\n2  1 [in-stock]       207\n3 NA                   22\n\nNow, we’ll be able to recode all of our variables with the same pair of functions! To do that, we’ll first need to take a look at the column-wise workflow made available by dplyr::across.\nIntroducing dplyr::across\nWhile there are several ways to apply a function across a set of variables in R, the simplest method comes from a new addition to the dplyr package that’s loaded when you run library(tidyverse). The function dplyr::across takes two arguments: a function, and a selection of columns where you want that function to be applied.\ndplyr is included when you load library(tidyverse)\nRemember that we want collapse the values 1 - In-stock and observed and 2 - In-stock but not observed for all of the variables ending with OBS, not just CONOBS. Using across and a selection of variables ending with OBS, we’ll apply the same lbl_relabel function we used on CONOBS above:\n\n\nsdp %>% \n  mutate(\n    across(ends_with(\"OBS\"), ~lbl_relabel(\n      .x,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    )) \n  ) %>% \n  count(CONOBS)\n\n\n# A tibble: 4 x 2\n                                    CONOBS     n\n*                                <dbl+lbl> <int>\n1  0 [out of stock]                            5\n2  1 [in-stock]                              207\n3 94 [Not interviewed (SDP questionnaire)]     4\n4 99 [NIU (not in universe)]                  18\n\n\n\n\n© 2018 RStudio (CC0 1.0)\nHere, we stick lbl_relabel inside a lambda function with syntax from purrr: the ~ designates a tidy lambda function, which in turn uses .x as a kind of pronoun referencing each of the variables returned by ends_with(\"OBS\"). We’re showing that CONOBS still gets recoded as before, but so do all of the other variables in its group!\nWe’ll use across again with lbl_na_if, but this time we want to produce NA values for all of the variables in our dataset. In place of ends_with(\"OBS\"), we’ll use the selection function everything(). This will take care of all the recoding we want to do, so we’ll also reassign our data with sdp <- sdp:\n\n\nsdp <- sdp %>% \n  mutate(\n    across(ends_with(\"OBS\"), ~lbl_relabel(\n      .x,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    )),\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"Not interviewed (SDP questionnaire)\",\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    ))\n  )\n\n\n\nLet’s pick a few variables to check out work:\n\n\nsdp %>% count(CONOBS)\n\n\n# A tibble: 3 x 2\n             CONOBS     n\n*         <dbl+lbl> <int>\n1  0 [out of stock]     5\n2  1 [in-stock]       207\n3 NA                   22\n\nsdp %>% count(IMPOBS)\n\n\n# A tibble: 3 x 2\n             IMPOBS     n\n*         <dbl+lbl> <int>\n1  0 [out of stock]     3\n2  1 [in-stock]       204\n3 NA                   27\n\nsdp %>% count(CONOUTDAY)\n\n\n# A tibble: 6 x 2\n  CONOUTDAY     n\n* <int+lbl> <int>\n1         1     1\n2         3     1\n3        10     1\n4        15     1\n5        60     1\n6        NA   229\n\nsdp %>% count(IMPOUTDAY)\n\n\n# A tibble: 4 x 2\n  IMPOUTDAY     n\n* <int+lbl> <int>\n1         1     1\n2        14     1\n3        90     1\n4        NA   231\n\nSummarize Variable Groups by Facility\nEverything looks great! Now that we’ve finished reformatting the data, remember that our ultimate goal is to get some sense of the scope of methods available at a particular facility.\nWe’d like to use something like across again here, but this time we’ll only want to apply our function to a selection of columns within the same row (because each row of our dataset represents one facility). To do this, we’ll divide the dataset rowwise, and then use the related function c_across to apply a calculation across columns within each row.\nFor instance, suppose we want to create NUM_METHODS_PROV to show the total number of methods provided at each facility. Let’s look at the PROV variables for the first few facilities:\n\n\nsdp %>% select(ends_with(\"PROV\"))\n\n\n# A tibble: 234 x 13\n    CONPROV  CYCBPROV DEPOPROV DIAPROV EMRGPROV  FCPROV FSTPROV FJPROV\n   <int+lb> <int+lbl> <int+lb> <int+l> <int+lb> <int+l> <int+l> <int+>\n 1  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 2  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 3  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n 4  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n 5  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  1 [Yes] 1 [Yes] 1 [Yes] 0 [No]\n 6  1 [Yes]   0 [No]   1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 7  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n 8  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 9  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n10  1 [Yes]   1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n# … with 224 more rows, and 5 more variables: IMPPROV <int+lbl>,\n#   IUDPROV <int+lbl>, MSTPROV <int+lbl>, PILLPROV <int+lbl>,\n#   SAYPROV <int+lbl>\n\nTo calculate NUM_METHODS_PROV, we can just find the sum of values across all of the PROV variables (thanks to our recoding work, the only possible values here are 1 for “yes”, or 0 for “no”). Notice that c_across takes only one argument: a selection function like ends_with(\"PROV\"). That’s because c_across works like the familiar concatenate function c() used to provide a vector of values to a function like sum(c(1,2,3)).\nFirst, use rowwise() to signal that we’ll only calculate the sum across variables in the same row. Then, use c_across() to find the sum() of PROV variables in each row:\n\n\nsdp %>% \n  rowwise() %>% \n  transmute(NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")), na.rm = T))\n\n\n# A tibble: 234 x 1\n# Rowwise: \n   NUM_METHODS_PROV\n              <int>\n 1               10\n 2               10\n 3                8\n 4                8\n 5               10\n 6                9\n 7                8\n 8               10\n 9                8\n10                8\n# … with 224 more rows\n\nWe can now create a summary variable for each of the four variable groups. Let’s create:\nNUM_METHODS_PROV - number of methods provided\nNUM_METHODS_INSTOCK - number of methods in-stock\nNUM_METHODS_OUT3MO - number of methods out of stock in the last 3 months\nMEAN_OUTDAY - the mean length of a stockout for all out of stock methods\n\n\nsdp %>% \n  rowwise() %>% \n  transmute(\n    NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")), na.rm = T),\n    NUM_METHODS_INSTOCK = sum(c_across(ends_with(\"OBS\")), na.rm = T),\n    NUM_METHODS_OUT3MO = sum(c_across(ends_with(\"OUT3MO\")), na.rm = T),\n    MEAN_OUTDAY = mean(c_across(ends_with(\"OUTDAY\")), na.rm = T)\n  )\n\n\n# A tibble: 234 x 4\n# Rowwise: \n   NUM_METHODS_PROV NUM_METHODS_INSTOCK NUM_METHODS_OUT3MO MEAN_OUTDAY\n              <int>               <dbl>              <int>       <dbl>\n 1               10                   8                  0         NaN\n 2               10                   7                  0           8\n 3                8                   8                  0         NaN\n 4                8                   8                  0         NaN\n 5               10                   9                  0         NaN\n 6                9                   7                  0         NaN\n 7                8                   7                  0         365\n 8               10                   8                  1         NaN\n 9                8                   8                  1         NaN\n10                8                   7                  0          30\n# … with 224 more rows\n\nMEAN_OUTDAY is NaN (not a number) if no methods were out of stock.\nSummarize Variable Groups by EAID\nIn our last post, we mentioned that the best use case for SDP data is to aggregate information collected from facilities working in the same geographic sampling units - or enumeration areas - used to select individuals for PMA Household and Female samples. In our next post, we’ll take a close look at the variable group EASERVED, which lists all of the enumeration area codes where a facility is known to provide health services. We’ll then introduce a strategy using tidyr::pivot_longer to summarize the full scope of services available to women living in a particular enumeration area.\nFor now, let’s simply consider all of the sampled facilities located in a particular enumeration area. That is, rather than calculating the number of methods provided by one facility NUM_METHODS_PROV, let’s create one variable for each method indicating whether the method was provided by at least one facility in a given enumeration area EAID in a given SAMPLE.\nFor instance, look at the number of facilities providing IUDs in enumeration area 7111 for the Burkina Faso sample collected in 2017:\n\n\nsdp %>% \n  filter(EAID == 7111, SAMPLE == 85405) %>% \n  select(EAID, SAMPLE, FACILITYID, PILLPROV)\n\n\n# A tibble: 4 x 4\n   EAID                            SAMPLE FACILITYID  PILLPROV\n  <dbl>                         <int+lbl>  <int+lbl> <int+lbl>\n1  7111 85405 [Burkina Faso 2017 Round 5]       7210   1 [Yes]\n2  7111 85405 [Burkina Faso 2017 Round 5]       7029   0 [No] \n3  7111 85405 [Burkina Faso 2017 Round 5]       7441   1 [Yes]\n4  7111 85405 [Burkina Faso 2017 Round 5]       7403   1 [Yes]\n\nWe want to use a summarize function to create a variable like ANY_PILLPROV, which should simply indicate whether any of these four facilities provide contraceptive pills. Three of them do provide pills, so we want ANY_PILLPROV to be TRUE.\n\n\nsdp %>% \n  filter(EAID == 7111, SAMPLE == 85405) %>% \n  summarize(ANY_PILLPROV = any(PILLPROV == 1))\n\n\n# A tibble: 1 x 1\n  ANY_PILLPROV\n  <lgl>       \n1 TRUE        \n\nNow that we’re familiar with across, we should be able to do the same thing to all PROV variables for this particular group of facilities. Let’s also introduce a naming convention where we glue the prefix ANY_ to the column name referenced by the pronoun .x:\n\n\nsdp %>% \n  filter(EAID == 7111, SAMPLE == 85405) %>% \n  summarize(across(ends_with(\"PROV\"), ~any(.x == 1), .names = \"ANY_{.col}\"))\n\n\n# A tibble: 1 x 13\n  ANY_CONPROV ANY_CYCBPROV ANY_DEPOPROV ANY_DIAPROV ANY_EMRGPROV\n  <lgl>       <lgl>        <lgl>        <lgl>       <lgl>       \n1 TRUE        TRUE         TRUE         FALSE       FALSE       \n# … with 8 more variables: ANY_FCPROV <lgl>, ANY_FSTPROV <lgl>,\n#   ANY_FJPROV <lgl>, ANY_IMPPROV <lgl>, ANY_IUDPROV <lgl>,\n#   ANY_MSTPROV <lgl>, ANY_PILLPROV <lgl>, ANY_SAYPROV <lgl>\n\n\nIt looks like none of the sampled facilities in enumeration area 7111 provided emergency contraception in 2017. This could be very important context for understanding the health services available to women sampled from that area!\nLet’s repeat the same procedure for every enumeration area in each of our samples. Rather than using a filter to select one EAID in one SAMPLE, we’ll use group_by to work with each EAID in each SAMPLE.\n\n\nsdp %>% \n  group_by(EAID, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~any(.x == 1), .names = \"ANY_{.col}\")\n  )\n\n\n# A tibble: 142 x 15\n# Groups:   EAID, SAMPLE [142]\n    EAID      SAMPLE ANY_CONPROV ANY_CYCBPROV ANY_DEPOPROV ANY_DIAPROV\n   <dbl>   <int+lbl> <lgl>       <lgl>        <lgl>        <lgl>      \n 1  7003 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n 2  7003 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 3  7006 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n 4  7006 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 5  7009 85405 [Bur… TRUE        FALSE        TRUE         FALSE      \n 6  7009 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 7  7016 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n 8  7016 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 9  7026 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n10  7042 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n# … with 132 more rows, and 9 more variables: ANY_EMRGPROV <lgl>,\n#   ANY_FCPROV <lgl>, ANY_FSTPROV <lgl>, ANY_FJPROV <lgl>,\n#   ANY_IMPPROV <lgl>, ANY_IUDPROV <lgl>, ANY_MSTPROV <lgl>,\n#   ANY_PILLPROV <lgl>, ANY_SAYPROV <lgl>\n\nThis is still quite a bit of information! Suppose we want to summarize it even further: let’s calculate NUM_METHODS_PROV again with our summary output. This time, NUM_METHODS_PROV will count the number of methods provided by at least one facility in each group.\n\n\nsdp %>% \n  group_by(EAID, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~any(.x == 1), .names = \"ANY_{.col}\")\n  ) %>% \n  transmute(NUM_METHODS_PROV= sum(c_across(ends_with(\"PROV\")), na.rm = T))\n\n\n# A tibble: 142 x 3\n# Groups:   EAID, SAMPLE [142]\n    EAID                            SAMPLE NUM_METHODS_PROV\n   <dbl>                         <int+lbl>            <int>\n 1  7003 85405 [Burkina Faso 2017 Round 5]                8\n 2  7003 85408 [Burkina Faso 2018 Round 6]                8\n 3  7006 85405 [Burkina Faso 2017 Round 5]                8\n 4  7006 85408 [Burkina Faso 2018 Round 6]                8\n 5  7009 85405 [Burkina Faso 2017 Round 5]                8\n 6  7009 85408 [Burkina Faso 2018 Round 6]               10\n 7  7016 85405 [Burkina Faso 2017 Round 5]                8\n 8  7016 85408 [Burkina Faso 2018 Round 6]                8\n 9  7026 85405 [Burkina Faso 2017 Round 5]                8\n10  7042 85405 [Burkina Faso 2017 Round 5]               10\n# … with 132 more rows\n\nThese summaries are exactly the type of SDP data we’d like to attach to a Household and Female dataset! Watch for our next post, where we’ll show how to create summaries by both EAID and EASERVED, and then match them to records from female respondents sampled from Burkina Faso in 2017 and 2018.\n\n\n\n",
    "preview": "posts/2021-01-28-across-sdp/images/logos.png",
    "last_modified": "2021-05-14T09:23:13-05:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 540
  },
  {
    "path": "posts/2021-01-26-sdp-data/",
    "title": "Service Delivery Point Data Explained",
    "description": "SDP samples are not nationally representative. Learn how to use them to describe the health service environment experienced by individuals.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-26",
    "categories": [
      "Individuals in Context",
      "Data Discovery",
      "Service Delivery Points"
    ],
    "contents": "\n\nContents\nWhat is an SDP?\nSurvey Topics\nSample Design\n\n\n\n\nWhen you visit pma.ipums.org and begin browsing data, you’ll notice that PMA data are available for several different units of analysis.\nYou can see which unit of analysis you’re currently browsing - or switch to a different unit of analysis - in this box:\n\n\n\nClick CHANGE, and you’ll see the different units of analysis that are available:\n\n\n\nThis Data Discovery post kicks off a series of posts all about the data available for the Family Planning - Service Delivery Point unit of analysis. As you’ll see, these data are meant to provide important context for the individuals included in the Family Planning - Person series: while SDP data are not nationally representative, they can help provide a rich portrait of the health service environment experienced by women and households.\nYou’ll find more blog posts about SDP data by following the Individuals in Context series. Look for upcoming posts about:\nWorking with variable groups created from multiple response questions\nMerging SDP summary data with Household and Female data\nMapping SDP Data with GPS Data from our partners at pmadata.org\nMerging SDP Data with spatial datasets from external sources\nAn example of the sort of spatial analysis you can perform with SDP data\nWhat is an SDP?\nA Service Delivery Point (SDP) is any type of facility that provides health services to a community: you’ll find a breakdown of the available facility types for each sample listed in FACILITYTYPE. Because countries may include regionally-specific facility types, we’ve integrated major groupings together in the variable FACILITYTYPEGEN. For example, you may find SDP data available from any of these general facility types:\nHospitals\nHealth Centers\nHealth Clinics\nOther Health Facilities\nPrivate Practices\nDispensaries\nPharmacies / Chemists / Drug Shops\nBoutiques / Shops\nOther\nPMA samples SDPs managed by governments, NGOs, faith-based organizations, private sector organizations, and a range of other institutions. You’ll find the managing authority for each SDP listed in AUTHORITY.\nSurvey Topics\nWhile all SDP surveys cover similar topics, individual questions may be posed somewhat differently - or not at all - for any given sample. That’s where IPUMS PMA comes in: we harmonize differences across samples and document the availability of every variable for each sample.\n\nYou’ll find the full text PDF of the original questionnaire administered to all SDPs in a particular sample here.\nIPUMS PMA also organizes SDP variables by topic. These topics currently include:\nFacility Characteristics\nGeneral Facility Characteristics\nGeography\nAreas Served\nStaffing\nMedical Equipment\nFunding\nManagement\nPerformance Feedback\nQuality of Care\nService Statistics\nMedical Records\nTransportation\n\nFamily Planning Services\nServices Provided\nContraceptive Stock\nReason for Stockout\nClients Served\nStock Supplier\nFees\nFacility Condition\n\nOther Health Services\nAbortion\nPost-abortion Care\nSTDs\nAntenatal Care\nLabor and Delivery\nPostpartum Care\nDelivery Medicines\nCommunity Health Workers\nVaccinations\nHealth Programs\nMedicines in Stock\nOther\n\nThese are listed in the TOPICS menu and are subject to growth & reorganization.\n\n\n\nAdditionally, there are a number of technical variables related to survey administration. For example, every SDP included in the sample frame receives a unique FACILITYID (this ID is preserved across survey rounds if a facility is surveyed more than once). However, some facilities never responded to the questionnaire if, for example, no individual respondent was present, competent, and available to be interviewed (see AVAILABLESQ); if no such person was available - or if such a person declined the interview - the variable CONSENTSQ will indicate that survey consent was never obtained. The variable RESULTSQ indicates whether the questionnaire was fully completed or, if not, it provides the reason.\nFor SDPs where CONSENTSQ is “No”, most variables will take the value “Not interviewed (SDP questionnaire)”.\nNote that the value “NIU (not in universe)” pertains to SDPs that were intentionally skipped because a question was deemed out-of-scope.\nYou may choose whether to include SDPs where RESULTSQ indicates that the questionnaire was not fully completed. Click CREATE DATA EXTRACT from you Data Cart:\n\n\n\nThen click CHANGE next to Sample Members:\n\n\n\nFinally, choose whether to include only “Facility Respondents” (those who fully completed the questionnaire), or “All Cases” instead:\n\n\n\nSample Design\nSo what conclusions can you draw from SDP data? First, it’s important to note that the SDP sample design is not nationally representative, and there are no sampling weights for SDP data.1 In other words, it is not possible to get a sense of the national health services profile in a particular country using SDP data.\nInstead, facilities were selected for the SDP survey using the same geographic enumeration areas used to select households for each Household and Female survey. To see how this works, let’s look at an example dataset collected from Burkina Faso in 2017, beginning with the set of female respondents to the Household questionnaire (other household members and female non-respondents have been excluded):\nRead more about the Household and Female sampling strategy.\n\n\nlibrary(tidyverse)\n\nbf17_hhf <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00011.xml\",\n  data = \"data/pma_00011.dat.gz\") %>% \n  filter(YEAR == 2017)\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nTo use this code, be sure to change both file paths to match your own extract!\nThe Dataset Notes for this sample describe a two-stage cluster design with urban-rural strata, producing a sample of women from 83 enumeration areas. If we count the number of unique values from EAID in our data, we see that there are 83 unique identification numbers - one for each enumeration area:\n\n\nn_distinct(bf17_hhf$EAID)\n\n\n[1] 83\n\nWe can also see how these enumeration areas are distributed throughout the 13 administrative regions of Burkina Faso. Note that we have more enumeration areas in the Central region (including the capital, Ouagadougou), and we have fewer enumeration areas in regions where the population is lower (Centre-Sud, Plateau-Central, Sud-Ouest, etc.):\n\n\nbf17_hhf %>% \n  group_by(GEOBF) %>% \n  summarize(.groups = \"keep\", n_EAID = n_distinct(EAID)) %>% \n  arrange(n_EAID)\n\n\n# A tibble: 13 x 2\n# Groups:   GEOBF [13]\n                    GEOBF n_EAID\n                <int+lbl>  <int>\n 1  7 [Centre-Sud]             2\n 2 11 [Plateau-Central]        3\n 3 13 [Sud-Ouest]              3\n 4  2 [Cascades]               4\n 5 12 [Sahel]                  4\n 6  5 [Centre-Nord]            5\n 7  4 [Centre-Est]             6\n 8 10 [Nord]                   6\n 9  1 [Boucle du Mouhoun]      7\n10  6 [Centre-Ouest]           7\n11  8 [Est]                    7\n12  9 [Hauts-Bassins]         11\n13  3 [Centre]                18\n\nAlthough the same number of households are randomly selected from within each enumeration area (typically 35), this concentration of enumeration areas within population-dense regions helps to ensure that the Household and Female data are nationally representative.\nLet’s now look at the sample of SDPs collected from Burkina Faso in that same year:\n\n\nbf17_sdp <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00008.xml\",\n  data = \"data/pma_00008.dat.gz\") %>% \n  filter(YEAR == 2017)\n\n\n\n\nRemember: to use this code, be sure to change both file paths to match your own extract!\nDataset Notes for the SDP sample explain that the same 83 enumeration areas used in the Household and Female Sample were used to select facilities for the SDP sample. Moreover, we can confirm that all of enumeration areas in the SDP data also appear in the HHF data:\n\n\nall(bf17_sdp$EAID %in% bf17_hhf$EAID)\n\n\n[1] TRUE\n\nBut is the reverse true? Is every enumeration area from the Household and Female Sample represented in the SDP data?\n\n\nall(bf17_hhf$EAID %in% bf17_sdp$EAID)\n\n\n[1] FALSE\n\nPerhaps surprisingly, the answer is no. To learn why, we have to dig a bit deeper into the SDP Dataset Notes. There, we see that a facility located within the physical boundaries of one of the 83 enumeration areas from the Household and Female Survey would have been included in the SDP sample. However, there may be enumeration areas - particularly in remote areas - where no facilities are located.\nFortunately, PMA also includes data about the service catchment area for some facilities.2 You can include this information by selecting the variable series EASERVED. If a given facility serves more than one enumeration area, EASERVED1 will contain the enumeration area ID code for the first enumeration area on its catchment list, EASERVED2 will contain the ID code for the second one, and so forth. If that same facility serves 5 enumeration areas, the variables EASERVED6, EASERVED7, and so forth would be “NIU (not in universe)”.\n\nThe IPUMS PMA extract system automatically determines the right maximum number of EASERVED variables by finding the facility with the largest service catchment list in your extract.\nWhat does this mean? As we’ll show in an upcoming post in this series, it’s possible to create a portrait of the health service environment provided to individuals sampled in the Household and Female surveys. This portrait extends beyond the list of facilities located in an individual’s geographic enumeration area, but users should take care to understand that the scope of facilities providing services to that enumeration area is somewhat limited by sample design.\n\nThe files do contain a weight variable for the sampling units EAWEIGHT, which is a probability weight representing the likelihood of an enumeration area being selected for sampling. The collectors of the original data do not recommend using EAWEIGHT to weight SDP variables.↩︎\nThis information is only available for SDPs where the managing authority listed in AUTHORITY is “government”.↩︎\n",
    "preview": "posts/2021-01-26-sdp-data/images/choose-unit.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 909,
    "preview_height": 632
  },
  {
    "path": "posts/2021-02-02-blogging-with-rmarkdown/",
    "title": "Blogging with RMarkdown",
    "description": "Quick tips for authoring and editing blog posts with RMarkdown",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-25",
    "categories": [],
    "contents": "\n\nContents\nCreating a blog post\nYAML MetadataTitle\nDescription\nCategories\nAuthor\nDate\nOutput (Table of Contents)\nPreview\n\nCode chunksChunk options\nData Visualizations from chunks\n\nFormatted TextHeadings\nBold and Italics\nInline code (variable names, packages, functions, etc)\nHyperlinks\nAsides and Footnotes\n\n\nRMarkdown documents (.Rmd) work just like regular R scripts (.R) in that you can use them as a space to develop code before sending it to the R Console. The main difference is that an RMarkdown file breaks code into discrete “chunks” of code that can be separated by blocks of text. When you run an RMarkdown document, the Console ignores anything that’s not included in a code chunk (so there’s no need to use the comment indicator #).\nRMarkdown is a powerful tool for sharing and teaching R code, but it has become even more useful with the advent of packages like knitr, which can transform RMarkdown files into Word documents, PDFs, sideshows, HTML pages, and more. The PMA Data Hub is built with knitr and another package called distill, which transforms RMarkdown files into fully formatted blog posts.\nIf you’ve never used RMarkdown, knitr, or distill before, you’ll need to install them with your R Console now:\n\n\ninstall.packages(\"RMarkdown\")\ninstall.packages(\"knitr\")\ninstall.packages(\"distill\")\n\n\n\nCreating a blog post\nIn this post, we’ll assume that you’ve already reviewed the Blog Post Workflow, so you’re familiar with the process for creating a new blog post on a new branch of the Git repository.\nAssuming you’ve created a new branch for your post, you’ll create and open an RMarkdown file when you run:\n\n\ndistill::create_post(\"Getting started with RMarkdown\")\n\n\n\nAt this point, RStudio should look something like this:\n\n\n\n\nCircled in red: notice that a new folder was created in the \"_posts\" directory shown in the Files tab, and your new files also are now being tracked in the Git tab.\nAn RMarkdown file opens with a template showing a YAML metadata header, a code chunk called “setup”, and some boilerplate text.\nYAML Metadata\nYour RMarkdown template contains a header consisting of a series of key: \"value\" pairs written in YAML. This is where we store metadata for each article appearing both at 1) the top of every blog post, and 2) on the blog homepage.\nHere’s an example of a complete YAML header for a post on the Data Hub:\n\n\n\nTitle\nThis will be the main Title shown in CSS style H1 at the top of your post. We automatically reformat to all-caps, so this is not case-sensitive.\nPlease do not use sentence punctuation (unless your title is a question).\nEnsure that your Title matches the “H1 Title” on the Data Hub Tracking Sheet.\nDescription\nThis is the subtitle shown in CSS style H2 (just below the H1 Title). This subtitle is case-senstive.\nPlease do not use sentence punctuation (unless your subtitle is a question).\nTry your best to avoid repeating the subtitle pattern “How to X”. Subtitles should emphasize the importance of a post in the particular context of analyzing PMA data if possible.\nCategories\nThe are the “tags” that will help readers filter posts and navigate through different thematic modules. What tags should you include?\nModule name (Column A of the tracking sheet)\nPost type (Column B of the tracking sheet)\nImportant package functions (package::function) or techniques\nAnalysis tools\nAlways check to see if your tags have been used in a previous post and, if so, make sure to match their existing style, spelling, etc.\nAuthor\nAlways include your name and your affiliation with the project. Optionally, feel free to link to a personal website or social media account!\nExamples:\n\nauthor:\n  - first_name: \"Yihui\"\n    last_name: \"Xie\"\n    url: https://github.com/yihui\n    affiliation: RStudio\n    affiliation_url: https://www.rstudio.com\n    orcid_id: 0000-0003-0645-5666\n  - name: \"JJ Allaire\"\n    url: https://github.com/jjallaire\n    affiliation: RStudio\n    affiliation_url: https://www.rstudio.com\n  - name: \"Rich Iannone\"\n    url: https://github.com/rich-iannone\n    affiliation: RStudio\n    affiliation_url: https://www.rstudio.com\n\nDate\nAdmin will update this to reflect date of publication.\nOutput (Table of Contents)\nThe PMA Data Hub uses a floating table of contents that follows up to 3 heading depths. Please format exactly as shown:\n\n\noutput:\n  distill::distill_article:\n    self_contained: false\n    toc: true\n    toc_depth: 3\n    toc_float: true\n\n\n\nPreview\nThis is the image that will appear alongside your post in the blog homepage. If you don’t specify an image, distill will automatically select the first image in your post; if there are no images, the space will be left blank (but please consider using one!)\nThe file path to your image should be relative to the RMarkdown file, itself. For example:\n\n\n\nThis shows the folder “2020-12-09-mapping-contraceptive-stockouts” in the \"_posts\" folder. The .Rmd file lives in the top level of the folder, and the desired image Rlogo.png lives in the images subfolder. The correct way to reference the image is:\n\n\npreview: images/static-map.png\n\n\n\nCode chunks\nEach chunk of code in your RMarkdown file must be offset as shown here (note the tic-marks and “r” in curly brackets):\n\n\n\n\nThe “r” tells R to interpret this chunk as R code (RMarkdown also supports other languages like Python, Julia, C++, and SQL).\nYou can quickly insert a code chunk via the Code menu in RStudio’s menu bar, or by using the keyboard shortcut shown there (e.g. command + option + I for mac users).\nChunk options\nYou can set specific rendering instructions for RMarkdown inside the curly brackets. Some common options include:\n\nr, eval = F # don't run the code in this chunk\nr, echo = F # hide the code, but not the results \nr, eval = T, echo = F # combine options with commas like this\nr, error = F, message = F, warning = F # hide errors, messages, & warnings\n\nA full list of chunk options are explained here.\nYou can also set default options for all of your code chunks at the top of your RMarkdown document (see the setup chunk that opens in a new template):\n\n\nknitr::opts_chunk$set(\n  echo = TRUE, \n  eval = FALSE,\n  error = FALSE,\n  message = FALSE, \n  warning = FALSE\n)\n\n\n\nData Visualizations from chunks\nThe Distill website explains how to format figures, tables, and diagrams via code chunk arguments.\nFormatted Text\nHere, we’ll show some examples for adding formatted text to the body of your RMarkdown file (i.e. everything that not included in the YAML header or a code chunk).\nHeadings\nUse the # symbol once for an H1 heading, twice ## for an H2 heading, or three times ### for an H3 heading.\nLook at the table of contents for this page: H1 headings are left aligned, and H2 headings are indented once; any H3 headings would be indented twice if we had them.\nBold and Italics\nItalics are offset by one * like this:\n\n*Italics* are offset by one `*` like this:\n\nBold text is offset by two ** like this:\n\n**Bold** text is offset by two `**` like this:\n\nInline code (variable names, packages, functions, etc)\nWe use a particular font for code chunks, and this font also gets applied in the text body to any mention of a variable name, package, or function - basically anything that might appear in the console.\nNote: the first time you use a variable name, package, or function in a post, it’s usually best in include a hyperlink to the underlying documentation. When you insert a hyperlink, do not offset text for inline code.\nInline code is offset by one ` like this:\n\n`Inline code` is offset by one ` like this:\n\nHyperlinks\nA hyperlink should be used the first time you mention a variable, package, function, or anything else that has underlying documentation at an external source.\nA hyperlink can be inserted like this:\nA [hyperlink](http://bitly.com/98K8eH) can be inserted like this:\nIf you want to link to another page on the PMA Data Hub, use relative links (do not include the full path). For example:\nA relative path to the ABOUT page:\nA relative path to the [ABOUT](about.html) page:\nOr, a relative path to one of the blog posts:\nOr, a relative path to one of the [blog posts](posts/2020-12-09-mapping-contraceptive-stockouts/index.html)\nHere is a relative path to one of the headings on that blog post:\nHere is a relative path to one of the [headings](posts/2020-12-09-mapping-contraceptive-stockouts/index.html#shiny-application) on that blog post\nAsides and Footnotes\nAsides are designed for very brief comments rendered to the side of the text body. They must be offset with <aside> tags like this:\n<aside>\nFYI: formatted text in an \"aside\" must use <b>HTML tags<\/b>\n<\/aside>\n\nFYI: formatted text in an “aside” must use HTML tags\nAsides are associated with a particular paragraph or code chunk, so they will create white-space in the text body if they become longer than their partner! For longer comments, consider using a footnote.1\nInsert a footnote here^[This is my footnote]\n\nFootnotes appear as hover-text, and they also populate at the bottom of the page.↩︎\n",
    "preview": "posts/2021-02-02-blogging-with-rmarkdown/images/new_rmarkdown.png",
    "last_modified": "2021-03-12T14:59:01-06:00",
    "input_file": {},
    "preview_width": 2560,
    "preview_height": 1600
  },
  {
    "path": "posts/2020-12-10-get-ipums-pma-data/",
    "title": "Import IPUMS PMA Data into R",
    "description": "How to download an IPUMS PMA data extract and start using it in R",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2020-11-10",
    "categories": [
      "R Tips",
      "R Packages",
      "Importing Data"
    ],
    "contents": "\n\nContents\nHow to access the data\nUser Guide\nYouTube Tutorials\n\nImporting the data\nFixed-width Data Format (dat)\nThe ipumsr package\n\n\nIPUMS PMA is the harmonized version of the multinational survey Performance Monitoring for Action (formerly known as Performance Monitoring and Accountability 2020 - PMA2020). IPUMS PMA lets researchers easily browse the contents of the survey series and craft customized microdata files they download for analysis.\nHow to access the data\nUser Guide\nVisit the IPUMS PMA data dissemination website to browse the available data, and then follow the posted user guide to get started with an extract of interest.\nNote: all users must register for a free account. See user guide for details.\nYouTube Tutorials\nVisit the IPUMS PMA YouTube page for a video playlist showing how to do things like:\nregister for a free IPUMS account\nselect from the available units of analysis\nbuild a data extract\nselect cases of interest\nuse the available survey weights\nImporting the data\nFixed-width Data Format (dat)\nOnce you have registered and finished selecting PMA samples and variables for your extract, click the “View Cart” button to begin checkout.\nReview the contents of your extract and clik the “Create Data Extract” button as shown:\n\n\n\nOn this final page be sure to change the data format to “.dat (fixed-width text)” if it is not selected by default:\n\n\n\nYou will receive an email when your extract request has been processed. Click the included link to find a download page like this one. You must download both the data file and the DDI codebook:\n\n\n\nThe ipumsr package\nThe R package ipumsr provides the tools you will need to import the data file and DDI codebook into R. You can install the package from CRAN with:\n\nClick here for more information on R packages.\ninstall.packages(\"ipumsr\")\nNote the location where your data file and DDI codebook were saved (in my case, they were saved in my local “Downloads” folder). Substitute your own paths into the function shown below:\n\n\ndat <- ipumsr::read_ipums_micro(\n  ddi = \"~/Downloads/pma_00001.xml\",\n  data_file = \"~/Downloads/pma_00001.dat.gz\"\n)\n\n\n\nYou’re done! The dataset is now accessible as the R object, dat.\n\n\n\n",
    "preview": "posts/2020-12-10-get-ipums-pma-data/images/create-data-extract.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 910,
    "preview_height": 790
  },
  {
    "path": "posts/2020-12-10-get-r-and-packages/",
    "title": "Getting Started with R",
    "description": "How to download R for free and install some of the R packages used on this blog",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2020-11-02",
    "categories": [
      "R Tips",
      "R Packages"
    ],
    "contents": "\n\nContents\nWhy analyze PMA data with R?\nGetting started with R\nOur favorite resources\n\nDo I really need statistical software?\nAre there alternatives?\n\nRStudio\nR packages\nEssentials\nipumsr\ntidyverse\nshiny\n\nWatch for updates here\n\n\nWhy analyze PMA data with R?\nLike all IPUMS data projects, IPUMS PMA data is available free of charge to users who agree to our terms of use. That’s because we believe that cost and institutional affiliation should not be barriers to answering pressing concerns around women’s health. (You can register here for a free IPUMS PMA user account.)\nIn fact, users can analyze IPUMS PMA data with any software they like! We’ve chosen to highlight R, in particular, because it is also free and popular with data analysts throughout the world. It’s available for Windows, MacOS, and a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux).\nNon-R users: IPUMS data extracts are available as CSV or fixed-width DAT with syntax files formatted for SPSS, Stata, and SAS.\nGetting started with R\nTo get a copy of R for yourself, visit the Comprehensive R Archive Network (CRAN) and choose the right download link for your operating system.\nIf you’re new to R (or want to refresh your skills), we recommend the excellent, free introductory text R for Data Science. It also introduces tidyverse conventions, which we use throughout this blog.\nOur favorite resources\nR for Data Science, for beginners\nAdvanced R, for a deeper dive\nRSpatial, for analysis with spatial data\nggplot2, for data visualization\nMastering Shiny, for interactive applications\nR Markdown: The Definitive Guide, for producing annotated code, word documents, presentations, web pages, and more\nR-bloggers, for regular news and tutorials\n\n\n\n© 2016 The R Foundation (CC-BY-SA 4.0)\nDo I really need statistical software?\nIf you’re new to data analysis, you might wonder exactly what you’re going to find in a toolkit like R.\nPlenty of people come to R after working with more common types of data analysis software, like Microsoft Excel or other spreadsheet programs. If you wanted to, you could absolutely download a CSV file from the IPUMS PMA extract system and open it in Excel. You would find individual respondents in rows and their responses for variables in columns, and you could make use of built-in spreadsheet functions to do things like:\nCalculate and visualize the distribution of a variable\nBuild pivot tables and graphs examining basic relationships between variables\nCreate new variables of your own that combine data from several variables\nHowever, you might also notice that a spreadsheet comes with certain limitations:\nThere is no variable “metadata”, including labels for the variables and each response option. For example, you might see that the responses to a certain variable include the numbers 0, 1, and 99 - what do these values actually mean?\nYou might find yourself repeating the same “point” and “click” procedure over and over. Or, maybe you’ve had to build a library of custom macro functions on your own to help automate those procedures.\nWhile you can perform arithmetic with built-in functions, there is little support for more advanced statistical procedures\nGraphics are limited within a set of pre-built templates\nMerging data from external sources (like spatial data) can be very tricky\nStatistical software is designed specifically to address these and other issues related to data cleaning and analysis. Learning a program like R takes a lot of practice, but doing so will almost certainly make your work much more efficient!\nAre there alternatives?\nYes! Many data analysts use proprietary statistical software like Stata, SAS, or SPSS. These tools are also powerful, and you may even find them easier to use than R.\n\nComing soon, we hope to include Stata code for many of the blog posts currently written in R.\nBeyond price, R has a few additional advantages that make it a particularly useful tool for working with PMA data:\nCommunity support: R users are particularly active on forums like Stack Overflow and R-bloggers. Groups like R-ladies even organize in-person meetups in cities around the world to help promote inclusion within the R community.\nCustomizability: Because R is open-source, you can change just about anything you like! With a little practice, you’ll be able to create functions and graphics that perfectly match your own needs.\nBeyond statistics: You can use R to build a website (like this one), manage and share a code repository on GitHub, scrape and compile a social media database, or automatically generate word documents, slide presentations, and more! There are practically endless ways to use functional programming in R that have nothing to do with statistics at all.\nIf you’re a beginner, learning R can be a daunting task. Keep at it! And never hesitate to ask questions.\nRStudio\nWe strongly recommend running R within RStudio, an integrated development environment (IDE) designed to make your experience with R much easier. Some of the reasons we use it, ourselves:\nIncludes a multi-pane window that puts your R console, source code, output, and help files all in one place\nSyntax highlighting and code completion\nSupport for R Projects, a crucial approach to organizing your work and sharing it with others\nIncludes RMarkdown, an R package that allows you write text-based documents with embedded snippets of code that can be passed directly to your R console\nComing soon: tools like the command palette, an improved package manager, and integrated citation management\nLike R, it is available at no cost for users on Windows, Mac, and Linux\n\nThis blog is, itself, an R Project with an individual R Markdown file for each page on the site. Look for a download button at the top of every post: you can download the original R Markdown file, open it in RStudio, and run all of the included code.\nR packages\nAn R package is a collection of functions created by other R users that you can download and install for yourself. Packages can be distributed in many ways, but all of the packages we highlight on this blog can be downloaded from CRAN (the same resource used to download “base” R). A package like ipumsr can be downloaded from CRAN by typing the following function into the R console:\n\nThis function saves package files in your default “library” location. If you’re using a Linux machine and don’t have root access, you’ll need to set up R to save packages to a location where you’re able to write files.\n\n\ninstall.packages(\"ipumsr\")\n\n\n\nPackages also come with help files detailing the purpose and possible inputs (or “arguments”) of each included function. Other included metadata explains what version of R you’ll need to use the package, and also whether the package borrows functions from any other packages that should also be installed (usually these are called “dependencies”).\nIn order to access the functions and help files for a package, you need to load it after installation with:\n\n\nlibrary(ipumsr)\n\n\n\nOn this blog, we will often show functions together with their package like this:\n\n\nipumsr::read_ipums_micro(\n  ddi = \"~/Downloads/pma_00001.xml\",\n  data_file = \"~/Downloads/pma_00001.dat.gz\"\n)\n\n\n\nThe function read_ipums_micro comes from the package ipumsr. It is not necessary for you to include the package each time you call a function (as long as you’ve already loaded the package with library()); we’re using this notation simply as a reminder (in case you want to consult the original package documentation).\n\nYou can use the package::function() notation if you ever want to access a function from a package without loading everything else in the package.\nEssentials\nHere are the packages you’ll need to install to reproduce the code on this blog:\nipumsr\nThe ipumsr package contains functions that make it easy to load IPUMS PMS data into R (mainly read_ipums_micro).\nIt also contains functions that will return variable metadata (like the variable descriptions you see while browsing for data on pma.ipums.org.\ntidyverse\nThe tidyverse package actually installs a family of related packages, including:\nggplot2, for data visualization\ndplyr, for data manipulation\ntidyr, for data tidying\nreadr, for data import\npurrr, for functional programming\ntibble, for tibbles (a modern re-imagining of data frames)\nstringr, for strings\nforcats, for factors\nThis blog uses tidyverse functions and syntax wherever possible because so-called “tidy” conventions are designed with the expressed purpose of making code and console output more human readable. Sometimes, human readability imposes a performance cost: in our experience, IPUMS PMA datasets are small enough that this is not an issue.\n\nFor larger datasets, we recommend exploring the package data.table.\nshiny\nInteractive graphics shown throughout this blog are built with the shiny package.\nWatch for updates here\nWe may add more package suggestions for future posts!\n\n\n\n",
    "preview": "posts/2020-12-10-get-r-and-packages/images/Rlogo.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 724,
    "preview_height": 561
  }
]
