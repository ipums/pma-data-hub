[
  {
    "path": "posts/2021-04-01-et-internal-migration/",
    "title": "Unmet need for family planning after internal migration",
    "description": "Summary and source code from a recent article using data from Ethiopia.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-04-02",
    "categories": [
      "Migration",
      "PMA Publications"
    ],
    "contents": "\n\nContents\nMotivation\nData\nDependent variable\nKey independent variable\nCovariates\nSub-sample\nReference groups\n\nRegression Model\nInterpretation\nPredicted Probabilities\n\n\n\n\n\nWhen great new research gets published with PMA data on a topic we’re covering here on the Data Analysis Hub, we’ll cover the highlights and explore some source code in a PMA Publications post.\n\nHave a recent publication using PMA data that you’d like to feature in a PMA Publications post? Please let us know!\nAs part of our new series on women’s migration experiences and their impact on family planning, let’s dig into a paper from University of Minnesota researchers Emily Groene and Devon Kristiansen (2021) published in the journal Population, Space and Place.\nMotivation\nAs we’ll see throughout this series, migration can be associated with major changes in an individual’s fertility intentions and family planning access, and it can either increase or decrease the likelihood of experiencing unmet need for family planning under different circumstances. Groene & Kristiansen focus their attention on the particular circumstances around rural-to-urban internal (within-country) migration, which is one of the prevailing modes of migration throughout the countries surveyed by PMA (McAuliffe and Ruhs 2017).\nConsider all of the potential changes a person might experience when moving from a rural to an urban area: Groene & Kristiansen outline literature that suggests quite a few ways that these changes might impact fertility behavior. Some are likely to increase demand for family planning, for example:\nIncreased availability and access to health services and long-acting contraceptives (Skiles et al. 2015)\nNew opportunities for employment, education, and greater wealth that can delay or limit plans for additional births (Schultz 1994)\nDiminished incentives for larger family sizes tied to rural culture and livelihoods (Abebe 2007)\nAcculturation, or adoption of destination cultural roles and values (Kohler 2000)\nOn the other hand, several offsetting factors may push to maintain or even decrease demand for family planning:\nSpousal separation tied to seasonal migration for employment (Sevoyan and Agadjanian 2013)\nFamily planning preferences established prior to migration (Kulu 2005)\nSelection into destinations where familiar cultural roles and values are prevalent (Courgeau 1989)\nEven when we focus our analysis on rural-urban internal migrants, it’s very hard to predict how these and other factors might react to determine the family planning needs for any given person. From a policy perspective, where planning is needed to identify and address unmet need for family planning services on a larger scale, Groene & Kristiansen offer important insights into the ways that migration experiences are tied to a particular place. Using female respondents from the Ethiopia 2017 and 2018 samples, they compare unmet need among rural-urban internal migrants to the unmet need experienced by non-migrants in both rural and urban settings. They find that migrants are less likely to experience unmet need compared to non-migrants, controlling for a number of demographic factors.\nUnmet need is the difference between an individual’s reproductive intentions and contraceptive behavior.\nIn this post, we’ll show how to recreate their analysis using an IPUMS PMA data extract in R.\nData\nThe Ethiopia 2017 and 2018 samples were among the first PMA samples to include questions related to women’s most recent migration experience, and about the region where they were born. Their responses are included in variables listed in the migration variable group:\nLIVEINREGION - How long living continuously in current region\nLIVEINREGIONYRS - Number of years continuously living in current region\nLASTREGION - Region/country of residence before current region\nLASTUR - Urban/rural status of residence before current region\nBIRTHREGIONET - Region of woman’s birth, Ethiopia\nBIRTHUR - Urban/rural status of region of woman’s birth\nMIGMAINRSN - The main reason why moved to current place of residence\nMIGPREKID - Gave birth before moved to current region\nMIGPREKIDNUM - Number of sons/daughters before moving to current region\n\nA 2013 sample of women from Kinshasa, DRC were also given questions related to their recent migration history, but these data have not been made available for public use. See Anglewicz et al. (2017).\nGroene & Kristiansen use the variable LIVEINREGION to determine whether a woman has always lived in the same place and, if not, they use BIRTHUR together with URBAN to identify those who ultimately moved from a rural place to an urban place. We’ve created a data extract containing these and all of the other variables discussed below (female respondents only); we’ll start by loading it and the following packages in R:\n\n\nlibrary(ipumsr)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(survey)\nlibrary(srvyr)\n\ndat <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00019.xml\",\n  data = \"data/pma_00019.dat.gz\"\n)\n\n\n\nIf you’re a registered user at pma.ipums.org, you can recreate the authors’ data extract by selecting the variables mentioned in this post.\nSee our guide for help importing IPUMS data extracts into R.\nWe’ll first label the various non-response values in this dataset with the value NA using ipumsr::lbl_na_if applied to all variables with dplyr::across:\n\n\ndat <- dat %>% \n  mutate(\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\",\n        \"NIU (not in universe) or missing\"\n      )\n    ))\n  )\n\n\n\nEthiopia samples are stratified by region (GEOET) and urban status (URBAN), resulting in 21 sampling strata from which primary sampling units (EAID) are selected. The authors exclude women from any STRATA where fewer than 200 women were sampled across both sample years:\n\nPMA uses a multistage, stratified cluster sample design. For more information, see PMA’s sample design memo.\n\n\ndat %>% count(STRATA) \n\n\n# A tibble: 21 x 2\n                                  STRATA     n\n *                             <int+lbl> <int>\n 1 23101 [Addis Ababa - urban, Ethiopia]  1833\n 2 23102 [Amhara - urban, Ethiopia]       1005\n 3 23103 [Amhara - rural, Ethiopia]       1651\n 4 23104 [Oromiya - urban, Ethiopia]      1216\n 5 23105 [Oromiya - rural, Ethiopia]      2275\n 6 23106 [Other, Ethiopia]                 839\n 7 23107 [SNNP - urban, Ethiopia]         1904\n 8 23108 [SNNP - rural, Ethiopia]         1241\n 9 23109 [Tigray - urban, Ethiopia]       1455\n10 23110 [Tigray - rural, Ethiopia]        810\n11 23111 [Dire Dawa - urban, Ethiopia]      22\n12 23112 [Dire Dawa - rural, Ethiopia]      29\n13 23113 [Afar - urban, Ethiopia]           32\n14 23114 [Afar - rural, Ethiopia]          218\n15 23115 [Somali - urban, Ethiopia]         73\n16 23116 [Somali - rural, Ethiopia]        123\n17 23117 [Gambella - urban, Ethiopia]       34\n18 23118 [Gambella - rural, Ethiopia]       23\n19 23119 [Harari - urban, Ethiopia]         29\n20 23120 [Harari - rural, Ethiopia]         26\n21 23121 [BG - rural, Ethiopia]            172\n\nNote that this will drop women from STRATA numbered 23111-23113 and 23115-23121:\n\n\ndat <- dat %>% \n  group_by(STRATA) %>% \n  mutate(STRATA_N = n()) %>% \n  ungroup() %>% \n  filter(STRATA_N > 200)\n\n\n\nDependent variable\nNow, consider the dependent variable UNMETYN, which is a constructed variable indicating whether each respondent currently has an unmet need for family planning. All respondents to the female questionnaire are included in the universe for UNMETYN, so women who are not able to become pregnant or are not sexually active are determined to have “no unmet need.”\nUNMETYN is a recoded binary indicator from UNMETNEED, which contains additional details on types of unmet need.\nWithin the combined sample of female respondents from both years, about 12% of women demonstrated unmet need for family planning:\n\n\ndat %>% summarize(mean_UNMETYN = mean(UNMETYN, na.rm = T)) \n\n\n# A tibble: 1 x 1\n  mean_UNMETYN\n         <dbl>\n1        0.120\n\nWe’ll use the survey package - and its tidy companion srvyr - to specify PMA sample design in our population estimates. The function srvyr::survey_mean uses information about the survey design (given by srvyr::as_survey_design) to estimate that an average woman aged 15-49 in Ethiopia has about a 15% chance of experiencing unmet need for family planning:\n\n\ndat %>% \n  as_survey_design(\n    id = EAID,\n    nest = T,\n    weight = FQWEIGHT,\n    strata = STRATA\n  ) %>% \n  summarize(pop_UNMETYN = survey_mean(UNMETYN, vartype = \"ci\", na.rm = T))\n\n\n# A tibble: 1 x 3\n  pop_UNMETYN pop_UNMETYN_low pop_UNMETYN_upp\n        <dbl>           <dbl>           <dbl>\n1       0.151           0.135           0.167\n\n\nThe 95% confidence interval is shown by pop_UNMETYN_low and pop_UNMETYN_upp\nKey independent variable\nIn order to conduct a three-way comparison between rural-urban migrants, rural non-migrants, and urban non-migrants, the authors construct a variable we’ll call MIGRANT_DIR.\nThe first component of MIGRANT_DIR evaluates whether each woman ever migrated from her place of birth. Using LIVEINREGION, any woman reporting that she has “always” lived in her current region is not a migrant, and any woman who has lived in her current region for a number of “months or years” is a migrant (note: we cannot determine the migration history for all of the remaining cases, so they will be excluded from further analysis).\n\n\ndat %>% count(LIVEINREGION)\n\n\n# A tibble: 4 x 2\n             LIVEINREGION     n\n*               <int+lbl> <int>\n1 10 [Always]             11777\n2 20 [Currently visiting]   124\n3 30 [Months or years]     2539\n4 NA                          7\n\nMigrants who were born in a rural place !BIRTHUR and now live in an urban place URBAN meet the definition for rural-urban migrant. Non-migrants are classified by their current URBAN status only. All other women are implicitly given the value NA and then filtered out of the dataset.\n\nWe implicitly assign NA to any cases that aren’t specified by the logical statements inside case_when().\n\n\ndat <- dat %>% \n   mutate(\n     across(c(BIRTHUR, URBAN), ~.x %>% zap_labels),\n     MIGRANT = case_when(\n       LIVEINREGION == 30 ~ T, \n       LIVEINREGION == 10 ~ F\n     ),\n     MIGRANT_DIR = case_when(\n       MIGRANT & !BIRTHUR & URBAN ~ \"rural to urban\",\n       !MIGRANT & !URBAN ~ \"nonmigrant - rural\",\n       !MIGRANT & URBAN ~ \"nonmigrant - urban\"\n     ) \n   ) %>% \n  filter(!is.na(MIGRANT_DIR))\n\n\n\n\nRemember: BIRTHUR and URBAN are labeled integers. Our use of zap_labels allows R to ignore their assigned labels and, instead, treat them as logicals where 1 == TRUE and 0 == FALSE.\n\n\ndat %>% count(MIGRANT_DIR)\n\n\n# A tibble: 3 x 2\n  MIGRANT_DIR            n\n* <chr>              <int>\n1 nonmigrant - rural  6437\n2 nonmigrant - urban  5340\n3 rural to urban      1421\n\nYou may notice that we’ve created MIGRANT_DIR as a string, or a character object. We’ll coerce it as a factor later so we can easily use each of the three classifications in a logistic regression model.\nCovariates\nThe authors control for a number of covariates in addition to MIGRANT_DIR. The following covariates are recoded versions of existing PMA variables:\nBIRTHS: number of children ever born CHEB (2017), or the woman’s total number of birth events BIRTHEVENT (2018, e.g. birth of twins is a single event)\nPARTNERED: recoded MARSTAT, indicating if the woman is either currently married or living with a partner\nRELGEN: recoded RELIGION as “muslim,” “christian,” or “other”\n\n\ndat <- dat %>% \n  mutate(\n    across(c(BIRTHEVENT, CHEB), ~.x %>% zap_labels),\n    BIRTHS = case_when(\n      YEAR == 2018 ~ BIRTHEVENT, \n      T ~ CHEB\n    ),\n    PARTNERED = case_when(\n      MARSTAT %in% 21:22 ~ T, \n      !is.na(MARSTAT) ~ F\n    ),\n    RELGEN = case_when(\n      RELIGION == 100 ~ \"muslim\",\n      RELIGION >= 200 & RELIGION < 300 ~ \"christian\",\n      T ~ \"other\"\n    )\n  )\n\n\n\nThe remaining covariates are used without further modification:\nAGE: the woman’s age (years)\nWEALTHQ: wealth quintile\nEDUCATTGEN: education level (general)\nHCVISITY: whether the woman visited a health facility in the last 12 months\nSUBNATIONAL: subnational region\nYEAR: survey year (2017 or 2018)\nSub-sample\nAs discussed above, the authors exclude any female respondents from small STRATA (n < 200) and those who are neither rural-urban migrants nor non-migrants. Additionally, they remove rural-urban migrants who moved to Ethiopia from another country. Women can indicate this information in two places: they may either list a foreign country in LASTREGION or indicate “abroad” as their region of birth in BIRTHREGIONET.\n\n\ndat %>% count(LASTREGION)\n\n\n# A tibble: 18 x 2\n                   LASTREGION     n\n *                  <int+lbl> <int>\n 1 101 [Tigray]                  50\n 2 102 [Afar]                     7\n 3 103 [Amhara]                 399\n 4 104 [Oromia]                 360\n 5 105 [Ethiopia Somali]          1\n 6 106 [Benishangul Gumuz]        4\n 7 107 [SNNPR]                  390\n 8 108 [Gambella]                 4\n 9 109 [Harari]                   7\n10 110 [Addis Ababa]            116\n11 111 [Dire Dawa]                7\n12 202 [Saudi Arabia]            33\n13 204 [Beirut]                   5\n14 205 [United Arab Emirates]     9\n15 206 [Sudan]                    5\n16 210 [Lebanon]                  1\n17 300 [Other]                   23\n18  NA                        11777\n\ndat %>% count(BIRTHREGIONET)\n\n\n# A tibble: 12 x 2\n                                               BIRTHREGIONET     n\n *                                                 <int+lbl> <int>\n 1  1 [Tigray Region]                                         2110\n 2  2 [Afar Region]                                            430\n 3  3 [Amhara Region]                                         2911\n 4  4 [Oromia Region]                                         3472\n 5  5 [Somali Region]                                          166\n 6  6 [Benishangul-Gumuz Region]                               148\n 7  7 [Southern Nations, Nationalities, and Peoples' Region]  3057\n 8  8 [Gambella Region]                                         26\n 9  9 [Harari Region]                                           35\n10 10 [Addis Ababa (city)]                                     814\n11 11 [Dire Dawa (city)]                                        27\n12 12 [Abroad]                                                   2\n\ndat <- dat %>% \n  mutate(\n    EXTERNAL = case_when(\n      LASTREGION %in% 200:900 | BIRTHREGIONET == 12 ~ T,\n      T ~ F\n    )\n  ) %>% \n  filter(!EXTERNAL)\n\n\n\nThe authors also exclude women whose PARTNERED status (i.e. sexual activity) cannot be determined, and women who indicate that they are either “infertile” in FERTPREF or “menopausal / hysterectomy” in TIMEMENSTRUATE. Women who are not at risk of pregnancy for these reasons cannot have unmet need, so they are removed from the sample.\n\n\ndat %>% count(PARTNERED)\n\n\n# A tibble: 3 x 2\n  PARTNERED     n\n* <lgl>     <int>\n1 FALSE      5513\n2 TRUE       7606\n3 NA            2\n\ndat %>% count(FERTPREF)\n\n\n# A tibble: 4 x 2\n                 FERTPREF     n\n*               <int+lbl> <int>\n1  1 [Have another child]  8983\n2  2 [No more children]    2702\n3  3 [Infertile]            225\n4 NA                       1211\n\ndat %>% count(TIMEMENSTRUATE)\n\n\n# A tibble: 8 x 2\n                TIMEMENSTRUATE     n\n*                    <int+lbl> <int>\n1  1 [Days]                     3113\n2  2 [Weeks]                    4111\n3  3 [Months]                   2955\n4  4 [Years]                    1276\n5  5 [Menopausal/hysterectomy]    85\n6  6 [Before last birth]        1231\n7  7 [Never menstruated]         317\n8 NA                              33\n\ndat <- dat %>% \n  mutate(\n    INFERTILE = case_when(\n      FERTPREF == 3 | TIMEMENSTRUATE == 5 ~ T,\n      T ~ F\n    )\n  ) %>% \n  filter(!is.na(PARTNERED), !INFERTILE)\n\n\n\nLastly, they exclude women with missing values on any of the remaining covariates.\n\n\ndat <- dat %>%\n  filter(\n    !if_any(c(UNMETYN, EDUCATTGEN, HCVISITY, BIRTHS, MCP), is.na),\n  )\n\n\n\nFrom 15,010 female respondents included in the original extract, this sub-sampling procedure leaves us with 12,630 remaing cases.\n\n\ndat %>% summarize(n = n())\n\n\n# A tibble: 1 x 1\n      n\n  <int>\n1 12630\n\nReference groups\nAs a final processing step, we’ll coerce each of our categorical variables (including MIGRANT_DIR) as factors. All but one of these is a labelled integer object where we’ll use the response with the lowest value as a reference group; because we created MIGRANT_DIR as a character object, we’ll specify its reference group manually:\n\n\ndat <- dat %>% \n  mutate(\n    across(\n      c(\n        MIGRANT_DIR, \n        RELGEN, \n        WEALTHQ, \n        EDUCATTGEN, \n        HCVISITY, \n        SUBNATIONAL, \n        PARTNERED,\n        YEAR\n      ), \n      ~as_factor(.) %>% droplevels()\n    ),\n    MIGRANT_DIR = fct_relevel(MIGRANT_DIR, \"nonmigrant - urban\")\n  )\n\n\n\nRegression Model\nFinally, we’re ready to build a regression model for UNMETYN using MIGRANT_DIR and the covariates discussed above!\nRecall that the function srvyr::survey_mean estimated that 15.1% of all women aged 15-49 in Ethiopia experience unmet need for family planning. This estimate used all of the women in our original sample prior to the sub-sampling procedure we just discussed. Now that we’ve created a sub-sample from the original dataset, let’s see how the population estimate has changed:\n\n\ndat %>% \n  as_survey_design(\n    id = EAID,\n    nest = T,\n    weight = FQWEIGHT,\n    strata = STRATA\n  ) %>% \n  summarize(pop_UNMETYN = survey_mean(UNMETYN, vartype = \"ci\", na.rm = T))\n\n\n# A tibble: 1 x 3\n  pop_UNMETYN pop_UNMETYN_low pop_UNMETYN_upp\n        <dbl>           <dbl>           <dbl>\n1       0.157           0.140           0.174\n\nNow that we’ve removed some cases (notably, all women who are infertile), the estimated population mean is close, but somewhat higher at 15.7%.\nGroene & Kristiansen build a multilevel logistic regression model for UNMETYN that breaks down this full-population estimate for each of the sub-groups represented by our independent variables. We’ll report the exponentiated coefficient estimates for each variable, which means that we’ll need to interpret each estimate as a change in the odds that a woman will experience UNMETYN relative to a woman in a reference group.\nWe’ll build the authors’ model m1 using the function survey::svyglm, which - like survey_mean - uses information about the sample design provided by as_survey_design to generate cluster-robust standard error estimates:\n\n\nm1 <- dat %>% \n  as_survey_design(\n    id = EAID,\n    nest = T,\n    weight = FQWEIGHT,\n    strata = STRATA\n  ) %>% \n  svyglm(\n    UNMETYN ~  \n      AGE + \n      MIGRANT_DIR +\n      RELGEN + \n      WEALTHQ +\n      EDUCATTGEN + \n      BIRTHS + \n      HCVISITY + \n      SUBNATIONAL +\n      PARTNERED + \n      YEAR,\n    design = .,\n    family = \"quasibinomial\",\n  ) \n\n\n\nWe tell svyglm to fit a logistic regression model with family = \"quasibinomial\".\nWhy “quasi” binomial? A simple binomial distribution yields the same point estimates and standard errors, but generates a warning because our use of sample weights produces a non-integer count of women with unmet need.\nTo simplify the output a bit, we’ll show a tidy table with just the term, point estimate, the 95% confidence interval, and the p-value (each rounded to two decimal places):\n\n\nm1 %>% \n  tidy(exp = T, conf.int = T) %>% \n  select(term, estimate, conf.low, conf.high, p.value) %>% \n  mutate(across(where(is.numeric), ~round(.x, 2))) \n\n\n# A tibble: 27 x 5\n   term                            estimate conf.low conf.high p.value\n   <chr>                              <dbl>    <dbl>     <dbl>   <dbl>\n 1 (Intercept)                         0.13    0.06       0.27    0   \n 2 AGE                                 0.96    0.94       0.97    0   \n 3 MIGRANT_DIRnonmigrant - rural       1.15    0.85       1.55    0.38\n 4 MIGRANT_DIRrural to urban           0.75    0.59       0.95    0.02\n 5 RELGENother                         0.88    0.580      1.33    0.54\n 6 RELGENchristian                     0.59    0.48       0.72    0   \n 7 WEALTHQLower quintile               0.99    0.79       1.25    0.96\n 8 WEALTHQMiddle quintile              1.06    0.84       1.33    0.63\n 9 WEALTHQHigher quintile              0.82    0.65       1.04    0.11\n10 WEALTHQHighest quintile             0.84    0.56       1.27    0.42\n11 EDUCATTGENPrimary/Middle school     1.05    0.88       1.26    0.59\n12 EDUCATTGENSecondary/post-prima…     0.76    0.55       1.05    0.1 \n13 EDUCATTGENTertiary/post-second…     1       0.68       1.49    0.98\n14 BIRTHS                              1.24    1.18       1.31    0   \n15 HCVISITYYes                         0.85    0.71       1.01    0.06\n16 SUBNATIONALAfar, Ethiopia           0.23    0.12       0.44    0   \n17 SUBNATIONALAmhara, Ethiopia         0.55    0.44       0.69    0   \n18 SUBNATIONALOromiya, Ethiopia        0.91    0.73       1.13    0.41\n19 SUBNATIONALSomali, Ethiopia         0.74    0.33       1.69    0.48\n20 SUBNATIONALBenishangul-Gumuz, …     0.55    0.290      1.08    0.08\n21 SUBNATIONALSNNP, Ethiopia           0.9     0.68       1.2     0.48\n22 SUBNATIONALGambella, Ethiopia       0.86    0.15       4.85    0.86\n23 SUBNATIONALHarari, Ethiopia         2.06    1.69       2.52    0   \n24 SUBNATIONALAddis Ababa, Ethiop…     0.98    0.69       1.4     0.93\n25 SUBNATIONALDire Dawa, Ethiopia      0.77    0.47       1.27    0.3 \n26 PARTNEREDTRUE                       6.69    4.76       9.41    0   \n27 YEAR2018                            0.88    0.74       1.05    0.15\n\nInterpretation\nControlling for all of the covariates we’ve discussed, the authors find that rural-urban internal migrants are less likely than both urban and rural non-migrants to experience unmet need for family planning!\nHow do we identify this finding in the model output? Notice that the estimated odds of experiencing UNMETYN for rural to urban migrants is 0.75, and that the associated 95% confidence interval ranges from 0.59 to 0.95: this represents the migrants’ odds compared to urban non-migrants. If the 95% confidence interval included the value 1.0, we would say that there’s more than a 5% chance that the migrants’ odds could be equal to the odds experienced by urban non-migrants. Because it does not include 1.0, we instead say that there’s a statistically significant difference between the two groups (at the 5% confidence threshold).\nBecause the authors selected urban non-migrants as a reference group, our model output shows the relationship between rural non-migrants and rural-urban migrants a bit less clearly. Although the point estimate for rural to urban migrants (0.75) is lower than the point estimate for nonmigrant - rural (1.15), their respective confidence intervals overlap. In order to see that they actually are statistically different, we’d need to run the model again with a different reference group.\nPredicted Probabilities\n\n\n\nWhile the output from our logistic regression model helps show the relative difference between groups, we’re not yet able to predict the absolute risk of UNMETYN for each group. Recall that, before building our model, we calculated that the average unmet need for all women in Ethiopia (excluding external migrants, infertile women, etc) was about 15.7%. We’ll now estimate the average unmet need experienced by all women in Ethiopia sorted into the three groups represented by MIGRANT_DIR.\nThe predict function allows us to make a prediction about each woman’s likelihood of experiencing unmet need according to the model m1. When we tell predict to return type = \"response\", it gives us the predicted probability that each woman should have UNMETYN.\n\n\ntibble(predicted = predict(m1, type = \"response\"))\n\n\n# A tibble: 12,630 x 1\n   predicted  \n   <svystat>  \n 1 0.072659141\n 2 0.338704987\n 3 0.033291206\n 4 0.376765517\n 5 0.025819303\n 6 0.528723102\n 7 0.009790659\n 8 0.088323159\n 9 0.044348246\n10 0.027093896\n# … with 12,620 more rows\n\nIf we wanted to compare each individual’s predicted probability to the value they actually do have for UNMETYN, we could attach our prediction back to our dataset. Remember that the original UNMETYN variable is binary, whereas the predictions are continuous probabilities that range from 0 to 1. Here, we hope to see that women whose predicted probability exceeds 0.50 have UNMETYN == 1, while those whose predicted probability is less than 0.50 have UNMETYN == 0:\n\n\ntibble(predicted = predict(m1, type = \"response\")) %>% \n  bind_cols(dat) %>% \n  select(predicted, UNMETYN) \n\n\n# A tibble: 12,630 x 2\n   predicted             UNMETYN\n   <svystat>           <int+lbl>\n 1 0.072659141 0 [No unmet need]\n 2 0.338704987 0 [No unmet need]\n 3 0.033291206 0 [No unmet need]\n 4 0.376765517 0 [No unmet need]\n 5 0.025819303 0 [No unmet need]\n 6 0.528723102 1 [Unmet need]   \n 7 0.009790659 0 [No unmet need]\n 8 0.088323159 0 [No unmet need]\n 9 0.044348246 0 [No unmet need]\n10 0.027093896 0 [No unmet need]\n# … with 12,620 more rows\n\nWe can also use predict to calculate predicted probabilities for hypothetical samples. For instance, the authors provide the predicted probabilities for a hypothetical sample of women that is completely identical to the real sample, except that they all share the same value for MIGRANT_DIR (all other variables are kept at their originial values). The mean predicted probability derived from this kind of hypothetical sample is known as a predictive margin.\nWhile the point estimates for each group in MIGRANT_DIR are easy to calculate with predict, the confidence intervals for those estimates are a bit harder to obtain. Here, we’ll use rsample::bootstraps to generate 100 replicates of our sample. This will allow us to rebuild our model 100 times:\n\n\nset.seed(1) # This ensures reproducible bootstrap sampling\n\nboots_dat <- dat %>% \n  rsample::bootstraps(100, EAID) %>% \n  transmute(\n    id = parse_number(id),\n    splits = map(splits, as_tibble),\n    model = map(splits, ~{\n      .x %>% \n        as_survey_design(\n          id = EAID,\n          nest = T,\n          weight = FQWEIGHT,\n          strata = STRATA\n        ) %>% \n        svyglm(\n          UNMETYN ~  \n            AGE + \n            MIGRANT_DIR +\n            RELGEN + \n            WEALTHQ +\n            EDUCATTGEN + \n            BIRTHS + \n            HCVISITY + \n            SUBNATIONAL +\n            PARTNERED + \n            YEAR,\n          design = .,\n          family = \"quasibinomial\",\n        ) \n    })\n  )\n\nboots_dat\n\n\n# A tibble: 100 x 3\n      id splits                 model   \n   <dbl> <list>                 <list>  \n 1     1 <tibble [12,630 × 38]> <svyglm>\n 2     2 <tibble [12,630 × 38]> <svyglm>\n 3     3 <tibble [12,630 × 38]> <svyglm>\n 4     4 <tibble [12,630 × 38]> <svyglm>\n 5     5 <tibble [12,630 × 38]> <svyglm>\n 6     6 <tibble [12,630 × 38]> <svyglm>\n 7     7 <tibble [12,630 × 38]> <svyglm>\n 8     8 <tibble [12,630 × 38]> <svyglm>\n 9     9 <tibble [12,630 × 38]> <svyglm>\n10    10 <tibble [12,630 × 38]> <svyglm>\n# … with 90 more rows\n\nNotice that each row of boots_dat contains a completely resampled version of dat contained in each row of the column splits. The column model contains the output from a model that’s uniquely fitted to the resampled data in splits.\nNext, we’ll use predict separately for each row in boots_dat. Because we generate three new rows each time - one prediction for each group in MIGRANT_DIR - the resulting data frame has 300 rows.\n\n\nboots_dat <- boots_dat %>% \n  rowwise() %>%\n  mutate(nested_predictions = list(map_df(levels(dat$MIGRANT_DIR), ~{\n    predict(\n      model, \n      type = \"response\", \n      newdata = splits %>% mutate(MIGRANT_DIR = .x)) %>% \n      tibble() %>% \n      bind_cols(splits) %>% \n      as_survey_design(\n        id = EAID,\n        nest = T,\n        weight = FQWEIGHT,\n        strata = STRATA\n      ) %>% \n      summarise(predicted = survey_mean(., vartype = \"ci\")) %>% \n      mutate(MIGRANT_DIR = .x) %>% \n      select(MIGRANT_DIR, predicted) \n  }))) %>% \n  unnest(nested_predictions) \n\nboots_dat\n\n\n# A tibble: 300 x 5\n      id splits                 model    MIGRANT_DIR        predicted\n   <dbl> <list>                 <list>   <chr>                  <dbl>\n 1     1 <tibble [12,630 × 38]> <svyglm> nonmigrant - urban    0.151 \n 2     1 <tibble [12,630 × 38]> <svyglm> nonmigrant - rural    0.163 \n 3     1 <tibble [12,630 × 38]> <svyglm> rural to urban        0.112 \n 4     2 <tibble [12,630 × 38]> <svyglm> nonmigrant - urban    0.149 \n 5     2 <tibble [12,630 × 38]> <svyglm> nonmigrant - rural    0.155 \n 6     2 <tibble [12,630 × 38]> <svyglm> rural to urban        0.0840\n 7     3 <tibble [12,630 × 38]> <svyglm> nonmigrant - urban    0.142 \n 8     3 <tibble [12,630 × 38]> <svyglm> nonmigrant - rural    0.155 \n 9     3 <tibble [12,630 × 38]> <svyglm> rural to urban        0.133 \n10     4 <tibble [12,630 × 38]> <svyglm> nonmigrant - urban    0.133 \n# … with 290 more rows\n\nFinally, we’ll calculate:\nthe predicted probability for each group from the mean of 100 bootstrap predictions\nthe standard error of each group’s predicted probability from the standard deviation of 100 bootstrap predictions\nthe 95% confidence interval from the product of each group’s standard error and qnrom(0.975)\n\n\ngroup_predictions <- boots_dat %>% \n  group_by(MIGRANT_DIR) %>% \n  summarise(\n    mean = mean(predicted),\n    se = sd(predicted),\n    lower = mean - se*qnorm(0.975),\n    upper = mean + se*qnorm(0.975)\n  )\n\ngroup_predictions\n\n\n# A tibble: 3 x 5\n  MIGRANT_DIR         mean      se  lower upper\n* <chr>              <dbl>   <dbl>  <dbl> <dbl>\n1 nonmigrant - rural 0.161 0.00568 0.149  0.172\n2 nonmigrant - urban 0.143 0.0125  0.119  0.168\n3 rural to urban     0.117 0.0171  0.0831 0.150\n\nAnd here are those intervals plotted with geom_errorbarh and geom_point:\n\n\nggplot(group_predictions) +\n  geom_errorbarh(\n    color = \"#A2269C\", \n    aes(height = .2, xmin = lower, xmax = upper, y = MIGRANT_DIR)\n  ) + \n  geom_point(\n    color = \"#A2269C\", \n    aes(x = mean, y = MIGRANT_DIR)\n  ) +\n  geom_text(\n    nudge_y = 0.2,\n    aes(label = round(mean, 3), x = mean, y = MIGRANT_DIR)\n  ) + \n  scale_x_continuous(breaks = seq(.08, .18, by = .02)) +\n  theme_minimal() + \n  labs(\n    subtitle = \"95% Confidence Interval\",\n    title = \"Predicted Probability of Unmet Need for Family Planning\",\n    y = \"\",\n    x = \"\"\n  ) + \n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 10)\n  ) \n\n\n\n\nAs you can see, the confidence intervals for each group in MIGRANT_DIR overlap quite a bit. However, the probability that a rural-urban internal migrant will experience unmet need for family planning seems to be generally lower than the other groups: we show a point-estimate of just 11.7% for migrants compared to 14.3% and 16.1% respectively for urban and rural non-migrants.\nTo learn more about the conceptual reasons why rural-urban internal migrants in Ethiopia might experience less unmet need for family planning compared to non-migrants, be sure to checkout out Groene & Kristiansen’s full article published at Populations, Space and Place! And, for more information about migration data available in other PMA samples, stay tuned for upcoming posts in this series.\n\n\n\nAbebe, Tatek. 2007. “Changing Livelihoods, Changing Childhoods: Patterns of Children’s Work in Rural Southern Ethiopia.” Children’s Geographies 5 (1-2): 77–93. https://doi.org/10.1080/14733280601108205.\n\n\nAnglewicz, Philip, Jamaica Corker, and Patrick Kayembe. 2017. “The Fertility of Internal Migrants to Kinshasa.” Genus 73 (1): 4. http://dx.doi.org/10.1186/s41118-017-0020-8.\n\n\nCourgeau, D. 1989. “Family Formation and Urbanization.” Population. English Selection 44 (1): 123–46. https://www.ncbi.nlm.nih.gov/pubmed/12157901.\n\n\nGroene, Emily A, and Devon Kristiansen. 2021. “Unmet Need for Family Planning After Internal Migration: Analysis of Ethiopia 2017–2018 PMA Survey Data.” Population, Space and Place 27 (1). https://onlinelibrary.wiley.com/doi/10.1002/psp.2376.\n\n\nKohler, Hans-Peter. 2000. “Social Interactions and Fluctuations in Birth Rates.” Population Studies 54 (2): 223–37. https://doi.org/10.1080/713779084.\n\n\nKulu, Hill. 2005. “Migration and Fertility: Competing Hypotheses Re-Examined.” European Journal of Population/Revue Européenne de Démographie 21 (1): 51–87. https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/content/pdf/10.1007/s10680-005-3581-8.pdf&casa_token=ofnVP7_3oy4AAAAA:nOmsfJTITgKjtiNcXj6u9GsVD9yHCkWsqmAtwTs6aG3vrQaL9DlhaOwcxFQMYweYNSt1mAtGNNonsZ-9.\n\n\nMcAuliffe, Marie, and Martin Ruhs. 2017. “World Migration Report 2018.” International Office of Migration, Geneva. https://publications.iom.int/fr/system/files/pdf/wmr_2018_en_chapter7.pdf.\n\n\nSchultz, T Paul. 1994. “Human Capital, Family Planning, and Their Effects on Population Growth.” The American Economic Review 84 (2): 255–60. http://www.jstor.org/stable/2117839.\n\n\nSevoyan, Arusyak, and Victor Agadjanian. 2013. “Contraception and Abortion in a Low-Fertility Setting: The Role of Seasonal Migration.” International Perspectives on Sexual and Reproductive Health 39 (3): 124–32. http://dx.doi.org/10.1363/3912413.\n\n\nSkiles, Martha Priedeman, Marc Cunningham, Andrew Inglis, Becky Wilkes, Ben Hatch, Ariella Bock, and Janine Barden-O’Fallon. 2015. “The Effect of Access to Contraceptive Services on Injectable Use and Demand for Family Planning in Malawi.” International Perspectives on Sexual and Reproductive Health 41 (1): 20–30. http://dx.doi.org/10.1363/4102015.\n\n\n\n\n",
    "preview": "posts/2021-04-01-et-internal-migration/images/pred_prob.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 2700,
    "preview_height": 900
  },
  {
    "path": "posts/2021-02-09-march-2021-data-release/",
    "title": "New SDP Data Available Spring 2021",
    "description": "Get details on new variables related to labor & delivery services, antenatal care, vaccinations, facility shipment schedules, and more!",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [
      "New Data",
      "Data Discovery"
    ],
    "contents": "\n\nContents\nLabor and Delivery\nAntenatal Care\nVaccinations\nFacility Stockouts & Expected Shipments\nNational Health Programs\n\n\n\n\nWe’re excited to announce the release of several new Service Delivery Point samples this month over at pma.ipums.org! As always, you’ll find the new data harmonized with older samples wherever the new surveys repeat questions you’ve seen before. The new samples also contain a big batch of new variables derived from questions that were posed for the very first time in PMA surveys, so we’d like to introduce a few of the highlights here.\n\n\n\nThe new samples included in this release represent data collected from:\nBurkina Faso 2020\nCongo (DR) 2019\nEthiopia 2019\nKenya 2019\nNigeria 2019\nUganda 2019\nLabor and Delivery\nWe’ve added a variable group in the Other Health Services topic offering more than 60 new variables related to Labor and Delivery. Many of these are currently available only for the Ethiopia 2019 sample, which piloted the new questions.\nFor example, you’ll find new variables about delivery personnel, including those showing whether a facility has a skilled birth attendant or a provider able to perform C-section delivery available 24 hours per day. Other variables describe the infrastructure available for labor and delivery, including the number of delivery rooms and beds, labor rooms, maternity waiting rooms, and newborn resuscitation tables. You’ll also find a number of variables describing the environment inside of a facility’s delivery rooms, including whether they are private, heated, and whether they have several specific delivery guidelines and protocols posted in the room.\nSeveral labor and delivery statistics are also provided for the month preceding the interview. These include the total number of facility deliveries, cesarian deliveries, stillbirths (both fresh and macerated), and neonatal deaths (reported separately for those occurring within 24 hours and one week). Other variables report whether certain services were provided any time within 3 months preceding the interview, including:\ninstrument / assisted delivery\ncaesarean section\nparenteral antibiotics for infections\nparenteral anticonvulsants for high blood pressure (Diazepam, Magnesium Sulfate, or other) or hypertension (Hydralazine, Nifedipine, Methyldopa, or other)\nparenteal / oral uterotonics for hemorrhage (Ergometrine, Misoprostol tablets, Oxytocin, or other)\ncortisteroids for fetal lung maturation\nmanual placenta removal\npartographs to monitor labor\nnewborn resuscitation\nblood transfusion\npostpartum implant insertion\npostpartum IUD insertion\npostpartum tubal ligation\nLastly, a number of variables indicate whether a particular service is typically provided at a facility. These include,\nneonatal intensive care, and whether it was available on the day of the interview\nreferrals for outgoing newborns and pregnant, laboring, and postpartum women; policies on referrals made from other facilities\nbreastfeeding assistance, newborn skin contact, and family planning discussions with new mothers prior to discharge\nAntenatal Care\nAs with Labor and Delivery, a new Antenatal Care group contains a number of variables that are currently available only for the Ethiopia 2019 sample, such as:\nwhether antenatal care was available on the day of the interview\nwhether trained staff are available to use ultrasound, and whether they were available on the day of the interview\nthe total number of rooms for antenatal care, and whether they are private\nwhether a number of different procedures are typically provided as a routine part of antenatal care (for example: blood pressure, weight, HIV testing, and several blood / urine tests)\nBeyond Ethiopia 2019, several of the other new samples included questions related to topics that are normally discussed with patients during an antenatal visit:\nimmediate and exclusive breastfeeding\nreturn to fertility after pregnancy\nhealthy timing and spacing of pregnancies\nfamily planning methods available to use while breastfeeding\nuse of the lactational amenorrhea method (LAM) for family planning, and plans for a transition to other methods\ninterest in a postpartum IUD or other long-acting family planning methods\nIn earlier survey rounds, PMA questionnaires included questions on whether these topics were discussed with a new mother after birth. The new samples differentiate between whether these topics were covered before the mother left the facility after delivery (e.g DISPPSPACE) or at a postnatal care visit later on (e.g. DISPNCSPACE).\nVaccinations\nThe Ethiopia 2019 sample also includes some of the first PMA variables related to vaccination. You’ll find indicators for whether a facility typically provides immunization services, whether those services were provided on the day of the interview, and whether a woman visiting the facility for her child’s immunization would be offered family planning services or counseling during the visit.\nThe availability of the following vaccines are also provided:\nBCG\nIPV and oral polio\nMeasles\nPCV\nPentavalent\nRota\nTetanus toxoid\nVitamin A\n\nEach of these may be “observed” by the interviewer, or else “reported” without observation.\nFacility Stockouts & Expected Shipments\nPMA has included variables related to contraceptive stockouts for many samples dating back to 2014. Four of the new samples dig deeper into the reasons why facilities experience stockouts, and also report the expected delivery time for methods that were out of stock on the day of the interview.\nFor each of the following methods, the expected delivery time is reported by two variables: a numeric value and a unit (e.g. days, weeks, months) that defines the value.\ndiaphragms\nDepo Provera\nemergency contraception\nfemale condoms\nfoam / jelly\nimplants\nIUDs\nmale condoms\npills\nSayana Press\nStandard Days/Cycle Beads\n\n\n\nIt’s recommended that users construct their own derived variables for expected delivery times using whichever unit of time they prefer. For example, notice that the values for expected delivery of Depo Provera DEPOVAL are reported either in weeks or months in DEPOUNIT:\n\n\ndat %>% count(DEPOVAL, DEPOUNIT)\n\n\n# A tibble: 18 x 3\n                      DEPOVAL                    DEPOUNIT     n\n                    <int+lbl>                   <int+lbl> <int>\n 1  0                          1 [Weeks]                      3\n 2  1                          1 [Weeks]                     55\n 3  1                          2 [Months]                    29\n 4  2                          1 [Weeks]                     16\n 5  2                          2 [Months]                     9\n 6  3                          1 [Weeks]                      8\n 7  3                          2 [Months]                     9\n 8  4                          1 [Weeks]                      2\n 9  4                          2 [Months]                     1\n10  6                          2 [Months]                     1\n11  7                          1 [Weeks]                      1\n12 12                          2 [Months]                     1\n13 14                          1 [Weeks]                      1\n14 30                          2 [Months]                     5\n15 60                          2 [Months]                     1\n16 99 [NIU (not in universe)] 97 [Don't know]                92\n17 99 [NIU (not in universe)] 98 [No response or missing]     1\n18 99 [NIU (not in universe)] 99 [NIU (not in universe)]   1492\n\nSuppose you wanted to create a derived variable called DEPO_WKS that simply reports the expected delivery time of Depo Provera in weeks. For any value that’s currently reported in months (DEPOUNIT == 2), you might decide to multiply the value in DEPOVAL by 4. Don’t forget to handle non-response values (e.g. 97, 98, 99) separately!\n\n\ndat %>% \n  mutate(DEPO_WKS = case_when(\n      DEPOUNIT == 2 ~ DEPOVAL * 4, \n      DEPOUNIT > 90 ~ NA_real_, \n      T ~ as.double(DEPOVAL) \n    )) %>% \n  count(DEPO_WKS)\n\n\n# A tibble: 15 x 2\n   DEPO_WKS     n\n *    <dbl> <int>\n 1        0     3\n 2        1    55\n 3        2    16\n 4        3     8\n 5        4    31\n 6        7     1\n 7        8     9\n 8       12     9\n 9       14     1\n10       16     1\n11       24     1\n12       48     1\n13      120     5\n14      240     1\n15       NA  1585\n\nDEPOVAL is an integer, but R coerces it into a double when you apply multiplication (what if multiplication creates non-integer values?). This is why we tell R to use the double class NA_real_ if DEPOUNIT > 90, and then to use as.double(DEPOVAL) if neither DEPOUNIT == 2 nor DEPOUNIT > 90. All of the values created by case_when have to be in the same class!\nFor facilities that were currently out of stock of a method that they normally provide, these new samples include variables explaining why the method was out of stock. With Depo Provera, for example, we can now see that a majority of stockouts across samples are caused by shipments that were ordered, but did not arrive:\n\n\ndat %>% count(OUTWHYDEPO)\n\n\n# A tibble: 8 x 2\n                                         OUTWHYDEPO     n\n*                                         <int+lbl> <int>\n1  1 [Did not place order for shipment]                23\n2  2 [Ordered but did not receive shipment]           154\n3  3 [Did not order right quantities]                   8\n4  4 [Ordered but did not receive right quantities]    17\n5  5 [Unexpected increase in consumption]               5\n6  9 [Other]                                           27\n7 97 [Don't know]                                       1\n8 99 [NIU (not in universe)]                         1492\n\nNational Health Programs\nLastly, we’ve created a new variable group related to participation in national health programs. While we may see more data in upcoming samples, these variables currently describe facility participation in programs provided by the National Hospital Insurance Fund (NHIF) for Kenya. Specifically, you’ll find an indicator for whether a facility in the Kenya 2019 sample provides family planning methods / services covered by NHIF and, if so, whether it provides each of these:\nEdu Afya\nLinda Mama (number of enrolled adolescents and adult women)\nStandard Program\nSuper Program\nother program\n\n\n\n",
    "preview": "posts/2021-02-09-march-2021-data-release/images/new_data_white.png",
    "last_modified": "2021-03-26T16:30:30-05:00",
    "input_file": {},
    "preview_width": 2555,
    "preview_height": 1437
  },
  {
    "path": "posts/2021-02-19-analyzing-the-individual-in-context/",
    "title": "Putting It All Together: Analyzing the Individual in Context",
    "description": "Analyzing women's contraceptive use while considering service delivery point and spatial contextual factors.",
    "author": [
      {
        "name": "Nina Brooks",
        "url": "http://www.ninarbrooks.com/"
      }
    ],
    "date": "2021-03-02",
    "categories": [
      "Individuals in Context",
      "Service Delivery Points",
      "Data Analysis",
      "survey",
      "dotwhisker"
    ],
    "contents": "\n\nContents\nSetup: Load Packages and Data\nRecoding covariates\nRegression Models\nIndividual factors: model with glm\nIndividual factors: model with svyglm\nAvailabillity: modeling with SDP variables\nAccessibility: modeling with external spatial variables\n\n\n\n\n\nThroughout our series on Individuals in Context, we’ve been looking at PMA Service Delivery Point (SDP) data as a resource for understanding the health services environment experienced by women surveyed in PMA Household and Female samples. We created summary variables that capture the SDPs that provide services within the same enumeration areas PMA uses to construct samples of individuals. We’ve also shown how to complement SDP data with additional information about women’s lived environment collected from external geospatial data sources.\nIn this final post, we’ll bring everything together and demonstrate the kind of analysis you might want to do with the contextual data we’ve collected in this series. Specifically, we’ll analyze women’s current contraceptive use, FPCURRUSE, taking into account:\nFPCURRUSE indicates whether a woman is currently using any method of family planning, or doing something to delay or avoid pregnancy.\nindividual factors about each woman collected in the Household and Female survey\navailability factors related to the supply of family planning services provided by SDPs in each woman’s enumeration area\naccessibility factors in each woman’s enumeration area - including measures of population density and transportation infrastructure - that we collected from external data sources\nThe availability of both detailed individual data on family planning and supply-side (service delivery) factors is one of the unique advantages of the PMA data.\nSetup: Load Packages and Data\nWe’ll load the packages tidyverse and ipumsr, as usual. Additionally, we’ll load tidymodels, which helps apply tidyverse principles to the models we’ll be building, and a few other packages we’ll discuss below.\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\nlibrary(tidymodels)\nlibrary(survey)\nlibrary(dotwhisker)\n\n\n\nWe’ll be using both of the Burkina Faso datasets we created in earlier posts in this series:\nbf_merged contains a handful of variables from each sampled woman merged with summary variables about the SDPs that serve her enumeration area (created in this post).\nint contains population and road density variables for each enumeration area (created in this post).\n\nRemember, to use the GPS data you must request access directly from our partners at pmadata.org. The version of int we’re using in this post is based on the real GPS locations but the GPS data itself is not included.\nAs a reminder, let’s take a glimpse at the variables we’ve currently got in each:\n\n\n\n\n\nglimpse(bf_merged)\n\n\nRows: 6,944\nColumns: 10\n$ EAID                <dbl+lbl> 7003, 7003, 7003, 7003, 7003, 7003,…\n$ SAMPLE              <int+lbl> 85405, 85405, 85405, 85405, 85405, …\n$ N_SDP               <int> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ NUM_METHODS_PROV    <int> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ NUM_METHODS_INSTOCK <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, …\n$ NUM_METHODS_OUT3MO  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MEAN_OUTDAY         <dbl> NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN,…\n$ PERSONID            <chr> \"0700300000019732017504\", \"070030000001…\n$ URBAN               <int+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FPCURRUSE           <int+lbl> 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,…\n\nThe variables N_SDP, NUM_METHODS_PROV, NUM_METHODS_INSTOCK, NUM_METHODS_OUT3MO, MEAN_OUTDAY, and URBAN all describe the the enumeration area (EAID) where a woman identified by PERSONID resides. The only other variable from the Household and Female questionnaire, itself, is FPCURRUSE. We’ll add more variables describing each woman in a moment.\n\n\n\n\n\nglimpse(int)\n\n\nRows: 83\nColumns: 7\n$ EAID        <dbl> 7003, 7006, 7009, 7016, 7026, 7042, 7048, 7056,…\n$ ROAD_LENGTH <dbl> 30.29857, 28.87695, 24.08644, 41.92500, 67.6741…\n$ PMACC       <chr> \"BF\", \"BF\", \"BF\", \"BF\", \"BF\", \"BF\", \"BF\", \"BF\",…\n$ PMAYEAR     <dbl> 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017,…\n$ REGION      <chr> \"5. centre-nord\", \"5. centre-nord\", \"8. est\", \"…\n$ DATUM       <chr> \"WGS84\", \"WGS84\", \"WGS84\", \"WGS84\", \"WGS84\", \"W…\n$ POP_DENSITY <dbl> 74.04364, 51.75554, 15.66303, 91.25882, 416.232…\n\nWe’ll be using ROAD_LENGTH and POP_DENSITY, but first we’ll need to merge int to bf_merged by matching up the EAID for each woman:\n\n\nbf_merged <- left_join(bf_merged, int, by = \"EAID\")\n\n\n\nLet’s now introduce some new variables obtained from each woman’s responses to the Household and Female questionnaire. We’ll merge a new data extract with the following variables collected from the Burkina Faso 2017 and 2018 surveys (female respondents only):\nAGE - Age (in years)\nMARSTAT - Marital status\nEDUCATTGEN - Highest level of school attended, general (4 categories)\nWEALTHQ - Wealth score quintile\nBIRTHEVENT - Number of birth events\n\nFor a refresher on accessing and importing PMA data in R, check out our post Import IPUMS PMA Data Into R.\nFollowing the practice we used when we made bf_merged, we’ll simply handle all of the different non-response codes in this new extract by recoding them as NA. Then, we’ll merge the extract to bf_merged by matching up each person by PERSONID:\n\n\nbf_merged <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00018.xml\",\n  data_file = \"data/pma_00018.dat.gz\") %>% \n  select(PERSONID, AGE, MARSTAT, EDUCATTGEN, WEALTHQ, BIRTHEVENT, STRATA) %>% \n  mutate(across(everything(), ~lbl_na_if(\n    .x,\n    ~.lbl %in% c(\n      \"Not interviewed (female questionnaire)\",\n      \"Not interviewed (household questionnaire)\",\n      \"Don't know\",\n      \"No response or missing\",\n      \"NIU (not in universe)\"\n    )\n  ))) %>% \n  right_join(bf_merged, by = \"PERSONID\")\n\n\n\nRecoding covariates\nAll five of the new variables we’ve introduced are loaded into R as members of both the integer and the haven_labelled class of objects. But really, only AGE and BIRTHEVENT should be treated like continuous measures in our analysis. For MARSTAT, EDUCATTGEN, and WEALTHQ, the integer values associated with each response are arbitrary; we’re much more interested in the labels associated with these numeric values because each of these three variables reflects a categorical measurement.\n\n\nbf_merged %>% \n  select(MARSTAT, EDUCATTGEN, WEALTHQ) %>% \n  map(class)\n\n\n$MARSTAT\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"integer\"       \n\n$EDUCATTGEN\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"integer\"       \n\n$WEALTHQ\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"integer\"       \n\nAs you might know, the normal way to handle categorical variables in a regression model is to create a binary dummy variable associated with each response, and R normally performs this task automatically when it encounters a variable that’s a member of the factor class.\nIf we want, we can simply coerce these variables as factors. When we do this and then use the factor in a regression model, R will select the lowest numbered response as a “reference group” and create binary dummy variables for the other responses. This makes sense with WEALTHQ, where we’d interpret the coefficient for each wealth quintile as an effect relative to being in the lowest quintile.\n\n\nbf_merged %>% count(WEALTHQ)\n\n\n# A tibble: 6 x 2\n                WEALTHQ     n\n*             <int+lbl> <int>\n1  1 [Lowest quintile]   1200\n2  2 [Lower quintile]    1031\n3  3 [Middle quintile]    984\n4  4 [Higher quintile]   1253\n5  5 [Highest quintile]  2474\n6 NA                        2\n\nbf_merged <- bf_merged %>% \n  mutate(WEALTHQ = as_factor(WEALTHQ)) \n\nbf_merged %>% count(WEALTHQ)\n\n\n# A tibble: 6 x 2\n  WEALTHQ              n\n* <fct>            <int>\n1 Lowest quintile   1200\n2 Lower quintile    1031\n3 Middle quintile    984\n4 Higher quintile   1253\n5 Highest quintile  2474\n6 <NA>                 2\n\nAlternatively, we might decide to make our own binary dummy variables. This makes sense when we might want to collapse several responses into one larger category, as with MARSTAT: here, for the purpose of analyzing FPCURRUSE, we probably only care about whether the woman is partnered (the reasons why she might not be partnered are less meaningful).\n\n\nbf_merged %>% count(MARSTAT)\n\n\n# A tibble: 5 x 2\n                             MARSTAT     n\n*                          <int+lbl> <int>\n1 10 [Never married]                  1876\n2 21 [Currently married]              4307\n3 22 [Currently living with partner]   410\n4 31 [Divorced or separated]           163\n5 32 [Widow or widower]                188\n\nbf_merged <- bf_merged %>%\n  mutate(MARSTAT = lbl_relabel(\n      MARSTAT,\n      lbl(1, \"partnered\") ~ .val %in% 21:22,\n      lbl(0, \"unpartnered\") ~ .val %in% c(10, 31, 32)\n  )) \n\nbf_merged %>% count(MARSTAT)\n\n\n# A tibble: 2 x 2\n          MARSTAT     n\n*       <dbl+lbl> <int>\n1 0 [unpartnered]  2227\n2 1 [partnered]    4717\n\nAnother reason to consider recoding categorical variables: what if one response option dominates a huge proportion of the responses in your data? Is it worth sacrificing additional degrees of freedom to accommodate dummy variables that could otherwise be merged together? This is the case with EDUCATTGEN, where over half of the responses are “never attended.” We’ll create a single, simplified binary variable where the responses are “some schooling” or “no schooling.”\n\n\nbf_merged %>% count(EDUCATTGEN)\n\n\n# A tibble: 5 x 2\n                    EDUCATTGEN     n\n*                    <int+lbl> <int>\n1  1 [Never attended]           3605\n2  2 [Primary/Middle school]    1212\n3  3 [Secondary/post-primary]   1893\n4  4 [Tertiary/post-secondary]   231\n5 NA                               3\n\nbf_merged <- bf_merged %>% \n  mutate(EDUCATTGEN = lbl_relabel(\n      EDUCATTGEN,\n      lbl(1, \"some schooling\") ~ .val %in% 2:4,\n      lbl(0, \"no school\") ~ .val == 1\n  )) \n\nbf_merged %>% count(EDUCATTGEN)\n\n\n# A tibble: 3 x 2\n           EDUCATTGEN     n\n*           <dbl+lbl> <int>\n1  0 [no school]       3605\n2  1 [some schooling]  3336\n3 NA                      3\n\nThe last thing we’ll do here is coerce SAMPLE as a factor so that we can control for arbitrary differences caused by selection into the two samples (recall that our dataset contains two samples from Burkina Faso 2017 and 2018). Because the women in each SAMPLE were surveyed in two different years, this essentially operates like a survey-year fixed effect.\n\n\nbf_merged <- bf_merged %>% \n  mutate(SAMPLE = as.factor(SAMPLE)) \n\nbf_merged %>% count(SAMPLE)\n\n\n# A tibble: 2 x 2\n  SAMPLE     n\n* <fct>  <int>\n1 85405   3556\n2 85408   3388\n\nRegression Models\nWe’re now ready to examine the relative effects individual factors on FPCURRUSE compared to the availability and accessibility of family planning services in each woman’s enumeration area. Let’s begin with a simple model containing the factors we added to the dataset above.\nIndividual factors: model with glm\nMost R users probably use the generalized linear model function glm for this purpose. To keep our demonstration as simple as possible, we’ll fit a model using the Ordinary Least-Squares (OLS) method that glm adopts by default. We’ll use the tidymodels function broom::tidy to clean up the output for our model’s coefficient estimates.\n\nRecall that FPCURRUSE is a binary response (“yes” or “no”), so you might consider fitting a logit model by adding the argument family = 'binomial' to glm().\n\n\nm1 <- glm(\n  FPCURRUSE ~\n    AGE + \n    MARSTAT + \n    EDUCATTGEN + \n    BIRTHEVENT  + \n    WEALTHQ + \n    SAMPLE,\n  data = bf_merged\n)\n\ntidy(m1)\n\n\n# A tibble: 10 x 5\n   term                    estimate std.error statistic  p.value\n   <chr>                      <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)              0.0421   0.0260       1.62  1.06e- 1\n 2 AGE                     -0.00280  0.000948    -2.95  3.17e- 3\n 3 MARSTAT                  0.201    0.0145      13.9   5.21e-43\n 4 EDUCATTGEN               0.159    0.0143      11.2   9.42e-29\n 5 BIRTHEVENT               0.0289   0.00390      7.40  1.48e-13\n 6 WEALTHQLower quintile    0.0319   0.0204       1.56  1.18e- 1\n 7 WEALTHQMiddle quintile   0.0222   0.0208       1.07  2.85e- 1\n 8 WEALTHQHigher quintile   0.0884   0.0198       4.46  8.25e- 6\n 9 WEALTHQHighest quintile  0.166    0.0191       8.67  5.24e-18\n10 SAMPLE85408              0.00515  0.0114       0.453 6.51e- 1\n\nBecause the outcome (FPCURRUSE) is binary, this linear regression is a linear probability model and the coefficients on each term should be interpreted as a percentage point change in the probability of current family planning use.\nFor each of the binary dummy variables we created, the coefficient estimate shows how much the probability FPCURRUSE == \"yes\" increases if the value of the dummy variable is 1. For example, in MARSTAT the value 1 represents “partnered” women, while the value 0 represented “unpartnered” women. The coefficient on MARSTAT is 0.201, meaning our model predicts that being partnered is associated with an increase in the expected probability of family planning use by 0.201.\nIs this a meaningful difference? Consider that the mean of FPCURRUSE is 0.34: this is the probability you might use to guess a woman’s likelihood for using family planning if we didn’t have access to any other variables. Relative to that, an increase of 0.201 is pretty substantive.\nWhat about the other coefficients? We also see a large increase in the probability of family planning use for women who have “some schooling,” and a smaller increase for those who have more children.\nNotice what happened with WEALTHQ, the variable we coerced as a factor above. As expected, R created a binary dummy variable from each response option except the reference group, which is the “lowest quintile.” It’s important to remember that each of these dummy variables represents the effect a being in a particular quintile relative to the “lowest quintile.” These results show that family planning use increases with wealth, which is expected (although the effects don’t become large or statistically significant until we get to the “high” and “highest” income quintiles).\nIndividual factors: model with svyglm\nThere is one problem with the model we created above: as we’ve discussed, PMA samples households randomly within the same enumeration area, and it’s likely that households located together will share many common features. This violates one of the basic assumptions of OLS regression, where we expect modeling errors to be uncorrelated (Cameron and Miller 2015). To address this, we’ll need to use a model that allows us to specify the complexities of PMA survey design. A common approach uses the survey package developed by Thomas Lumley.1\n\nWe’ll use the package svyglm to specify PMA survey design whenever we create analytic models on the PMA Data Analysis Hub!\nLumley’s modeling function survey::svyglm is similar to glm, except that it takes a special design argument where glm takes a data argument. We use the function survey::svydesign to specify the data, the cluster ids from EAID, and the sampling strata STRATA (if we were using the sample weights from FQWEIGHT, we could do that here, too):\n\n\nm2 <- svyglm(\n  FPCURRUSE ~\n    AGE + \n    MARSTAT + \n    EDUCATTGEN + \n    BIRTHEVENT  + \n    WEALTHQ + \n    SAMPLE,\n  design = svydesign(\n    ids = ~EAID,\n    strata = ~STRATA,\n    data = bf_merged\n  )\n)\n\ntidy(m2)\n\n\n# A tibble: 10 x 5\n   term                    estimate std.error statistic  p.value\n   <chr>                      <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)              0.0421    0.0366      1.15  2.54e- 1\n 2 AGE                     -0.00280   0.00114    -2.45  1.65e- 2\n 3 MARSTAT                  0.201     0.0157     12.8   3.19e-20\n 4 EDUCATTGEN               0.159     0.0167      9.53  2.13e-14\n 5 BIRTHEVENT               0.0289    0.00405     7.13  6.41e-10\n 6 WEALTHQLower quintile    0.0319    0.0244      1.31  1.95e- 1\n 7 WEALTHQMiddle quintile   0.0222    0.0282      0.788 4.33e- 1\n 8 WEALTHQHigher quintile   0.0884    0.0302      2.92  4.62e- 3\n 9 WEALTHQHighest quintile  0.166     0.0283      5.86  1.30e- 7\n10 SAMPLE85408              0.00515   0.0145      0.356 7.23e- 1\n\nTo see how this impacts our model estimates, let’s visualize the confidence interval for each coefficient with dotwhisker::dwplot. We’ll use the same function a few times here, and we’ll want to repeat the same visual elements each time, so we’ll just wrap everything together in a custom function we’re calling pma_dwplot():\n\n\npma_dwplot <- function(...){\n  dwplot(\n    bind_rows(...),\n    dodge_size = 0.8,\n    vline = geom_vline(xintercept = 0, colour = \"grey60\", linetype = 2)) +\n    scale_color_viridis_d(option = \"plasma\", end = .7) +\n    theme_minimal()\n}\n\npma_dwplot(\n  tidy(m1) %>% mutate(model = \"glm\"),\n  tidy(m2) %>% mutate(model = \"svyglm\")\n)\n\n\n\n\nWe can see from this plot that the confidence intervals obtained from svyglm are wider than those we got from glm, but the point estimates for each coefficient are unchanged. We also added a dashed line at 0 to make it really easy to see when coefficients are statistically insignificant at the 5% level (if so, the “whiskers” of a 95% confidence interval will cross 0).\nAvailabillity: modeling with SDP variables\nWhile these individual factors are important, we should also expect the availability and accessibility of family planning services to partially determine their use (Bongaarts 2011). Back in an earlier post, we observed that the women in our sample appeared to be 5% more likely to use family planning if they lived in an enumeration area where no SDPs reported a recent contraceptive stockout, compared to women living in areas where at least one SDP did experience a recent stockout. Now we’ll see if that difference is statistically significant, controlling for other factors.\n\nFor our purposes, a “recent stockout” is a stockout of any contraceptive method normally provided by an SDP if the stockout occurred within 3 months prior to the survey.\nFirst, we’ll create a binary variable STOCKOUT indicating whether each woman lives in an enumeration area where at least one SDP reported a recent stockout:\n\n\nbf_merged <- bf_merged %>%\n  mutate(STOCKOUT = case_when(\n    NUM_METHODS_OUT3MO > 0 ~ 1,\n    NUM_METHODS_OUT3MO == 0 ~ 0\n  ))\n\nbf_merged %>% count(STOCKOUT)\n\n\n# A tibble: 3 x 2\n  STOCKOUT     n\n*    <dbl> <int>\n1        0  4561\n2        1  1725\n3       NA   658\n\nNext, we’ll add STOCKOUT to our previous model, along with NUM_METHODS_PROV (the number of methods available from at least one SDP serving the woman’s enumeration area) and N_SDP (the number of sampled SDPs serving the woman’s enumeration area).\n\n\nm3 <- svyglm(\n  FPCURRUSE ~\n    AGE + \n    MARSTAT + \n    EDUCATTGEN + \n    BIRTHEVENT  + \n    WEALTHQ + \n    SAMPLE + \n    STOCKOUT + \n    NUM_METHODS_PROV + \n    N_SDP,\n  design = svydesign(\n    ids = ~EAID,\n    strata = ~STRATA,\n    data = bf_merged\n  )\n)\n\ntidy(m3)\n\n\n# A tibble: 13 x 5\n   term                     estimate std.error statistic  p.value\n   <chr>                       <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)              0.113      0.105      1.08   2.85e- 1\n 2 AGE                     -0.00249    0.00114   -2.19   3.23e- 2\n 3 MARSTAT                  0.201      0.0161    12.5    1.94e-19\n 4 EDUCATTGEN               0.168      0.0169     9.93   6.35e-15\n 5 BIRTHEVENT               0.0272     0.00414    6.57   7.77e- 9\n 6 WEALTHQLower quintile    0.0380     0.0244     1.56   1.24e- 1\n 7 WEALTHQMiddle quintile   0.0211     0.0287     0.736  4.64e- 1\n 8 WEALTHQHigher quintile   0.0879     0.0300     2.93   4.58e- 3\n 9 WEALTHQHighest quintile  0.162      0.0270     6.02   7.55e- 8\n10 SAMPLE85408              0.000951   0.0175     0.0542 9.57e- 1\n11 STOCKOUT                -0.0444     0.0206    -2.16   3.44e- 2\n12 NUM_METHODS_PROV        -0.00199    0.0104    -0.192  8.48e- 1\n13 N_SDP                   -0.0165     0.0120    -1.38   1.72e- 1\n\nIndeed, women living in an enumeration area where we’re aware of recent stockouts are less likely to be currently using family planning! The effect isn’t quite as large as some of the individual level factors we’ve examined, but it is statistically significant (p < 0.05).\nDoes the introduction of SDP variables change our estimates for the individual factors we examined previously? A new dwplot seems to show little difference:\n\n\npma_dwplot(\n  tidy(m2) %>% mutate(model = \"Individual-only\"),\n  tidy(m3) %>% mutate(model = \"SDP + Individual\")\n)\n\n\n\n\nAccessibility: modeling with external spatial variables\nAvailability of family planning methods (or lack thereof) is not the same as accessibility. The variables we created in our last post using external geospatial data allow us to explore some factors related to accessibility, which is what we’ll add now. We’ll complement these external variables with URBAN, indicating whether the woman lives in an urban area.\n\n\nm4 <- svyglm(\n  FPCURRUSE ~\n    AGE + \n    MARSTAT + \n    EDUCATTGEN + \n    BIRTHEVENT  + \n    WEALTHQ + \n    SAMPLE + \n    STOCKOUT + \n    NUM_METHODS_PROV + \n    N_SDP + \n    POP_DENSITY + \n    ROAD_LENGTH + \n    URBAN,\n  design = svydesign(\n    ids = ~EAID,\n    strata = ~STRATA,\n    data = bf_merged\n  )\n)\n\ntidy(m4) \n\n\n# A tibble: 16 x 5\n   term                       estimate  std.error statistic  p.value\n   <chr>                         <dbl>      <dbl>     <dbl>    <dbl>\n 1 (Intercept)              0.0833     0.117         0.712  4.79e- 1\n 2 AGE                     -0.00303    0.00112      -2.71   8.61e- 3\n 3 MARSTAT                  0.210      0.0157       13.4    1.97e-20\n 4 EDUCATTGEN               0.162      0.0174        9.30   1.28e-13\n 5 BIRTHEVENT               0.0284     0.00417       6.82   3.41e- 9\n 6 WEALTHQLower quintile    0.0343     0.0249        1.38   1.72e- 1\n 7 WEALTHQMiddle quintile   0.0124     0.0291        0.426  6.72e- 1\n 8 WEALTHQHigher quintile   0.0405     0.0312        1.30   1.99e- 1\n 9 WEALTHQHighest quintile  0.0734     0.0384        1.91   6.07e- 2\n10 SAMPLE85408              0.00133    0.0179        0.0744 9.41e- 1\n11 STOCKOUT                -0.0483     0.0211       -2.29   2.53e- 2\n12 NUM_METHODS_PROV        -0.00225    0.0104       -0.216  8.30e- 1\n13 N_SDP                   -0.0158     0.0124       -1.27   2.09e- 1\n14 POP_DENSITY             -0.00000144 0.00000522   -0.276  7.83e- 1\n15 ROAD_LENGTH              0.00110    0.00144       0.760  4.50e- 1\n16 URBAN                    0.0773     0.0370        2.09   4.03e- 2\n\npma_dwplot(\n  tidy(m2) %>% mutate(model = \"Individual-only\"),\n  tidy(m3) %>% mutate(model = \"SDP + Individual\"),\n  tidy(m4) %>% mutate(model = \"All\")\n) \n\n\n\n\nThis figure with all three models reveals that marital status, education, number of births, and living in an enumeration area that faced recent stockouts are all significantly associated with current family planning use. However, it’s pretty difficult to compare the effects across all the variables. The coefficients on age, population density, and road length are particularly hard to examine and compare. dotwhisker includes a very handy function that re-scales continuous variables on the right-hand side of your regression model to make them more comparable to binary predictors. Specifically, dotwhisker::by_2sd() re-scales continuous input variables by 2 standard deviations following Gelman (2008).2\nWhile we’re adding some last touches to make the plot more readable, we’ll also provide a title, clearer variable names on the Y axis, a tighter scale on the X axis, and a caption at the bottom.\n\n\nlist(\n  tidy(m2) %>% mutate(model = \"Individual-only\"),\n  tidy(m3) %>% mutate(model = \"SDP + Individual\"),\n  tidy(m4) %>% mutate(model = \"All\")\n) %>% \n  map(~by_2sd(.x, bf_merged)) %>% \n  bind_rows() %>% \n  relabel_predictors(\n    c(\n      AGE = \"Age\",  \n      MARSTAT = \"Married\", \n      EDUCATTGEN = \"Some Schooling\", \n      BIRTHEVENT = \"No. of Children\",\n      `WEALTHQLower quintile` = \"Lower Wealth Quintile\", \n      `WEALTHQMiddle quintile` = \"Middle Wealth Quintile\",\n      `WEALTHQHigher quintile` = \"Higher Wealth Quintile\", \n      `WEALTHQHighest quintile` = \"Highest Wealth Quintile\", \n      STOCKOUT = \"Recent Stockout\", \n      NUM_METHODS_PROV = \"No. of FP Methods\",\n      N_SDP = \"No. of SDP Providers\",\n      POP_DENSITY = \"Population Density (w/i 10 km)\",\n      ROAD_LENGTH = \"Road length (w/i 10 km)\",\n      URBAN = \"Lives in Urban EA\",\n      SAMPLE85408 = \"2018 Sample\"\n    )\n  ) %>% \n  dwplot(\n    dodge_size = 0.8,\n    vline = geom_vline(xintercept = 0, colour = \"grey60\", linetype = 2)\n  ) + \n  scale_color_viridis_d(option = \"plasma\", end = .7) +\n  theme_minimal() +\n  labs(\n    x = \"Coefficient\",\n    color = NULL,\n    title = \"Marital Status Is the Strongest Predictor of Family Planning Use\",\n    subtitle = \"Impact of Individual and Contextual Factors on Family Planning Use\",\n    caption = \"Source: IPUMS PMA (Burkina Faso 2017-2018), DIVA-GIS (road length), and WorldPop (population density)\"\n  ) +\n  scale_x_continuous(limits = c(-0.1, 0.3)) + # to make space for the legend\n  theme(\n    legend.position = c(0.8, 0.2),\n    title = element_text(size = 8),\n    legend.text = element_text(size = 8),\n    plot.caption = element_text(hjust = 0), #left align the caption\n    legend.background = element_rect(colour = \"grey80\")\n  )\n\n\n\n\nNow that we’ve re-scaled the continuous input variables by two standard deviations, we can much more easily see the relationship between age and family planning use. Across all models a one-year increase in a woman’s age is associated with a five percentage point lower expected probability of using family planning. This effect is statistically significant at the 5% level in all three models as well.\nThe relationships we observed with marital status, education, and number of children are quite stable across all the models – even as we added variables representing the service environment and broader context of contraceptive availability the coefficients did not meaningfully change.\nThis is in pretty striking contrast to what happens to the wealth quintile variables. When we included only woman and SDP characteristics, being in either the higher and highest wealth quintiles was associated with large and statistically significant increases in the probability of using family planning. But as we added geospatial variables and the URBAN variable in particular, the coefficients become smaller and the confidence intervals become wider. This indicates that there was likely omitted variable bias because wealth is correlated with living in an urban area but when we excluded URBAN the wealth quintile variables were capturing some of this relationship with family planning use.\nEven though this analysis was relatively simple, it was quite informative about different drivers of family planning use. You could easily extend this analysis to include other factors that influence family planning use, incorporate fixed or random effects, or take advantage of the multiple years of survey data!\nAs always, let us know if you have any questions on this post or if you’re working any fertility related analyses and you have a question that we can help address with the blog!\n\n\n\nBongaarts, John. 2011. “Can Family Planning Programs Reduce High Desired Family Size in Sub-Saharan Africa?” International Perspectives on Sexual and Reproductive Health 37 (4): 209–16. https://doi.org/10.1363/3720911.\n\n\nCameron, A. Colin, and Douglas L. Miller. 2015. “A Practitioner’s Guide to Cluster-Robust Inference.” Journal of Human Resources 50 (2): 317–72.\n\n\nGelman, Andrew. 2008. “Scaling Regression Inputs by Dividing by Two Standard Deviations.” Statistics in Medicine 27 (15): 2865–73. https://doi.org/10.1002/sim.3107.\n\n\nLumley, Thomas. 2011. Complex Surveys: A Guide to Analysis Using R. Wiley Series in Survey Methodology. John Wiley & Sons.\n\n\nWe highly recommend Lumley’s (2011) book, Complex Surveys: A Guide to Analysis Using R.↩︎\nWe recommend checking out the full paper, but the short explanation is that with binary predictors you are comparing a value of 0 to a value of 1 when interpreting coefficients. A 1-unit change in a binary predictor is equivalent to a 2 standard deviation change because the standard deviation of a binary variable with equal probabilities is 0.5.↩︎\n",
    "preview": "posts/2021-02-19-analyzing-the-individual-in-context/images/results.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 1950,
    "preview_height": 1199
  },
  {
    "path": "posts/2021-02-04-merging-external-spatial-data/",
    "title": "Merging external spatial data",
    "description": "How to integrate external spatial data with PMA data.",
    "author": [
      {
        "name": "Nina Brooks",
        "url": "http://www.ninarbrooks.com/"
      }
    ],
    "date": "2021-02-15",
    "categories": [
      "Individuals in Context",
      "Service Delivery Points",
      "Data Manipulation",
      "sf",
      "raster",
      "Spatial"
    ],
    "contents": "\n\nContents\nData\nSetup: Load packages and data\nPopulation Density: working with raster data\nRoad Networks: Working with vector data\n\n\n\n\nOur last post showed how to read, merge, and map the PMA GPS data - and how mapping can shed light on interesting spatial variation. A big advantage of the PMA GPS data is that you can also merge in other sources of spatial data, which opens up enormous opportunities for analyzing how contextual and environmental factors affect topics of interest in the PMA data. In this post, we’ll show how to merge in two different types of spatial data and construct variables of interest.\nData\nWe’ll be using toy PMA GPS data for this post. To use real PMA GPS data you must request access directly from our partners at pmadata.org. The toy data we’ll use here contains randomly sampled locations within Burkina Faso which have no actual relationship to the EAs in the PMA data. This means none of the interpretations of spatial patterns will hold, but all the code will run.\nWe will also be introducing two different spatial datasets that represent different kinds of spatial data. The first is population density data from WorldPop.1 If you want to download the data from the WorldPop site, we’re using the “Unconstrained individual countries 2000-2020 (1 km resolution)” data from 2017 for Burkina Faso. This is raster data, which means the data are stored as a grid of values which are rendered on a map as pixels. You can think of this as a matrix that is spatially referenced – that is each pixel represents a specific area of land on the Earth. Lots of spatial data are stored as rasters including climate data (e.g., temperature and rainfall), elevation, and satellite images. Note that the raster data is saved as a .tiff (which is a common way of storing raster data). The resolution of the raster maps to the area that each pixel represents in the real world. The population density is 1 km resolution, which means that each pixel represents a 1 km by 1 km square on the ground. The figure below shows the impact of different spatial resolutions for the same raster data.2\n\n\n\nFigure 1: Source: NEON\n\n\n\n\nThere are tons of resources on earth data science in R. We recommend the resources by Earth Lab and NEON by NSF. This post is an excellent introduction to working with rasters in R!\nPopulation density is also conceptually important to the SDP data on contraceptive supply that we’ve been examining through this series of posts. Population density may provide a more nuanced characterization of urbanization than the URBAN variable. Additionally, density may be correlated with longer wait times at clinics, which may also impact contraceptive use at the individual level.\nThe second spatial dataset we’ll introduce is data on road networks in Burkina Faso from the Digital Chart of the World and made publicly available by DIVA-GIS, an excellent source for publicly available spatial datasets. Road networks serve as a proxy for accessibility to health clinics – an important component of the contraceptive service environment – that may be more nuanced than the binary urban/rural distinction. To download the road data, go to DIVA-GIS Data and select Burkina Faso from the Country dropdown and Roads from the Subject dropdown. The road data is called vector data and is stored in a shapefile (.shp). Vector data is used to represent real world features and are three basic types: points, lines, and polygons. The road data we’re using in this post is an example of vector line data.\n\nRemember the administrative boundaries we used in the previous post were polygons and the GPS points for the PMA enumeration areas were points. Both are vector data!\nSetup: Load packages and data\nWe’ll be using many of the packages from the last few posts, as well as a new package for specifically working with raster data – appropriately called raster – and one called units, which enables easy conversion between objects of different units. Make sure to install the raster and units packages first and then load everything we’ll be using today:\n\n\nlibrary(sf) # primary spatial package\nlibrary(raster) # for working with raster data\nlibrary(viridis) # for color palettes\nlibrary(units) # to easily convert between units\nlibrary(tidyverse)\n\n\n\nLet’s start by reading in the raster using raster::raster() and check out the meta-data.\n\n\npop_density <- raster(\"bfa_pd_2017_1km.tif\")\npop_density\n\n\nclass      : RasterLayer \ndimensions : 682, 951, 648582  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -5.517917, 2.407083, 9.407917, 15.09125  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : /Users/Matt/R/pma-data-hub/_posts/2021-02-04-merging-external-spatial-data/bfa_pd_2017_1km.tif \nnames      : bfa_pd_2017_1km \nvalues     : 0.281988, 8820.016  (min, max)\n\nBecause rasters are essentially just matrices, you can think of the dimensions in the same way. At a spatial resolution of 1 km, this raster covers all of Burkina Faso with 648,582 cells. The resolution describes the size of the cells (the length of one side of each square cell). You may be wondering why this is showing up as 0.00833 when the data has a spatial scale of 1 km by 1 km. This is because the units that the resolution is reported in depend on the coordinate reference system of the data. More on this in a moment.\nThe extent (or spatial extent) refers to the geographic area that the raster covers. The values are in the same coordinate reference system as the raster. The coordinate reference system or crs is the next piece of meta-data we have. “A coordinate reference system (CRS) is a coordinate-based local, regional or global system used to locate geographical entities.”3 The crs for this raster is +proj=longlat +datum=WGS84 +no_defs. The crs contains several pieces of information including the datum (WGS84) and the projection.4 The appropriate CRS to use for any given spatial task depends on what part of the world the data represent and what kind of spatial operations you’ll be performing. It’s really important to know what crs your data are in and make sure that all your spatial data are in the same  crs if you use more than one kind. Otherwise, they won’t line up on a map and any spatial analysis or processing you do will be incorrect.\nThe projection of this raster data is described as longlat, which actually is not a projection. A projection refers to how the Earth’s surface is flattened so it can be represented as a 2-dimensional raster grid. These data use a geographic coordinate system, simply the raw latitude and longitude coordinates, rather than a projected coordinate system, which would transform the coordinates into a 2-dimensional plane. Latitude and longitude locate positions on the Earth using angles, so the spacing of each line of latitude as you move north or south along the Earth is not uniform. The units of this reference system are in degrees (of latitude and longitude), so the 0.00833 resolution we saw above is reporting the spatial resolution in degrees, rather than meters or kilometers. This crs is not ideal for measuring distances because the distance covered by a single degree of latitude or longitude varies greatly across the Earth’s surface. This also means that the stated 1 km resolution is only nominal. At the equator, 0.00833 degrees is approximately equal to 1 km, but this distance, and the ground area represented by each pixel, will vary. Fortunately, Burkina Faso is relatively close to the equator, so the pixels will be quite close to 1 km by 1 km.\nThe last piece of meta-data to look at are the values – this is reporting the minimum and maximum values across all of the cells. Because these are population density data, it can be interpreted as the number of people in each pixel divided by the area of each pixel (which we know is 1 km2)\nNow that we’ve reviewed the raster attributes, let’s see what it looks like. We can use the basic plot function to do this.\n\n\nplot(pop_density)\n\n\n\n\nWe can see three locations stand out in terms of population density. First is Ouagadougou the capital of Burkina Faso and largest city, right in the center. Then we can see higher density around Bobo Dioulasso and Banfora in the southwest of the country, which are the second and third largest cities in the country.\nNext we’ll load the roads data using sf::st_read().\n\n\nroads <- st_read(\"BFA_roads/BFA_roads.shp\", quiet = TRUE)\nroads\n\n\nSimple feature collection with 1149 features and 5 fields\ngeometry type:  MULTILINESTRING\ndimension:      XY\nbbox:           xmin: -5.482261 ymin: 9.407643 xmax: 2.393089 ymax: 15.08071\ngeographic CRS: WGS 84\nFirst 10 features:\n       MED_DESCRI      RTT_DESCRI F_CODE_DES ISO   ISOCOUNTRY\n1  Without Median Secondary Route       Road BFA BURKINA FASO\n2  Without Median Secondary Route       Road BFA BURKINA FASO\n3  Without Median Secondary Route       Road BFA BURKINA FASO\n4  Without Median Secondary Route       Road BFA BURKINA FASO\n5  Without Median Secondary Route       Road BFA BURKINA FASO\n6  Without Median Secondary Route       Road BFA BURKINA FASO\n7  Without Median Secondary Route       Road BFA BURKINA FASO\n8  Without Median Secondary Route       Road BFA BURKINA FASO\n9  Without Median Secondary Route       Road BFA BURKINA FASO\n10 Without Median Secondary Route       Road BFA BURKINA FASO\n                         geometry\n1  MULTILINESTRING ((-0.720550...\n2  MULTILINESTRING ((-0.583273...\n3  MULTILINESTRING ((-0.397415...\n4  MULTILINESTRING ((-0.142728...\n5  MULTILINESTRING ((-0.403059...\n6  MULTILINESTRING ((-0.171111...\n7  MULTILINESTRING ((-0.116756...\n8  MULTILINESTRING ((0.0672155...\n9  MULTILINESTRING ((-1.245636...\n10 MULTILINESTRING ((-1.50246 ...\n\nThis sf object also contains meta-data (shown at the top). In terms of meta-data, the geometry type field tells us this data is a MULTILINESTRING object, which makes sense since these are roads. The bbox (short for bounding box), is the same information as the extent field for the raster data – it tells us the bounds of the geographic area that this spatial data covers. We see the geographic CRS which is the coordinate reference system of the data. For this roads dataset it is WGS84, which is the same as the population density raster data.\nThe roads data contains several variables: MED_DESCRI, RTT_DESCRI, F_CODE_DES, ISO, ISOCOUNTRY, and geometry. The first three variables provide some information about the types of roads in this data. ISO and ISOCOUNTRY simply provide country codes and names for the data. Finally, we see the geometry variable, which is the variable that contains the spatial information in an sf object.\nWe can also plot this roads data to see what it looks like.\n\n\nplot(roads)\n\n\n\n\nBy calling the basic plot function, we get a panel of plots of the road network, with one plot for each variable. We can see some variation in color MED_DESCRI and RTT_DESCRI, indicating that there multiple values for those variables. If we wanted just a single plot of the road network, we can get that by calling plot on the geometry variable:\n\n\nplot(roads$geometry)\n\n\n\n\nFinally, we’ll load the “toy” GPS data and convert it to an sf object. The option crs = 4326 means that we are creating this with the WGS84 coordinate reference system because 4326 is the EPSG code for WGS84.\n\nMost crs are assigned an “EPSG code”, which is a unique ID that can be used to identify a CRS.\n\n\ngps <- read_csv(\"bf_gps_fake.csv\") %>%\n  rename(EAID = EA_ID) %>% # rename to be consistent with other PMA data\n  st_as_sf(\n    coords = c(\"GPSLONG\", \"GPSLAT\"), \n    crs = 4326)\n\n\n\n\n\n\nPopulation Density: working with raster data\nWe want to construct a variable that captures the population density at each enumeration area in the data. We’ll use sf::st_buffer() to do this, which will construct a buffer circle around each GPS point. The PMA GPS data are randomly displaced to protect the privacy of respondents, so it’s imperative to consider this displacement when working with the GPS data to do spatial operations. Because the maximum displacement distance is 10 km, if we construct buffers with a radius of 10 km we can be 100% confident that the true locations of each GPS point fall within that buffer.\n\nUrban EAs are displaced from their true location up to 2 km. Rural EAs are displaced from their true location up to 5 km. Additionally, a random sample of 1% of rural EAs are displaced up to 10km.\n\n\nbuffers <- st_buffer(gps, dist = 10000)\n\n\nWarning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle\n= endCapStyle, : st_buffer does not correctly buffer longitude/\nlatitude data\ndist is assumed to be in decimal degrees (arc_degrees).\n\nggplot() +\n  geom_sf(data = buffers) +\n  geom_sf(data = gps)\n\n\n\n\nThis giant circle is certainly not what we would expect! What’s going on here? Earlier in this post we mentioned that the WGS84 crs is a geographic coordinate system that simply uses the latitude and longitude coordinates to identify locations and the units are in degrees, rather than meters or kilometers. This circle thus has a radius of 10,000 degrees and since the Earth only spans 360 degrees it is fully covered by this circle. As we mentioned, the WGS84 crs is not ideal for measuring distances. R alerted us of this problem with two warnings: st_buffer does not correctly buffer longitude/latitude data and dist is assumed to be in decimal degrees (arc_degrees). This is why it’s so important to pay attention to the crs of your data.\nTo properly construct a buffer circle around these GPS points, we need to transform the data to a different projection that uses meters or kilometers. And, because it’s essential that all of our data are in the same crs, we need to transform or reproject everything. For vector data, we can do this using sf::st_transform() and for raster data we’ll do this with raster::projectRaster(). For the transformation, we’re using a crs that is projected to meters and is appropriate to the local geography of Burkina Faso. You can read about it on the epsg.io site. After reprojecting, we’ll calculate the buffer again and plot it to make sure this looks right.\n\n\n# transform the GPS data\ngps_tr <- gps %>% st_transform(crs = 32630)\ngps_tr\n\n\nSimple feature collection with 83 features and 5 fields\ngeometry type:  POINT\ndimension:      XY\nbbox:           xmin: 260943.3 ymin: 1114669 xmax: 1025472 ymax: 1653511\nprojected CRS:  WGS 84 / UTM zone 30N\n# A tibble: 83 x 6\n   PMACC PMAYEAR REGION                EAID DATUM           geometry\n * <chr>   <dbl> <chr>                <dbl> <chr>        <POINT [m]>\n 1 BF       2017 5. centre-nord        7610 WGS84 (837531.4 1567675)\n 2 BF       2017 1. boucle-du-mouhoun  7820 WGS84 (491871.7 1488848)\n 3 BF       2017 3. centre             7271 WGS84   (982414 1349907)\n 4 BF       2017 3. centre             7799 WGS84   (739431 1352652)\n 5 BF       2017 8. est                7243 WGS84 (545866.2 1219668)\n 6 BF       2017 6. centre-ouest       7026 WGS84 (352638.7 1209502)\n 7 BF       2017 3. centre             7859 WGS84 (833822.1 1377527)\n 8 BF       2017 3. centre             7725 WGS84 (980025.8 1406727)\n 9 BF       2017 6. centre-ouest       7390 WGS84 (439876.7 1190609)\n10 BF       2017 11. plateau-central   7104 WGS84 (835483.2 1469280)\n# … with 73 more rows\n\n# reproject the raster data\npop_density_tr <- projectRaster(\n  pop_density, \n  crs = \"+proj=utm +zone=30 +datum=WGS84 +units=m +no_defs\"\n)\npop_density_tr\n\n\nclass      : RasterLayer \ndimensions : 699, 970, 678030  (nrow, ncol, ncell)\nresolution : 907, 922  (x, y)\nextent     : 218942.9, 1098733, 1035714, 1680192  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=30 +datum=WGS84 +units=m +no_defs \nsource     : memory\nnames      : bfa_pd_2017_1km \nvalues     : 0.9709167, 8775.492  (min, max)\n\n\nNote: the projectRaster function takes crs as a character string, rather than the EPSG code 32630. We’re using the PROJ.4 code shown in the “Export” menu on the epsg.io site.\n\n\n# calculate 10 km (10,000 meter) buffer circles\nbuffers_tr <- st_buffer(gps_tr, dist = 10000) # because the units are in meters\n\n# plot\nggplot() +\n  geom_sf(data = buffers_tr) +\n  geom_sf(data = gps_tr, color = \"red\")\n\n\n\n\nLooking at the meta-data for both the gps_tr and raster_tr objects, we can see they have the same new projected crs: UTM zone 30N. The raster_tr meta-data also includes information on the units (+units=m) confirming that distances are measured in meters. Turning to the plot, we can see the GPS coordinates marked in red and each has a circle around it.\nNow that we have correctly estimated 10 km buffer circles, we can calculate the average population density within each buffer using the raster::extract() command and specifying fun = mean. This produces an 83 x 1 vector of results, which means we have one population density value for each enumeration area. Printing the first 5 results shows there is some substantial variation in population density.\n\n\nbuffer_density <- raster::extract(\n  pop_density_tr, \n  buffers_tr, \n  fun = mean, \n  na.rm = TRUE,\n  cellnumbers = TRUE\n)\ndim(buffer_density)\n\n\n[1] 83  1\n\nhead(buffer_density)\n\n\n          [,1]\n[1,]  35.58080\n[2,]  21.36878\n[3,]  17.56161\n[4,] 113.36298\n[5,]  31.73017\n[6,]  57.09591\n\nNote, that we don’t actually need to create the buffers first to extract the mean values of the raster. We can do it all in one step, shown below. Just make sure to use the gps_tr object instead of the buffer_tr object! But, we’ll use those buffers again with the road data.\n\n\nbuffer_density_alt <- raster::extract(\n  pop_density_tr, gps_tr, \n  buffer = 10000,\n  fun = mean, \n  na.rm = TRUE\n)\nhead(buffer_density_alt)\n\n\n[1]  35.58080  21.36878  17.56161 113.21781  31.73017  57.09591\n\nFinally, so we can merge everything together by EAID, let’s add the population density calculation directly to the gps_tr data. Note that the raster::extract() command preserves the order of the inputs, so we know the first row of the density calculation corresponds to the first row of the gps_tr data.\n\n\ngps_tr$pop_density <- raster::extract(\n  pop_density_tr, gps_tr, \n  buffer = 10000,\n  fun = mean, na.rm = TRUE\n)\n\n\n\nRoad Networks: Working with vector data\nBefore we do anything with the road data, let’s make sure to reproject it to match the rest of our data.\n\n\nroads_tr <- roads %>%\n  st_transform(crs = 32630)\n\n\n\nBecause enumeration areas with better access to roads may make it easier for women to reach local service delivery providers. We are going to calculate the total length of roads within each buffer as a proxy for this accessibility. Because each of these buffers was constructed with the same 10 km radius, they have the same area, which means the sum of road length can also be thought of as a road density measure.\nFirst, we need to identify which portions of the road fall into each buffer. We’ll use sf::st_intersection(), which returns a new sf object that contains observations from the first argument that touch (geographically) the second argument.\n\nNote that there is also an sf::intersects() command. This is different than the one we’re using because it returns a logical matrix that indicates whether each geometry pair intersects. See more on these types of operations in the sf vignette.\n\n\nint <- st_intersection(roads_tr, buffers_tr)\nint\n\n\nSimple feature collection with 238 features and 10 fields\ngeometry type:  LINESTRING\ndimension:      XY\nbbox:           xmin: 251238.7 ymin: 1104708 xmax: 1033002 ymax: 1658158\nprojected CRS:  WGS 84 / UTM zone 30N\nFirst 10 features:\n        MED_DESCRI      RTT_DESCRI F_CODE_DES ISO   ISOCOUNTRY PMACC\n240 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n241 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n268 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n711 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n640    With Median   Primary Route       Road BFA BURKINA FASO    BF\n649    With Median   Primary Route       Road BFA BURKINA FASO    BF\n728 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n958 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n959 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n964 Without Median Secondary Route       Road BFA BURKINA FASO    BF\n    PMAYEAR               REGION EAID DATUM\n240    2017 1. boucle-du-mouhoun 7820 WGS84\n241    2017 1. boucle-du-mouhoun 7820 WGS84\n268    2017 1. boucle-du-mouhoun 7820 WGS84\n711    2017            3. centre 7271 WGS84\n640    2017            3. centre 7799 WGS84\n649    2017            3. centre 7799 WGS84\n728    2017            3. centre 7799 WGS84\n958    2017               8. est 7243 WGS84\n959    2017               8. est 7243 WGS84\n964    2017               8. est 7243 WGS84\n                          geometry\n240 LINESTRING (501051.3 149280...\n241 LINESTRING (481961.6 149016...\n268 LINESTRING (489661.7 148171...\n711 LINESTRING (981157 1359825,...\n640 LINESTRING (732417.5 135977...\n649 LINESTRING (746990.9 135527...\n728 LINESTRING (746990.9 135527...\n958 LINESTRING (553191.7 122647...\n959 LINESTRING (554462.3 122043...\n964 LINESTRING (535888.5 122030...\n\nThe returned object (int) is a data.frame with 238 observations (far fewer than the original 1149 in the roads_tr data). Note that it also contains all the variables from both roads_tr and buffers_tr, so this operates a bit like an inner_join, which means it only includes observations that are in both datasets. We can see the implications of this by making a quick map. The full road network is shown in gray, the buffer circles are in black and the roads that fall into the circles are highlighted in red. Based on this map, it looks like there are a few buffer circles that don’t contain any roads. We want to be sure we account for this.\n\n\n# plot intersection with buffers and road networks \nggplot() +\n  geom_sf(data = buffers_tr) +\n  geom_sf(data = roads_tr, color = \"grey\") +\n  geom_sf(data = int, color = \"red\")\n\n\n\n\nWe can merge in the full list of EAIDs to make sure we don’t miss this one (or any others) using sf:st_join(), which works like dplyr::left_join(). It’s important that when we do the join, the first argument is int, so that it will retain the LINESTRING geometry from this dataset, which we need to calculate the road length. Then, we’ll calculate the length of the road networks contained in each buffer. We can do this with sf::st_length(). Because many of the buffer circles contain multiple roads, we first need calculate the length of each road then we need to aggregate to get the length of all roads in a given enumeration area. We’ll convert from meters to km for greater readability. It’s important to note that any EAs with buffers that don’t contain any roads will not be in the int dataset, so we’ll do a dplyr::full_join() with gps_tr to make sure we get them all.\nBecause int and gps_tr are both sf objects, it’s not possible to do a standard join – you can only use sf::join() when you have two sf objects. That’s why we convert both to data.frames for the dplyr::full_join() and then back into an sf object. Finally, we’ll convert int back into an sf object, retaining the POINT geometry from gps_tr, and replace all NA road lengths as 0.\n\n\n# join, calculate length, & summarize\nint <- int %>%\n  mutate(road_length = st_length(geometry)) %>%\n  group_by(EAID) %>%\n  summarise(road_length = sum(road_length, na.rm = T)) %>%\n  mutate(road_length = set_units(road_length, \"km\")) %>%\n  as.data.frame() %>%\n  full_join(as.data.frame(gps_tr), by = \"EAID\") %>%\n  st_sf(sf_column_name = 'geometry.y') %>%\n  dplyr::select(-geometry.x) %>%\n  mutate(road_length = ifelse(is.na(road_length), 0, road_length))\n\nint\n\n\nSimple feature collection with 83 features and 7 fields\ngeometry type:  POINT\ndimension:      XY\nbbox:           xmin: 260943.3 ymin: 1114669 xmax: 1025472 ymax: 1653511\nprojected CRS:  WGS 84 / UTM zone 30N\nFirst 10 features:\n   EAID road_length PMACC PMAYEAR               REGION DATUM\n1  7003    22.41682    BF    2017       5. centre-nord WGS84\n2  7006    13.35579    BF    2017       5. centre-nord WGS84\n3  7009    20.45756    BF    2017               8. est WGS84\n4  7016    20.31970    BF    2017 1. boucle-du-mouhoun WGS84\n5  7026    18.99583    BF    2017      6. centre-ouest WGS84\n6  7042    14.52567    BF    2017               8. est WGS84\n7  7048    24.29931    BF    2017        4. centre-est WGS84\n8  7056    15.21114    BF    2017     9. hauts-bassins WGS84\n9  7082    44.15771    BF    2017          2. cascades WGS84\n10 7092    26.30945    BF    2017        7. centre-sud WGS84\n   pop_density               geometry.y\n1     29.55058 POINT (422741.8 1383385)\n2     29.98035 POINT (371721.6 1276582)\n3     37.77861 POINT (348264.9 1114669)\n4    154.07399 POINT (346019.4 1218739)\n5     57.09591 POINT (352638.7 1209502)\n6     63.08637 POINT (752580.5 1219236)\n7     74.19692 POINT (479619.4 1234717)\n8     35.07390 POINT (359915.1 1392593)\n9   2731.76393 POINT (669089.2 1375002)\n10   152.23504 POINT (559070.9 1346179)\n\nThe added benefit of the full_join() with gps_tr is that it brings in the pop_density variable we created earlier. So now everything is in one dataset!\nThis can now be merged into other PMA data, such as the individual level dataset bf_merged we worked with in the other posts in this module, and the variables can be used for analysis!\nAs always, let us know if you have any questions and if you’re doing anything exciting with the PMA spatial data!\nSpecial thanks to Tracy Kugler, Nicholas Nagle, and Jonathan Schroeder for excellent help with this post.\n\nLinard, C., Gilbert, M., Snow, R. W., Noor, A. M., & Tatem, A. J. (2012). Population distribution, settlement patterns and accessibility across Africa in 2010. PloS one, 7(2), e31743.↩︎\nNEON: https://www.neonscience.org/resources/learning-hub/tutorials/raster-res-extent-pixels-r↩︎\nWikipedia↩︎\nhttps://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/intro-to-coordinate-reference-systems/↩︎\n",
    "preview": "posts/2021-02-04-merging-external-spatial-data/images/road_map.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 936,
    "preview_height": 574
  },
  {
    "path": "posts/2021-02-02-blog-post-workflow/",
    "title": "Blog post workflow",
    "description": "How to create or review a PMA Data Hub blog post",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-02-02",
    "categories": [
      "internal"
    ],
    "contents": "\n\nContents\nAll users: first time setupR, RStudio, and required packages\nInitialize your UMN GitHub account\nUsing UMN GitHub from RStudio\nThe PMA Data Hub Repository\n\nAuthors: Creating a new postCreate a new branch\nCreate a new folder in \"_posts\"\nPush your post to GitHub\n\nEditorsPull the author’s branch to your computer\nLocate and edit the new post\nPush the edited post back to GitHub\n\nAll users: making revisions\nSite AdminSetup\nMerging\nOther tips\n\n\nAll users: first time setup\nR, RStudio, and required packages\nTo get a copy of R, visit the Comprehensive R Archive Network (CRAN) and choose the right download link for your operating system.\nThe PMA Data Hub is organized as an RStudio Project, so you’ll also need to use RStudio (not base R).\nNote: a copy of RStudio running R version 4.0.2 (or higher) lives on the MPC gp1 server here. Members of the MPC GitHub organization can access an article specifically about using RStudio on gp1 (e.g. how to build a package library) here.\nWhen you’ve got RStudio set up, install these packages:\n\n\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"knitr\")\ninstall.packages(\"distill\")\ninstall.packages(\"usethis\")\n\n\n\nTroubleshooting note: the package rmarkdown comes along with RStudio, but you may receive an older version than we need to build the site. So, when you try install.packages(\"rmarkdown\"), you may get a message asking to restart R (avoiding a conflict with the prior version). We’ve found that it’s best to reply ‘No’ to the restart prompt, wait for rmarkdown to install, and then re-launch RStudio. If you do choose to restart, you may experience a recursive loop of restart prompts!\nInitialize your UMN GitHub account\nContributors to the PMA Data Hub will work on an internal copy of the public site - it’s visible only to certain people affiliated with the MPC. A smaller team of Data Hub “admin” (currently Matt & Nina) will take care of migrating content from the private site to our public site: we’re always here to help with formatting, editing, and version control!\nSo what is UMN GitHub? GitHub, itself, is a company that hosts projects on proprietary server: when you make a repository “public”, anyone in the world can visit your project on a GitHub server. GitHub also makes its underlying software available to institutions that want to provide a similar service restricted to institutional members. In practice, UMN operates its own GitHub server where organizations like the MPC can host projects that are more private in scope.\nUMN GitHub is an instance of Enterprise GitHub, whereas the public version of our blog lives in a space that folks sometimes call “public GitHub”. It’s common for people to have one account for “public GitHub” and one account for their job associated with an “enterprise GitHub”. To initialize an account for UMN GitHub, visit github.umn.edu and log in with your University Internet ID and password.\nUsing UMN GitHub from RStudio\nFirst things first: you must install install Git on your computer if it isn’t there already. Mac OS comes with git installed,1 while other users should download the right Git for their operation system. If you’re using RStudio on the gp1 server, Git is already installed.\nNext, open the Global Options menu in RStudio and locate the Git/SVN tab. Ensure that the box shown below is checked, and then enter the location of the executable file2 for your Git installation:\n\n\n\nLastly, you should provide Git with a username, email, and Personal Access Token (PAT) for your UMN GitHub account. If you’ve installed usethis as shown above, you’ll be able to set these up with R commands (changes will be applied to globally wherever you use Git on your operating system). First, set the username and email address for your UMN GitHub account. For example, mine are:\nWhy a PAT? GitHub plans to deprecate password authentication in the near future. You could use one for now (like the example below), but you’ll need one soon!\n\n\ngert::git_config_global_set(\"user.name\", \"Matt Gunther\")\ngert::git_config_global_set(\"user.email\", \"mgunther@umn.edu\")\n\n\n\nThen, create a PAT for your account with:\n\n\nusethis::create_github_token(host = \"https://github.umn.edu\")\n\n\n\nYour browser will open to a webpage. Check all the boxes you see, then click the green Generate Token button. On the next page, notice the very long string shown in the green box: this is your PAT. Don’t close this page yet!. Return to R and call:\n\n\ngitcreds::gitcreds_set(\"https://github.umn.edu\")\n\n\n\nYou’ll be asked to enter a new password or token: copy and paste your PAT from your browser and press Enter. From now on, RStudio and Git will be able to access your UMN GitHub account automatically! (If you have a personal GitHub account at github.com, you could repeat this process substituting https://github.com for https://github.umn.edu, and Git will automatically choose the right credentials based on the repository associated with your project).\nThe PMA Data Hub Repository\nOpen RStudio and navigate to File > New Project, then select Version Control:\n\n\n\nChoose Git to clone our project from a GitHub repository:\n\n\n\nOn the next menu page, enter the address for the enterprise repository exactly as shown (do not clone the public repository):\nhttps://github.umn.edu/mpc/pma-data-hub/\nAlso enter the project directory name “pma-data-hub” as shown:\npma-data-hub\nIn the third field, choose a location where you would like to save this file on your computer (mine was “~/R” - insert your own path, instead). Finally, click Create Project.\n\nWhen choosing a place to save this project, do not save to a network drive. This seems to cause RStudio to crash!\n\n\n\nIf you have not configured Git to automatically use your UMN GitHub credentials with the steps shown above, you may be prompted to provide them in a pop-up window:\n\n\n\n\nUntil you configure Git with these credentials, you’ll have to do this every time you interact with GitHub. Additionally, password authentication for GitHub will be deprecated in the near future so you’ll need to do it soon!\nAfter a short bit, RStudio will relaunch and open the new project. If you adjust the windows to show the tabs Git (left) and Files (right), you should see something like this:\n\n\n\nYou have now downloaded a copy of the Enterprise repository to your computer!\nMoreover, because you’ve connected these files to a GitHub repository, the RStudio Project will now keep track of changes you make to the files in this folder, and it will prompt you to upload your changes back to GitHub: as you add, edit, or delete files, a list of changes will appear in the Git tab.\n\nTo open an RStudio project, click on the file pma-data-hub.Rproj. If you ever forget, RStudio won’t know to look for a Git history associated with all of the underlying files.\nNotice the word master shown in the Git tab - this shows that any changes we make to files will be recorded in our local copy of the “master” version of the repository. If we made changes here and then pushed them to GitHub, they would be reflected on the “master” version we’ve saved there, too.\nIn general, Matt and Nina will be responsible for merging finished blog posts to the master branch and deploying its contents to the “live” blog that’s seen by users (for details, see site admin instructions below). All other contributors should create their own branch when writing a new blog post; Matt or Nina will merge them to “master” after they’ve been reviewed and approved by an editor. Read on!\nAuthors: Creating a new post\nCreate a new branch\nNotice that the Git tab in RStudio has a purple icon:\n\n\n\nClick this icon to create a new branch. You can name it anything you like, but we recommend using your URL slug if possible (e.g. “blog-post-workflow” is the end of the URL for this webpage). Leave the box next to “Sync branch with remote” checked, as this will create your branch both locally and on our GitHub page:\n\n\n\nRStudio now displays the new branch in place of “master” to show that we’re working on the new branch, instead!\nCreate a new folder in \"_posts\"\nNow that you’ve created a new branch in the Git window, take a look at the File window.\n\n\n\nThe program we use to build the blog is called Distill, and it takes care of all the back-end work as long as we put every new blog post inside of a unique folder within the \"_posts\" directory. Opening \"_posts\", you can see that every post is contained within a time-stamped subfolder:\n\n\n\nTo create one of these folders for your new post, enter the following command into R:\n\n\ndistill::create_post(\"Blog post workflow\")\n\n\n\nThis does two things: it creates the folder automatically (circled in red), and it opens a new RMarkdown file where you can begin writing your post (circled in green).\n\n\n\nIn red: notice the folder appears in both your File tab and your Git tab. (Don’t worry about the date on this folder - it’s for internal use and does not need to match the publication date.)\nIn green: this is the RMarkdown file where you’ll write your post.\nCheck out our Quick-start Guide for Blogging with RMarkdown!\nAs you’re writing your post, you can preview it as a fully formatted webpage by hitting the Knit button at the top of your RMarkdown file:\n\n\n\n\nYou can switch between previewing the page in the RStudio Viewer tab, or in your computer’s default web browser. Click the settings icon next to “Knit” for preview options.\nPush your post to GitHub\nWhen you’re finished writing, follow these steps to share your post with the team on our Enterprise GitHub page (reminder: it won’t go “live” until Matt or Nina merges your post to master and publishes it to the public GitHub page).\nPress the “Knit” button one more time to render a final HTML version of the page. (At this point, you may see a number of automatically created files related to your RMarkdown file in the Git tab.)\nNow, enter the following commands directly into the R console, but please adjust the brief commit “message” as necessary to describe your change!\n\n\ngert::git_add(\".\")\ngert::git_commit(\"Draft published: blog-post-workflow\")\ngert::git_push()\n\n\n\nNow, if you visit our Enterprise GitHub page, your post will appear in a new branch! (It will not yet appear on the copy of the blog we have posted there, which is rendered only from master.)\n\n\n\nEditors\nPull the author’s branch to your computer\nAny time that you want to review an author’s post, you’ll always need to get the latest copy of their branch from our Enterprise GitHub page. If this is the first time you’ve read a draft for the post, the author’s branch won’t yet be listed in RStudio.\n\n\n\n\nThis editor’s RStudio only knows about the “master” branch so far.\nThe Pull button in RStudio’s Git tab will gather information about all of the new branches on GitHub, and it will download a copy of each one onto your computer:\n\n\n\nClicking it will bring up a dialogue screen. RStudio reports that it discovered a new branch blog-post-workflow living at the remote repository origin.\n\n\n\nReturning to RStudio’s main window, notice that you can now toggle between the working on the remote master branch, or the new remote branch called blog-post-workflow. When you’re ready to edit the author’s post, use this menu to select their branch.\n\n\n\nRStudio automatically creates a local version of this branch on your computer, and it reports that your changes will be tracked and pushed to the remote branch of the same name.\n\n\n\nLocate and edit the new post\nNow, RStudio shows that you’re working on the author’s branch in the Git tab, and you’ll see their post listed in the _posts folder on the Files tab. Navigate to the .Rmd file for their post, then click it to begin making edits.\n\n\n\nCheck out our Quick-start Guide for Blogging with RMarkdown!\nAs you’re writing your post, you can preview it as a fully formatted webpage by hitting the Knit button at the top of your RMarkdown file:\n\n\n\n\nYou can switch between previewing the page in the RStudio Viewer tab, or in your computer’s default web browser. Click the settings icon next to “Knit” for preview options.\nPush the edited post back to GitHub\nWhen you’re finished editing, follow these steps to send the revised file back to GitHub. (Reminder: it won’t appear on the website until Matt or Nina merges the post to master and publishes it to the public GitHub page).\nPress the “Knit” button one more time to render a final HTML version of the page. (At this point, you may see a number of automatically created files related to your RMarkdown file in the Git tab.)\nNow, enter the following commands directly into the R console, but please adjust the brief commit “message” as necessary to describe your change!\n\n\ngert::git_add(\".\")\ngert::git_commit(\"Draft edited: blog-post-workflow\")\ngert::git_push()\n\n\n\nNow, if you visit our Enterprise GitHub page, you’ll see that your edited files appear on the author’s branch. (They will not yet appear on the copy of the blog we have posted there, which is rendered only from master.)\n\n\n\nAll users: making revisions\nWhen you’ve finished pushing something to GitHub, please email your collaborators to let them know about next steps!\nTo get your collaborator’s latest updates from GitHub, you should launch the pma-data-hub RStudio project file called pma-data-hub.Rproj. If you forget to open the project file, RStudio will not be able to access the contents of your Git folder (it won’t know that there’s a GitHub repository for the project at all).\nWhen you open the project file, RStudio will again display a Git tab. Click on the Pull button to get your collaborator’s latest changes.\n\n\n\nSwitch from the master branch to the branch associated with your post:\n\n\n\nAfter you’re finished incorporating their feedback into the RMarkdown file (.Rmd), click Knit and then run these commands in Terminal again to send your work back to GitHub:\n\n\ngert::git_add(\".\")\ngert::git_commit(\"Draft complete: blog-post-workflow\")\ngert::git_push()\n\n\n\nPlease let Matt & Nina know when your revisions are complete and ready to appear on the live blog!\nSite Admin\nThese instructions will introduce an additional remote to your local repository. Only use them if you’ll be involved with managing updates to the live blog (e.g. Matt & Nina). All of these functions can be used the in RStudio Terminal.\nSetup\nBefore adding a second remote, it’s best to rename the UMN GitHub remote something like private. You can check the current name with:\ngit remote\nIf the UMN GitHub is still called origin (by default), rename it with:\ngit remote rename origin private\nLikewise, you should give the private branch master a different name, as the second remote will have a master branch, too. To see all of the remote branches currently in use:\ngit remote show private\nCheckout master and change its name to something like private-master:\ngit checkout master\ngit branch -m \"private-master\"\nNow, add the public remote and fetch its branches (there should only be one, called master):\ngit remote add public https://github.com/ipums/pma-data-hub\ngit fetch public\nCreate a local branch called public-master corresponding with master on the public remote:\ngit branch public-master public/master\nAt this point, you’ll notice that RStudio shows two remotes (with the branches you’ve fetched) and all of the local branches you’ve created so far.\n\n\n\n\nIn this example, we set the local “private-master” branch to track “master” on the private remote. The “public-master” branch tracks “master” on the public remote.\nMerging\nOur main goal is to avoid creating divergent commit histories between the internal repository and the public repository. In practice, that means a typical workflow will involve these steps:\nSquash and Merge the author’s branch to private/master\nAdd the new post to index.Rmd\nBuild, Commit, and Push to private/master\nMerge private/master to public/master\nPush to public/master\nOur authoring / editing workflow generates a commit each time someone adds a change to the branch git log (you can run git log on any branch to see its full commit history). We will squash and merge this commit history into a single commit on private-master.\nFor example, with a new post on the branch blog-post-workflow:\ngit checkout private-master\ngit merge --squash blog-post-workflow\nWe have two posts that should not be included in the blog index: this one, and the Quick-start Guide for Blogging with RMarkdown. In order to make that work, we have to whitelist the posts we do want in the file index.Rmd.3\nNow, in RStudio, hit the Build Website button. Look over the site to make sure everything looks good. (We won’t build on public-master to avoid merge conflicts, so get those edits in now.)\n\n\n\nWhen you’re ready, add all files and commit your changes with a message like:\ngit add .\ngit commit -m \"new post: blog-post-workflow\"\nAlthough your local branch has a different name, you can push your commit to master on the private remote with this command:\ngit push private HEAD:master\nWait a few minutes, and you’ll see that the GitHub Pages site hosted at UMN GitHub should update to reflect your changes.\nTo merge your private-master to public-master:\ngit checkout public-master\ngit merge private-master\nFinally, push your changes to the live site:\ngit push public HEAD:master\nOther tips\nNeed to rollback to a previous commit? Look for it in the git log, and do a hard reset:\ngit log\ngit reset --hard 58ba4f0396b985fb5ab82c88f7bbc5c9cc619e71\nFor a checklist of updated files and their commit status:\ngit status\n\nYou can check its location by running “which git” in Terminal, and “git –version” to check the installed version. If git is somehow not installed, use the “Install git using Homebrew” instructions here↩︎\nMac users: type “which git” in terminal and enter the result; Windows users: look for git.exe (most likely in Program Files)↩︎\nAnnoying, yes? Hopefully, an exclusion logic will become available in the next distll release.↩︎\n",
    "preview": "posts/2021-02-02-blog-post-workflow/images/git-menu.png",
    "last_modified": "2021-03-12T14:59:01-06:00",
    "input_file": {},
    "preview_width": 1180,
    "preview_height": 1160
  },
  {
    "path": "posts/2021-01-29-mapping-sdp-variables/",
    "title": "Mapping Service Delivery Point Data",
    "description": "Map spatial variation in the service delivery environment across enumeration areas.",
    "author": [
      {
        "name": "Nina Brooks",
        "url": "http://www.ninarbrooks.com/"
      }
    ],
    "date": "2021-01-30",
    "categories": [
      "Individuals in Context",
      "Service Delivery Points",
      "Data Manipulation",
      "Mapping",
      "sf"
    ],
    "contents": "\n\nContents\nData\nSetup\nMerge and Map\nBasic Maps\nMerge GPS and SDP Data\nMap SDP data\n\nPutting it All Together\n\n\n\n\nIn our last post, we showed how PMA Service Delivery Point (SDP) data can be aggregated to the enumeration area they serve (captured by EASERVED) and linked to individual-level data from a PMA Household and Female survey. In this post, we’ll continue thinking about the spatial distribution of SDP summary data. We’ll first show how to merge our example data to a GPS dataset obtained from pmadata.org, and we’ll then use the new dataset to visualize a few of our variables on a map of Burkina Faso.\nData\nBuilding on the steps we’ve covered in the last two posts in this series, we’ll be working with an example dataset we’re calling bf_merged that contains records from female respondents to the 2017 and 2018 Burkina Faso Household and Female surveys merged with five variables we’ve created from the 2017 and 2018 SDP surveys. These five variables describe services provided within the enumeration area where each woman resides:\nNUM_METHODS_PROV - number of methods provided by at least one SDP\nNUM_METHODS_INSTOCK - number of methods in-stock with at least one SDP\nNUM_METHODS_OUT3MO - number of methods out of stock any time in the last 3 months with at least one SDP\nMEAN_OUTDAY - the mean length of a stockout for any family planning method (measured in days)\nN_SDP - number of SDPs\nThe remaining four variables in bf_merged were taken directly from a data extract containing only female respondents:\nPERSONID - unique identifer for each woman\nEAID - unique identifier for each woman’s enumeration area\nURBAN - whether each woman lives in an urban enumeration area\nFPCURRUSE - whether each woman is currently using any family planning method\nWe’ll also be working with toy PMA GPS datasets for Burkina Faso. PMA GPS data include one GPS coordinate per enumeration area. The Burkina Faso Round 5 and 6 surveys sampled the same enumeration areas, which means we can link the GPS data to both rounds. To use real PMA GPS data you must request access directly from our partners at pmadata.org. For the purpose of use in this post, we’ve created a “toy” GPS dataset: the toy data contains randomly sampled locations within Burkina Faso that have no actual relationship to the EAs in the PMA data.\nThe last dataset we’ll use in this post are the administrative boundaries for Burkina Faso. Shapefiles with administrative boundaries are widely available for download, but we’ll use the ones made available from IPUMS PMA.\nSetup\nMake sure you have all of the following packages installed. Once installed, load the packages we’ll be using today:\n\n\nlibrary(ipumsr)\nlibrary(sf) # primary spatial package\nlibrary(viridis) # for color palettes\nlibrary(tabulator) # for pipe-friendly tabs & cross-tabs\nlibrary(tidyverse)\n\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nIf you followed along with our last post, glimpse(bf_merged) should list the first few records for all of the variables we have so far:\n\n\n\n\n\nglimpse(bf_merged)\n\n\nRows: 6,944\nColumns: 10\n$ EAID                <dbl+lbl> 7003, 7003, 7003, 7003, 7003, 7003,…\n$ SAMPLE              <int+lbl> 85405, 85405, 85405, 85405, 85405, …\n$ N_SDP               <int> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ NUM_METHODS_PROV    <int> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ NUM_METHODS_INSTOCK <int> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, …\n$ NUM_METHODS_OUT3MO  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MEAN_OUTDAY         <dbl> NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN,…\n$ PERSONID            <chr> \"0700300000019732017504\", \"070030000001…\n$ URBAN               <int+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ FPCURRUSE           <int+lbl> 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,…\n\nRemember that we merged the EA-level data into the individual-level data, but the GPS datasets provide coordinates for the enumeration area. So the first thing we’ll do is aggregate bf_merged to the EA-level, and assign the aggregated data to a new object called bf_ea.\n\n\nbf_ea <- bf_merged %>%\n  dplyr::select(-PERSONID) %>%\n  group_by(EAID, SAMPLE) %>%\n  summarise_all(mean, na.rm = T) %>%\n  filter(!is.na(N_SDP)) \n\n\n\n\nIn this example, we’ll exclude any EAs where no facilities in our SDP sample provide services with filter(!is.na(N_SDP))\nNow, let’s read in the GPS data from the data folder and see what the it contains.\n\n\ngps <- read_csv(\"bf_gps_fake.csv\")\ngps\n\n\n# A tibble: 83 x 7\n   PMACC PMAYEAR REGION               EA_ID DATUM GPSLAT GPSLONG\n   <chr>   <dbl> <chr>                <dbl> <chr>  <dbl>   <dbl>\n 1 BF       2017 5. centre-nord        7610 WGS84   14.2  0.126 \n 2 BF       2017 1. boucle-du-mouhoun  7820 WGS84   13.5 -3.08  \n 3 BF       2017 3. centre             7271 WGS84   12.2  1.43  \n 4 BF       2017 3. centre             7799 WGS84   12.2 -0.799 \n 5 BF       2017 8. est                7243 WGS84   11.0 -2.58  \n 6 BF       2017 6. centre-ouest       7026 WGS84   10.9 -4.35  \n 7 BF       2017 3. centre             7859 WGS84   12.4  0.0703\n 8 BF       2017 3. centre             7725 WGS84   12.7  1.42  \n 9 BF       2017 6. centre-ouest       7390 WGS84   10.8 -3.55  \n10 BF       2017 11. plateau-central   7104 WGS84   13.3  0.0957\n# … with 73 more rows\n\n\nIf you requested access to the actual GPS datasets, make sure to replace the bf_gps_fake.csv with the filename of the real data!\n\n\n\nThe gps data has 7 variables:\nPMACC: the country code\nPMAYEAR: the 4-digit year of data collection\nREGION: sub-national administrative division name\nEA_ID: the enumeration area ID (and how we’ll merge this data into other PMA datasets)\nGPSLAT: the displaced EA’s centroid latitude coordinate in decimal degrees\nGPSLONG: the displaced EA’s centroid longitude coordinate in decimal degrees\nDATUM: the coordinate reference system and geographic datum. This variable is always “WGS84” for the World Geodetic System 1984.\n\nNote that while the PMAYEAR variable is 2017 for all EAs, because the same EAs were sampled in the 2017 (Round 5) and 2018 (Round 6) surveys, we can link these coordinates to both samples.\nNote that the GPSLAT and GPSLONG are displaced coordinates of the EA centroid. This is because PMA randomly displaces the geographic coordinates to preserve the privacy of survey respondents. Coordinates are displaced randomly by both angle and distance. Urban EAs are displaced from their true location up to 2 km. Rural EAs are displaced from their true location up to 5 km. Additionally, a random sample of 1% of rural EAs are displaced up to 10km. The PMA GPS data come with documentation that explains the displacement in more detail. The primary spatial package we’ll use is simple features or sf. We’ll use sf::st_as_sf() to convert the GPS data to a spatial data object (known as a simple feature collection).\n\n\ngps <- gps %>%\n    rename(EAID = EA_ID) %>% # rename to be consistent with other PMA data\n    st_as_sf(\n      coords = c(\"GPSLONG\", \"GPSLAT\"), \n      crs = 4326) # 4326 is the coordinate reference system (CRS) identifier for WGS84\ngps\n\n\nSimple feature collection with 83 features and 5 fields\ngeometry type:  POINT\ndimension:      XY\nbbox:           xmin: -5.185229 ymin: 10.08082 xmax: 1.829187 ymax: 14.93619\ngeographic CRS: WGS 84\n# A tibble: 83 x 6\n   PMACC PMAYEAR REGION               EAID DATUM              geometry\n * <chr>   <dbl> <chr>               <dbl> <chr>           <POINT [°]>\n 1 BF       2017 5. centre-nord       7610 WGS84  (0.1263576 14.15999)\n 2 BF       2017 1. boucle-du-mouho…  7820 WGS84   (-3.075099 13.4676)\n 3 BF       2017 3. centre            7271 WGS84   (1.430382 12.17557)\n 4 BF       2017 3. centre            7799 WGS84 (-0.7991777 12.22721)\n 5 BF       2017 8. est               7243 WGS84  (-2.580105 11.03307)\n 6 BF       2017 6. centre-ouest      7026 WGS84  (-4.348525 10.93844)\n 7 BF       2017 3. centre            7859 WGS84 (0.07032293 12.44352)\n 8 BF       2017 3. centre            7725 WGS84   (1.417154 12.68821)\n 9 BF       2017 6. centre-ouest      7390 WGS84  (-3.549929 10.77006)\n10 BF       2017 11. plateau-central  7104 WGS84 (0.09573113 13.27184)\n# … with 73 more rows\n\n\n\n\nNow that gps is a simple features object, we’ve lost the GPSLAT and GPSLONG variables and gained a variable called geometry, which contains the spatial information for this data.\n\n\n\nThe last thing we need is the Burkina Faso shapefile, which are available from IPUMS PMA. You’ll need to download the shapefile (geobf.zip) from the IPUMS site and save it in your working directory to use it. We can use sf::st_read() to read the shapefile into R as an sf object. Note that here the geometry variable is a POLYGON, whereas in the gps data it is a POINT.\n\nNote that what we call a shapefile is actually a collection of many files. More on this in a future post! But for now, just know that you’ll need all the files that come in the zipped download and can refer to the collectively with “geobf.shp”.\n\n\nbf_shp <- st_read(\"geobf/geobf.shp\") \n\n\n\nMerge and Map\nNow that we have all our data, we’ll show you how to map variables… but before we do that, let’s do some basic, exploratory mapping.\nBasic Maps\nggplot2 has support for sf objects, which makes it really easy to map things using the ggplot2 system. ggplot2::geom_sf() will automatically identify what kind of spatial data you’re plotting and handle it appropriately. For example, let’s plot the gps data (which are points) and the administrative region (which are polygons).\nggplot2 is included when you load library(tidyverse)\n\n\n# Plot EA centroids\nggplot() +\n  geom_sf(data = gps)\n\n\n\n# Plot regions of Burkina Faso\nggplot() +\n  geom_sf(data = bf_shp)\n\n\n\n\nThe building-block approach of ggplot2 (“Grammar of Graphics”) also makes it really easy to layer different spatial features on the same map.\n\n\n# Plot regions of Burkina Faso & EA centroids on the same map\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = gps)\n\n\n\n\nMerge GPS and SDP Data\nTo map the EA-level variables constructed in the last post, we need to merge the bf_ea data and the gps data by EAID. First, let’s rename the EASEARVED variable to match the GPS data and then use a dplyr::right_join to merge in the SDP data. We need to use a right_join() because the sf object must be listed first in our join command to retain the sf class, but we want to ensure that all rows from bf_ea are preserved.\n\nRemember, the SDP data contains information from both 2017 and 2018, while the GPS data has a single observation per EA.\n\n\nbf_ea <- right_join(gps, bf_ea, by = \"EAID\")\n\n\n\nMap SDP data\nRemember, the bf_ea data contains information from 2017 & 2018 for the same EA, which can clog up the map depending on how we use this information. To start out, let’s use only the 2017 data and add information about the number of service delivery providers that serve a given EA (N_SDP). By passing N_SDP to the size aesthetic, we can more easily visualize how EAs vary in their access to service delivery providers.\n\n\nbf_ea2017 <- bf_ea %>%\n  filter(SAMPLE == 85405) # this sample corresponds to the 2017 wave\n\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = bf_ea2017, \n          aes(size = N_SDP),\n          alpha = 0.4) \n\n\n\n\nFrom the map, it looks like there may be a few locations where there EAs are both close together and served by many SDPs, which are likely in urban areas. For example, the capital of Burkina Faso, Ouagadougou, is in the center of the map where there are a number of EAs on top of each other. But, it’s a little hard to see the variation in size when there are so many values for N_SDP and so many EAs on top of each other. Let’s do two things to make this more readable. First, we’ll create smaller categories of the N_SDP variable, and second, we’ll map the URBAN variable to the color aesthetic.\n\n\nbf_ea2017 <- bf_ea2017 %>%\n  mutate(\n    N_SDP_CAT = case_when(\n      N_SDP <= 2 ~ 1,\n      N_SDP >2 & N_SDP <= 4 ~ 2,\n      N_SDP >4 ~ 3),\n    N_SDP_CAT = factor(N_SDP_CAT,\n                       levels = c(1, 2, 3),\n                       labels = c(\"Low\", \"Mid\", \"High\"),\n                       ordered = T), # needs to be an ORDERED factor to map to the size aesthetic\n    MEAN_OUTDAY = ifelse(is.na(MEAN_OUTDAY), 0, MEAN_OUTDAY),\n    URBAN = factor(URBAN, \n                   levels = c(0,1),\n                   labels = c(\"Rural\", \"Urban\"))\n  )\n\n\n# let's take a look at the distribution of this new categorical variable\nbf_ea2017 %>% \n  tab(URBAN, N_SDP_CAT)\n\n\nSimple feature collection with 5 features and 5 fields\ngeometry type:  MULTIPOINT\ndimension:      XY\nbbox:           xmin: -5.18865 ymin: 9.883331 xmax: 1.634087 ymax: 14.39679\ngeographic CRS: WGS 84\n# A tibble: 5 x 6\n  URBAN N_SDP_CAT     N                        geometry  prop cum_prop\n  <fct> <ord>     <int>                <MULTIPOINT [°]> <dbl>    <dbl>\n1 Rural Mid          29 ((-5.18865 11.54962), (-4.3401…  0.35     0.35\n2 Urban Mid          23 ((-5.178223 10.66001), (-4.781…  0.28     0.63\n3 Urban Low          15 ((-4.307564 11.18051), (-4.299…  0.18     0.81\n4 Rural Low          13 ((-4.969563 10.45619), (-4.804…  0.16     0.96\n5 Urban High          3 ((-4.262726 11.14746), (-1.528…  0.04     1   \n\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = bf_ea2017, \n          aes(size = N_SDP_CAT,\n              color = URBAN),\n          alpha = 0.4) +\n  scale_color_viridis_d() \n\n\n\n\nFrom this map we can see that urban areas are generally served by more SDPs – in fact, no rural EAs fall into the “High” category – although the difference is perhaps not as stark as one might have expected. But, what is the service environment like? Do urban areas have more stockouts than rural areas? Do SDPs in urban areas offer a greater selection of family planning methods? Did the service environment change between 2017 and 2018? Mapping can shed a lot of light on these questions.\nLet’s look at the NUM_METHODS_PROV variable created in the last post. This variable captures the number of family planning methods provided by at least one SDP that serves a given EA.\n\n\nbf_ea2017 %>%\n  tab(NUM_METHODS_PROV) %>%\n  arrange(NUM_METHODS_PROV)\n\n\nSimple feature collection with 5 features and 4 fields\ngeometry type:  MULTIPOINT\ndimension:      XY\nbbox:           xmin: -5.18865 ymin: 9.883331 xmax: 1.634087 ymax: 14.39679\ngeographic CRS: WGS 84\n# A tibble: 5 x 5\n  NUM_METHODS_PROV     N                       geometry  prop cum_prop\n             <dbl> <int>               <MULTIPOINT [°]> <dbl>    <dbl>\n1                8     2 ((-4.299839 11.18039), (-2.96…  0.02     1   \n2                9    13 ((-4.340111 11.8743), (-4.281…  0.16     0.92\n3               10    34 ((-5.18865 11.54962), (-4.969…  0.41     0.41\n4               11    29 ((-5.178223 10.66001), (-3.84…  0.35     0.76\n5               12     5 ((-2.757122 11.53829), (-2.26…  0.06     0.98\n\nSince there is not a large range of number of FP methods provided, let’s dichotomize this so we can map it to the shape aesthetic.\n\n\nbf_ea2017 <- bf_ea2017 %>%\n  mutate(\n    NUM_METHODS_CAT = case_when(\n      NUM_METHODS_PROV <= 9 ~ 1,\n      NUM_METHODS_PROV >9  ~ 2),\n    NUM_METHODS_CAT = factor(NUM_METHODS_CAT,\n                       levels = c(1, 2),\n                       labels = c(\"Low (<=9)\", \"High (>9)\"),\n                       ordered = T)\n  )\n\n\nggplot() +\n  geom_sf(data = bf_shp) +\n  geom_sf(data = bf_ea2017, \n          aes(size = NUM_METHODS_CAT,\n              shape = URBAN,\n              color = NUM_METHODS_OUT3MO),\n          alpha = 0.4) +\n  scale_color_viridis_c() \n\n\n\n\nPutting it All Together\nNow we have a map that shows spatial variation in availability of different methods of family planning and prevalence of stock-outs, as well as demonstrates how these characteristic differ across urban vs. rural EAs. It’s super quick to make a basic map like this, but let’s clean up a few things to make it look nicer.\n\n\nggplot() +\n  geom_sf(data = bf_shp, fill = \"#f2f2f5\") +\n  geom_sf(data = bf_ea2017, \n          aes(size = NUM_METHODS_CAT,\n              shape = URBAN,\n              color = NUM_METHODS_OUT3MO),\n          alpha = 0.4) +\n  scale_color_viridis_c(direction = -1) + # reversing the direction makes the high #s stand out more\n  labs(title = \"Spatial Variation in Family Planning Service Environment\",\n       subtitle = \"Burkina Faso 2017\",\n       caption = \"Source: IPUMS PMA\",\n       shape = \"\",\n       size = \"Methods\\nProvided\",\n       color = \"Out of Stock\\n(Past 3 Months)\",\n       x = NULL,\n       y = NULL) +\n  theme_minimal() +\n  theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank())\n\n\n\n\n\n\n\nThis map suggests there is spatial correlation to the stockouts – with 2 regions responsible for the majority of EAs with stockouts. It also looks like these EAs tend to have more methods provided by the SDPs that serve them. Finally, let’s use both years of data and see if there is any temporal variation. To do this, we’ll use the original bf_ea dataset (instead of sdp2017) and re-create the same NUM_METHODS_CAT factor variable that dichotomizes the NUM_METHODS_PROV variable. Then, we’ll use facet_wrap() to make a multi-panel plot, with one panel per year.\n\n\nbf_ea <- bf_ea %>%\n  mutate(\n    NUM_METHODS_CAT = case_when(\n      NUM_METHODS_PROV <= 9 ~ 1,\n      NUM_METHODS_PROV >9  ~ 2),\n    NUM_METHODS_CAT = factor(NUM_METHODS_CAT,\n                       levels = c(1, 2),\n                       labels = c(\"Low (<=9)\", \"High (>9)\"),\n                       ordered = T),\n    YEAR = case_when(\n      SAMPLE == 85405 ~ 2017,\n      SAMPLE == 85408 ~ 2018\n    ),\n    URBAN = factor(URBAN, \n                   levels = c(0,1),\n                   labels = c(\"Urban\", \"Rural\"))\n  )\n\nggplot() +\n  geom_sf(data = bf_shp, fill = \"#f2f2f5\") +\n  geom_sf(data = bf_ea, \n          aes(size = NUM_METHODS_CAT,\n              shape = URBAN,\n              color = NUM_METHODS_OUT3MO),\n          alpha = 0.4) +\n  facet_wrap(~ YEAR) +\n  # reversing the direction makes the high #s stand out more\n  scale_color_viridis_c(direction = -1) + \n  guides(color = guide_colorbar(barheight = .75,\n                                barwidth = 4.5,\n                                label.position = \"top\",\n                                label.hjust = 0)) + \n  labs(title = \"Spatial Variation in Family Planning Service Environment\",\n       subtitle = \"Burkina Faso 2017-2018\",\n       caption = \"Source: IPUMS PMA\",\n       shape = \"\",\n       size = \"Methods\\nProvided\",\n       color = \"# Methods\\nOut of Stock\\n(Past 3 Months)\",\n       x = NULL,\n       y = NULL) +\n  theme_minimal() +\n  theme(\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    legend.title = element_text(size = 8),\n    legend.position = \"bottom\") \n\n\n\n\nWith the 2018 data included, it looks like the service environment may have improved between 2017 and 2018 with fewer stockouts. However, it also looks the EAs that faced more stockouts in 2017 are not always the same as those facing stockouts in 2018. But, there is still a spatial pattern to the stockouts in 2018. It also looks like some EAs had fewer family planning methods available from SDPs in 2018 than in 2017, specifically in the western part of the country.\nFuture posts may explore other supply-side factors that could influence the SDPs (and look at how these change over time) or examine demand-side factors by merging in the individual-level data.\nAs always, let us know what kinds of questions about fertility and family planning you’re answering – especially if you’re doing anything spatial!\n\n\n\n",
    "preview": "posts/2021-01-29-mapping-sdp-variables/images/bf_fp_map.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 2100,
    "preview_height": 1350
  },
  {
    "path": "posts/2021-01-28-summarize-by-easerved/",
    "title": "Merging Service Delivery Point Data to Household & Female Records",
    "description": "Create aggregate measures for women living the areas served by SDPs",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-29",
    "categories": [
      "Individuals in Context",
      "Data Manipulation",
      "Service Delivery Points",
      "tidyr"
    ],
    "contents": "\n\nContents\nReviewing SDP Sample Design\nSetup: Create and Load a Data Extract\nEAID and EASERVED\nPivot Longer: EASERVED in Rows\nSummarise by EASERVED and SAMPLE\nMerging to Household and Female Data\n\n\n\n\nWelcome to the third post in a series all about using PMA Service Delivery Point (SDP) data to better understand Individuals in Context. In our last post, we discussed a few of the variable groups related to contraceptive availability, and we showed how to use functions like dplyr::across to recode and summarize these variable groups in preparation for merging with Household and Female data.\nBefore we dive in, let’s quickly revisit the geographic sampling units - or enumeration areas - we’ll be using to link SDPs with their counterparts in the Household and Female data.\nReviewing SDP Sample Design\nRemember: the SDP sample design selects facilities meant to reflect the health service environment experienced by individuals included in Household and Female samples. If you were designing survey with this goal in mind, how would you select facilities?\nWell, you might target a sample of facilities located within the same geographic sampling units PMA used to define Household and Female samples from the same country in the same year. Presumably, the health services available to a woman living in enumeration area X would be captured pretty well if we surveyed a list of facilities also located in enumeration area X.\nBut what happens if a lot of women living in enumeration area X travel to enumeration area Y to receive family planning services? In that case, you’d want to know as much as possible about the service catchment areas for facilities in that country. Then, you could select facilities based on whether they provide services to enumeration area X, rather than relying simply to those that are located there.\nIn fact, PMA partners with government health agencies to obtain information about the service catchment area for all of the public-sector health facilities in each participating country. As a result, public SDPs are sampled if one of the enumeration areas used in a corresponding Household and Female sample appears in their service catchment area.\nBecause service catchment data are only available for public facilities, PMA uses a different method to select private-sector facilities. A private facility will be selected for a SDP sample only if it is located inside the boundaries of an enumeration area included in a corresponding Household and Female sample.\nSetup: Create and Load a Data Extract\nLet’s take a look at an example SDP dataset to see how all of this information gets reported. We’ll use the same data we highlighted in our last post, which includes facilities sampled from Burkina Faso in 2017 and 2018. First, load the following packages into R:\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\n\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nAgain in this post, we’ll be working with all of the available contraceptive services and stock variables ending with the suffixes PROV, OBS, OUT3MO, and OUTDAY. We’ll also add the variable group EASERVED, which - as we’ll see - stores information about the service catchment area for facilities where that information was available. Finally, we’ll add a few more variables that we’ll explore a bit later: AUTHORITY, FACILITYTYPE, and FACILITYTYPEGEN.\nWe’ll first load the data using ipumsr::read_ipums_micro:\n\n\nsdp <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00008.xml\",\n  data = \"data/pma_00008.dat.gz\") \n\n\n\n\nRemember: change these file paths to match your own data extract!\nThen, following the steps outlined in our last post, we’ll apply a couple of recoding functions from ipumsr.\n\n\nsdp <- sdp %>% \n  select(-EASERVED) %>% # error from extract system\n  mutate(\n    across(ends_with(\"OBS\"), ~lbl_relabel(\n      .x,\n      lbl(1, \"in stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    )),\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"Not interviewed (SDP questionnaire)\",\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    ))\n  )\n\n\n\nEAID and EASERVED\nFor the moment, let’s just take a look at the basic structure of our data, selecting only the variables FACILITYID, SAMPLE, AUTHORITY, CONSENTSQ, and EAID. For this preview, we’ll also arrange the data in ascending order of FACILITYID and SAMPLE:\nFACILITYID, SAMPLE, CONSENTSQ, and EAID are automatically included in every SDP data extract.\n\n\nsdp %>% \n  select(FACILITYID, SAMPLE, AUTHORITY, CONSENTSQ, EAID) %>% \n  arrange(FACILITYID, SAMPLE)\n\n\n# A tibble: 234 x 5\n   FACILITYID                      SAMPLE    AUTHORITY CONSENTSQ  EAID\n    <int+lbl>                   <int+lbl>    <int+lbl> <int+lbl> <dbl>\n 1       7006 85405 [Burkina Faso 2017 R… 1 [Governme…   1 [Yes]  7390\n 2       7006 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7390\n 3       7027 85405 [Burkina Faso 2017 R… 1 [Governme…   1 [Yes]  7332\n 4       7027 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7332\n 5       7029 85405 [Burkina Faso 2017 R… 4 [Private]    1 [Yes]  7111\n 6       7029 85408 [Burkina Faso 2018 R… 4 [Private]    0 [No]   7111\n 7       7036 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7412\n 8       7046 85408 [Burkina Faso 2018 R… 4 [Private]    1 [Yes]  7798\n 9       7048 85405 [Burkina Faso 2017 R… 1 [Governme…   1 [Yes]  7009\n10       7051 85408 [Burkina Faso 2018 R… 1 [Governme…   1 [Yes]  7798\n# … with 224 more rows\n\nEach row in our data represents one facility from one sample. Notice that some - but not all - facilities appear once in sample 85405 (from 2017), and again in sample 85408 (from 2018).\nThe variable AUTHORITY shows the managing authority for each facility. Following the discussion above, we’ll expect to find information about the service catchment area for each facility where the managing authority is 1 - Government.\nAlso notice CONSENTSQ, which indicates whether a respondent at each facility consented to be interviewed. When you first obtain a data extract, you should expect most variables to be marked Not interviewed (SDP questionnaire) for facilities where CONSENTSQ shows 0 - No. However, we’ve already taken the extra step of marking all non-response values NA: we should now expect to see NA substituted for Not interviewed (SDP questionnaire).\nLastly, take particular note of the variable EAID: in SDP data, EAID shows the identification code associated with the enumeration area where a facility is located.\nWe’ll find information about the service catchment area for each facility in a different set of variables, each starting with with prefix EASERVED:\n\n\nsdp %>% \n  select(starts_with(\"EASERVED\")) \n\n\n# A tibble: 234 x 18\n   EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n   <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl>\n 1      7380      7323      7491      7605      7142      7279\n 2      7879      7516      7111      7554      7934      7791\n 3      7483        NA        NA        NA        NA        NA\n 4      7185        NA        NA        NA        NA        NA\n 5      7725      7859      7472      7175        NA        NA\n 6      7082        NA        NA        NA        NA        NA\n 7      7650        NA        NA        NA        NA        NA\n 8      7955        NA        NA        NA        NA        NA\n 9      7323        NA        NA        NA        NA        NA\n10      7774        NA        NA        NA        NA        NA\n# … with 224 more rows, and 12 more variables: EASERVED7 <int+lbl>,\n#   EASERVED8 <int+lbl>, EASERVED9 <int+lbl>, EASERVED10 <int+lbl>,\n#   EASERVED11 <int+lbl>, …\n\nYou’ll notice that our extract contains 18 EASERVED variables. Why 18? If you created your own data extract, you’ll remember that you only selected one variable called EASERVED: once you’ve selected samples, the IPUMS extract system automatically determines the correct number of EASERVED variables for your dataset based on the facility with the largest service catchment list.\n\nSome samples include facilities serving as many as 42 enumeration areas, requiring 42 EASERVED variables!\nAs we’ve discussed, PMA only receives service catchment information about public-sector facilities. In their case, each EASERVED variable contains an ID code for one of the enumeration areas in its service catchment list, or else it’s NA. We’ll look at these public-sector facilities first:\n\n\nsdp %>% count(AUTHORITY)\n\n\n# A tibble: 3 x 2\n        AUTHORITY     n\n*       <int+lbl> <int>\n1 1 [Government]    202\n2 3 [Faith-based]     3\n3 4 [Private]        29\n\nThe vast majority of SDPs in our sample are public-sector facilities. They comprise 202 of the 234 facilities in our sample.\n\n\nsdp %>% \n  filter(AUTHORITY == 1) %>% \n  select(starts_with(\"EASERVED\")) \n\n\n# A tibble: 202 x 18\n   EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n   <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl> <int+lbl>\n 1      7380      7323      7491      7605      7142      7279\n 2      7879      7516      7111      7554      7934      7791\n 3      7483        NA        NA        NA        NA        NA\n 4      7185        NA        NA        NA        NA        NA\n 5      7725      7859      7472      7175        NA        NA\n 6      7082        NA        NA        NA        NA        NA\n 7      7650        NA        NA        NA        NA        NA\n 8      7955        NA        NA        NA        NA        NA\n 9      7323        NA        NA        NA        NA        NA\n10      7774        NA        NA        NA        NA        NA\n# … with 192 more rows, and 12 more variables: EASERVED7 <int+lbl>,\n#   EASERVED8 <int+lbl>, EASERVED9 <int+lbl>, EASERVED10 <int+lbl>,\n#   EASERVED11 <int+lbl>, …\n\nUsing two of the dplyr functions discussed in our last post - summarize and across - we’ll get a better sense of the catchment areas for our public-sector SDPs. Let’s see how many missing values exist for each of these EASERVED variables:\ndplyr is included when you load library(tidyverse)\n\n\nsdp %>% \n  filter(AUTHORITY == 1) %>% \n  summarise(across(starts_with(\"EASERVED\"), ~sum(is.na(.x))))\n\n\n# A tibble: 1 x 18\n  EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n      <int>     <int>     <int>     <int>     <int>     <int>\n1         0       156       173       181       190       192\n# … with 12 more variables: EASERVED7 <int>, EASERVED8 <int>,\n#   EASERVED9 <int>, EASERVED10 <int>, EASERVED11 <int>, …\n\nWe see that every public facility serves at least one enumeration area (there are no missing values for EASERVED1). However, there are 156 missing values for EASERVED2, which tells us that 156 public facilities only serve one enumeration area. Likewise: 173 facilities serve 2 enumeration areas or fewer, 181 serve 3 or fewer, and so forth.\nWhat about the 32 non-public facilities?\n\n\nsdp %>% \n  filter(AUTHORITY != 1) %>% \n  summarise(across(starts_with(\"EASERVED\"), ~sum(is.na(.x))))\n\n\n# A tibble: 1 x 18\n  EASERVED1 EASERVED2 EASERVED3 EASERVED4 EASERVED5 EASERVED6\n      <int>     <int>     <int>     <int>     <int>     <int>\n1         4        32        32        32        32        32\n# … with 12 more variables: EASERVED7 <int>, EASERVED8 <int>,\n#   EASERVED9 <int>, EASERVED10 <int>, EASERVED11 <int>, …\n\nPMA receives no information about the service catchment areas for these facilities, so - as you might expect - there are 32 missing values for EASERVED2 onward. Note, however, that there are only 4 missing values for EASERVED1: for non-public facilities, EASERVED1 usually contains that same enumeration area code shown in EAID (this is the enumeration area where the facility is, itself, located).\nThe exception to this rule comes from facilities where CONSENTSQ shows that no respondent provided consent to be interviewed. If we’d like, we can copy EAID to EASERVED1 for these facilities using dplyr::case_when:\n\n\nsdp <- sdp %>% \n  mutate(EASERVED1 = case_when(\n    is.na(EASERVED1) ~ EAID,\n    T ~ as.double(EASERVED1)\n  ))\n\n\n\n\nWe coerce EASERVED1 as a double, matching the class provided by EAID.\nNow, every SDP has at least one enumeration area included in the EASERVED group. This will be important in our next step, where we’ll see how to summarize the SDP data by groups of facilities serving the same enumeration area.\nPivot Longer: EASERVED in Rows\nNow that we’re familiar with EASERVED variables, let’s take a look at the kinds of summary statistics we might want to construct from variables related to contraceptive service availability. For example, consider EMRGPROV, which indicates whether a facility provides emergency contraceptives to clients.\nRemember that, right now, each row of our SDP dataset represents responses from one facility per sample. We’ll ultimately want to count the number of facilities providing emergency contraceptives to clients in each enumeration area, so we should use the tidyr function pivot_longer to reshape the data in a way that repeats each facility’s response to EMRGPROV once for every enumeration area that it serves.\ntidyr is included when you load library(tidyverse)\nTake, for example, the first 5 facilities in our dataset: for now, let’s just look at the first two EASERVED variables, along with each facility’s FACILITYID, EAID, and EMRGPROV response:\n\n\nsdp %>% \n  slice(1:5) %>% \n   select(FACILITYID, EASERVED1, EASERVED2, EMRGPROV)\n\n\n# A tibble: 5 x 4\n  FACILITYID EASERVED1 EASERVED2  EMRGPROV\n   <int+lbl>     <dbl> <int+lbl> <int+lbl>\n1       7250      7380      7323   0 [No] \n2       7399      7879      7516   0 [No] \n3       7506      7483        NA   0 [No] \n4       7982      7185        NA   0 [No] \n5       7065      7725      7859   1 [Yes]\n\nAmong these 5 facilities, only facility 7065 provides emergency contraceptives. This facility happens to provide services to 2 enumeration areas: 7725 and 7859. When we use pivot_longer, we’ll reshape the data to emphasize a different conclusion: our example shows two enumeration areas where individuals can access emergency contraceptives. We convey this information by placing each enumeration area from EASERVED1 or EASERVED2 in its own row:\n\n\nsdp %>% \n  slice(1:5) %>% \n  select(FACILITYID, EASERVED1, EASERVED2, EMRGPROV) %>% \n  pivot_longer(\n    cols = starts_with(\"EASERVED\"),\n    values_to = \"EASERVED\",\n    names_to = NULL\n  )\n\n\n# A tibble: 10 x 3\n   FACILITYID  EMRGPROV  EASERVED\n    <int+lbl> <int+lbl> <dbl+lbl>\n 1       7250   0 [No]       7380\n 2       7250   0 [No]       7323\n 3       7399   0 [No]       7879\n 4       7399   0 [No]       7516\n 5       7506   0 [No]       7483\n 6       7506   0 [No]         NA\n 7       7982   0 [No]       7185\n 8       7982   0 [No]         NA\n 9       7065   1 [Yes]      7725\n10       7065   1 [Yes]      7859\n\n\nHere, values_to gives the name of a new column where we store the values. If we wanted, we could use names_to to create another column storing the original variable names (EASERVED1 and EASERVED2) for each value.\nNow, we find that each of the values previously stored in EASERVED1 and EASERVED2 appear in a new column, EASERVED. Each facility occupies two rows: one for each of the enumeration areas that it serves.\nWhat about the rows where EASERVED contains NA? These rows are meaningless: we’re repeating each facility’s response to EMRGPROV twice to represent two enumeration areas, but facilities 7506 and 7982 only serve one enumeration area apiece. We should include the argument values_drop_na = T to drop these rows when we use pivot_longer():\n\n\nsdp %>% \n  slice(1:5) %>% \n  select(FACILITYID, EASERVED1, EASERVED2, EMRGPROV) %>% \n  pivot_longer(\n    cols = starts_with(\"EASERVED\"),\n    values_to = \"EASERVED\",\n    names_to = NULL,\n    values_drop_na = T\n  )\n\n\n# A tibble: 8 x 3\n  FACILITYID  EMRGPROV  EASERVED\n   <int+lbl> <int+lbl> <dbl+lbl>\n1       7250   0 [No]       7380\n2       7250   0 [No]       7323\n3       7399   0 [No]       7879\n4       7399   0 [No]       7516\n5       7506   0 [No]       7483\n6       7982   0 [No]       7185\n7       7065   1 [Yes]      7725\n8       7065   1 [Yes]      7859\n\nNow that we know how to pivot_longer, let’s apply the function to our full dataset:\n\n\nsdp <- sdp %>%\n  pivot_longer(\n    cols = starts_with(\"EASERVED\"),\n    values_to = \"EASERVED\",\n    values_drop_na = T,\n    names_to = NULL\n  ) %>%\n  distinct() # in case any facility listed the same EASERVED twice\n\n\n\nDropping each row where EASERVED is missing, we’re left with 372 rows where information about each SDP gets repeated once for every enumeration area that it serves. (Remember: our original dataset contained only 234 rows because SDPs occupied just one row apiece).\n\n\nsdp %>% select(FACILITYID, EASERVED, everything())\n\n\n# A tibble: 372 x 58\n   FACILITYID EASERVED      SAMPLE COUNTRY  YEAR ROUND  EAID CONSENTSQ\n    <int+lbl> <dbl+lb>   <int+lbl> <int+l> <int> <dbl> <dbl> <int+lbl>\n 1       7250     7380 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 2       7250     7323 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 3       7250     7491 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 4       7250     7605 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 5       7250     7142 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 6       7250     7279 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 7       7250     7370 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 8       7250     7725 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n 9       7250     7811 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n10       7250     7859 85405 [Bur… 1 [Bur…  2017     5  7142   1 [Yes]\n# … with 362 more rows, and 50 more variables: STRATA <int+lbl>,\n#   FACILITYTYPE <int+lbl>, FACILITYTYPEGEN <int+lbl>,\n#   AUTHORITY <int+lbl>, CONPROV <int+lbl>, …\n\nSummarise by EASERVED and SAMPLE\nNow that we’ve reshaped our data, we’ll be able to create some simple summary statistics about each of the enumeration areas served by the facilities in our sample. First, let’s group_by(EASERVED, SAMPLE) and count() the number of facilities providing services to each enumeration area in each of our samples:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\", \n    N_SDP = n()\n  )\n\n\n# A tibble: 149 x 3\n# Groups:   EASERVED, SAMPLE [149]\n    EASERVED                            SAMPLE N_SDP\n   <dbl+lbl>                         <int+lbl> <int>\n 1      7003 85405 [Burkina Faso 2017 Round 5]     3\n 2      7003 85408 [Burkina Faso 2018 Round 6]     2\n 3      7006 85405 [Burkina Faso 2017 Round 5]     2\n 4      7006 85408 [Burkina Faso 2018 Round 6]     1\n 5      7009 85405 [Burkina Faso 2017 Round 5]     3\n 6      7009 85408 [Burkina Faso 2018 Round 6]     2\n 7      7016 85405 [Burkina Faso 2017 Round 5]     3\n 8      7016 85408 [Burkina Faso 2018 Round 6]     2\n 9      7026 85405 [Burkina Faso 2017 Round 5]     3\n10      7026 85408 [Burkina Faso 2018 Round 6]     3\n# … with 139 more rows\n\nContinuing with the variable EMRGPROV, we can now also count the number of sampled facilities providing emergency contraception to each EASERVED:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    N_EMRGPROV = sum(EMRGPROV)\n  )\n\n\n# A tibble: 149 x 3\n# Groups:   EASERVED, SAMPLE [149]\n    EASERVED                            SAMPLE N_EMRGPROV\n   <dbl+lbl>                         <int+lbl>      <int>\n 1      7003 85405 [Burkina Faso 2017 Round 5]          0\n 2      7003 85408 [Burkina Faso 2018 Round 6]          0\n 3      7006 85405 [Burkina Faso 2017 Round 5]          0\n 4      7006 85408 [Burkina Faso 2018 Round 6]          0\n 5      7009 85405 [Burkina Faso 2017 Round 5]          2\n 6      7009 85408 [Burkina Faso 2018 Round 6]          1\n 7      7016 85405 [Burkina Faso 2017 Round 5]          0\n 8      7016 85408 [Burkina Faso 2018 Round 6]          0\n 9      7026 85405 [Burkina Faso 2017 Round 5]          0\n10      7026 85408 [Burkina Faso 2018 Round 6]          0\n# … with 139 more rows\n\nWhat if we want to include a count of the facilities providing each of the different contraceptive methods in our data? Building on a technique showcased in our last post, we could use dplyr::across to iterate over all variables ending with the suffix PROV:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~sum(.x), .names = \"N_{.col}\")\n  ) \n\n\n# A tibble: 149 x 15\n# Groups:   EASERVED, SAMPLE [149]\n   EASERVED      SAMPLE N_CONPROV N_CYCBPROV N_DEPOPROV N_DIAPROV\n   <dbl+lb>   <int+lbl>     <int>      <int>      <int>     <int>\n 1     7003 85405 [Bur…         3          3          3         0\n 2     7003 85408 [Bur…         2          2          2         0\n 3     7006 85405 [Bur…         2          2          2         0\n 4     7006 85408 [Bur…         1          1          1         0\n 5     7009 85405 [Bur…         3          1          3         0\n 6     7009 85408 [Bur…         2          2          2         0\n 7     7016 85405 [Bur…         3          3          3         0\n 8     7016 85408 [Bur…         2          1          2         0\n 9     7026 85405 [Bur…         3          3          3         0\n10     7026 85408 [Bur…         3          3          3         0\n# … with 139 more rows, and 9 more variables: N_EMRGPROV <int>,\n#   N_FCPROV <int>, N_FSTPROV <int>, N_FJPROV <int>, N_IMPPROV <int>,\n#   …\n\nWe’ll reduce this information even further, creating a variable NUM_METHODS_PROV indicating the number of methods provided by at least one sampled facility:\n\n\nsdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~sum(.x), .names = \"N_{.col}\")\n  ) %>% \n  transmute(\n    EASERVED,\n    SAMPLE,\n    NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")) > 0, na.rm = T)\n  )\n\n\n# A tibble: 149 x 3\n# Groups:   EASERVED, SAMPLE [149]\n    EASERVED                            SAMPLE NUM_METHODS_PROV\n   <dbl+lbl>                         <int+lbl>            <int>\n 1      7003 85405 [Burkina Faso 2017 Round 5]               10\n 2      7003 85408 [Burkina Faso 2018 Round 6]                8\n 3      7006 85405 [Burkina Faso 2017 Round 5]               10\n 4      7006 85408 [Burkina Faso 2018 Round 6]                8\n 5      7009 85405 [Burkina Faso 2017 Round 5]                9\n 6      7009 85408 [Burkina Faso 2018 Round 6]               10\n 7      7016 85405 [Burkina Faso 2017 Round 5]                9\n 8      7016 85408 [Burkina Faso 2018 Round 6]                8\n 9      7026 85405 [Burkina Faso 2017 Round 5]               10\n10      7026 85408 [Burkina Faso 2018 Round 6]               10\n# … with 139 more rows\n\nIn our last post, we introduced 4 variable groups related to the availability of different contraceptive methods. We’ll now create a summary variable for each one, and then show how to attach our new variables to a Household and Female dataset:\nN_SDP - number of SDPs\nNUM_METHODS_PROV - number of methods provided by at least one SDP\nNUM_METHODS_INSTOCK - number of methods in-stock with at least one SDP\nNUM_METHODS_OUT3MO - number of methods out of stock in the last 3 months with at least one SDP\nMEAN_OUTDAY - the mean length of a stockout for all out of stock methods\n\n\nsdp <- sdp %>% \n  group_by(EASERVED, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    N_SDP = n(),\n    across(ends_with(\"PROV\"), ~sum(.x, na.rm = T), .names = \"N_{.col}\"),\n    across(ends_with(\"OBS\"), ~sum(.x, na.rm = T), .names = \"N_{.col}\"),\n    across(ends_with(\"OUT3MO\"), ~sum(.x, na.rm = T), .names = \"N_{.col}\"),\n    across(ends_with(\"OUTDAY\"), ~mean(.x, na.rm = T), .names = \"N_{.col}\"),\n  ) %>% \n  transmute(\n    EASERVED,\n    SAMPLE,\n    N_SDP,\n    NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")) > 0, na.rm = T),\n    NUM_METHODS_INSTOCK = sum(c_across(ends_with(\"OBS\")) > 0, na.rm = T),\n    NUM_METHODS_OUT3MO = sum(c_across(ends_with(\"OUT3MO\")) > 0, na.rm = T),\n    MEAN_OUTDAY = mean(c_across(ends_with(\"OUTDAY\")), na.rm = T)\n  ) %>% \n  ungroup()\n\n\n\nMerging to Household and Female Data\nConsider the following female respondent dataset collected from Burkina Faso in 2017 and 2018. It contains a variable FPCURRUSE indicating whether the woman is currently using a method of family planning:\n\n\nhhf <- read_ipums_micro(\n  ddi = \"data/pma_00011.xml\",\n  data = \"data/pma_00011.dat.gz\"\n) %>% \n  select(PERSONID, EAID, URBAN, SAMPLE, FPCURRUSE) %>% \n  mutate(\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    ))\n  )\n\n\n\n\n\nhhf\n\n\n# A tibble: 6,944 x 5\n   PERSONID           EAID    URBAN                   SAMPLE FPCURRUSE\n   <chr>             <dbl> <int+lb>                <int+lbl> <int+lbl>\n 1 0762000000029022…  7620 1 [Urba… 85405 [Burkina Faso 201…  NA      \n 2 0735800000017142…  7358 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 3 0710400000020992…  7104 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 4 0704800000014092…  7048 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n 5 0715600000020782…  7156 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 6 0727900000021452…  7279 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n 7 0743100000024642…  7431 0 [Rura… 85405 [Burkina Faso 201…   1 [Yes]\n 8 0721200000025792…  7212 0 [Rura… 85405 [Burkina Faso 201…   0 [No] \n 9 0704200000014542…  7042 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n10 0797200000013032…  7972 1 [Urba… 85405 [Burkina Faso 201…   0 [No] \n# … with 6,934 more rows\n\nYou’ll notice that each row represents one female respondent with a unique PERSONID (non-respondents and other household members have been removed beforehand). We’ve also got EAID, which represents the enumeration area where each respondent resides; the variable URBAN indicates whether the enumeration area is primarily “urban” or “rural”.\nThe variable SAMPLE contains the same values seen in our SDP data:\n85405 - Burkina Faso 2017 Round 5\n85408 - Burkina Faso 2018 Round 6\nWhen we merge, we’ll want to match each woman to both a SAMPLE and an EASERVED from the SDP data. We’ll rename EASERVED to match the variable EAID in the HHF data:\n\n\nbf_merged <- sdp %>% \n  rename(EAID = EASERVED) %>% \n  right_join(hhf, by = c(\"EAID\", \"SAMPLE\"))\n\n\n\nNow, each woman’s record contains all of the variables we created above summarizing the SDPs that serve her enumeration area. For example, for all sampled women living in EAID == 7003 in 2017, the value in NUM_METHODS_OUT3MO shows the number of family planning methods that were out of stock with any SDP serving the woman’s enumeration area within three months prior to the survey:\n\n\nbf_merged %>% \n  filter(EAID == 7003, SAMPLE == 85405) %>% \n  select(PERSONID, EAID, SAMPLE, NUM_METHODS_OUT3MO)\n\n\n# A tibble: 55 x 4\n   PERSONID             EAID                  SAMPLE NUM_METHODS_OUT3…\n   <chr>            <dbl+lb>               <int+lbl>             <int>\n 1 070030000001973…     7003 85405 [Burkina Faso 20…                 0\n 2 070030000001973…     7003 85405 [Burkina Faso 20…                 0\n 3 070030000002640…     7003 85405 [Burkina Faso 20…                 0\n 4 070030000001075…     7003 85405 [Burkina Faso 20…                 0\n 5 070030000001609…     7003 85405 [Burkina Faso 20…                 0\n 6 070030000000835…     7003 85405 [Burkina Faso 20…                 0\n 7 070030000001273…     7003 85405 [Burkina Faso 20…                 0\n 8 070030000000527…     7003 85405 [Burkina Faso 20…                 0\n 9 070030000002561…     7003 85405 [Burkina Faso 20…                 0\n10 070030000002391…     7003 85405 [Burkina Faso 20…                 0\n# … with 45 more rows\n\nYou’ll notice that 55 women were surveyed in EAID 7003 in 2017, and each one has the same value (0) for NUM_METHODS_OUT3MO.\nWe’ll dig deeper into the types of research questions that our new combined dataset can answer in our upcoming Data Analysis post. For now, take a look at the apparent relationship between FPCURRUSE and NUM_METHODS_OUT3MO for all of the women with non-missing responses for both variables:\n\n\nbf_merged %>% \n  filter(!is.na(FPCURRUSE) & !is.na(NUM_METHODS_OUT3MO)) %>% \n  group_by(NUM_METHODS_OUT3MO > 0) %>% \n  count(FPCURRUSE) %>% \n  mutate(pct = n/sum(n))\n\n\n# A tibble: 4 x 4\n# Groups:   NUM_METHODS_OUT3MO > 0 [2]\n  `NUM_METHODS_OUT3MO > 0` FPCURRUSE     n   pct\n  <lgl>                    <int+lbl> <int> <dbl>\n1 FALSE                      0 [No]   2721 0.648\n2 FALSE                      1 [Yes]  1475 0.352\n3 TRUE                       0 [No]   1124 0.700\n4 TRUE                       1 [Yes]   482 0.300\n\nNotably, among those respondents living in an enumeration area that experienced zero stockouts within the 3 months prior to the SDP survey, 35% indicated that they were actively using a family planning method. Compare that to the set of respondents living in an area where at least one method was out of stock during the same time period: only 30% were using a family planning method.\nWhile a 5% difference may or may not prove to be statistically significant under further analysis, it’s not entirely surprising that the reliable availability of contraceptive methods from service providers might influence the contraceptive prevalence rate among women in a given area.\nAs always, let us know what kinds of questions about fertility and family planning you’re answering with data merged from service providers!\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-28-across-sdp/",
    "title": "Recode and Summarize Variables from Multiple Response Questions",
    "description": "Use dplyr::across to summarize variables with a similar naming pattern.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-28",
    "categories": [
      "Individuals in Context",
      "Data Manipulation",
      "Service Delivery Points",
      "dplyr",
      "ipumsr"
    ],
    "contents": "\n\nContents\nSDP Multiple Response Questions\nSetup: Load an Example Dataset into R\nRecoding Variables with ipumsr\nIntroducing dplyr::across\nSummarize Variable Groups by Facility\nSummarize Variable Groups by EAID\n\n\n\n\nIn our last post, we introduced PMA Service Delivery Point (SDP) data as an important resource for understanding the health services environment experienced by individuals sampled in PMA Household and Female data. For our second post in this Individuals in Context series, we’ll now take deeper dive into one of the important topics in SDP data: the range and availability of contraceptive methods provided at each facility.\nA common feature of the variables in this topic - and in many other topics - is that you’ll find several binary indicators constructed from the same multiple resposne item on the SDP questionnaire. We’ll see that IPUMS PMA uses a common naming convention to help users group these variables together for use in functions like dplyr::across.\nSDP Multiple Response Questions\nEvery SDP respondent receives a question associated with the variable FPOFFERED, which indicates whether to facility usually offers family planning services or products:\nDo you usually offer family planning services / products?\n\n  [] Yes\n  [] No\n  [] No response\nIf yes, they’ll then receive a multiple response-type question asking about the contraceptive methods provided to clients. The range of options provided on the questionnaire may vary across samples, but most look something like this:\nWhich of the following methods are provided to clients at this facility? \n\n  [] Female sterilization\n  [] Male sterilization\n  [] Implant\n  [] IUD\n  [] Injectables - Depo Provera\n  [] Injectables - Sayana Press\n  [] Pill\n  [] Emergency Contraception\n  [] Male Condom\n  [] Female Condom\n  [] Diaphragm\n  [] Foam/Jelly\n  [] Std. Days / Cycle beads\n  [] None of the above\n  [] No response\n\nIf the response to FPOFFERED was not “Yes”, this question will be skipped and marked “NIU (not in universe)”.\nThis is a multiple response question: each method in the list could be answered individually (Yes or No), or the respondent could reply None of the above or provide No response. The IPUMS PMA extract system generates one variable for each of the methods in the list:\nFSTPROV\nMSTPROV\nIMPPROV\nIUDPROV\nDEPOPROV\nSAYPROV\nPILLPROV\nEMRGPROV\nCONPROV\nFCPROV\nDIAPROV\nFJPROV\nCYCBPROV\nThe questionnaire continues for each one of the methods provided at a given facility. Next, it checks for the current availability of each of the provided methods:\nYou mentioned that you typically provide the [METHOD] at this facility,\ncan you show it to me? If no, probe: Is the [METHOD] out of stock today?\n\n  [] In-stock and observed\n  [] In-stock but not observed\n  [] Out of stock\n  [] No Response\nThe variables associated with each response end with the same suffix OBS:\nIMPOBS\nIUDOBS\nDEPOOBS\nSAYOBS\nPILLOBS\nEMRGOBS\nCONOBS\nFCOBS\nDIAOBS\nFJOBS\nCYCBOBS\nSterilization methods were not included in this question.\nNote: if a given method was not provided at a facility, it would be skipped and marked “NIU (not in universe)”.\nYou can always visit a variable’s Universe tab for details.\nIf a facility did have a particular method in-stock, it received a question asking whether supplies were unavailable any time in the previous three months:\nHas the [METHOD] been out of stock at any time in the last 3 months?\n\n  [] Yes \n  [] No \n  [] Don't know\n  [] No response\nThis question becomes a series of variables ending with the suffix OUT3MO:\nIMPOUT3MO\nIUDOUT3MO\nDEPOOUT3MO\nSAYOUT3MO\nPILLOUT3MO\nEMRGOUT3MO\nCONOUT3MO\nFCOUT3MO\nDIAOUT3MO\nFJOUT3MO\nCYCBOUT3MO\nAgain, sterilization methods were not included in this question.\nNote: if a given method was not in-stock at a facility where it’s normally provided, it would be skipped and marked “NIU (not in universe)”.\nOn the other hand, if a facility that normally provides a given method did not have supplies in-stock during the interview, it received a different question about the duration of the current stockout:\nHow many days has the [METHOD] been out of stock?\n\n  Number of days____\nThe resulting variables - each ending with the suffix OUTDAY - take an integer value representing the stockout duration in days (except where the value is a non-response code, see below):\nIMPOUTDAY\nIUDOUTDAY\nDEPOOUTDAY\nSAYOUTDAY\nPILLOUTDAY\nEMRGOUTDAY\nCONOUTDAY\nFCOUTDAY\nDIAOUTDAY\nFJOUTDAY\nCYCBOUTDAY\nAgain, sterilization methods were not included in this question.\nNote: if a given method was in-stock at a facility where it’s normally provided, it would be skipped and marked “NIU (not in universe)”.\nSetup: Load an Example Dataset into R\nAs you can see, we’re left with quite a few variables from just these 4 questions! That’s very useful if you’re interested in the availability of one method, in particular, but what if you want to get a picture of the full range of methods provided at a particular facility?\nFortunately, the repeated use of variable suffixes (PROV, OBS, OUT3MO, and OUTDAY) make these variables highly suitable for column-wise processing with dplyr::across.\nLet’s start with an example data extract containing all of the variables listed above, collected from just two samples:\nBurkina Faso - 2018 R6\nBurkina Faso - 2017 R5\nOnce you’ve downloaded an extract, open RStudio and load the packages tidyverse and ipumsr:\n\n\nlibrary(tidyverse)\nlibrary(ipumsr)\n\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nNext, use the file paths for your data extract to load it into R:\n\n\nsdp <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00008.xml\",\n  data = \"data/pma_00008.dat.gz\"\n)\n\n\n\n\nRemember: change these file paths to match your own extract!\nUsing dplyr::ends_with, we’ll select only FACILITYID, SAMPLE, EAID, and the variables using one of the four suffixes PROV, OBS, OUT3MO, or OUTDAY.\n\n\nsdp <- sdp %>%  \n  select(\n    FACILITYID,\n    SAMPLE, \n    EAID, \n    ends_with(\"PROV\"),\n    ends_with(\"OBS\"),\n    ends_with(\"OUT3MO\"),\n    ends_with(\"OUTDAY\")\n  )\n\n\n\nThat leaves us with 234 rows - each a facility from one of our two samples - and 49 variables:\n\n\nsdp\n\n\n# A tibble: 234 x 49\n   FACILITYID      SAMPLE  EAID CONPROV CYCBPROV DEPOPROV DIAPROV\n    <int+lbl>   <int+lbl> <dbl> <int+l> <int+lb> <int+lb> <int+l>\n 1       7250 85405 [Bur…  7142 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 2       7399 85405 [Bur…  7879 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 3       7506 85405 [Bur…  7483 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 4       7982 85405 [Bur…  7185 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 5       7065 85405 [Bur…  7859 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 6       7729 85405 [Bur…  7082 1 [Yes]  0 [No]   1 [Yes]  0 [No]\n 7       7490 85405 [Bur…  7650 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 8       7311 85405 [Bur…  7955 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n 9       7524 85405 [Bur…  7323 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n10       7932 85405 [Bur…  7774 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]\n# … with 224 more rows, and 42 more variables: EMRGPROV <int+lbl>,\n#   FCPROV <int+lbl>, FSTPROV <int+lbl>, FJPROV <int+lbl>,\n#   IMPPROV <int+lbl>, IUDPROV <int+lbl>, MSTPROV <int+lbl>,\n#   PILLPROV <int+lbl>, SAYPROV <int+lbl>, CONOBS <int+lbl>,\n#   CYCBOBS <int+lbl>, DEPOOBS <int+lbl>, DIAOBS <int+lbl>,\n#   EMRGOBS <int+lbl>, FCOBS <int+lbl>, FJOBS <int+lbl>,\n#   IMPOBS <int+lbl>, IUDOBS <int+lbl>, PILLOBS <int+lbl>,\n#   SAYOBS <int+lbl>, CONOUT3MO <int+lbl>, CYCBOUT3MO <int+lbl>,\n#   DEPOOUT3MO <int+lbl>, DIAOUT3MO <int+lbl>, EMRGOUT3MO <int+lbl>,\n#   FCOUT3MO <int+lbl>, FJOUT3MO <int+lbl>, IMPOUT3MO <int+lbl>,\n#   IUDOUT3MO <int+lbl>, PILLOUT3MO <int+lbl>, SAYOUT3MO <int+lbl>,\n#   CONOUTDAY <int+lbl>, CYCBOUTDAY <int+lbl>, DEPOOUTDAY <int+lbl>,\n#   DIAOUTDAY <int+lbl>, EMRGOUTDAY <int+lbl>, FCOUTDAY <int+lbl>,\n#   FJOUTDAY <int+lbl>, IMPOUTDAY <int+lbl>, IUDOUTDAY <int+lbl>,\n#   PILLOUTDAY <int+lbl>, SAYOUTDAY <int+lbl>\n\nRecoding Variables with ipumsr\nA key feature to remember about IPUMS PMA extracts is that variables often have value labels, which are text labels assigned to the different values taken by a variable. When we load the extract into R with an ipumsr function, these variables are imported as labelled objects rather than the more common factor class of objects.\nMore information on the difference between factors and IPUMS labelled variables.\nAs a result, IPUMS data users need to take some unusual steps when recoding a variable or handling NA values. Happily, the ipumsr package provide a few functions (starting with the prefix lbl_) that make this process very easy.\nLet’s take a look at the variable CONOBS:\n\n\nsdp %>% count(CONOBS)\n\n\n# A tibble: 5 x 2\n                                    CONOBS     n\n*                                <int+lbl> <int>\n1  1 [In-stock and observed]                 204\n2  2 [In-stock but not observed]               3\n3  3 [Out of stock]                            5\n4 94 [Not interviewed (SDP questionnaire)]     4\n5 99 [NIU (not in universe)]                  18\n\nNotice that we have two values representing SDPs with male condoms “in-stock”: SDPs where the interviewer personally observed the condoms get 1, while those where condoms where reported in-stock - but not actually observed by the interviewer - get 2.\nDepending on your research question, the interviewer’s personal observation of each method may or may not be important. You might decide that you’d prefer to recode this variable into a simple binary measure that could be easily plugged into a regression model as a dummy variable later on. To do that, you could use the ipumsr function lbl_relabel:\n\n\nsdp %>% \n  mutate(CONOBS = lbl_relabel(\n      CONOBS,\n      lbl(1, \"In-stock\") ~ .val %in% 1:2,\n      lbl(0, \"Out of stock\") ~ .val == 3\n    )) %>% \n  count(CONOBS)\n\n\n# A tibble: 4 x 2\n                                    CONOBS     n\n*                                <dbl+lbl> <int>\n1  0 [Out of stock]                            5\n2  1 [In-stock]                              207\n3 94 [Not interviewed (SDP questionnaire)]     4\n4 99 [NIU (not in universe)]                  18\n\nThat collapses the values 1 and 2 together, and it moves the value 3 (“Out of stock”) to 0. However, we’ve still got a the values 94 and 99, which are each a different type of non-response. The easiest strategy here would be to recode any value larger than 90 as NA, and we could do that with another ipumsr function, lbl_na_if:\n\n\nsdp %>% \n  mutate(\n    CONOBS = lbl_relabel(\n      CONOBS,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    ),\n    CONOBS = lbl_na_if(\n      CONOBS,\n      ~.val > 90\n    )\n  ) %>% \n  count(CONOBS)\n\n\n# A tibble: 3 x 2\n             CONOBS     n\n*         <dbl+lbl> <int>\n1  0 [out of stock]     5\n2  1 [in-stock]       207\n3 NA                   22\n\nThis works great for our example variable, CONOBS. Unfortunately, though, we can’t always rely on the rule ~.val > 90 to handle missing responses. For variables like CONOUTDAY, a value above 90 could be a valid response: what if a facility experienced a stockout lasting 94 days? For this reason, the non-response values for CONOUTDAY are padded with additional digits:\n\n\nsdp %>% count(CONOUTDAY)\n\n\n# A tibble: 7 x 2\n                                   CONOUTDAY     n\n*                                  <int+lbl> <int>\n1    1                                           1\n2    3                                           1\n3   10                                           1\n4   15                                           1\n5   60                                           1\n6 9994 [Not interviewed (SDP questionnaire)]     4\n7 9999 [NIU (not in universe)]                 225\n\nWe could write a different lbl_na_if function for our OUTDAY variables, but ipumsr provides a much nicer workaround: we can specify non-response labels rather than values, as long as we make sure to use all of the different non-response labels appearing throughout our dataset:\n\n\nsdp %>% \n  mutate(\n    CONOBS = lbl_relabel(\n      CONOBS,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    ),\n    CONOBS = lbl_na_if(\n      CONOBS,\n      ~.lbl %in% c(\n        \"Not interviewed (SDP questionnaire)\",\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    )\n  ) %>% \n  count(CONOBS)\n\n\n# A tibble: 3 x 2\n             CONOBS     n\n*         <dbl+lbl> <int>\n1  0 [out of stock]     5\n2  1 [in-stock]       207\n3 NA                   22\n\nNow, we’ll be able to recode all of our variables with the same pair of functions! To do that, we’ll first need to take a look at the column-wise workflow made available by dplyr::across.\nIntroducing dplyr::across\nWhile there are several ways to apply a function across a set of variables in R, the simplest method comes from a new addition to the dplyr package that’s loaded when you run library(tidyverse). The function dplyr::across takes two arguments: a function, and a selection of columns where you want that function to be applied.\ndplyr is included when you load library(tidyverse)\nRemember that we want collapse the values 1 - In-stock and observed and 2 - In-stock but not observed for all of the variables ending with OBS, not just CONOBS. Using across and a selection of variables ending with OBS, we’ll apply the same lbl_relabel function we used on CONOBS above:\n\n\nsdp %>% \n  mutate(\n    across(ends_with(\"OBS\"), ~lbl_relabel(\n      .x,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    )) \n  ) %>% \n  count(CONOBS)\n\n\n# A tibble: 4 x 2\n                                    CONOBS     n\n*                                <dbl+lbl> <int>\n1  0 [out of stock]                            5\n2  1 [in-stock]                              207\n3 94 [Not interviewed (SDP questionnaire)]     4\n4 99 [NIU (not in universe)]                  18\n\nHere, we stick lbl_relabel inside a lambda function with syntax from purrr: the ~ designates a tidy lambda function, which in turn uses .x as a kind of pronoun referencing each of the variables returned by ends_with(\"OBS\"). We’re showing that CONOBS still gets recoded as before, but so do all of the other variables in its group!\nWe’ll use across again with lbl_na_if, but this time we want to produce NA values for all of the variables in our dataset. In place of ends_with(\"OBS\"), we’ll use the selection function everything(). This will take care of all the recoding we want to do, so we’ll also reassign our data with sdp <- sdp:\n\n\nsdp <- sdp %>% \n  mutate(\n    across(ends_with(\"OBS\"), ~lbl_relabel(\n      .x,\n      lbl(1, \"in-stock\") ~ .val %in% 1:2,\n      lbl(0, \"out of stock\") ~ .val == 3\n    )),\n    across(everything(), ~lbl_na_if(\n      .x,\n      ~.lbl %in% c(\n        \"Not interviewed (SDP questionnaire)\",\n        \"Don't know\",\n        \"No response or missing\",\n        \"NIU (not in universe)\"\n      )\n    ))\n  )\n\n\n\nLet’s pick a few variables to check out work:\n\n\nsdp %>% count(CONOBS)\n\n\n# A tibble: 3 x 2\n             CONOBS     n\n*         <dbl+lbl> <int>\n1  0 [out of stock]     5\n2  1 [in-stock]       207\n3 NA                   22\n\nsdp %>% count(IMPOBS)\n\n\n# A tibble: 3 x 2\n             IMPOBS     n\n*         <dbl+lbl> <int>\n1  0 [out of stock]     3\n2  1 [in-stock]       204\n3 NA                   27\n\nsdp %>% count(CONOUTDAY)\n\n\n# A tibble: 6 x 2\n  CONOUTDAY     n\n* <int+lbl> <int>\n1         1     1\n2         3     1\n3        10     1\n4        15     1\n5        60     1\n6        NA   229\n\nsdp %>% count(IMPOUTDAY)\n\n\n# A tibble: 4 x 2\n  IMPOUTDAY     n\n* <int+lbl> <int>\n1         1     1\n2        14     1\n3        90     1\n4        NA   231\n\nSummarize Variable Groups by Facility\nEverything looks great! Now that we’ve finished reformatting the data, remember that our ultimate goal is to get some sense of the scope of methods available at a particular facility.\nWe’d like to use something like across again here, but this time we’ll only want to apply our function to a selection of columns within the same row (because each row of our dataset represents one facility). To do this, we’ll divide the dataset rowwise, and then use the related function c_across to apply a calculation across columns within each row.\nFor instance, suppose we want to create NUM_METHODS_PROV to show the total number of methods provided at each facility. Let’s look at the PROV variables for the first few facilities:\n\n\nsdp %>% select(ends_with(\"PROV\"))\n\n\n# A tibble: 234 x 13\n   CONPROV CYCBPROV DEPOPROV DIAPROV EMRGPROV  FCPROV FSTPROV FJPROV\n   <int+l> <int+lb> <int+lb> <int+l> <int+lb> <int+l> <int+l> <int+>\n 1 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 2 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 3 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n 4 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n 5 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  1 [Yes] 1 [Yes] 1 [Yes] 0 [No]\n 6 1 [Yes]  0 [No]   1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 7 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n 8 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 1 [Yes] 0 [No]\n 9 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n10 1 [Yes]  1 [Yes]  1 [Yes]  0 [No]  0 [No]  1 [Yes] 0 [No]  0 [No]\n# … with 224 more rows, and 5 more variables: IMPPROV <int+lbl>,\n#   IUDPROV <int+lbl>, MSTPROV <int+lbl>, PILLPROV <int+lbl>,\n#   SAYPROV <int+lbl>\n\nTo calculate NUM_METHODS_PROV, we can just find the sum of values across all of the PROV variables (thanks to our recoding work, the only possible values here are 1 for “yes”, or 0 for “no”). Notice that c_across takes only one argument: a selection function like ends_with(\"PROV\"). That’s because c_across works like the familiar concatenate function c() used to provide a vector of values to a function like sum(c(1,2,3)).\nFirst, use rowwise() to signal that we’ll only calculate the sum across variables in the same row. Then, use c_across() to find the sum() of PROV variables in each row:\n\n\nsdp %>% \n  rowwise() %>% \n  transmute(NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")), na.rm = T))\n\n\n# A tibble: 234 x 1\n# Rowwise: \n   NUM_METHODS_PROV\n              <int>\n 1               10\n 2               10\n 3                8\n 4                8\n 5               10\n 6                9\n 7                8\n 8               10\n 9                8\n10                8\n# … with 224 more rows\n\nWe can now create a summary variable for each of the four variable groups. Let’s create:\nNUM_METHODS_PROV - number of methods provided\nNUM_METHODS_INSTOCK - number of methods in-stock\nNUM_METHODS_OUT3MO - number of methods out of stock in the last 3 months\nMEAN_OUTDAY - the mean length of a stockout for all out of stock methods\n\n\nsdp %>% \n  rowwise() %>% \n  transmute(\n    NUM_METHODS_PROV = sum(c_across(ends_with(\"PROV\")), na.rm = T),\n    NUM_METHODS_INSTOCK = sum(c_across(ends_with(\"OBS\")), na.rm = T),\n    NUM_METHODS_OUT3MO = sum(c_across(ends_with(\"OUT3MO\")), na.rm = T),\n    MEAN_OUTDAY = mean(c_across(ends_with(\"OUTDAY\")), na.rm = T)\n  )\n\n\n# A tibble: 234 x 4\n# Rowwise: \n   NUM_METHODS_PROV NUM_METHODS_INSTOCK NUM_METHODS_OUT3MO MEAN_OUTDAY\n              <int>               <dbl>              <int>       <dbl>\n 1               10                   8                  0         NaN\n 2               10                   7                  0           8\n 3                8                   8                  0         NaN\n 4                8                   8                  0         NaN\n 5               10                   9                  0         NaN\n 6                9                   7                  0         NaN\n 7                8                   7                  0         365\n 8               10                   8                  1         NaN\n 9                8                   8                  1         NaN\n10                8                   7                  0          30\n# … with 224 more rows\n\nMEAN_OUTDAY is NaN (not a number) if no methods were out of stock.\nSummarize Variable Groups by EAID\nIn our last post, we mentioned that the best use case for SDP data is to aggregate information collected from facilities working in the same geographic sampling units - or enumeration areas - used to select individuals for PMA Household and Female samples. In our next post, we’ll take a close look at the variable group EASERVED, which lists all of the enumeration area codes where a facility is known to provide health services. We’ll then introduce a strategy using tidyr::pivot_longer to summarize the full scope of services available to women living in a particular enumeration area.\nFor now, let’s simply consider all of the sampled facilities located in a particular enumeration area. That is, rather than calculating the number of methods provided by one facility NUM_METHODS_PROV, let’s create one variable for each method indicating whether the method was provided by at least one facility in a given enumeration area EAID in a given SAMPLE.\nFor instance, look at the number of facilities providing IUDs in enumeration area 7111 for the Burkina Faso sample collected in 2017:\n\n\nsdp %>% \n  filter(EAID == 7111, SAMPLE == 85405) %>% \n  select(EAID, SAMPLE, FACILITYID, PILLPROV)\n\n\n# A tibble: 4 x 4\n   EAID                            SAMPLE FACILITYID  PILLPROV\n  <dbl>                         <int+lbl>  <int+lbl> <int+lbl>\n1  7111 85405 [Burkina Faso 2017 Round 5]       7210   1 [Yes]\n2  7111 85405 [Burkina Faso 2017 Round 5]       7029   0 [No] \n3  7111 85405 [Burkina Faso 2017 Round 5]       7441   1 [Yes]\n4  7111 85405 [Burkina Faso 2017 Round 5]       7403   1 [Yes]\n\nWe want to use a summarize function to create a variable like ANY_PILLPROV, which should simply indicate whether any of these four facilities provide contraceptive pills. Three of them do provide pills, so we want ANY_PILLPROV to be TRUE.\n\n\nsdp %>% \n  filter(EAID == 7111, SAMPLE == 85405) %>% \n  summarize(ANY_PILLPROV = any(PILLPROV == 1))\n\n\n# A tibble: 1 x 1\n  ANY_PILLPROV\n  <lgl>       \n1 TRUE        \n\nNow that we’re familiar with across, we should be able to do the same thing to all PROV variables for this particular group of facilities. Let’s also introduce a naming convention where we glue the prefix ANY_ to the column name referenced by the pronoun .x:\n\n\nsdp %>% \n  filter(EAID == 7111, SAMPLE == 85405) %>% \n  summarize(across(ends_with(\"PROV\"), ~any(.x == 1), .names = \"ANY_{.col}\"))\n\n\n# A tibble: 1 x 13\n  ANY_CONPROV ANY_CYCBPROV ANY_DEPOPROV ANY_DIAPROV ANY_EMRGPROV\n  <lgl>       <lgl>        <lgl>        <lgl>       <lgl>       \n1 TRUE        TRUE         TRUE         FALSE       FALSE       \n# … with 8 more variables: ANY_FCPROV <lgl>, ANY_FSTPROV <lgl>,\n#   ANY_FJPROV <lgl>, ANY_IMPPROV <lgl>, ANY_IUDPROV <lgl>,\n#   ANY_MSTPROV <lgl>, ANY_PILLPROV <lgl>, ANY_SAYPROV <lgl>\n\n\nIt looks like none of the sampled facilities in enumeration area 7111 provided emergency contraception in 2017. This could be very important context for understanding the health services available to women sampled from that area!\nLet’s repeat the same procedure for every enumeration area in each of our samples. Rather than using a filter to select one EAID in one SAMPLE, we’ll use group_by to work with each EAID in each SAMPLE.\n\n\nsdp %>% \n  group_by(EAID, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~any(.x == 1), .names = \"ANY_{.col}\")\n  )\n\n\n# A tibble: 142 x 15\n# Groups:   EAID, SAMPLE [142]\n    EAID      SAMPLE ANY_CONPROV ANY_CYCBPROV ANY_DEPOPROV ANY_DIAPROV\n   <dbl>   <int+lbl> <lgl>       <lgl>        <lgl>        <lgl>      \n 1  7003 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n 2  7003 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 3  7006 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n 4  7006 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 5  7009 85405 [Bur… TRUE        FALSE        TRUE         FALSE      \n 6  7009 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 7  7016 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n 8  7016 85408 [Bur… TRUE        TRUE         TRUE         FALSE      \n 9  7026 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n10  7042 85405 [Bur… TRUE        TRUE         TRUE         FALSE      \n# … with 132 more rows, and 9 more variables: ANY_EMRGPROV <lgl>,\n#   ANY_FCPROV <lgl>, ANY_FSTPROV <lgl>, ANY_FJPROV <lgl>,\n#   ANY_IMPPROV <lgl>, ANY_IUDPROV <lgl>, ANY_MSTPROV <lgl>,\n#   ANY_PILLPROV <lgl>, ANY_SAYPROV <lgl>\n\nThis is still quite a bit of information! Suppose we want to summarize it even further: let’s calculate NUM_METHODS_PROV again with our summary output. This time, NUM_METHODS_PROV will count the number of methods provided by at least one facility in each group.\n\n\nsdp %>% \n  group_by(EAID, SAMPLE) %>% \n  summarize(\n    .groups = \"keep\",\n    across(ends_with(\"PROV\"), ~any(.x == 1), .names = \"ANY_{.col}\")\n  ) %>% \n  transmute(NUM_METHODS_PROV= sum(c_across(ends_with(\"PROV\")), na.rm = T))\n\n\n# A tibble: 142 x 3\n# Groups:   EAID, SAMPLE [142]\n    EAID                            SAMPLE NUM_METHODS_PROV\n   <dbl>                         <int+lbl>            <int>\n 1  7003 85405 [Burkina Faso 2017 Round 5]                8\n 2  7003 85408 [Burkina Faso 2018 Round 6]                8\n 3  7006 85405 [Burkina Faso 2017 Round 5]                8\n 4  7006 85408 [Burkina Faso 2018 Round 6]                8\n 5  7009 85405 [Burkina Faso 2017 Round 5]                8\n 6  7009 85408 [Burkina Faso 2018 Round 6]               10\n 7  7016 85405 [Burkina Faso 2017 Round 5]                8\n 8  7016 85408 [Burkina Faso 2018 Round 6]                8\n 9  7026 85405 [Burkina Faso 2017 Round 5]                8\n10  7042 85405 [Burkina Faso 2017 Round 5]               10\n# … with 132 more rows\n\nThese summaries are exactly the type of SDP data we’d like to attach to a Household and Female dataset! Watch for our next post, where we’ll show how to create summaries by both EAID and EASERVED, and then match them to records from female respondents sampled from Burkina Faso in 2017 and 2018.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-26-sdp-data/",
    "title": "Service Delivery Point Data Explained",
    "description": "SDP samples are not nationally representative. Learn how to use them to describe the health service environment experienced by individuals.",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-26",
    "categories": [
      "Individuals in Context",
      "Data Discovery",
      "Service Delivery Points"
    ],
    "contents": "\n\nContents\nWhat is an SDP?\nSurvey Topics\nSample Design\n\n\n\n\nWhen you visit pma.ipums.org and begin browsing data, you’ll notice that PMA data are available for several different units of analysis.\nYou can see which unit of analysis you’re currently browsing - or switch to a different unit of analysis - in this box:\n\n\n\nClick CHANGE, and you’ll see the different units of analysis that are available:\n\n\n\nThis Data Discovery post kicks off a series of posts all about the data available for the Family Planning - Service Delivery Point unit of analysis. As you’ll see, these data are meant to provide important context for the individuals included in the Family Planning - Person series: while SDP data are not nationally representative, they can help provide a rich portrait of the health service environment experienced by women and households.\nYou’ll find more blog posts about SDP data by following the Individuals in Context series. Look for upcoming posts about:\nWorking with variable groups created from multiple response questions\nMerging SDP summary data with Household and Female data\nMapping SDP Data with GPS Data from our partners at pmadata.org\nMerging SDP Data with spatial datasets from external sources\nAn example of the sort of spatial analysis you can perform with SDP data\nWhat is an SDP?\nA Service Delivery Point (SDP) is any type of facility that provides health services to a community: you’ll find a breakdown of the available facility types for each sample listed in FACILITYTYPE. Because countries may include regionally-specific facility types, we’ve integrated major groupings together in the variable FACILITYTYPEGEN. For example, you may find SDP data available from any of these general facility types:\nHospitals\nHealth Centers\nHealth Clinics\nOther Health Facilities\nPrivate Practices\nDispensaries\nPharmacies / Chemists / Drug Shops\nBoutiques / Shops\nOther\nPMA samples SDPs managed by governments, NGOs, faith-based organizations, private sector organizations, and a range of other institutions. You’ll find the managing authority for each SDP listed in AUTHORITY.\nSurvey Topics\nWhile all SDP surveys cover similar topics, individual questions may be posed somewhat differently - or not at all - for any given sample. That’s where IPUMS PMA comes in: we harmonize differences across samples and document the availability of every variable for each sample.\n\nYou’ll find the full text PDF of the original questionnaire administered to all SDPs in a particular sample here.\nIPUMS PMA also organizes SDP variables by topic. These topics currently include:\nFacility Characteristics\nGeneral Facility Characteristics\nGeography\nAreas Served\nStaffing\nMedical Equipment\nFunding\nManagement\nPerformance Feedback\nQuality of Care\nService Statistics\nMedical Records\nTransportation\n\nFamily Planning Services\nServices Provided\nContraceptive Stock\nReason for Stockout\nClients Served\nStock Supplier\nFees\nFacility Condition\n\nOther Health Services\nAbortion\nPost-abortion Care\nSTDs\nAntenatal Care\nLabor and Delivery\nPostpartum Care\nDelivery Medicines\nCommunity Health Workers\nVaccinations\nHealth Programs\nMedicines in Stock\nOther\n\nThese are listed in the TOPICS menu and are subject to growth & reorganization.\n\n\n\nAdditionally, there are a number of technical variables related to survey administration. For example, every SDP included in the sample frame receives a unique FACILITYID (this ID is preserved across survey rounds if a facility is surveyed more than once). However, some facilities never responded to the questionnaire if, for example, no individual respondent was present, competent, and available to be interviewed (see AVAILABLESQ); if no such person was available - or if such a person declined the interview - the variable CONSENTSQ will indicate that survey consent was never obtained. The variable RESULTSQ indicates whether the questionnaire was fully completed or, if not, it provides the reason.\nFor SDPs where CONSENTSQ is “No”, most variables will take the value “Not interviewed (SDP questionnaire)”.\nNote that the value “NIU (not in universe)” pertains to SDPs that were intentionally skipped because a question was deemed out-of-scope.\nYou may choose whether to include SDPs where RESULTSQ indicates that the questionnaire was not fully completed. Click CREATE DATA EXTRACT from you Data Cart:\n\n\n\nThen click CHANGE next to Sample Members:\n\n\n\nFinally, choose whether to include only “Facility Respondents” (those who fully completed the questionnaire), or “All Cases” instead:\n\n\n\nSample Design\nSo what conclusions can you draw from SDP data? First, it’s important to note that the SDP sample design is not nationally representative, and there are no sampling weights for SDP data.1 In other words, it is not possible to get a sense of the national health services profile in a particular country using SDP data.\nInstead, facilities were selected for the SDP survey using the same geographic enumeration areas used to select households for each Household and Female survey. To see how this works, let’s look at an example dataset collected from Burkina Faso in 2017, beginning with the set of female respondents to the Household questionnaire (other household members and female non-respondents have been excluded):\nRead more about the Household and Female sampling strategy.\n\n\nlibrary(tidyverse)\n\nbf17_hhf <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00011.xml\",\n  data = \"data/pma_00011.dat.gz\") %>% \n  filter(YEAR == 2017)\n\n\n\nCheck out our posts on R Packages and Loading IPUMS PMA data into R.\nTo use this code, be sure to change both file paths to match your own extract!\nThe Dataset Notes for this sample describe a two-stage cluster design with urban-rural strata, producing a sample of women from 83 enumeration areas. If we count the number of unique values from EAID in our data, we see that there are 83 unique identification numbers - one for each enumeration area:\n\n\nn_distinct(bf17_hhf$EAID)\n\n\n[1] 83\n\nWe can also see how these enumeration areas are distributed throughout the 13 administrative regions of Burkina Faso. Note that we have more enumeration areas in the Central region (including the capital, Ouagadougou), and we have fewer enumeration areas in regions where the population is lower (Centre-Sud, Plateau-Central, Sud-Ouest, etc.):\n\n\nbf17_hhf %>% \n  group_by(GEOBF) %>% \n  summarize(.groups = \"keep\", n_EAID = n_distinct(EAID)) %>% \n  arrange(n_EAID)\n\n\n# A tibble: 13 x 2\n# Groups:   GEOBF [13]\n                    GEOBF n_EAID\n                <int+lbl>  <int>\n 1  7 [Centre-Sud]             2\n 2 11 [Plateau-Central]        3\n 3 13 [Sud-Ouest]              3\n 4  2 [Cascades]               4\n 5 12 [Sahel]                  4\n 6  5 [Centre-Nord]            5\n 7  4 [Centre-Est]             6\n 8 10 [Nord]                   6\n 9  1 [Boucle du Mouhoun]      7\n10  6 [Centre-Ouest]           7\n11  8 [Est]                    7\n12  9 [Hauts-Bassins]         11\n13  3 [Centre]                18\n\nAlthough the same number of households are randomly selected from within each enumeration area (typically 35), this concentration of enumeration areas within population-dense regions helps to ensure that the Household and Female data are nationally representative.\nLet’s now look at the sample of SDPs collected from Burkina Faso in that same year:\n\n\nbf17_sdp <- ipumsr::read_ipums_micro(\n  ddi = \"data/pma_00008.xml\",\n  data = \"data/pma_00008.dat.gz\") %>% \n  filter(YEAR == 2017)\n\n\n\n\nRemember: to use this code, be sure to change both file paths to match your own extract!\nDataset Notes for the SDP sample explain that the same 83 enumeration areas used in the Household and Female Sample were used to select facilities for the SDP sample. Moreover, we can confirm that all of enumeration areas in the SDP data also appear in the HHF data:\n\n\nall(bf17_sdp$EAID %in% bf17_hhf$EAID)\n\n\n[1] TRUE\n\nBut is the reverse true? Is every enumeration area from the Household and Female Sample represented in the SDP data?\n\n\nall(bf17_hhf$EAID %in% bf17_sdp$EAID)\n\n\n[1] FALSE\n\nPerhaps surprisingly, the answer is no. To learn why, we have to dig a bit deeper into the SDP Dataset Notes. There, we see that a facility located within the physical boundaries of one of the 83 enumeration areas from the Household and Female Survey would have been included in the SDP sample. However, there may be enumeration areas - particularly in remote areas - where no facilities are located.\nFortunately, PMA also includes data about the service catchment area for some facilities.2 You can include this information by selecting the variable series EASERVED. If a given facility serves more than one enumeration area, EASERVED1 will contain the enumeration area ID code for the first enumeration area on its catchment list, EASERVED2 will contain the ID code for the second one, and so forth. If that same facility serves 5 enumeration areas, the variables EASERVED6, EASERVED7, and so forth would be “NIU (not in universe)”.\n\nThe IPUMS PMA extract system automatically determines the right maximum number of EASERVED variables by finding the facility with the largest service catchment list in your extract.\nWhat does this mean? As we’ll show in an upcoming post in this series, it’s possible to create a portrait of the health service environment provided to individuals sampled in the Household and Female surveys. This portrait extends beyond the list of facilities located in an individual’s geographic enumeration area, but users should take care to understand that the scope of facilities providing services to that enumeration area is somewhat limited by sample design.\n\nThe files do contain a weight variable for the sampling units EAWEIGHT, which is a probability weight representing the likelihood of an enumeration area being selected for sampling. The collectors of the original data do not recommend using EAWEIGHT to weight SDP variables.↩︎\nThis information is only available for SDPs where the managing authority listed in AUTHORITY is “government”.↩︎\n",
    "preview": "posts/2021-01-26-sdp-data/images/choose-unit.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 909,
    "preview_height": 632
  },
  {
    "path": "posts/2021-02-02-blogging-with-rmarkdown/",
    "title": "Blogging with RMarkdown",
    "description": "Quick tips for authoring and editing blog posts with RMarkdown",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2021-01-25",
    "categories": [],
    "contents": "\n\nContents\nCreating a blog post\nYAML MetadataTitle\nDescription\nCategories\nAuthor\nDate\nOutput (Table of Contents)\nPreview\n\nCode chunksChunk options\nData Visualizations from chunks\n\nFormatted TextHeadings\nBold and Italics\nInline code (variable names, packages, functions, etc)\nHyperlinks\nAsides and Footnotes\n\n\nRMarkdown documents (.Rmd) work just like regular R scripts (.R) in that you can use them as a space to develop code before sending it to the R Console. The main difference is that an RMarkdown file breaks code into discrete “chunks” of code that can be separated by blocks of text. When you run an RMarkdown document, the Console ignores anything that’s not included in a code chunk (so there’s no need to use the comment indicator #).\nRMarkdown is a powerful tool for sharing and teaching R code, but it has become even more useful with the advent of packages like knitr, which can transform RMarkdown files into Word documents, PDFs, sideshows, HTML pages, and more. The PMA Data Hub is built with knitr and another package called distill, which transforms RMarkdown files into fully formatted blog posts.\nIf you’ve never used RMarkdown, knitr, or distill before, you’ll need to install them with your R Console now:\n\n\ninstall.packages(\"RMarkdown\")\ninstall.packages(\"knitr\")\ninstall.packages(\"distill\")\n\n\n\nCreating a blog post\nIn this post, we’ll assume that you’ve already reviewed the Blog Post Workflow, so you’re familiar with the process for creating a new blog post on a new branch of the Git repository.\nAssuming you’ve created a new branch for your post, you’ll create and open an RMarkdown file when you run:\n\n\ndistill::create_post(\"Getting started with RMarkdown\")\n\n\n\nAt this point, RStudio should look something like this:\n\n\n\n\nCircled in red: notice that a new folder was created in the \"_posts\" directory shown in the Files tab, and your new files also are now being tracked in the Git tab.\nAn RMarkdown file opens with a template showing a YAML metadata header, a code chunk called “setup”, and some boilerplate text.\nYAML Metadata\nYour RMarkdown template contains a header consisting of a series of key: \"value\" pairs written in YAML. This is where we store metadata for each article appearing both at 1) the top of every blog post, and 2) on the blog homepage.\nHere’s an example of a complete YAML header for a post on the Data Hub:\n\n\n\nTitle\nThis will be the main Title shown in CSS style H1 at the top of your post. We automatically reformat to all-caps, so this is not case-sensitive.\nPlease do not use sentence punctuation (unless your title is a question).\nEnsure that your Title matches the “H1 Title” on the Data Hub Tracking Sheet.\nDescription\nThis is the subtitle shown in CSS style H2 (just below the H1 Title). This subtitle is case-senstive.\nPlease do not use sentence punctuation (unless your subtitle is a question).\nTry your best to avoid repeating the subtitle pattern “How to X”. Subtitles should emphasize the importance of a post in the particular context of analyzing PMA data if possible.\nCategories\nThe are the “tags” that will help readers filter posts and navigate through different thematic modules. What tags should you include?\nModule name (Column A of the tracking sheet)\nPost type (Column B of the tracking sheet)\nImportant package functions (package::function) or techniques\nAnalysis tools\nAlways check to see if your tags have been used in a previous post and, if so, make sure to match their existing style, spelling, etc.\nAuthor\nAlways include your name and your affiliation with the project. Optionally, feel free to link to a personal website or social media account!\nExamples:\n\nauthor:\n  - first_name: \"Yihui\"\n    last_name: \"Xie\"\n    url: https://github.com/yihui\n    affiliation: RStudio\n    affiliation_url: https://www.rstudio.com\n    orcid_id: 0000-0003-0645-5666\n  - name: \"JJ Allaire\"\n    url: https://github.com/jjallaire\n    affiliation: RStudio\n    affiliation_url: https://www.rstudio.com\n  - name: \"Rich Iannone\"\n    url: https://github.com/rich-iannone\n    affiliation: RStudio\n    affiliation_url: https://www.rstudio.com\n\nDate\nAdmin will update this to reflect date of publication.\nOutput (Table of Contents)\nThe PMA Data Hub uses a floating table of contents that follows up to 3 heading depths. Please format exactly as shown:\n\n\noutput:\n  distill::distill_article:\n    self_contained: false\n    toc: true\n    toc_depth: 3\n    toc_float: true\n\n\n\nPreview\nThis is the image that will appear alongside your post in the blog homepage. If you don’t specify an image, distill will automatically select the first image in your post; if there are no images, the space will be left blank (but please consider using one!)\nThe file path to your image should be relative to the RMarkdown file, itself. For example:\n\n\n\nThis shows the folder “2020-12-09-mapping-contraceptive-stockouts” in the \"_posts\" folder. The .Rmd file lives in the top level of the folder, and the desired image Rlogo.png lives in the images subfolder. The correct way to reference the image is:\n\n\npreview: images/static-map.png\n\n\n\nCode chunks\nEach chunk of code in your RMarkdown file must be offset as shown here (note the tic-marks and “r” in curly brackets):\n\n\n\n\nThe “r” tells R to interpret this chunk as R code (RMarkdown also supports other languages like Python, Julia, C++, and SQL).\nYou can quickly insert a code chunk via the Code menu in RStudio’s menu bar, or by using the keyboard shortcut shown there (e.g. command + option + I for mac users).\nChunk options\nYou can set specific rendering instructions for RMarkdown inside the curly brackets. Some common options include:\n\nr, eval = F # don't run the code in this chunk\nr, echo = F # hide the code, but not the results \nr, eval = T, echo = F # combine options with commas like this\nr, error = F, message = F, warning = F # hide errors, messages, & warnings\n\nA full list of chunk options are explained here.\nYou can also set default options for all of your code chunks at the top of your RMarkdown document (see the setup chunk that opens in a new template):\n\n\nknitr::opts_chunk$set(\n  echo = TRUE, \n  eval = FALSE,\n  error = FALSE,\n  message = FALSE, \n  warning = FALSE\n)\n\n\n\nData Visualizations from chunks\nThe Distill website explains how to format figures, tables, and diagrams via code chunk arguments.\nFormatted Text\nHere, we’ll show some examples for adding formatted text to the body of your RMarkdown file (i.e. everything that not included in the YAML header or a code chunk).\nHeadings\nUse the # symbol once for an H1 heading, twice ## for an H2 heading, or three times ### for an H3 heading.\nLook at the table of contents for this page: H1 headings are left aligned, and H2 headings are indented once; any H3 headings would be indented twice if we had them.\nBold and Italics\nItalics are offset by one * like this:\n\n*Italics* are offset by one `*` like this:\n\nBold text is offset by two ** like this:\n\n**Bold** text is offset by two `**` like this:\n\nInline code (variable names, packages, functions, etc)\nWe use a particular font for code chunks, and this font also gets applied in the text body to any mention of a variable name, package, or function - basically anything that might appear in the console.\nNote: the first time you use a variable name, package, or function in a post, it’s usually best in include a hyperlink to the underlying documentation. When you insert a hyperlink, do not offset text for inline code.\nInline code is offset by one ` like this:\n\n`Inline code` is offset by one ` like this:\n\nHyperlinks\nA hyperlink should be used the first time you mention a variable, package, function, or anything else that has underlying documentation at an external source.\nA hyperlink can be inserted like this:\nA [hyperlink](http://bitly.com/98K8eH) can be inserted like this:\nIf you want to link to another page on the PMA Data Hub, use relative links (do not include the full path). For example:\nA relative path to the ABOUT page:\nA relative path to the [ABOUT](about.html) page:\nOr, a relative path to one of the blog posts:\nOr, a relative path to one of the [blog posts](posts/2020-12-09-mapping-contraceptive-stockouts/index.html)\nHere is a relative path to one of the headings on that blog post:\nHere is a relative path to one of the [headings](posts/2020-12-09-mapping-contraceptive-stockouts/index.html#shiny-application) on that blog post\nAsides and Footnotes\nAsides are designed for very brief comments rendered to the side of the text body. They must be offset with <aside> tags like this:\n<aside>\nFYI: formatted text in an \"aside\" must use <b>HTML tags<\/b>\n<\/aside>\n\nFYI: formatted text in an “aside” must use HTML tags\nAsides are associated with a particular paragraph or code chunk, so they will create white-space in the text body if they become longer than their partner! For longer comments, consider using a footnote.1\nInsert a footnote here^[This is my footnote]\n\nFootnotes appear as hover-text, and they also populate at the bottom of the page.↩︎\n",
    "preview": "posts/2021-02-02-blogging-with-rmarkdown/images/new_rmarkdown.png",
    "last_modified": "2021-03-12T14:59:01-06:00",
    "input_file": {},
    "preview_width": 2560,
    "preview_height": 1600
  },
  {
    "path": "posts/2020-12-10-get-ipums-pma-data/",
    "title": "Import IPUMS PMA Data into R",
    "description": "How to download an IPUMS PMA data extract and start using it in R",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2020-11-10",
    "categories": [
      "R Tips",
      "R Packages",
      "Importing Data"
    ],
    "contents": "\n\nContents\nHow to access the data\nUser Guide\nYouTube Tutorials\n\nImporting the data\nFixed-width Data Format (dat)\nThe ipumsr package\n\n\nIPUMS PMA is the harmonized version of the multinational survey Performance Monitoring for Action (formerly known as Performance Monitoring and Accountability 2020 - PMA2020). IPUMS PMA lets researchers easily browse the contents of the survey series and craft customized microdata files they download for analysis.\nHow to access the data\nUser Guide\nVisit the IPUMS PMA data dissemination website to browse the available data, and then follow the posted user guide to get started with an extract of interest.\nNote: all users must register for a free account. See user guide for details.\nYouTube Tutorials\nVisit the IPUMS PMA YouTube page for a video playlist showing how to do things like:\nregister for a free IPUMS account\nselect from the available units of analysis\nbuild a data extract\nselect cases of interest\nuse the available survey weights\nImporting the data\nFixed-width Data Format (dat)\nOnce you have registered and finished selecting PMA samples and variables for your extract, click the “View Cart” button to begin checkout.\nReview the contents of your extract and clik the “Create Data Extract” button as shown:\n\n\n\nOn this final page be sure to change the data format to “.dat (fixed-width text)” if it is not selected by default:\n\n\n\nYou will receive an email when your extract request has been processed. Click the included link to find a download page like this one. You must download both the data file and the DDI codebook:\n\n\n\nThe ipumsr package\nThe R package ipumsr provides the tools you will need to import the data file and DDI codebook into R. You can install the package from CRAN with:\n\nClick here for more information on R packages.\ninstall.packages(\"ipumsr\")\nNote the location where your data file and DDI codebook were saved (in my case, they were saved in my local “Downloads” folder). Substitute your own paths into the function shown below:\n\n\ndat <- ipumsr::read_ipums_micro(\n  ddi = \"~/Downloads/pma_00001.xml\",\n  data_file = \"~/Downloads/pma_00001.dat.gz\"\n)\n\n\n\nYou’re done! The dataset is now accessible as the R object, dat.\n\n\n\n",
    "preview": "posts/2020-12-10-get-ipums-pma-data/images/create-data-extract.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 910,
    "preview_height": 790
  },
  {
    "path": "posts/2020-12-10-get-r-and-packages/",
    "title": "Getting Started with R",
    "description": "How to download R for free and install some of the R packages used on this blog",
    "author": [
      {
        "name": "Matt Gunther",
        "url": {}
      }
    ],
    "date": "2020-11-02",
    "categories": [
      "R Tips",
      "R Packages"
    ],
    "contents": "\n\nContents\nWhy analyze PMA data with R?\nGetting started with R\nOur favorite resources\n\nDo I really need statistical software?\nAre there alternatives?\n\nRStudio\nR packages\nEssentials\nipumsr\ntidyverse\nshiny\n\nWatch for updates here\n\n\nWhy analyze PMA data with R?\nLike all IPUMS data projects, IPUMS PMA data is available free of charge to users who agree to our terms of use. That’s because we believe that cost and institutional affiliation should not be barriers to answering pressing concerns around women’s health. (You can register here for a free IPUMS PMA user account.)\nIn fact, users can analyze IPUMS PMA data with any software they like! We’ve chosen to highlight R, in particular, because it is also free and popular with data analysts throughout the world. It’s available for Windows, MacOS, and a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux).\nNon-R users: IPUMS data extracts are available as CSV or fixed-width DAT with syntax files formatted for SPSS, Stata, and SAS.\nGetting started with R\nTo get a copy of R for yourself, visit the Comprehensive R Archive Network (CRAN) and choose the right download link for your operating system.\nIf you’re new to R (or want to refresh your skills), we recommend the excellent, free introductory text R for Data Science. It also introduces tidyverse conventions, which we use throughout this blog.\nOur favorite resources\nR for Data Science, for beginners\nAdvanced R, for a deeper dive\nRSpatial, for analysis with spatial data\nggplot2, for data visualization\nMastering Shiny, for interactive applications\nR Markdown: The Definitive Guide, for producing annotated code, word documents, presentations, web pages, and more\nR-bloggers, for regular news and tutorials\n\n\n\n© 2016 The R Foundation (CC-BY-SA 4.0)\nDo I really need statistical software?\nIf you’re new to data analysis, you might wonder exactly what you’re going to find in a toolkit like R.\nPlenty of people come to R after working with more common types of data analysis software, like Microsoft Excel or other spreadsheet programs. If you wanted to, you could absolutely download a CSV file from the IPUMS PMA extract system and open it in Excel. You would find individual respondents in rows and their responses for variables in columns, and you could make use of built-in spreadsheet functions to do things like:\nCalculate and visualize the distribution of a variable\nBuild pivot tables and graphs examining basic relationships between variables\nCreate new variables of your own that combine data from several variables\nHowever, you might also notice that a spreadsheet comes with certain limitations:\nThere is no variable “metadata”, including labels for the variables and each response option. For example, you might see that the responses to a certain variable include the numbers 0, 1, and 99 - what do these values actually mean?\nYou might find yourself repeating the same “point” and “click” procedure over and over. Or, maybe you’ve had to build a library of custom macro functions on your own to help automate those procedures.\nWhile you can perform arithmetic with built-in functions, there is little support for more advanced statistical procedures\nGraphics are limited within a set of pre-built templates\nMerging data from external sources (like spatial data) can be very tricky\nStatistical software is designed specifically to address these and other issues related to data cleaning and analysis. Learning a program like R takes a lot of practice, but doing so will almost certainly make your work much more efficient!\nAre there alternatives?\nYes! Many data analysts use proprietary statistical software like Stata, SAS, or SPSS. These tools are also powerful, and you may even find them easier to use than R.\n\nComing soon, we hope to include Stata code for many of the blog posts currently written in R.\nBeyond price, R has a few additional advantages that make it a particularly useful tool for working with PMA data:\nCommunity support: R users are particularly active on forums like Stack Overflow and R-bloggers. Groups like R-ladies even organize in-person meetups in cities around the world to help promote inclusion within the R community.\nCustomizability: Because R is open-source, you can change just about anything you like! With a little practice, you’ll be able to create functions and graphics that perfectly match your own needs.\nBeyond statistics: You can use R to build a website (like this one), manage and share a code repository on GitHub, scrape and compile a social media database, or automatically generate word documents, slide presentations, and more! There are practically endless ways to use functional programming in R that have nothing to do with statistics at all.\nIf you’re a beginner, learning R can be a daunting task. Keep at it! And never hesitate to ask questions.\nRStudio\nWe strongly recommend running R within RStudio, an integrated development environment (IDE) designed to make your experience with R much easier. Some of the reasons we use it, ourselves:\nIncludes a multi-pane window that puts your R console, source code, output, and help files all in one place\nSyntax highlighting and code completion\nSupport for R Projects, a crucial approach to organizing your work and sharing it with others\nIncludes RMarkdown, an R package that allows you write text-based documents with embedded snippets of code that can be passed directly to your R console\nComing soon: tools like the command palette, an improved package manager, and integrated citation management\nLike R, it is available at no cost for users on Windows, Mac, and Linux\n\nThis blog is, itself, an R Project with an individual R Markdown file for each page on the site. Look for a download button at the top of every post: you can download the original R Markdown file, open it in RStudio, and run all of the included code.\nR packages\nAn R package is a collection of functions created by other R users that you can download and install for yourself. Packages can be distributed in many ways, but all of the packages we highlight on this blog can be downloaded from CRAN (the same resource used to download “base” R). A package like ipumsr can be downloaded from CRAN by typing the following function into the R console:\n\nThis function saves package files in your default “library” location. If you’re using a Linux machine and don’t have root access, you’ll need to set up R to save packages to a location where you’re able to write files.\n\n\ninstall.packages(\"ipumsr\")\n\n\n\nPackages also come with help files detailing the purpose and possible inputs (or “arguments”) of each included function. Other included metadata explains what version of R you’ll need to use the package, and also whether the package borrows functions from any other packages that should also be installed (usually these are called “dependencies”).\nIn order to access the functions and help files for a package, you need to load it after installation with:\n\n\nlibrary(ipumsr)\n\n\n\nOn this blog, we will often show functions together with their package like this:\n\n\nipumsr::read_ipums_micro(\n  ddi = \"~/Downloads/pma_00001.xml\",\n  data_file = \"~/Downloads/pma_00001.dat.gz\"\n)\n\n\n\nThe function read_ipums_micro comes from the package ipumsr. It is not necessary for you to include the package each time you call a function (as long as you’ve already loaded the package with library()); we’re using this notation simply as a reminder (in case you want to consult the original package documentation).\n\nYou can use the package::function() notation if you ever want to access a function from a package without loading everything else in the package.\nEssentials\nHere are the packages you’ll need to install to reproduce the code on this blog:\nipumsr\nThe ipumsr package contains functions that make it easy to load IPUMS PMS data into R (mainly read_ipums_micro).\nIt also contains functions that will return variable metadata (like the variable descriptions you see while browsing for data on pma.ipums.org.\ntidyverse\nThe tidyverse package actually installs a family of related packages, including:\nggplot2, for data visualization\ndplyr, for data manipulation\ntidyr, for data tidying\nreadr, for data import\npurrr, for functional programming\ntibble, for tibbles (a modern re-imagining of data frames)\nstringr, for strings\nforcats, for factors\nThis blog uses tidyverse functions and syntax wherever possible because so-called “tidy” conventions are designed with the expressed purpose of making code and console output more human readable. Sometimes, human readability imposes a performance cost: in our experience, IPUMS PMA datasets are small enough that this is not an issue.\n\nFor larger datasets, we recommend exploring the package data.table.\nshiny\nInteractive graphics shown throughout this blog are built with the shiny package.\nWatch for updates here\nWe may add more package suggestions for future posts!\n\n\n\n",
    "preview": "posts/2020-12-10-get-r-and-packages/images/Rlogo.png",
    "last_modified": "2021-03-26T16:30:28-05:00",
    "input_file": {},
    "preview_width": 724,
    "preview_height": 561
  }
]
